{"db":[{"meta":{"exported_on":1594845066913,"version":"3.25.0"},"data":{"actions":[{"id":"5f0f6652ad2fa40039199e2a","resource_id":"5e382b29f81c630018abe076","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2020-07-15T20:25:54.000Z"},{"id":"5f0f665dad2fa40039199e2b","resource_id":"5e382b29f81c630018abe034","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2020-07-15T20:26:05.000Z"},{"id":"5f0f6666ad2fa40039199e2c","resource_id":"5e382b29f81c630018abe06f","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2020-07-15T20:26:14.000Z"},{"id":"5f0f666dad2fa40039199e2d","resource_id":"5e382b29f81c630018abe06c","resource_type":"post","actor_id":"1","actor_type":"user","event":"deleted","context":null,"created_at":"2020-07-15T20:26:21.000Z"},{"id":"5f0f6672ad2fa40039199e2e","resource_id":"1","resource_type":"user","actor_id":"1","actor_type":"user","event":"edited","context":null,"created_at":"2020-07-15T20:26:26.000Z"}],"api_keys":[{"id":"5e3838d4bc678a002d6a7e73","type":"admin","secret":"21e6ec77c1da0b353121fd8f2419933ceb6f9094df0b6946af31687077bc82de","role_id":"5e3838d2bc678a002d6a7e25","integration_id":"5e3838d4bc678a002d6a7e72","last_seen_at":null,"last_seen_version":null,"created_at":"2020-02-03T15:14:29.000Z","updated_at":"2020-02-03T15:14:29.000Z"},{"id":"5e3838d8bc678a002d6a7e8b","type":"admin","secret":"2bab634a655d0f6b9b464a9e3bcf9e8d1ea220acaaf8e59abca891c0de6c9187","role_id":"5e3838d8bc678a002d6a7e86","integration_id":"5e3838d8bc678a002d6a7e8a","last_seen_at":null,"last_seen_version":null,"created_at":"2020-02-03T15:14:33.000Z","updated_at":"2020-02-03T15:14:33.000Z"},{"id":"5e3838d9bc678a002d6a7e92","type":"admin","secret":"d2b44615d041e2e35509809ca8a5ff96b09ba7c10085ff551d9a7b1cf057d915","role_id":"5e3838d8bc678a002d6a7e8f","integration_id":"5e3838d9bc678a002d6a7e91","last_seen_at":null,"last_seen_version":null,"created_at":"2020-02-03T15:14:33.000Z","updated_at":"2020-02-03T15:14:33.000Z"}],"app_fields":[],"app_settings":[],"apps":[],"brute":[{"key":"LnyjTE2MjSBahGVSdIXmhYXYCrkorvC7OXYwBJNWQgg=","firstRequest":1594844582717,"lastRequest":1594844656332,"lifetime":1594848256338,"count":2},{"key":"5mX0GR+GU7lMwG9H0+dFfBWSr826eIV5/XWHVw9eN/8=","firstRequest":1594844656064,"lastRequest":1594844656064,"lifetime":1594848256072,"count":1}],"emails":[],"integrations":[{"id":"5e3838d4bc678a002d6a7e72","type":"builtin","name":"Zapier","slug":"zapier","icon_image":null,"description":"Built-in Zapier integration","created_at":"2020-02-03T15:14:28.000Z","updated_at":"2020-02-03T15:14:28.000Z"},{"id":"5e3838d8bc678a002d6a7e8a","type":"internal","name":"Ghost Backup","slug":"ghost-backup","icon_image":null,"description":"Internal DB Backup integration","created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d9bc678a002d6a7e91","type":"internal","name":"Ghost Scheduler","slug":"ghost-scheduler","icon_image":null,"description":"Internal Scheduler integration","created_at":"2020-02-03T15:14:33.000Z","updated_at":"2020-02-03T15:14:33.000Z"}],"invites":[],"labels":[],"members":[],"members_labels":[],"members_stripe_customers":[],"members_stripe_customers_subscriptions":[],"migrations":[{"id":1,"name":"1-create-tables.js","version":"init","currentVersion":"1.21"},{"id":2,"name":"2-create-fixtures.js","version":"init","currentVersion":"1.21"},{"id":3,"name":"1-post-excerpt.js","version":"1.3","currentVersion":"1.21"},{"id":4,"name":"1-codeinjection-post.js","version":"1.4","currentVersion":"1.21"},{"id":5,"name":"1-og-twitter-post.js","version":"1.5","currentVersion":"1.21"},{"id":6,"name":"1-add-backup-client.js","version":"1.7","currentVersion":"1.21"},{"id":7,"name":"1-add-permissions-redirect.js","version":"1.9","currentVersion":"1.21"},{"id":8,"name":"1-custom-template-post.js","version":"1.13","currentVersion":"1.21"},{"id":9,"name":"2-theme-permissions.js","version":"1.13","currentVersion":"1.21"},{"id":10,"name":"1-add-webhooks-table.js","version":"1.18","currentVersion":"1.21"},{"id":11,"name":"1-webhook-permissions.js","version":"1.19","currentVersion":"1.21"},{"id":12,"name":"1-remove-settings-keys.js","version":"1.20","currentVersion":"1.21"},{"id":13,"name":"1-add-contributor-role.js","version":"1.21","currentVersion":"1.21"},{"id":14,"name":"1-multiple-authors-DDL.js","version":"1.22","currentVersion":"1.26"},{"id":15,"name":"1-multiple-authors-DML.js","version":"1.22","currentVersion":"1.26"},{"id":16,"name":"1-update-koenig-beta-html.js","version":"1.25","currentVersion":"1.26"},{"id":17,"name":"2-demo-post.js","version":"1.25","currentVersion":"1.26"},{"id":18,"name":"1-rename-amp-column.js","version":"2.0","currentVersion":"2.31"},{"id":19,"name":"2-update-posts.js","version":"2.0","currentVersion":"2.31"},{"id":20,"name":"3-remove-koenig-labs.js","version":"2.0","currentVersion":"2.31"},{"id":21,"name":"4-permalink-setting.js","version":"2.0","currentVersion":"2.31"},{"id":22,"name":"5-remove-demo-post.js","version":"2.0","currentVersion":"2.31"},{"id":23,"name":"6-replace-fixture-posts.js","version":"2.0","currentVersion":"2.31"},{"id":24,"name":"1-add-sessions-table.js","version":"2.2","currentVersion":"2.31"},{"id":25,"name":"2-add-integrations-and-api-key-tables.js","version":"2.2","currentVersion":"2.31"},{"id":26,"name":"3-insert-admin-integration-role.js","version":"2.2","currentVersion":"2.31"},{"id":27,"name":"4-insert-integration-and-api-key-permissions.js","version":"2.2","currentVersion":"2.31"},{"id":28,"name":"5-add-mobiledoc-revisions-table.js","version":"2.2","currentVersion":"2.31"},{"id":29,"name":"1-add-webhook-columns.js","version":"2.3","currentVersion":"2.31"},{"id":30,"name":"2-add-webhook-edit-permission.js","version":"2.3","currentVersion":"2.31"},{"id":31,"name":"1-add-webhook-permission-roles.js","version":"2.6","currentVersion":"2.31"},{"id":32,"name":"1-add-members-table.js","version":"2.8","currentVersion":"2.31"},{"id":33,"name":"1-remove-empty-strings.js","version":"2.13","currentVersion":"2.31"},{"id":34,"name":"1-add-actions-table.js","version":"2.14","currentVersion":"2.31"},{"id":35,"name":"2-add-actions-permissions.js","version":"2.14","currentVersion":"2.31"},{"id":36,"name":"1-add-type-column-to-integrations.js","version":"2.15","currentVersion":"2.31"},{"id":37,"name":"2-insert-zapier-integration.js","version":"2.15","currentVersion":"2.31"},{"id":38,"name":"1-add-members-perrmissions.js","version":"2.16","currentVersion":"2.31"},{"id":39,"name":"1-normalize-settings.js","version":"2.17","currentVersion":"2.31"},{"id":40,"name":"2-posts-add-canonical-url.js","version":"2.17","currentVersion":"2.31"},{"id":41,"name":"1-restore-settings-from-backup.js","version":"2.18","currentVersion":"2.31"},{"id":42,"name":"1-update-editor-permissions.js","version":"2.21","currentVersion":"2.31"},{"id":43,"name":"1-add-member-permissions-to-roles.js","version":"2.22","currentVersion":"2.31"},{"id":44,"name":"1-insert-ghost-db-backup-role.js","version":"2.27","currentVersion":"2.31"},{"id":45,"name":"2-insert-db-backup-integration.js","version":"2.27","currentVersion":"2.31"},{"id":46,"name":"3-add-subdirectory-to-relative-canonical-urls.js","version":"2.27","currentVersion":"2.31"},{"id":47,"name":"1-add-db-backup-content-permission.js","version":"2.28","currentVersion":"2.31"},{"id":48,"name":"2-add-db-backup-content-permission-to-roles.js","version":"2.28","currentVersion":"2.31"},{"id":49,"name":"3-insert-ghost-scheduler-role.js","version":"2.28","currentVersion":"2.31"},{"id":50,"name":"4-insert-scheduler-integration.js","version":"2.28","currentVersion":"2.31"},{"id":51,"name":"5-add-scheduler-permission-to-roles.js","version":"2.28","currentVersion":"2.31"},{"id":52,"name":"6-add-type-column.js","version":"2.28","currentVersion":"2.31"},{"id":53,"name":"7-populate-type-column.js","version":"2.28","currentVersion":"2.31"},{"id":54,"name":"8-remove-page-column.js","version":"2.28","currentVersion":"2.31"},{"id":55,"name":"1-add-post-page-column.js","version":"2.29","currentVersion":"2.31"},{"id":56,"name":"2-populate-post-page-column.js","version":"2.29","currentVersion":"2.31"},{"id":57,"name":"3-remove-page-type-column.js","version":"2.29","currentVersion":"2.31"},{"id":58,"name":"1-remove-name-and-password-from-members-table.js","version":"2.31","currentVersion":"2.31"},{"id":59,"name":"01-add-members-stripe-customers-table.js","version":"2.32","currentVersion":"2.38"},{"id":60,"name":"02-add-name-to-members-table.js","version":"2.32","currentVersion":"2.38"},{"id":61,"name":"01-correct-members-stripe-customers-table.js","version":"2.33","currentVersion":"2.38"},{"id":62,"name":"01-add-stripe-customers-subscriptions-table.js","version":"2.34","currentVersion":"2.38"},{"id":63,"name":"02-add-email-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"2.38"},{"id":64,"name":"03-add-name-to-members-stripe-customers-table.js","version":"2.34","currentVersion":"2.38"},{"id":65,"name":"01-add-note-to-members-table.js","version":"2.35","currentVersion":"2.38"},{"id":66,"name":"01-add-self-signup-and-from address-to-members-settings.js","version":"2.37","currentVersion":"2.38"},{"id":67,"name":"01-remove-user-ghost-auth-columns.js","version":"3.0","currentVersion":"3.4"},{"id":68,"name":"02-drop-token-auth-tables.js","version":"3.0","currentVersion":"3.4"},{"id":69,"name":"03-drop-client-auth-tables.js","version":"3.0","currentVersion":"3.4"},{"id":70,"name":"04-add-posts-meta-table.js","version":"3.0","currentVersion":"3.4"},{"id":71,"name":"05-populate-posts-meta-table.js","version":"3.0","currentVersion":"3.4"},{"id":72,"name":"06-remove-posts-meta-columns.js","version":"3.0","currentVersion":"3.4"},{"id":73,"name":"07-add-posts-type-column.js","version":"3.0","currentVersion":"3.4"},{"id":74,"name":"08-populate-posts-type-column.js","version":"3.0","currentVersion":"3.4"},{"id":75,"name":"09-remove-posts-page-column.js","version":"3.0","currentVersion":"3.4"},{"id":76,"name":"10-remove-empty-strings.js","version":"3.0","currentVersion":"3.4"},{"id":77,"name":"11-update-posts-html.js","version":"3.0","currentVersion":"3.4"},{"id":78,"name":"12-populate-members-table-from-subscribers.js","version":"3.0","currentVersion":"3.4"},{"id":79,"name":"13-drop-subscribers-table.js","version":"3.0","currentVersion":"3.4"},{"id":80,"name":"14-remove-subscribers-flag.js","version":"3.0","currentVersion":"3.4"},{"id":81,"name":"01-add-send-email-when-published-to-posts.js","version":"3.1","currentVersion":"3.4"},{"id":82,"name":"02-add-email-subject-to-posts-meta.js","version":"3.1","currentVersion":"3.4"},{"id":83,"name":"03-add-email-preview-permissions.js","version":"3.1","currentVersion":"3.4"},{"id":84,"name":"04-add-subscribed-flag-to-members.js","version":"3.1","currentVersion":"3.4"},{"id":85,"name":"05-add-emails-table.js","version":"3.1","currentVersion":"3.4"},{"id":86,"name":"06-add-email-permissions.js","version":"3.1","currentVersion":"3.4"},{"id":87,"name":"07-add-uuid-field-to-members.js","version":"3.1","currentVersion":"3.4"},{"id":88,"name":"08-add-uuid-values-to-members.js","version":"3.1","currentVersion":"3.4"},{"id":89,"name":"09-add-further-email-permissions.js","version":"3.1","currentVersion":"3.4"},{"id":90,"name":"10-add-email-error-data-column.js","version":"3.1","currentVersion":"3.4"},{"id":91,"name":"01-add-cancel-at-period-end-to-subscriptions.js","version":"3.2","currentVersion":"3.4"},{"id":92,"name":"1-add-labels-table.js","version":"3.6","currentVersion":"3.6"},{"id":93,"name":"2-add-members-labels-table.js","version":"3.6","currentVersion":"3.6"},{"id":94,"name":"3-add-labels-permissions.js","version":"3.6","currentVersion":"3.6"},{"id":95,"name":"01-fix-incorrect-member-labels-foreign-keys.js","version":"3.7","currentVersion":"3.7"},{"id":96,"name":"01-add-geolocation-to-members.js","version":"3.8","currentVersion":"3.9"},{"id":97,"name":"01-add-member-sigin-url-permissions.js","version":"3.9","currentVersion":"3.9"},{"id":98,"name":"01-remove-broken-complimentary-plan-from-members-settings.js","version":"3.11","currentVersion":"3.12"},{"id":99,"name":"01-add-identity-permission.js","version":"3.12","currentVersion":"3.12"},{"id":100,"name":"02-remove-legacy-is-paid-flag-from-settings.js","version":"3.12","currentVersion":"3.12"},{"id":101,"name":"01-add-email-preview-permissions-to-roles.js","version":"3.18","currentVersion":"3.19"},{"id":102,"name":"02-add-members_stripe_connect-auth-permissions.js","version":"3.18","currentVersion":"3.19"},{"id":103,"name":"01-update-member-from-email-address.js","version":"3.19","currentVersion":"3.19"},{"id":104,"name":"01-removed-legacy-values-from-settings-table.js","version":"3.22","currentVersion":"3.24"},{"id":105,"name":"02-settings-key-renames.js","version":"3.22","currentVersion":"3.24"},{"id":106,"name":"03-add-group-and-flags-to-settings.js","version":"3.22","currentVersion":"3.24"},{"id":107,"name":"04-populate-settings-groups-and-flags.js","version":"3.22","currentVersion":"3.24"},{"id":108,"name":"05-migrate-members-subscription-settings.js","version":"3.22","currentVersion":"3.24"},{"id":109,"name":"06-migrate-stripe-connect-settings.js","version":"3.22","currentVersion":"3.24"},{"id":110,"name":"07-update-type-for-settings.js","version":"3.22","currentVersion":"3.24"},{"id":111,"name":"01-migrate-bulk-email-settings.js","version":"3.23","currentVersion":"3.24"},{"id":112,"name":"02-remove-bulk-email-settings.js","version":"3.23","currentVersion":"3.24"},{"id":113,"name":"03-update-portal-button-setting.js","version":"3.23","currentVersion":"3.24"},{"id":114,"name":"04-add-meta-columns-to-tags-table.js","version":"3.23","currentVersion":"3.24"},{"id":115,"name":"01-populate-group-for-new-portal-settings.js","version":"3.24","currentVersion":"3.24"},{"id":116,"name":"01-add-members-stripe-webhook-settings.js","version":"3.25","currentVersion":"3.25"}],"migrations_lock":[{"lock_key":"km01","locked":0,"acquired_at":"2020-07-15T13:01:44.000Z","released_at":"2020-07-15T13:01:45.000Z"}],"permissions":[{"id":"5e382b26f81c630018abdf1c","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf1d","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf1e","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf1f","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf20","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf21","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf22","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf23","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf24","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf25","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf26","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf27","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf28","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf29","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b27f81c630018abdf2a","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf2b","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf2c","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf2d","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf2e","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf2f","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf30","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf31","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf32","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf33","name":"Activate themes","object_type":"theme","action_type":"activate","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf34","name":"Upload themes","object_type":"theme","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf35","name":"Download themes","object_type":"theme","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf36","name":"Delete themes","object_type":"theme","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf37","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf38","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf39","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3a","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3b","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3c","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3d","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3e","name":"Browse clients","object_type":"client","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf3f","name":"Read clients","object_type":"client","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf40","name":"Edit clients","object_type":"client","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf41","name":"Add clients","object_type":"client","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf42","name":"Delete clients","object_type":"client","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf43","name":"Browse subscribers","object_type":"subscriber","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf44","name":"Read subscribers","object_type":"subscriber","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf45","name":"Edit subscribers","object_type":"subscriber","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf46","name":"Add subscribers","object_type":"subscriber","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf47","name":"Delete subscribers","object_type":"subscriber","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf48","name":"Browse invites","object_type":"invite","action_type":"browse","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf49","name":"Read invites","object_type":"invite","action_type":"read","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4a","name":"Edit invites","object_type":"invite","action_type":"edit","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4b","name":"Add invites","object_type":"invite","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4c","name":"Delete invites","object_type":"invite","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4d","name":"Download redirects","object_type":"redirect","action_type":"download","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4e","name":"Upload redirects","object_type":"redirect","action_type":"upload","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf4f","name":"Add webhooks","object_type":"webhook","action_type":"add","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e382b27f81c630018abdf50","name":"Delete webhooks","object_type":"webhook","action_type":"destroy","object_id":null,"created_at":"2020-02-03T14:16:07.000Z","updated_at":"2020-02-03T14:16:07.000Z"},{"id":"5e3838d2bc678a002d6a7e58","name":"Browse integrations","object_type":"integration","action_type":"browse","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e59","name":"Browse API keys","object_type":"api_key","action_type":"browse","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5a","name":"Read integrations","object_type":"integration","action_type":"read","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5b","name":"Read API keys","object_type":"api_key","action_type":"read","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5c","name":"Edit integrations","object_type":"integration","action_type":"edit","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5d","name":"Edit API keys","object_type":"api_key","action_type":"edit","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5e","name":"Add integrations","object_type":"integration","action_type":"add","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e5f","name":"Add API keys","object_type":"api_key","action_type":"add","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e60","name":"Delete integrations","object_type":"integration","action_type":"destroy","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d2bc678a002d6a7e61","name":"Delete API keys","object_type":"api_key","action_type":"destroy","object_id":null,"created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d3bc678a002d6a7e6c","name":"Edit webhooks","object_type":"webhook","action_type":"edit","object_id":null,"created_at":"2020-02-03T15:14:27.000Z","updated_at":"2020-02-03T15:14:27.000Z"},{"id":"5e3838d4bc678a002d6a7e6f","name":"Browse Actions","object_type":"action","action_type":"browse","object_id":null,"created_at":"2020-02-03T15:14:28.000Z","updated_at":"2020-02-03T15:14:28.000Z"},{"id":"5e3838d8bc678a002d6a7e77","name":"Browse Members","object_type":"member","action_type":"browse","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e78","name":"Read Members","object_type":"member","action_type":"read","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e79","name":"Edit Members","object_type":"member","action_type":"edit","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e7a","name":"Add Members","object_type":"member","action_type":"add","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e7b","name":"Delete Members","object_type":"member","action_type":"destroy","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e8c","name":"Backup database","object_type":"db","action_type":"backupContent","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e90","name":"Publish posts","object_type":"post","action_type":"publish","object_id":null,"created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e389e2722a7fb002debb411","name":"Email preview","object_type":"email_preview","action_type":"read","object_id":null,"created_at":"2020-02-03T22:26:47.000Z","updated_at":"2020-02-03T22:26:47.000Z"},{"id":"5e389e2722a7fb002debb412","name":"Send test email","object_type":"email_preview","action_type":"sendTestEmail","object_id":null,"created_at":"2020-02-03T22:26:47.000Z","updated_at":"2020-02-03T22:26:47.000Z"},{"id":"5e389e2722a7fb002debb413","name":"Browse emails","object_type":"email","action_type":"browse","object_id":null,"created_at":"2020-02-03T22:26:47.000Z","updated_at":"2020-02-03T22:26:47.000Z"},{"id":"5e389e2722a7fb002debb414","name":"Read emails","object_type":"email","action_type":"read","object_id":null,"created_at":"2020-02-03T22:26:47.000Z","updated_at":"2020-02-03T22:26:47.000Z"},{"id":"5e389e2722a7fb002debb415","name":"Retry emails","object_type":"email","action_type":"retry","object_id":null,"created_at":"2020-02-03T22:26:47.000Z","updated_at":"2020-02-03T22:26:47.000Z"},{"id":"5e46b752e96657002d116b8c","name":"Browse labels","object_type":"label","action_type":"browse","object_id":null,"created_at":"2020-02-14T15:05:54.000Z","updated_at":"2020-02-14T15:05:54.000Z"},{"id":"5e46b752e96657002d116b8d","name":"Read labels","object_type":"label","action_type":"read","object_id":null,"created_at":"2020-02-14T15:05:54.000Z","updated_at":"2020-02-14T15:05:54.000Z"},{"id":"5e46b752e96657002d116b8e","name":"Edit labels","object_type":"label","action_type":"edit","object_id":null,"created_at":"2020-02-14T15:05:54.000Z","updated_at":"2020-02-14T15:05:54.000Z"},{"id":"5e46b752e96657002d116b8f","name":"Add labels","object_type":"label","action_type":"add","object_id":null,"created_at":"2020-02-14T15:05:54.000Z","updated_at":"2020-02-14T15:05:54.000Z"},{"id":"5e46b752e96657002d116b90","name":"Delete labels","object_type":"label","action_type":"destroy","object_id":null,"created_at":"2020-02-14T15:05:54.000Z","updated_at":"2020-02-14T15:05:54.000Z"},{"id":"5e5e50998a92dc002de459e3","name":"Read member signin urls","object_type":"member_signin_url","action_type":"read","object_id":null,"created_at":"2020-03-03T12:42:01.000Z","updated_at":"2020-03-03T12:42:01.000Z"},{"id":"5e7c7d02fe64120039f24200","name":"Read identities","object_type":"identity","action_type":"read","object_id":null,"created_at":"2020-03-26T09:59:30.000Z","updated_at":"2020-03-26T09:59:30.000Z"},{"id":"5ee206e40f4bc6002d3438c8","name":"Auth Stripe Connect for Members","object_type":"members_stripe_connect","action_type":"auth","object_id":null,"created_at":"2020-06-11T11:26:44.000Z","updated_at":"2020-06-11T11:26:44.000Z"}],"permissions_apps":[],"permissions_roles":[{"id":"5e382b27f81c630018abdf52","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf1c"},{"id":"5e382b27f81c630018abdf53","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf1d"},{"id":"5e382b27f81c630018abdf54","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf1e"},{"id":"5e382b27f81c630018abdf55","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf1f"},{"id":"5e382b27f81c630018abdf56","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf20"},{"id":"5e382b27f81c630018abdf57","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf21"},{"id":"5e382b27f81c630018abdf58","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf22"},{"id":"5e382b27f81c630018abdf59","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf23"},{"id":"5e382b27f81c630018abdf5a","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf24"},{"id":"5e382b27f81c630018abdf5b","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf25"},{"id":"5e382b27f81c630018abdf5c","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf26"},{"id":"5e382b27f81c630018abdf5d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf27"},{"id":"5e382b27f81c630018abdf5e","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf28"},{"id":"5e382b27f81c630018abdf5f","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b26f81c630018abdf29"},{"id":"5e382b27f81c630018abdf60","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2a"},{"id":"5e382b27f81c630018abdf61","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2b"},{"id":"5e382b27f81c630018abdf62","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2c"},{"id":"5e382b27f81c630018abdf63","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2d"},{"id":"5e382b27f81c630018abdf64","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2e"},{"id":"5e382b27f81c630018abdf65","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf2f"},{"id":"5e382b27f81c630018abdf66","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf30"},{"id":"5e382b28f81c630018abdf67","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf31"},{"id":"5e382b28f81c630018abdf68","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf32"},{"id":"5e382b28f81c630018abdf69","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf33"},{"id":"5e382b28f81c630018abdf6a","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf34"},{"id":"5e382b28f81c630018abdf6b","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf35"},{"id":"5e382b28f81c630018abdf6c","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf36"},{"id":"5e382b28f81c630018abdf6d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf37"},{"id":"5e382b28f81c630018abdf6e","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf38"},{"id":"5e382b28f81c630018abdf6f","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf39"},{"id":"5e382b28f81c630018abdf70","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3a"},{"id":"5e382b28f81c630018abdf71","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3b"},{"id":"5e382b28f81c630018abdf72","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3c"},{"id":"5e382b28f81c630018abdf73","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3d"},{"id":"5e382b28f81c630018abdf74","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3e"},{"id":"5e382b28f81c630018abdf75","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf3f"},{"id":"5e382b28f81c630018abdf76","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf40"},{"id":"5e382b28f81c630018abdf77","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf41"},{"id":"5e382b28f81c630018abdf78","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf42"},{"id":"5e382b28f81c630018abdf79","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf43"},{"id":"5e382b28f81c630018abdf7a","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf44"},{"id":"5e382b28f81c630018abdf7b","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf45"},{"id":"5e382b28f81c630018abdf7c","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf46"},{"id":"5e382b28f81c630018abdf7d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf47"},{"id":"5e382b28f81c630018abdf7e","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf48"},{"id":"5e382b28f81c630018abdf7f","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf49"},{"id":"5e382b28f81c630018abdf80","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4a"},{"id":"5e382b28f81c630018abdf81","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4b"},{"id":"5e382b28f81c630018abdf82","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4c"},{"id":"5e382b28f81c630018abdf83","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4d"},{"id":"5e382b28f81c630018abdf84","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4e"},{"id":"5e382b28f81c630018abdf85","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf4f"},{"id":"5e382b28f81c630018abdf86","role_id":"5e382b26f81c630018abdf17","permission_id":"5e382b27f81c630018abdf50"},{"id":"5e382b28f81c630018abdf87","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf23"},{"id":"5e382b28f81c630018abdf88","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf24"},{"id":"5e382b28f81c630018abdf89","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf25"},{"id":"5e382b28f81c630018abdf8a","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf26"},{"id":"5e382b28f81c630018abdf8b","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf27"},{"id":"5e382b28f81c630018abdf8c","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf28"},{"id":"5e382b28f81c630018abdf8d","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf29"},{"id":"5e382b28f81c630018abdf8e","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf2b"},{"id":"5e382b28f81c630018abdf8f","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf2c"},{"id":"5e382b28f81c630018abdf90","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf2d"},{"id":"5e382b28f81c630018abdf91","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf2e"},{"id":"5e382b28f81c630018abdf92","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf2f"},{"id":"5e382b28f81c630018abdf93","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf30"},{"id":"5e382b28f81c630018abdf94","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf37"},{"id":"5e382b28f81c630018abdf95","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf38"},{"id":"5e382b28f81c630018abdf96","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf39"},{"id":"5e382b28f81c630018abdf97","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3a"},{"id":"5e382b28f81c630018abdf98","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3b"},{"id":"5e382b28f81c630018abdf99","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3c"},{"id":"5e382b28f81c630018abdf9a","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3d"},{"id":"5e382b28f81c630018abdf9b","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3e"},{"id":"5e382b28f81c630018abdf9c","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf3f"},{"id":"5e382b28f81c630018abdf9d","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf40"},{"id":"5e382b28f81c630018abdf9e","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf41"},{"id":"5e382b28f81c630018abdf9f","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf42"},{"id":"5e382b28f81c630018abdfa0","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf46"},{"id":"5e382b28f81c630018abdfa1","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf48"},{"id":"5e382b28f81c630018abdfa2","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf49"},{"id":"5e382b28f81c630018abdfa3","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf4a"},{"id":"5e382b28f81c630018abdfa4","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf4b"},{"id":"5e382b28f81c630018abdfa5","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf4c"},{"id":"5e382b28f81c630018abdfa6","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b27f81c630018abdf31"},{"id":"5e382b28f81c630018abdfa7","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b26f81c630018abdf23"},{"id":"5e382b28f81c630018abdfa8","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b26f81c630018abdf24"},{"id":"5e382b28f81c630018abdfa9","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b26f81c630018abdf26"},{"id":"5e382b28f81c630018abdfaa","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b26f81c630018abdf28"},{"id":"5e382b28f81c630018abdfab","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b26f81c630018abdf29"},{"id":"5e382b28f81c630018abdfac","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf2b"},{"id":"5e382b28f81c630018abdfad","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf2c"},{"id":"5e382b28f81c630018abdfae","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf2d"},{"id":"5e382b28f81c630018abdfaf","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf2f"},{"id":"5e382b28f81c630018abdfb0","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf37"},{"id":"5e382b28f81c630018abdfb1","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf38"},{"id":"5e382b28f81c630018abdfb2","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf3d"},{"id":"5e382b28f81c630018abdfb3","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf3e"},{"id":"5e382b28f81c630018abdfb4","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf3f"},{"id":"5e382b28f81c630018abdfb5","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf40"},{"id":"5e382b28f81c630018abdfb6","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf41"},{"id":"5e382b28f81c630018abdfb7","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf42"},{"id":"5e382b28f81c630018abdfb8","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf46"},{"id":"5e382b28f81c630018abdfb9","role_id":"5e382b26f81c630018abdf19","permission_id":"5e382b27f81c630018abdf31"},{"id":"5e382b28f81c630018abdfba","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b26f81c630018abdf23"},{"id":"5e382b28f81c630018abdfbb","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b26f81c630018abdf24"},{"id":"5e382b28f81c630018abdfbc","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b26f81c630018abdf26"},{"id":"5e382b28f81c630018abdfbd","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b26f81c630018abdf28"},{"id":"5e382b28f81c630018abdfbe","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b26f81c630018abdf29"},{"id":"5e382b28f81c630018abdfbf","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf2b"},{"id":"5e382b28f81c630018abdfc0","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf2c"},{"id":"5e382b28f81c630018abdfc1","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf2d"},{"id":"5e382b28f81c630018abdfc2","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf37"},{"id":"5e382b28f81c630018abdfc3","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf38"},{"id":"5e382b28f81c630018abdfc4","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf3d"},{"id":"5e382b28f81c630018abdfc5","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf3e"},{"id":"5e382b28f81c630018abdfc6","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf3f"},{"id":"5e382b28f81c630018abdfc7","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf40"},{"id":"5e382b28f81c630018abdfc8","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf41"},{"id":"5e382b28f81c630018abdfc9","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf42"},{"id":"5e382b28f81c630018abdfca","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf46"},{"id":"5e382b28f81c630018abdfcb","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e382b27f81c630018abdf31"},{"id":"5e3838d2bc678a002d6a7e26","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf1f"},{"id":"5e3838d2bc678a002d6a7e27","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf20"},{"id":"5e3838d2bc678a002d6a7e28","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf21"},{"id":"5e3838d2bc678a002d6a7e29","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf22"},{"id":"5e3838d2bc678a002d6a7e2a","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf23"},{"id":"5e3838d2bc678a002d6a7e2b","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf24"},{"id":"5e3838d2bc678a002d6a7e2c","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf25"},{"id":"5e3838d2bc678a002d6a7e2d","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf26"},{"id":"5e3838d2bc678a002d6a7e2e","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf27"},{"id":"5e3838d2bc678a002d6a7e2f","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf28"},{"id":"5e3838d2bc678a002d6a7e30","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b26f81c630018abdf29"},{"id":"5e3838d2bc678a002d6a7e31","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2a"},{"id":"5e3838d2bc678a002d6a7e32","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2b"},{"id":"5e3838d2bc678a002d6a7e33","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2c"},{"id":"5e3838d2bc678a002d6a7e34","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2d"},{"id":"5e3838d2bc678a002d6a7e35","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2e"},{"id":"5e3838d2bc678a002d6a7e36","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf2f"},{"id":"5e3838d2bc678a002d6a7e37","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf30"},{"id":"5e3838d2bc678a002d6a7e38","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf31"},{"id":"5e3838d2bc678a002d6a7e39","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf32"},{"id":"5e3838d2bc678a002d6a7e3a","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf33"},{"id":"5e3838d2bc678a002d6a7e3b","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf34"},{"id":"5e3838d2bc678a002d6a7e3c","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf35"},{"id":"5e3838d2bc678a002d6a7e3d","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf36"},{"id":"5e3838d2bc678a002d6a7e3e","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf37"},{"id":"5e3838d2bc678a002d6a7e3f","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf38"},{"id":"5e3838d2bc678a002d6a7e40","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf39"},{"id":"5e3838d2bc678a002d6a7e41","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3a"},{"id":"5e3838d2bc678a002d6a7e42","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3b"},{"id":"5e3838d2bc678a002d6a7e43","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3c"},{"id":"5e3838d2bc678a002d6a7e44","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3d"},{"id":"5e3838d2bc678a002d6a7e45","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3e"},{"id":"5e3838d2bc678a002d6a7e46","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf3f"},{"id":"5e3838d2bc678a002d6a7e47","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf40"},{"id":"5e3838d2bc678a002d6a7e48","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf41"},{"id":"5e3838d2bc678a002d6a7e49","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf42"},{"id":"5e3838d2bc678a002d6a7e4a","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf43"},{"id":"5e3838d2bc678a002d6a7e4b","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf44"},{"id":"5e3838d2bc678a002d6a7e4c","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf45"},{"id":"5e3838d2bc678a002d6a7e4d","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf46"},{"id":"5e3838d2bc678a002d6a7e4e","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf47"},{"id":"5e3838d2bc678a002d6a7e4f","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf48"},{"id":"5e3838d2bc678a002d6a7e50","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf49"},{"id":"5e3838d2bc678a002d6a7e51","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4a"},{"id":"5e3838d2bc678a002d6a7e52","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4b"},{"id":"5e3838d2bc678a002d6a7e53","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4c"},{"id":"5e3838d2bc678a002d6a7e54","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4d"},{"id":"5e3838d2bc678a002d6a7e55","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4e"},{"id":"5e3838d2bc678a002d6a7e56","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf4f"},{"id":"5e3838d2bc678a002d6a7e57","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e382b27f81c630018abdf50"},{"id":"5e3838d2bc678a002d6a7e62","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e58"},{"id":"5e3838d2bc678a002d6a7e63","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5a"},{"id":"5e3838d2bc678a002d6a7e64","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5c"},{"id":"5e3838d2bc678a002d6a7e65","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5e"},{"id":"5e3838d2bc678a002d6a7e66","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e60"},{"id":"5e3838d2bc678a002d6a7e67","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e59"},{"id":"5e3838d2bc678a002d6a7e68","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5b"},{"id":"5e3838d2bc678a002d6a7e69","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5d"},{"id":"5e3838d2bc678a002d6a7e6a","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e5f"},{"id":"5e3838d2bc678a002d6a7e6b","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d2bc678a002d6a7e61"},{"id":"5e3838d3bc678a002d6a7e6d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d3bc678a002d6a7e6c"},{"id":"5e3838d3bc678a002d6a7e6e","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d3bc678a002d6a7e6c"},{"id":"5e3838d4bc678a002d6a7e70","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d4bc678a002d6a7e6f"},{"id":"5e3838d4bc678a002d6a7e71","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d4bc678a002d6a7e6f"},{"id":"5e3838d8bc678a002d6a7e74","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf20"},{"id":"5e3838d8bc678a002d6a7e75","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf21"},{"id":"5e3838d8bc678a002d6a7e76","role_id":"5e382b26f81c630018abdf18","permission_id":"5e382b26f81c630018abdf22"},{"id":"5e3838d8bc678a002d6a7e7c","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e77"},{"id":"5e3838d8bc678a002d6a7e7d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e78"},{"id":"5e3838d8bc678a002d6a7e7e","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e79"},{"id":"5e3838d8bc678a002d6a7e7f","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e7a"},{"id":"5e3838d8bc678a002d6a7e80","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e7b"},{"id":"5e3838d8bc678a002d6a7e81","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e77"},{"id":"5e3838d8bc678a002d6a7e82","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e78"},{"id":"5e3838d8bc678a002d6a7e83","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e79"},{"id":"5e3838d8bc678a002d6a7e84","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e7a"},{"id":"5e3838d8bc678a002d6a7e85","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e7b"},{"id":"5e3838d8bc678a002d6a7e87","role_id":"5e3838d8bc678a002d6a7e86","permission_id":"5e382b26f81c630018abdf1c"},{"id":"5e3838d8bc678a002d6a7e88","role_id":"5e3838d8bc678a002d6a7e86","permission_id":"5e382b26f81c630018abdf1d"},{"id":"5e3838d8bc678a002d6a7e89","role_id":"5e3838d8bc678a002d6a7e86","permission_id":"5e382b26f81c630018abdf1e"},{"id":"5e3838d8bc678a002d6a7e8d","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e8c"},{"id":"5e3838d8bc678a002d6a7e8e","role_id":"5e3838d8bc678a002d6a7e86","permission_id":"5e3838d8bc678a002d6a7e8c"},{"id":"5e3838d9bc678a002d6a7e93","role_id":"5e382b26f81c630018abdf17","permission_id":"5e3838d8bc678a002d6a7e90"},{"id":"5e3838d9bc678a002d6a7e94","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e3838d8bc678a002d6a7e90"},{"id":"5e3838d9bc678a002d6a7e95","role_id":"5e382b26f81c630018abdf18","permission_id":"5e3838d8bc678a002d6a7e90"},{"id":"5e3838d9bc678a002d6a7e96","role_id":"5e3838d8bc678a002d6a7e8f","permission_id":"5e3838d8bc678a002d6a7e90"},{"id":"5e46b753e96657002d116b91","role_id":"5e382b26f81c630018abdf17","permission_id":"5e46b752e96657002d116b8c"},{"id":"5e46b753e96657002d116b92","role_id":"5e382b26f81c630018abdf17","permission_id":"5e46b752e96657002d116b8d"},{"id":"5e46b753e96657002d116b93","role_id":"5e382b26f81c630018abdf17","permission_id":"5e46b752e96657002d116b8e"},{"id":"5e46b753e96657002d116b94","role_id":"5e382b26f81c630018abdf17","permission_id":"5e46b752e96657002d116b8f"},{"id":"5e46b753e96657002d116b95","role_id":"5e382b26f81c630018abdf17","permission_id":"5e46b752e96657002d116b90"},{"id":"5e46b753e96657002d116b96","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e46b752e96657002d116b8c"},{"id":"5e46b753e96657002d116b97","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e46b752e96657002d116b8d"},{"id":"5e46b753e96657002d116b98","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e46b752e96657002d116b8e"},{"id":"5e46b753e96657002d116b99","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e46b752e96657002d116b8f"},{"id":"5e46b753e96657002d116b9a","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e46b752e96657002d116b90"},{"id":"5ee206e40f4bc6002d3438c0","role_id":"5e382b26f81c630018abdf17","permission_id":"5e389e2722a7fb002debb411"},{"id":"5ee206e40f4bc6002d3438c1","role_id":"5e382b26f81c630018abdf17","permission_id":"5e389e2722a7fb002debb412"},{"id":"5ee206e40f4bc6002d3438c2","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e389e2722a7fb002debb411"},{"id":"5ee206e40f4bc6002d3438c3","role_id":"5e3838d2bc678a002d6a7e25","permission_id":"5e389e2722a7fb002debb412"},{"id":"5ee206e40f4bc6002d3438c4","role_id":"5e382b26f81c630018abdf18","permission_id":"5e389e2722a7fb002debb411"},{"id":"5ee206e40f4bc6002d3438c5","role_id":"5e382b26f81c630018abdf18","permission_id":"5e389e2722a7fb002debb412"},{"id":"5ee206e40f4bc6002d3438c6","role_id":"5e382b26f81c630018abdf19","permission_id":"5e389e2722a7fb002debb411"},{"id":"5ee206e40f4bc6002d3438c7","role_id":"5e382b26f81c630018abdf1a","permission_id":"5e389e2722a7fb002debb411"}],"permissions_users":[],"posts":[{"id":"5e382b29f81c630018abe035","uuid":"5f697efb-4894-4f43-8082-24c825d3522c","title":"Securing your ASP.NET MVC 3 application","slug":"securing-your-asp-net-mvc-3-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The following blog post is most likely well covered on the internet, but I thought I would write about it anyway just so I now where to find it next time I need it. As you probably have figured the topic that will be handled in this post is securing your ASP.NET MVC 3 application. When talking about security there are two things we need to consider; authentication and authorization. This post will handle the authentication part since authorization is specific to your application and most of the time it will be enought just to use the `AuthorizeAttribute`.\\n\\n###Authentication done right in ASP.NET MVC 3\\nAuthentication is the first step in my short security chain, first I authenticate the user to assign an identity, roles or claims to the user. If I'm not developing a simple web site where every user has access to every page I most often want to restrict access to all pages except those that I have explicitly white listed for anonymous access. So how do I go along and implementing white listing i ASP.NET MVC 3? It turns out that it is really easy since ASP.NET MVC 3 introduced global action filters which you can apply to all controllers and actions in the application, compared to local action filters which you apply only to a controller or action. When you have a global action filter in place you can apply a local action filter to those actions or controllers that you do want anonymous users have access to, like the log in action. The global action filter that I will use to restrict the access to the application look like:\\n\\n    public class RequireAuthenticationAttribute : AuthorizeAttribute\\n    {\\n        public override void OnAuthorization(AuthorizationContext filterContext)\\n        {\\n            var skipAuthorization = filterContext.ActionDescriptor.IsDefined(typeof(AllowAnonymousAttribute), true) ||\\n                                    filterContext.ActionDescriptor.ControllerDescriptor.IsDefined(\\n                                        typeof(AllowAnonymousAttribute), true);\\n            if (!skipAuthorization)\\n            {\\n                base.OnAuthorization(filterContext);\\n            }\\n        }\\n    }\\n\\t\\nIt is pretty straight forward but here are some comments about it. Everywhere it says *Authorization* really means *Authentication* if you ask me, but it is handled the same way in the framework. The `OnAuthorization` method checks if a local filter called `AllowAnonymousAttribute` (which is coming next) is defined on either the action or on the controller. If the local filter is defined we want to allow anonymous access and can therefor skip authentication, that is, the call to `base.OnAuthorization`. The code for the `AllowAnonymousAttribute` is probably one of the most simple class you have ever seen:\\n\\n    [AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = true, Inherited = true)]\\n    public class AllowAnonymousAttribute : Attribute\\n    {\\n    }\\n\\nAll the class do is defining an attribute that I can apply to classes (controllers) or methods (actions). It doesn't have to have any functionality since all I do in the `RequireAuthenticationAttribute` is checking if the `AllowAnonymousAttribute` is present.\\n\\t\\nTo apply this global action filter you need to add the filters to your global action filters collection in your global.asax file. So in your global.asax file you need it to look something like this:\\n\\n\\tprotected void Application_Start()\\n\\t{\\n\\t\\t...\\n\\t\\tRegisterGlobalFilters(GlobalFilters.Filters);\\n\\t\\t...\\n\\t}\\n\\n\\tpublic static void RegisterGlobalFilters(GlobalFilterCollection filters)\\n\\t{\\n\\t\\tfilters.Add(new RequireAuthenticationAttribute());\\n\\t\\t...\\n\\t}\\n\\nIn the `Application_Start` method I make a call to my `RegisterGlobalFilters` method and register the `RequireAuthenticationAttribute` so if you try to access any page, that is handled by MVC, you will be denied access to it since you are not authenticated. That is the exact behavior we want, but now it is time to start white listing actions. The actions I want to white list are the `LogOn` and `Register` actions in my `AccountController`:\\n\\n    public class AccountController : Controller\\n    {\\n        [AllowAnonymous]\\n        public ActionResult LogOn()\\n        {\\n\\t\\t\\t...\\n        }\\n\\n        [HttpPost]\\n        [AllowAnonymous]\\n        public ActionResult LogOn(LogOnModel model, string returnUrl)\\n        {\\n\\t\\t\\t...\\n        }\\n\\n        [AllowAnonymous]\\n        public ActionResult Register()\\n        {\\n\\t\\t\\t...\\n        }\\n\\n        [HttpPost]\\n        [AllowAnonymous]\\n        public ActionResult Register(RegisterModel model)\\n        {\\n\\t\\t\\t...\\n        }\\n\\n\\t\\t...\\n\\t\\t...\\n    }\\n\\nIf you now try to access the application again you should be redirected to the log on page if you have a default ASP.NET MVC 3 application.\\n\\nThat is all there is to it basically. Applying this in a filter instead of having this functionality in a base controller as you would do in ASP.NET MVC 2 is much more \\\"correct\\\". When you have it in a global filter it is basically impossible to do a mistake compared to when you are forced to inherit a specific controller which you might forget.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The following blog post is most likely well covered on the internet, but I thought I would write about it anyway just so I now where to find it next time I need it. As you probably have figured the topic that will be handled in this post is securing your ASP.NET MVC 3 application. When talking about security there are two things we need to consider; authentication and authorization. This post will handle the authentication part since authorization is specific to your application and most of the time it will be enought just to use the <code>AuthorizeAttribute</code>.</p>\n<h3 id=\"authenticationdonerightinaspnetmvc3\">Authentication done right in ASP.NET MVC 3</h3>\n<p>Authentication is the first step in my short security chain, first I authenticate the user to assign an identity, roles or claims to the user. If I'm not developing a simple web site where every user has access to every page I most often want to restrict access to all pages except those that I have explicitly white listed for anonymous access. So how do I go along and implementing white listing i ASP.NET MVC 3? It turns out that it is really easy since ASP.NET MVC 3 introduced global action filters which you can apply to all controllers and actions in the application, compared to local action filters which you apply only to a controller or action. When you have a global action filter in place you can apply a local action filter to those actions or controllers that you do want anonymous users have access to, like the log in action. The global action filter that I will use to restrict the access to the application look like:</p>\n<pre><code>public class RequireAuthenticationAttribute : AuthorizeAttribute\n{\n    public override void OnAuthorization(AuthorizationContext filterContext)\n    {\n        var skipAuthorization = filterContext.ActionDescriptor.IsDefined(typeof(AllowAnonymousAttribute), true) ||\n                                filterContext.ActionDescriptor.ControllerDescriptor.IsDefined(\n                                    typeof(AllowAnonymousAttribute), true);\n        if (!skipAuthorization)\n        {\n            base.OnAuthorization(filterContext);\n        }\n    }\n}\n</code></pre>\n<p>It is pretty straight forward but here are some comments about it. Everywhere it says <em>Authorization</em> really means <em>Authentication</em> if you ask me, but it is handled the same way in the framework. The <code>OnAuthorization</code> method checks if a local filter called <code>AllowAnonymousAttribute</code> (which is coming next) is defined on either the action or on the controller. If the local filter is defined we want to allow anonymous access and can therefor skip authentication, that is, the call to <code>base.OnAuthorization</code>. The code for the <code>AllowAnonymousAttribute</code> is probably one of the most simple class you have ever seen:</p>\n<pre><code>[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = true, Inherited = true)]\npublic class AllowAnonymousAttribute : Attribute\n{\n}\n</code></pre>\n<p>All the class do is defining an attribute that I can apply to classes (controllers) or methods (actions). It doesn't have to have any functionality since all I do in the <code>RequireAuthenticationAttribute</code> is checking if the <code>AllowAnonymousAttribute</code> is present.</p>\n<p>To apply this global action filter you need to add the filters to your global action filters collection in your global.asax file. So in your global.asax file you need it to look something like this:</p>\n<pre><code>protected void Application_Start()\n{\n\t...\n\tRegisterGlobalFilters(GlobalFilters.Filters);\n\t...\n}\n\npublic static void RegisterGlobalFilters(GlobalFilterCollection filters)\n{\n\tfilters.Add(new RequireAuthenticationAttribute());\n\t...\n}\n</code></pre>\n<p>In the <code>Application_Start</code> method I make a call to my <code>RegisterGlobalFilters</code> method and register the <code>RequireAuthenticationAttribute</code> so if you try to access any page, that is handled by MVC, you will be denied access to it since you are not authenticated. That is the exact behavior we want, but now it is time to start white listing actions. The actions I want to white list are the <code>LogOn</code> and <code>Register</code> actions in my <code>AccountController</code>:</p>\n<pre><code>public class AccountController : Controller\n{\n    [AllowAnonymous]\n    public ActionResult LogOn()\n    {\n\t\t...\n    }\n\n    [HttpPost]\n    [AllowAnonymous]\n    public ActionResult LogOn(LogOnModel model, string returnUrl)\n    {\n\t\t...\n    }\n\n    [AllowAnonymous]\n    public ActionResult Register()\n    {\n\t\t...\n    }\n\n    [HttpPost]\n    [AllowAnonymous]\n    public ActionResult Register(RegisterModel model)\n    {\n\t\t...\n    }\n\n\t...\n\t...\n}\n</code></pre>\n<p>If you now try to access the application again you should be redirected to the log on page if you have a default ASP.NET MVC 3 application.</p>\n<p>That is all there is to it basically. Applying this in a filter instead of having this functionality in a base controller as you would do in ASP.NET MVC 2 is much more &quot;correct&quot;. When you have it in a global filter it is basically impossible to do a mistake compared to when you are forced to inherit a specific controller which you might forget.</p>\n<!--kg-card-end: markdown-->","comment_id":"2","plaintext":"The following blog post is most likely well covered on the internet, but I\nthought I would write about it anyway just so I now where to find it next time I\nneed it. As you probably have figured the topic that will be handled in this\npost is securing your ASP.NET MVC 3 application. When talking about security\nthere are two things we need to consider; authentication and authorization. This\npost will handle the authentication part since authorization is specific to your\napplication and most of the time it will be enought just to use the \nAuthorizeAttribute.\n\nAuthentication done right in ASP.NET MVC 3\nAuthentication is the first step in my short security chain, first I\nauthenticate the user to assign an identity, roles or claims to the user. If I'm\nnot developing a simple web site where every user has access to every page I\nmost often want to restrict access to all pages except those that I have\nexplicitly white listed for anonymous access. So how do I go along and\nimplementing white listing i ASP.NET MVC 3? It turns out that it is really easy\nsince ASP.NET MVC 3 introduced global action filters which you can apply to all\ncontrollers and actions in the application, compared to local action filters\nwhich you apply only to a controller or action. When you have a global action\nfilter in place you can apply a local action filter to those actions or\ncontrollers that you do want anonymous users have access to, like the log in\naction. The global action filter that I will use to restrict the access to the\napplication look like:\n\npublic class RequireAuthenticationAttribute : AuthorizeAttribute\n{\n    public override void OnAuthorization(AuthorizationContext filterContext)\n    {\n        var skipAuthorization = filterContext.ActionDescriptor.IsDefined(typeof(AllowAnonymousAttribute), true) ||\n                                filterContext.ActionDescriptor.ControllerDescriptor.IsDefined(\n                                    typeof(AllowAnonymousAttribute), true);\n        if (!skipAuthorization)\n        {\n            base.OnAuthorization(filterContext);\n        }\n    }\n}\n\n\nIt is pretty straight forward but here are some comments about it. Everywhere it\nsays Authorization really means Authentication if you ask me, but it is handled\nthe same way in the framework. The OnAuthorization method checks if a local\nfilter called AllowAnonymousAttribute (which is coming next) is defined on\neither the action or on the controller. If the local filter is defined we want\nto allow anonymous access and can therefor skip authentication, that is, the\ncall to base.OnAuthorization. The code for the AllowAnonymousAttribute is\nprobably one of the most simple class you have ever seen:\n\n[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = true, Inherited = true)]\npublic class AllowAnonymousAttribute : Attribute\n{\n}\n\n\nAll the class do is defining an attribute that I can apply to classes\n(controllers) or methods (actions). It doesn't have to have any functionality\nsince all I do in the RequireAuthenticationAttribute is checking if the \nAllowAnonymousAttribute is present.\n\nTo apply this global action filter you need to add the filters to your global\naction filters collection in your global.asax file. So in your global.asax file\nyou need it to look something like this:\n\nprotected void Application_Start()\n{\n\t...\n\tRegisterGlobalFilters(GlobalFilters.Filters);\n\t...\n}\n\npublic static void RegisterGlobalFilters(GlobalFilterCollection filters)\n{\n\tfilters.Add(new RequireAuthenticationAttribute());\n\t...\n}\n\n\nIn the Application_Start method I make a call to my RegisterGlobalFilters method\nand register the RequireAuthenticationAttribute so if you try to access any\npage, that is handled by MVC, you will be denied access to it since you are not\nauthenticated. That is the exact behavior we want, but now it is time to start\nwhite listing actions. The actions I want to white list are the LogOn and \nRegister actions in my AccountController:\n\npublic class AccountController : Controller\n{\n    [AllowAnonymous]\n    public ActionResult LogOn()\n    {\n\t\t...\n    }\n\n    [HttpPost]\n    [AllowAnonymous]\n    public ActionResult LogOn(LogOnModel model, string returnUrl)\n    {\n\t\t...\n    }\n\n    [AllowAnonymous]\n    public ActionResult Register()\n    {\n\t\t...\n    }\n\n    [HttpPost]\n    [AllowAnonymous]\n    public ActionResult Register(RegisterModel model)\n    {\n\t\t...\n    }\n\n\t...\n\t...\n}\n\n\nIf you now try to access the application again you should be redirected to the\nlog on page if you have a default ASP.NET MVC 3 application.\n\nThat is all there is to it basically. Applying this in a filter instead of\nhaving this functionality in a base controller as you would do in ASP.NET MVC 2\nis much more \"correct\". When you have it in a global filter it is basically\nimpossible to do a mistake compared to when you are forced to inherit a specific\ncontroller which you might forget.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T08:57:38.000Z","updated_at":"2014-03-01T09:00:43.000Z","published_at":"2012-10-24T06:57:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe036","uuid":"bc3e8b5f-4a41-46a5-83a6-96a2e937c98d","title":"Creating custom unobtrusive file extension validation in ASP.NET MVC 3 and jQuery","slug":"creating-custom-unobtrusive-file-extension-validation-in-asp-net-mvc-3-and-jquery","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Just recently I was given the task to move a part of an ASP.NET WebForm to ASP.NET MVC since we are moving over to ASP.NET MVC. The part that I would move over had a form that contained a file upload controller and there were some custom implemented JavaScript hacks to verify that the file had the right file extension. I thought I could do better using data annotations, unobtrusive jQuery and implement a custom validator that also work on the client side so this post is about writing custom validation on the server and unobtrusive validation for ASP.NET MVC 3 on the client.\\n\\n###Server side validation\\nThe first step to get this working is to get the server side validation to work since you allways want server side validation to make sure the user hasn't bypassed your client side validation. The attribute we need to implement will look something like:\\n\\n    [AttributeUsage(AttributeTargets.Field | AttributeTargets.Property, AllowMultiple = false, Inherited = true)]\\n    public class FileExtensionsAttribute : ValidationAttribute\\n    {\\n        private List<string> ValidExtensions { get; set; }\\n\\n        public FileExtensionsAttribute(string fileExtensions)\\n        {\\n            ValidExtensions = fileExtensions.Split('|').ToList();\\n        }\\n\\n        public override bool IsValid(object value)\\n        {\\n            HttpPostedFileBase file = value as HttpPostedFileBase;\\n            if (file != null)\\n            {\\n                var fileName = file.FileName;\\n                var isValidExtension = ValidExtensions.Any(y => fileName.EndsWith(y));\\n                return isValidExtension;\\n            }\\n            return true;\\n        }\\n    }\\n\\nYou apply the property like such\\n\\n    [FileExtensions(\\\"txt|doc\\\")]\\n    public HttPostedFileBase File { get; set; }\\n\\t\\nWhen this attribute is applied to a property or field of type `HttpPostedFileBase` and is given a string with extensions separated by \\\"|\\\" it will take those extensions and validate the `HttpPostedFileBase` property that it is applied to. As you can see I am returning true if the file is `null`, and that is because if a file should be required should have the `Require` attribute as well. But all this is still on the server and we want to bring it to the client which is the next step.\\n\\nBefore we continue with the next step, which is Unobtrusive client side validation, I thought I would show you a simple editor template to make it easier to get the correct markup for the `HttpPostedFileBase`:\\n\\n     @model System.Web.HttpPostedFileBase\\n     @Html.TextBoxFor(model => model, new {type = \\\"file\\\"})\\n\\nSave the file as HttpPostedFileBase.cshtml in the folder Views/Shared/EditorTemplates. When you have saved the file you will be able to write `@Html.EditorFor(model => model.FileProperty)` and you will get the right html.\\n\\n\\n###Unobtrusive client side validation\\n[Unobtrusive JavaScript](http://en.wikipedia.org/wiki/Unobtrusive_JavaScript) is not formally defined but it basically says that you should separate your logic (JavaScript) from your view (the html code) and then bind JavaScript after the page has loaded. This way you don't get any JavaScript code in your view and if you do it correctly the page will still be supported in browsers that don't support JavaScript.\\n\\nOne thing about client side validation on the web is that you should NOT rely on it, it should only be there to make a better user experience. Enough said, let get to the point.\\n\\nThere are four steps that you need to do to implement unobtrusive client side validation: \\n\\n * Enable unobtrusive validation and add the scripts.\\n * Implement a `ModelClientValidationRule` that is part of the bridge to the JavaScript.\\n * Implement the rule in JavaScript.\\n * Implement an adapter that is the second part of the bridge to the JavaScript rule.\\n\\n#### Enable unobtrusive validation and add the scripts.\\nThis step is the easiest one. You can enable unobtrusive validation in two ways, either in code:\\n\\n    Html.EnableClientValidation(true);\\n    Html.EnableUnobtrusiveJavaScript(true);\\n\\t\\nor in web.config:\\n\\n    <appSettings>\\n        <add key=\\\"ClientValidationEnabled\\\" value=\\\"true\\\"/>\\n        <add key=\\\"UnobtrusiveJavaScriptEnabled\\\" value=\\\"true\\\"/>\\n    </appSettings>\\n\\n**When you have enabled it you need to include the scripts jquery.validate.js, jquery.validate.unobtrusive.js and of course your own script that will contain the custom validation code.**\\n\\t\\n#### Implementing the ModelClientValidationRule\\nThe code for the `ModelClientValidationRule` is: \\n\\n    public class ModelClientFileExtensionValidationRule : ModelClientValidationRule\\n    {\\n        public ModelClientFileExtensionValidationRule(string errorMessage, List<string> fileExtensions)\\n        {\\n            ErrorMessage = errorMessage;\\n            ValidationType = \\\"fileextensions\\\";\\n            ValidationParameters.Add(\\\"fileextensions\\\", string.Join(\\\",\\\", fileExtensions));\\n        }\\n    }\\n\\t\\nIt is somewhat straightforward; the `ErrorMessage` is the message that is shown on the client, the `ValidationType` is what it says it is, that is, the type of the validation. When the `ModelClientValidationRule` is done we need to add it to the `ValidationAttribute`:\\n\\n    public class FileExtensionsAttribute : ValidationAttribute, IClientValidatable\\n    {\\n\\t\\t... // Same as before\\n\\t\\t\\n        public IEnumerable<ModelClientValidationRule> GetClientValidationRules(ModelMetadata metadata, ControllerContext context)\\n        {\\n            var rule = new ModelClientFileExtensionValidationRule(ErrorMessage, ValidExtensions);\\n            yield return rule;\\n        }\\n    }\\n    }\\n\\n#### Implementing the adapter\\nThe JavaScript adapter is in this case basically the definition of the rule. It doesn't necessary have the actual logic that do the testing, but it sets up everything that needs to be tested. \\n\\n    $(function () {\\n        jQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\\n            // Set up test parameters\\n    \\t\\tvar params = {\\n                fileextensions: options.params.fileextensions.split(',')\\n            };\\n\\n\\t\\t\\t// Match parameters to the method to execute\\n            options.rules['fileextensions'] = params;\\n            if (options.message) {\\n\\t\\t\\t    // If there is a message, set it for the rule\\n                options.messages['fileextensions'] = options.message;\\n            }\\n        });\\n    } (jQuery));\\n\\nThe code above is executed when the document is ready and adds an \\\"adapter\\\" to the unobtrusive framework. The first paramter is the name of the adapter in this case is \\\"fileextensions\\\" and it should match the `ValidationType` in the `ModelClientFileExtensionValidationRule`. In the `ModelClientFileExtensionValidationRule` we defined a `ValidationParameter` that was named \\\"fileextensions\\\", that is why we have an array with \\\"fileextensions\\\" in it as second parameter. The second parameter should contain the names of all the parameters that are added to the `ValidationParameters` property. The third parameter is the function that basically configures the rule, note that it doesn't execute the rule. \\n\\n#### Implementing the rule in JavaScript\\nAdding a rule to the a test is as simple as adding the adapter. The code below includes both the adapter and the rule.\\n\\n\\t$(function () {\\n\\t\\tjQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\\n\\t\\t\\tvar params = {\\n\\t\\t\\t\\tfileextensions: options.params.fileextensions.split(',')\\n\\t\\t\\t};\\n\\n\\t\\t\\toptions.rules['fileextensions'] = params;\\n\\t\\t\\tif (options.message) {\\n\\t\\t\\t\\toptions.messages['fileextensions'] = options.message;\\n\\t\\t\\t}\\n\\t\\t});\\n\\n\\t\\tjQuery.validator.addMethod(\\\"fileextensions\\\", function (value, element, param) {\\n\\t\\t\\tvar extension = getFileExtension(value);\\n\\t\\t\\tvar validExtension = $.inArray(extension, param.fileextensions) !== -1;\\n\\t\\t\\treturn validExtension;\\n\\t\\t});\\n\\n\\t\\tfunction getFileExtension(fileName) {\\n\\t\\t\\tvar extension = (/[.]/.exec(fileName)) ? /[^.]+$/.exec(fileName) : undefined;\\n\\t\\t\\tif (extension != undefined) {\\n\\t\\t\\t\\treturn extension[0];\\n\\t\\t\\t}\\n\\t\\t\\treturn extension;\\n\\t\\t};\\n\\t} (jQuery));\\n\\nIt is the row `jQuery.validator.addMethod(...)` that is relevant here. We add a rule that we define with the name \\\"fileextensions\\\". The parameters to the rule are the value that is to be validated, the element that is being validated and the parameters that we set up in the adapter. In our case the validation is really straightforward, first we get the file extension using the helper method `getFileExtension`, then we check if the extension is present in the list for file extensions that on the `fileextensions` property on the `param` parameter and return the result. \\n\\nI think this is all there is to it. If you have any question or comments just use the form below.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Just recently I was given the task to move a part of an ASP.NET WebForm to ASP.NET MVC since we are moving over to ASP.NET MVC. The part that I would move over had a form that contained a file upload controller and there were some custom implemented JavaScript hacks to verify that the file had the right file extension. I thought I could do better using data annotations, unobtrusive jQuery and implement a custom validator that also work on the client side so this post is about writing custom validation on the server and unobtrusive validation for ASP.NET MVC 3 on the client.</p>\n<h3 id=\"serversidevalidation\">Server side validation</h3>\n<p>The first step to get this working is to get the server side validation to work since you allways want server side validation to make sure the user hasn't bypassed your client side validation. The attribute we need to implement will look something like:</p>\n<pre><code>[AttributeUsage(AttributeTargets.Field | AttributeTargets.Property, AllowMultiple = false, Inherited = true)]\npublic class FileExtensionsAttribute : ValidationAttribute\n{\n    private List&lt;string&gt; ValidExtensions { get; set; }\n\n    public FileExtensionsAttribute(string fileExtensions)\n    {\n        ValidExtensions = fileExtensions.Split('|').ToList();\n    }\n\n    public override bool IsValid(object value)\n    {\n        HttpPostedFileBase file = value as HttpPostedFileBase;\n        if (file != null)\n        {\n            var fileName = file.FileName;\n            var isValidExtension = ValidExtensions.Any(y =&gt; fileName.EndsWith(y));\n            return isValidExtension;\n        }\n        return true;\n    }\n}\n</code></pre>\n<p>You apply the property like such</p>\n<pre><code>[FileExtensions(&quot;txt|doc&quot;)]\npublic HttPostedFileBase File { get; set; }\n</code></pre>\n<p>When this attribute is applied to a property or field of type <code>HttpPostedFileBase</code> and is given a string with extensions separated by &quot;|&quot; it will take those extensions and validate the <code>HttpPostedFileBase</code> property that it is applied to. As you can see I am returning true if the file is <code>null</code>, and that is because if a file should be required should have the <code>Require</code> attribute as well. But all this is still on the server and we want to bring it to the client which is the next step.</p>\n<p>Before we continue with the next step, which is Unobtrusive client side validation, I thought I would show you a simple editor template to make it easier to get the correct markup for the <code>HttpPostedFileBase</code>:</p>\n<pre><code> @model System.Web.HttpPostedFileBase\n @Html.TextBoxFor(model =&gt; model, new {type = &quot;file&quot;})\n</code></pre>\n<p>Save the file as HttpPostedFileBase.cshtml in the folder Views/Shared/EditorTemplates. When you have saved the file you will be able to write <code>@Html.EditorFor(model =&gt; model.FileProperty)</code> and you will get the right html.</p>\n<h3 id=\"unobtrusiveclientsidevalidation\">Unobtrusive client side validation</h3>\n<p><a href=\"http://en.wikipedia.org/wiki/Unobtrusive_JavaScript\">Unobtrusive JavaScript</a> is not formally defined but it basically says that you should separate your logic (JavaScript) from your view (the html code) and then bind JavaScript after the page has loaded. This way you don't get any JavaScript code in your view and if you do it correctly the page will still be supported in browsers that don't support JavaScript.</p>\n<p>One thing about client side validation on the web is that you should NOT rely on it, it should only be there to make a better user experience. Enough said, let get to the point.</p>\n<p>There are four steps that you need to do to implement unobtrusive client side validation:</p>\n<ul>\n<li>Enable unobtrusive validation and add the scripts.</li>\n<li>Implement a <code>ModelClientValidationRule</code> that is part of the bridge to the JavaScript.</li>\n<li>Implement the rule in JavaScript.</li>\n<li>Implement an adapter that is the second part of the bridge to the JavaScript rule.</li>\n</ul>\n<h4 id=\"enableunobtrusivevalidationandaddthescripts\">Enable unobtrusive validation and add the scripts.</h4>\n<p>This step is the easiest one. You can enable unobtrusive validation in two ways, either in code:</p>\n<pre><code>Html.EnableClientValidation(true);\nHtml.EnableUnobtrusiveJavaScript(true);\n</code></pre>\n<p>or in web.config:</p>\n<pre><code>&lt;appSettings&gt;\n    &lt;add key=&quot;ClientValidationEnabled&quot; value=&quot;true&quot;/&gt;\n    &lt;add key=&quot;UnobtrusiveJavaScriptEnabled&quot; value=&quot;true&quot;/&gt;\n&lt;/appSettings&gt;\n</code></pre>\n<p><strong>When you have enabled it you need to include the scripts jquery.validate.js, jquery.validate.unobtrusive.js and of course your own script that will contain the custom validation code.</strong></p>\n<h4 id=\"implementingthemodelclientvalidationrule\">Implementing the ModelClientValidationRule</h4>\n<p>The code for the <code>ModelClientValidationRule</code> is:</p>\n<pre><code>public class ModelClientFileExtensionValidationRule : ModelClientValidationRule\n{\n    public ModelClientFileExtensionValidationRule(string errorMessage, List&lt;string&gt; fileExtensions)\n    {\n        ErrorMessage = errorMessage;\n        ValidationType = &quot;fileextensions&quot;;\n        ValidationParameters.Add(&quot;fileextensions&quot;, string.Join(&quot;,&quot;, fileExtensions));\n    }\n}\n</code></pre>\n<p>It is somewhat straightforward; the <code>ErrorMessage</code> is the message that is shown on the client, the <code>ValidationType</code> is what it says it is, that is, the type of the validation. When the <code>ModelClientValidationRule</code> is done we need to add it to the <code>ValidationAttribute</code>:</p>\n<pre><code>public class FileExtensionsAttribute : ValidationAttribute, IClientValidatable\n{\n\t... // Same as before\n\t\n    public IEnumerable&lt;ModelClientValidationRule&gt; GetClientValidationRules(ModelMetadata metadata, ControllerContext context)\n    {\n        var rule = new ModelClientFileExtensionValidationRule(ErrorMessage, ValidExtensions);\n        yield return rule;\n    }\n}\n}\n</code></pre>\n<h4 id=\"implementingtheadapter\">Implementing the adapter</h4>\n<p>The JavaScript adapter is in this case basically the definition of the rule. It doesn't necessary have the actual logic that do the testing, but it sets up everything that needs to be tested.</p>\n<pre><code>$(function () {\n    jQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\n        // Set up test parameters\n\t\tvar params = {\n            fileextensions: options.params.fileextensions.split(',')\n        };\n\n\t\t// Match parameters to the method to execute\n        options.rules['fileextensions'] = params;\n        if (options.message) {\n\t\t    // If there is a message, set it for the rule\n            options.messages['fileextensions'] = options.message;\n        }\n    });\n} (jQuery));\n</code></pre>\n<p>The code above is executed when the document is ready and adds an &quot;adapter&quot; to the unobtrusive framework. The first paramter is the name of the adapter in this case is &quot;fileextensions&quot; and it should match the <code>ValidationType</code> in the <code>ModelClientFileExtensionValidationRule</code>. In the <code>ModelClientFileExtensionValidationRule</code> we defined a <code>ValidationParameter</code> that was named &quot;fileextensions&quot;, that is why we have an array with &quot;fileextensions&quot; in it as second parameter. The second parameter should contain the names of all the parameters that are added to the <code>ValidationParameters</code> property. The third parameter is the function that basically configures the rule, note that it doesn't execute the rule.</p>\n<h4 id=\"implementingtheruleinjavascript\">Implementing the rule in JavaScript</h4>\n<p>Adding a rule to the a test is as simple as adding the adapter. The code below includes both the adapter and the rule.</p>\n<pre><code>$(function () {\n\tjQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\n\t\tvar params = {\n\t\t\tfileextensions: options.params.fileextensions.split(',')\n\t\t};\n\n\t\toptions.rules['fileextensions'] = params;\n\t\tif (options.message) {\n\t\t\toptions.messages['fileextensions'] = options.message;\n\t\t}\n\t});\n\n\tjQuery.validator.addMethod(&quot;fileextensions&quot;, function (value, element, param) {\n\t\tvar extension = getFileExtension(value);\n\t\tvar validExtension = $.inArray(extension, param.fileextensions) !== -1;\n\t\treturn validExtension;\n\t});\n\n\tfunction getFileExtension(fileName) {\n\t\tvar extension = (/[.]/.exec(fileName)) ? /[^.]+$/.exec(fileName) : undefined;\n\t\tif (extension != undefined) {\n\t\t\treturn extension[0];\n\t\t}\n\t\treturn extension;\n\t};\n} (jQuery));\n</code></pre>\n<p>It is the row <code>jQuery.validator.addMethod(...)</code> that is relevant here. We add a rule that we define with the name &quot;fileextensions&quot;. The parameters to the rule are the value that is to be validated, the element that is being validated and the parameters that we set up in the adapter. In our case the validation is really straightforward, first we get the file extension using the helper method <code>getFileExtension</code>, then we check if the extension is present in the list for file extensions that on the <code>fileextensions</code> property on the <code>param</code> parameter and return the result.</p>\n<p>I think this is all there is to it. If you have any question or comments just use the form below.</p>\n<!--kg-card-end: markdown-->","comment_id":"3","plaintext":"Just recently I was given the task to move a part of an ASP.NET WebForm to\nASP.NET MVC since we are moving over to ASP.NET MVC. The part that I would move\nover had a form that contained a file upload controller and there were some\ncustom implemented JavaScript hacks to verify that the file had the right file\nextension. I thought I could do better using data annotations, unobtrusive\njQuery and implement a custom validator that also work on the client side so\nthis post is about writing custom validation on the server and unobtrusive\nvalidation for ASP.NET MVC 3 on the client.\n\nServer side validation\nThe first step to get this working is to get the server side validation to work\nsince you allways want server side validation to make sure the user hasn't\nbypassed your client side validation. The attribute we need to implement will\nlook something like:\n\n[AttributeUsage(AttributeTargets.Field | AttributeTargets.Property, AllowMultiple = false, Inherited = true)]\npublic class FileExtensionsAttribute : ValidationAttribute\n{\n    private List<string> ValidExtensions { get; set; }\n\n    public FileExtensionsAttribute(string fileExtensions)\n    {\n        ValidExtensions = fileExtensions.Split('|').ToList();\n    }\n\n    public override bool IsValid(object value)\n    {\n        HttpPostedFileBase file = value as HttpPostedFileBase;\n        if (file != null)\n        {\n            var fileName = file.FileName;\n            var isValidExtension = ValidExtensions.Any(y => fileName.EndsWith(y));\n            return isValidExtension;\n        }\n        return true;\n    }\n}\n\n\nYou apply the property like such\n\n[FileExtensions(\"txt|doc\")]\npublic HttPostedFileBase File { get; set; }\n\n\nWhen this attribute is applied to a property or field of type HttpPostedFileBase \nand is given a string with extensions separated by \"|\" it will take those\nextensions and validate the HttpPostedFileBase property that it is applied to.\nAs you can see I am returning true if the file is null, and that is because if a\nfile should be required should have the Require attribute as well. But all this\nis still on the server and we want to bring it to the client which is the next\nstep.\n\nBefore we continue with the next step, which is Unobtrusive client side\nvalidation, I thought I would show you a simple editor template to make it\neasier to get the correct markup for the HttpPostedFileBase:\n\n @model System.Web.HttpPostedFileBase\n @Html.TextBoxFor(model => model, new {type = \"file\"})\n\n\nSave the file as HttpPostedFileBase.cshtml in the folder\nViews/Shared/EditorTemplates. When you have saved the file you will be able to\nwrite @Html.EditorFor(model => model.FileProperty) and you will get the right\nhtml.\n\nUnobtrusive client side validation\nUnobtrusive JavaScript [http://en.wikipedia.org/wiki/Unobtrusive_JavaScript] is\nnot formally defined but it basically says that you should separate your logic\n(JavaScript) from your view (the html code) and then bind JavaScript after the\npage has loaded. This way you don't get any JavaScript code in your view and if\nyou do it correctly the page will still be supported in browsers that don't\nsupport JavaScript.\n\nOne thing about client side validation on the web is that you should NOT rely on\nit, it should only be there to make a better user experience. Enough said, let\nget to the point.\n\nThere are four steps that you need to do to implement unobtrusive client side\nvalidation:\n\n * Enable unobtrusive validation and add the scripts.\n * Implement a ModelClientValidationRule that is part of the bridge to the\n   JavaScript.\n * Implement the rule in JavaScript.\n * Implement an adapter that is the second part of the bridge to the JavaScript\n   rule.\n\nEnable unobtrusive validation and add the scripts.\nThis step is the easiest one. You can enable unobtrusive validation in two ways,\neither in code:\n\nHtml.EnableClientValidation(true);\nHtml.EnableUnobtrusiveJavaScript(true);\n\n\nor in web.config:\n\n<appSettings>\n    <add key=\"ClientValidationEnabled\" value=\"true\"/>\n    <add key=\"UnobtrusiveJavaScriptEnabled\" value=\"true\"/>\n</appSettings>\n\n\nWhen you have enabled it you need to include the scripts jquery.validate.js,\njquery.validate.unobtrusive.js and of course your own script that will contain\nthe custom validation code.\n\nImplementing the ModelClientValidationRule\nThe code for the ModelClientValidationRule is:\n\npublic class ModelClientFileExtensionValidationRule : ModelClientValidationRule\n{\n    public ModelClientFileExtensionValidationRule(string errorMessage, List<string> fileExtensions)\n    {\n        ErrorMessage = errorMessage;\n        ValidationType = \"fileextensions\";\n        ValidationParameters.Add(\"fileextensions\", string.Join(\",\", fileExtensions));\n    }\n}\n\n\nIt is somewhat straightforward; the ErrorMessage is the message that is shown on\nthe client, the ValidationType is what it says it is, that is, the type of the\nvalidation. When the ModelClientValidationRule is done we need to add it to the \nValidationAttribute:\n\npublic class FileExtensionsAttribute : ValidationAttribute, IClientValidatable\n{\n\t... // Same as before\n\t\n    public IEnumerable<ModelClientValidationRule> GetClientValidationRules(ModelMetadata metadata, ControllerContext context)\n    {\n        var rule = new ModelClientFileExtensionValidationRule(ErrorMessage, ValidExtensions);\n        yield return rule;\n    }\n}\n}\n\n\nImplementing the adapter\nThe JavaScript adapter is in this case basically the definition of the rule. It\ndoesn't necessary have the actual logic that do the testing, but it sets up\neverything that needs to be tested.\n\n$(function () {\n    jQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\n        // Set up test parameters\n\t\tvar params = {\n            fileextensions: options.params.fileextensions.split(',')\n        };\n\n\t\t// Match parameters to the method to execute\n        options.rules['fileextensions'] = params;\n        if (options.message) {\n\t\t    // If there is a message, set it for the rule\n            options.messages['fileextensions'] = options.message;\n        }\n    });\n} (jQuery));\n\n\nThe code above is executed when the document is ready and adds an \"adapter\" to\nthe unobtrusive framework. The first paramter is the name of the adapter in this\ncase is \"fileextensions\" and it should match the ValidationType in the \nModelClientFileExtensionValidationRule. In the \nModelClientFileExtensionValidationRule we defined a ValidationParameter that was\nnamed \"fileextensions\", that is why we have an array with \"fileextensions\" in it\nas second parameter. The second parameter should contain the names of all the\nparameters that are added to the ValidationParameters property. The third\nparameter is the function that basically configures the rule, note that it\ndoesn't execute the rule.\n\nImplementing the rule in JavaScript\nAdding a rule to the a test is as simple as adding the adapter. The code below\nincludes both the adapter and the rule.\n\n$(function () {\n\tjQuery.validator.unobtrusive.adapters.add('fileextensions', ['fileextensions'], function (options) {\n\t\tvar params = {\n\t\t\tfileextensions: options.params.fileextensions.split(',')\n\t\t};\n\n\t\toptions.rules['fileextensions'] = params;\n\t\tif (options.message) {\n\t\t\toptions.messages['fileextensions'] = options.message;\n\t\t}\n\t});\n\n\tjQuery.validator.addMethod(\"fileextensions\", function (value, element, param) {\n\t\tvar extension = getFileExtension(value);\n\t\tvar validExtension = $.inArray(extension, param.fileextensions) !== -1;\n\t\treturn validExtension;\n\t});\n\n\tfunction getFileExtension(fileName) {\n\t\tvar extension = (/[.]/.exec(fileName)) ? /[^.]+$/.exec(fileName) : undefined;\n\t\tif (extension != undefined) {\n\t\t\treturn extension[0];\n\t\t}\n\t\treturn extension;\n\t};\n} (jQuery));\n\n\nIt is the row jQuery.validator.addMethod(...) that is relevant here. We add a\nrule that we define with the name \"fileextensions\". The parameters to the rule\nare the value that is to be validated, the element that is being validated and\nthe parameters that we set up in the adapter. In our case the validation is\nreally straightforward, first we get the file extension using the helper method \ngetFileExtension, then we check if the extension is present in the list for file\nextensions that on the fileextensions property on the param parameter and return\nthe result.\n\nI think this is all there is to it. If you have any question or comments just\nuse the form below.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:04:32.000Z","updated_at":"2014-03-01T09:06:40.000Z","published_at":"2012-10-25T07:04:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe037","uuid":"5b9df385-dfc9-41a9-bc2b-ee3715ebd76c","title":"ASP.NET MVC helper for active tab in tab menu","slug":"asp-net-mvc-helper-for-active-tab-in-tab-menu","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"This will be my first post on my own blog in this giant thing called \\\"the web\\\". I will mostly publish things like this, but I will take me my freedom to write anything I feel like :).\\n\\nNow to the problem, when using the template that comes with asp.net mvc the tabs will not get any specific styling when you are on the active tab. If you are not looking for a very advanced solution it is not that hard to write a simple helper class to handle the active tab selection, so that is what I did. The base  functionality is in the following helper method:\\n\\n\\tpublic static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions, string cssClass)\\n\\t{\\n\\t    string currentAction = helper.ViewContext.Controller.\\n\\t            ValueProvider.GetValue(\\\"action\\\").RawValue.ToString();\\n\\t    string currentController = helper.ViewContext.Controller.\\n\\t            ValueProvider.GetValue(\\\"controller\\\").RawValue.ToString();\\n\\n\\t    string cssClassToUse = currentController == activeController \\n\\t         && activeActions.Contains(currentAction)\\n\\t                               ? cssClass\\n\\t                               : string.Empty;\\n\\t    return cssClassToUse;\\n\\t}\\nThe ideas behind the code is that you should be able to write something like:\\n\\n    Html.ActiveTab(\\\"controllerName\\\", new string[]{\\\"actionName1\\\", \\\"actionName2\\\"}, \\\"active\\\")\\n\\nto return \\\"active\\\" if the current action is actionName1 or actionName2 in the controller controllerName. Example markup at the bottom, but first some more static methods to make it easier to use the extension method:\\n\\n\\tprivate const string DefaultCssClass = \\\"active\\\";\\n\\n\\tpublic static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions)\\n\\t{\\n\\t    return helper.ActiveTab(activeController, activeActions, DefaultCssClass);\\n\\t}\\n\\n\\tpublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction)\\n\\t{\\n\\t    return helper.ActiveTab(activeController, new string[] { activeAction }, DefaultCssClass);\\n\\t}\\n\\n\\tpublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction, string cssClass)\\n\\t{\\n\\t    return helper.ActiveTab(activeController, new string[] {activeAction}, cssClass);\\n\\t}\\nThe markup I use for my menu is the following:\\n\\n\\t<div id=\\\"menu\\\">\\n\\t   <ul>\\n\\t      <li class=\\\"first <%=Html.ActiveTab(\\\"Company\\\", \\\"Index\\\") %>\\\">\\n\\t         <%=Html.ActionLink(\\\"Home\\\", \\\"Index\\\", \\\"Company\\\") %>\\n\\t      </li>\\n\\t      <li class=\\\"last <%=Html.ActiveTab(\\\"Example\\\", \\\"Index\\\") %>\\\">\\n\\t         <%=Html.ActionLink(\\\"Example\\\", \\\"Index\\\", \\\"Example\\\") %>\\n\\t      </li>\\n\\t   </ul>\\n\\t</div>\\n\\nThat's basically all there is to it. If you have any suggestions for improvement feel free to comment.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>This will be my first post on my own blog in this giant thing called &quot;the web&quot;. I will mostly publish things like this, but I will take me my freedom to write anything I feel like :).</p>\n<p>Now to the problem, when using the template that comes with asp.net mvc the tabs will not get any specific styling when you are on the active tab. If you are not looking for a very advanced solution it is not that hard to write a simple helper class to handle the active tab selection, so that is what I did. The base  functionality is in the following helper method:</p>\n<pre><code>public static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions, string cssClass)\n{\n    string currentAction = helper.ViewContext.Controller.\n            ValueProvider.GetValue(&quot;action&quot;).RawValue.ToString();\n    string currentController = helper.ViewContext.Controller.\n            ValueProvider.GetValue(&quot;controller&quot;).RawValue.ToString();\n\n    string cssClassToUse = currentController == activeController \n         &amp;&amp; activeActions.Contains(currentAction)\n                               ? cssClass\n                               : string.Empty;\n    return cssClassToUse;\n}\n</code></pre>\n<p>The ideas behind the code is that you should be able to write something like:</p>\n<pre><code>Html.ActiveTab(&quot;controllerName&quot;, new string[]{&quot;actionName1&quot;, &quot;actionName2&quot;}, &quot;active&quot;)\n</code></pre>\n<p>to return &quot;active&quot; if the current action is actionName1 or actionName2 in the controller controllerName. Example markup at the bottom, but first some more static methods to make it easier to use the extension method:</p>\n<pre><code>private const string DefaultCssClass = &quot;active&quot;;\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions)\n{\n    return helper.ActiveTab(activeController, activeActions, DefaultCssClass);\n}\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction)\n{\n    return helper.ActiveTab(activeController, new string[] { activeAction }, DefaultCssClass);\n}\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction, string cssClass)\n{\n    return helper.ActiveTab(activeController, new string[] {activeAction}, cssClass);\n}\n</code></pre>\n<p>The markup I use for my menu is the following:</p>\n<pre><code>&lt;div id=&quot;menu&quot;&gt;\n   &lt;ul&gt;\n      &lt;li class=&quot;first &lt;%=Html.ActiveTab(&quot;Company&quot;, &quot;Index&quot;) %&gt;&quot;&gt;\n         &lt;%=Html.ActionLink(&quot;Home&quot;, &quot;Index&quot;, &quot;Company&quot;) %&gt;\n      &lt;/li&gt;\n      &lt;li class=&quot;last &lt;%=Html.ActiveTab(&quot;Example&quot;, &quot;Index&quot;) %&gt;&quot;&gt;\n         &lt;%=Html.ActionLink(&quot;Example&quot;, &quot;Index&quot;, &quot;Example&quot;) %&gt;\n      &lt;/li&gt;\n   &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre>\n<p>That's basically all there is to it. If you have any suggestions for improvement feel free to comment.</p>\n<!--kg-card-end: markdown-->","comment_id":"4","plaintext":"This will be my first post on my own blog in this giant thing called \"the web\".\nI will mostly publish things like this, but I will take me my freedom to write\nanything I feel like :).\n\nNow to the problem, when using the template that comes with asp.net mvc the tabs\nwill not get any specific styling when you are on the active tab. If you are not\nlooking for a very advanced solution it is not that hard to write a simple\nhelper class to handle the active tab selection, so that is what I did. The base\nfunctionality is in the following helper method:\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions, string cssClass)\n{\n    string currentAction = helper.ViewContext.Controller.\n            ValueProvider.GetValue(\"action\").RawValue.ToString();\n    string currentController = helper.ViewContext.Controller.\n            ValueProvider.GetValue(\"controller\").RawValue.ToString();\n\n    string cssClassToUse = currentController == activeController \n         && activeActions.Contains(currentAction)\n                               ? cssClass\n                               : string.Empty;\n    return cssClassToUse;\n}\n\n\nThe ideas behind the code is that you should be able to write something like:\n\nHtml.ActiveTab(\"controllerName\", new string[]{\"actionName1\", \"actionName2\"}, \"active\")\n\n\nto return \"active\" if the current action is actionName1 or actionName2 in the\ncontroller controllerName. Example markup at the bottom, but first some more\nstatic methods to make it easier to use the extension method:\n\nprivate const string DefaultCssClass = \"active\";\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string[] activeActions)\n{\n    return helper.ActiveTab(activeController, activeActions, DefaultCssClass);\n}\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction)\n{\n    return helper.ActiveTab(activeController, new string[] { activeAction }, DefaultCssClass);\n}\n\npublic static string ActiveTab(this HtmlHelper helper, string activeController, string activeAction, string cssClass)\n{\n    return helper.ActiveTab(activeController, new string[] {activeAction}, cssClass);\n}\n\n\nThe markup I use for my menu is the following:\n\n<div id=\"menu\">\n   <ul>\n      <li class=\"first <%=Html.ActiveTab(\"Company\", \"Index\") %>\">\n         <%=Html.ActionLink(\"Home\", \"Index\", \"Company\") %>\n      </li>\n      <li class=\"last <%=Html.ActiveTab(\"Example\", \"Index\") %>\">\n         <%=Html.ActionLink(\"Example\", \"Index\", \"Example\") %>\n      </li>\n   </ul>\n</div>\n\n\nThat's basically all there is to it. If you have any suggestions for improvement\nfeel free to comment.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:08:20.000Z","updated_at":"2014-03-01T09:08:53.000Z","published_at":"2012-10-25T07:08:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe038","uuid":"420a85e5-d481-4b52-82aa-b45ae77a7042","title":"Creating a NuGet package","slug":"creating-a-nuget-package","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you read my last post you probably know that NuGet is here to stay. To start using NuGet packages, either throught the Package Manager Console or the \\\"Add Library Package Reference\\\" dialogue box, shouldn't be that hard to figure out. But what about creating packages? It turns out it is almost as simple as using them. I will try to cover the most basic scenarios in this post about how to create your own package and get it up on [nuget.org](http://www.nuget.org/). In the rest of the post I assume you have signed up on [nuget.org](http://www.nuget.org) and you have found your access key under your account.\\n\\n### Ok, so let's get busy\\nIn the previous version of NuGet, it is currently on version 1.2, it wasn't possible to create a package directly from the csproj-file, but now it is. So in the most basic scenarios all you need to do create a package is run to runt the `nuget pack` command on a csproj-file like:\\n\\n    c:\\\\path\\\\to\\\\your\\\\project>nuget pack DoSomeAwesomeStuff.csproj\\n\\nRunning that command will make the project build and create a .nupkg file that is your first NuGet package!. That wasn't hard, was it? But it is more to it, when running that command NuGet will look in you project and see if you have any NuGet references in your project and include those as NuGet references, which is great! But, there is one feature that is lacking if you ask me... and that is that the content folder, and most likely the tool folder as well, is not added by convention when running this command. Luckily for us it is not that hard to create a work around for this issue. Instead of running the above command you can use:\\n\\n    c:\\\\path\\\\to\\\\your\\\\project>nuget spec DoSomeAwesomeStuff.csproj\\n\\nwhich will create a NuGet spec-file for that project. The file will look something like: \\n\\n\\t<?xml version=\\\"1.0\\\"?>\\n\\t<package xmlns=\\\"http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd\\\">\\n\\t  <metadata>\\n\\t\\t<id>$id$</id>\\n\\t\\t<version>$version$</version>\\n\\t\\t<authors>$author$</authors>\\n\\t\\t<owners>$author$</owners>\\n\\t\\t<licenseUrl>http://LICENSE_URL_HERE_OR_DELETE_THIS_LINE</licenseUrl>\\n\\t\\t<projectUrl>http://PROJECT_URL_HERE_OR_DELETE_THIS_LINE</projectUrl>\\n\\t\\t<iconUrl>http://ICON_URL_HERE_OR_DELETE_THIS_LINE</iconUrl>\\n\\t\\t<requireLicenseAcceptance>false</requireLicenseAcceptance>\\n\\t\\t<description>$description$</description>\\n\\t\\t<tags>Tag1 Tag2</tags>\\n\\t  </metadata>\\n\\t</package>\\n\\nAll the metadata tags, the ones like `$tagName$`, will be replaced by the value in the `assembly.cs` file if you don't change it to some constant value. When you have this file you can start modifying it to include some content files for example. Content files are files that are placed in the content folder in the created NuGet package, all files in the content folder will be copied to project where the package is installed. So one of my final spec-files look like this: \\n\\n\\t<?xml version=\\\"1.0\\\"?>\\n\\t<package xmlns=\\\"http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd\\\">\\n\\t  <metadata>\\n\\t\\t<id>$id$</id>\\n\\t\\t<version>$version$</version>\\n\\t\\t<authors>Tomas Jansson</authors>\\n\\t\\t<owners>Tomas Jansson</owners>\\n\\t\\t<licenseUrl>https://github.com/mastoj/SimpleCompression/blob/master/SimpleCompression/license.txt</licenseUrl>\\n\\t\\t<projectUrl>https://github.com/mastoj/SimpleCompression</projectUrl>\\n\\t\\t<iconUrl>https://github.com/mastoj/SimpleCompression/raw/master/SimpleCompression/SimpleCompression_icon.png</iconUrl>\\n\\t\\t<requireLicenseAcceptance>false</requireLicenseAcceptance>\\n\\t\\t<description>$description$</description>\\n\\t\\t<tags>ASP.NET MVC3 Compression javascript css</tags>\\n\\t  </metadata>\\n\\t  <files>\\n\\t\\t<file src=\\\"content\\\\App_Start\\\\*.*\\\" target=\\\"content\\\\App_Start\\\" />\\n\\t  </files>\\n\\t</package>\\n\\nAs you can see I've swapped some of the metadata parameters against static values that is not present in the `assembly.cs` file, so it is no problem at all to mix static values with generic ones. But the most important part is that I've added an element to the root node, `files`, which specify where I have the content I want to include and where I want it to go in the package. That's all you need to do if you want to include one or more files, in the above example I'm including a startup file for example.\\n\\nWhen I now run the `nuget pack` command on the csproj-file, *not* the spec-file, NuGet will look at the spec file and include the content files I have specified and the NuGet references will still be added since they are specified in the .csproj-file. It will also use the metadata from the `assembly.cs` to update the metadata for the package.\\n\\nThe last step in the process is to submit the package to [nuget.org](http://www.nuget.org/), and that is as simple as creating the package. I recommend to do it in two steps the first time, set api access key and the submit. When you have set the api access key you don't have to write it every time you submit or update a package. So here are the commands:\\n\\n    c:\\\\path\\\\to\\\\your\\\\project>nuget setApiKey xxxxxx-xxxx-xxxx-xxxx-xxxxxx\\n    c:\\\\path\\\\to\\\\your\\\\project>nuget push YourProject.x.x.nupkg\\n\\n\\nI think that is all to it. Of course there are some other ways as well, like creating spec-file from the dll/exe-file for example, but this is the way I think is most efficient way to do it and will probably be the way I'll create NuGet packages in the future. Of course it would be nice if you could specify everything in the project so you didn't have to update the spec-file, but that seems to be something I'll have to live with.\\n\\nIn a coming post I'll show an example of how you can use a NuGet package to extend the functionality of the Package Manager Console, that is creating your own PowerShell commands that gets included in the Console.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>If you read my last post you probably know that NuGet is here to stay. To start using NuGet packages, either throught the Package Manager Console or the &quot;Add Library Package Reference&quot; dialogue box, shouldn't be that hard to figure out. But what about creating packages? It turns out it is almost as simple as using them. I will try to cover the most basic scenarios in this post about how to create your own package and get it up on <a href=\"http://www.nuget.org/\">nuget.org</a>. In the rest of the post I assume you have signed up on <a href=\"http://www.nuget.org\">nuget.org</a> and you have found your access key under your account.</p>\n<h3 id=\"oksoletsgetbusy\">Ok, so let's get busy</h3>\n<p>In the previous version of NuGet, it is currently on version 1.2, it wasn't possible to create a package directly from the csproj-file, but now it is. So in the most basic scenarios all you need to do create a package is run to runt the <code>nuget pack</code> command on a csproj-file like:</p>\n<pre><code>c:\\path\\to\\your\\project&gt;nuget pack DoSomeAwesomeStuff.csproj\n</code></pre>\n<p>Running that command will make the project build and create a .nupkg file that is your first NuGet package!. That wasn't hard, was it? But it is more to it, when running that command NuGet will look in you project and see if you have any NuGet references in your project and include those as NuGet references, which is great! But, there is one feature that is lacking if you ask me... and that is that the content folder, and most likely the tool folder as well, is not added by convention when running this command. Luckily for us it is not that hard to create a work around for this issue. Instead of running the above command you can use:</p>\n<pre><code>c:\\path\\to\\your\\project&gt;nuget spec DoSomeAwesomeStuff.csproj\n</code></pre>\n<p>which will create a NuGet spec-file for that project. The file will look something like:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;package xmlns=&quot;http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd&quot;&gt;\n  &lt;metadata&gt;\n\t&lt;id&gt;$id$&lt;/id&gt;\n\t&lt;version&gt;$version$&lt;/version&gt;\n\t&lt;authors&gt;$author$&lt;/authors&gt;\n\t&lt;owners&gt;$author$&lt;/owners&gt;\n\t&lt;licenseUrl&gt;http://LICENSE_URL_HERE_OR_DELETE_THIS_LINE&lt;/licenseUrl&gt;\n\t&lt;projectUrl&gt;http://PROJECT_URL_HERE_OR_DELETE_THIS_LINE&lt;/projectUrl&gt;\n\t&lt;iconUrl&gt;http://ICON_URL_HERE_OR_DELETE_THIS_LINE&lt;/iconUrl&gt;\n\t&lt;requireLicenseAcceptance&gt;false&lt;/requireLicenseAcceptance&gt;\n\t&lt;description&gt;$description$&lt;/description&gt;\n\t&lt;tags&gt;Tag1 Tag2&lt;/tags&gt;\n  &lt;/metadata&gt;\n&lt;/package&gt;\n</code></pre>\n<p>All the metadata tags, the ones like <code>$tagName$</code>, will be replaced by the value in the <code>assembly.cs</code> file if you don't change it to some constant value. When you have this file you can start modifying it to include some content files for example. Content files are files that are placed in the content folder in the created NuGet package, all files in the content folder will be copied to project where the package is installed. So one of my final spec-files look like this:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;package xmlns=&quot;http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd&quot;&gt;\n  &lt;metadata&gt;\n\t&lt;id&gt;$id$&lt;/id&gt;\n\t&lt;version&gt;$version$&lt;/version&gt;\n\t&lt;authors&gt;Tomas Jansson&lt;/authors&gt;\n\t&lt;owners&gt;Tomas Jansson&lt;/owners&gt;\n\t&lt;licenseUrl&gt;https://github.com/mastoj/SimpleCompression/blob/master/SimpleCompression/license.txt&lt;/licenseUrl&gt;\n\t&lt;projectUrl&gt;https://github.com/mastoj/SimpleCompression&lt;/projectUrl&gt;\n\t&lt;iconUrl&gt;https://github.com/mastoj/SimpleCompression/raw/master/SimpleCompression/SimpleCompression_icon.png&lt;/iconUrl&gt;\n\t&lt;requireLicenseAcceptance&gt;false&lt;/requireLicenseAcceptance&gt;\n\t&lt;description&gt;$description$&lt;/description&gt;\n\t&lt;tags&gt;ASP.NET MVC3 Compression javascript css&lt;/tags&gt;\n  &lt;/metadata&gt;\n  &lt;files&gt;\n\t&lt;file src=&quot;content\\App_Start\\*.*&quot; target=&quot;content\\App_Start&quot; /&gt;\n  &lt;/files&gt;\n&lt;/package&gt;\n</code></pre>\n<p>As you can see I've swapped some of the metadata parameters against static values that is not present in the <code>assembly.cs</code> file, so it is no problem at all to mix static values with generic ones. But the most important part is that I've added an element to the root node, <code>files</code>, which specify where I have the content I want to include and where I want it to go in the package. That's all you need to do if you want to include one or more files, in the above example I'm including a startup file for example.</p>\n<p>When I now run the <code>nuget pack</code> command on the csproj-file, <em>not</em> the spec-file, NuGet will look at the spec file and include the content files I have specified and the NuGet references will still be added since they are specified in the .csproj-file. It will also use the metadata from the <code>assembly.cs</code> to update the metadata for the package.</p>\n<p>The last step in the process is to submit the package to <a href=\"http://www.nuget.org/\">nuget.org</a>, and that is as simple as creating the package. I recommend to do it in two steps the first time, set api access key and the submit. When you have set the api access key you don't have to write it every time you submit or update a package. So here are the commands:</p>\n<pre><code>c:\\path\\to\\your\\project&gt;nuget setApiKey xxxxxx-xxxx-xxxx-xxxx-xxxxxx\nc:\\path\\to\\your\\project&gt;nuget push YourProject.x.x.nupkg\n</code></pre>\n<p>I think that is all to it. Of course there are some other ways as well, like creating spec-file from the dll/exe-file for example, but this is the way I think is most efficient way to do it and will probably be the way I'll create NuGet packages in the future. Of course it would be nice if you could specify everything in the project so you didn't have to update the spec-file, but that seems to be something I'll have to live with.</p>\n<p>In a coming post I'll show an example of how you can use a NuGet package to extend the functionality of the Package Manager Console, that is creating your own PowerShell commands that gets included in the Console.</p>\n<!--kg-card-end: markdown-->","comment_id":"5","plaintext":"If you read my last post you probably know that NuGet is here to stay. To start\nusing NuGet packages, either throught the Package Manager Console or the \"Add\nLibrary Package Reference\" dialogue box, shouldn't be that hard to figure out.\nBut what about creating packages? It turns out it is almost as simple as using\nthem. I will try to cover the most basic scenarios in this post about how to\ncreate your own package and get it up on nuget.org [http://www.nuget.org/]. In\nthe rest of the post I assume you have signed up on nuget.org\n[http://www.nuget.org] and you have found your access key under your account.\n\nOk, so let's get busy\nIn the previous version of NuGet, it is currently on version 1.2, it wasn't\npossible to create a package directly from the csproj-file, but now it is. So in\nthe most basic scenarios all you need to do create a package is run to runt the \nnuget pack command on a csproj-file like:\n\nc:\\path\\to\\your\\project>nuget pack DoSomeAwesomeStuff.csproj\n\n\nRunning that command will make the project build and create a .nupkg file that\nis your first NuGet package!. That wasn't hard, was it? But it is more to it,\nwhen running that command NuGet will look in you project and see if you have any\nNuGet references in your project and include those as NuGet references, which is\ngreat! But, there is one feature that is lacking if you ask me... and that is\nthat the content folder, and most likely the tool folder as well, is not added\nby convention when running this command. Luckily for us it is not that hard to\ncreate a work around for this issue. Instead of running the above command you\ncan use:\n\nc:\\path\\to\\your\\project>nuget spec DoSomeAwesomeStuff.csproj\n\n\nwhich will create a NuGet spec-file for that project. The file will look\nsomething like:\n\n<?xml version=\"1.0\"?>\n<package xmlns=\"http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd\">\n  <metadata>\n\t<id>$id$</id>\n\t<version>$version$</version>\n\t<authors>$author$</authors>\n\t<owners>$author$</owners>\n\t<licenseUrl>http://LICENSE_URL_HERE_OR_DELETE_THIS_LINE</licenseUrl>\n\t<projectUrl>http://PROJECT_URL_HERE_OR_DELETE_THIS_LINE</projectUrl>\n\t<iconUrl>http://ICON_URL_HERE_OR_DELETE_THIS_LINE</iconUrl>\n\t<requireLicenseAcceptance>false</requireLicenseAcceptance>\n\t<description>$description$</description>\n\t<tags>Tag1 Tag2</tags>\n  </metadata>\n</package>\n\n\nAll the metadata tags, the ones like $tagName$, will be replaced by the value in\nthe assembly.cs file if you don't change it to some constant value. When you\nhave this file you can start modifying it to include some content files for\nexample. Content files are files that are placed in the content folder in the\ncreated NuGet package, all files in the content folder will be copied to project\nwhere the package is installed. So one of my final spec-files look like this:\n\n<?xml version=\"1.0\"?>\n<package xmlns=\"http://schemas.microsoft.com/packaging/2010/07/nuspec.xsd\">\n  <metadata>\n\t<id>$id$</id>\n\t<version>$version$</version>\n\t<authors>Tomas Jansson</authors>\n\t<owners>Tomas Jansson</owners>\n\t<licenseUrl>https://github.com/mastoj/SimpleCompression/blob/master/SimpleCompression/license.txt</licenseUrl>\n\t<projectUrl>https://github.com/mastoj/SimpleCompression</projectUrl>\n\t<iconUrl>https://github.com/mastoj/SimpleCompression/raw/master/SimpleCompression/SimpleCompression_icon.png</iconUrl>\n\t<requireLicenseAcceptance>false</requireLicenseAcceptance>\n\t<description>$description$</description>\n\t<tags>ASP.NET MVC3 Compression javascript css</tags>\n  </metadata>\n  <files>\n\t<file src=\"content\\App_Start\\*.*\" target=\"content\\App_Start\" />\n  </files>\n</package>\n\n\nAs you can see I've swapped some of the metadata parameters against static\nvalues that is not present in the assembly.cs file, so it is no problem at all\nto mix static values with generic ones. But the most important part is that I've\nadded an element to the root node, files, which specify where I have the content\nI want to include and where I want it to go in the package. That's all you need\nto do if you want to include one or more files, in the above example I'm\nincluding a startup file for example.\n\nWhen I now run the nuget pack command on the csproj-file, not the spec-file,\nNuGet will look at the spec file and include the content files I have specified\nand the NuGet references will still be added since they are specified in the\n.csproj-file. It will also use the metadata from the assembly.cs to update the\nmetadata for the package.\n\nThe last step in the process is to submit the package to nuget.org\n[http://www.nuget.org/], and that is as simple as creating the package. I\nrecommend to do it in two steps the first time, set api access key and the\nsubmit. When you have set the api access key you don't have to write it every\ntime you submit or update a package. So here are the commands:\n\nc:\\path\\to\\your\\project>nuget setApiKey xxxxxx-xxxx-xxxx-xxxx-xxxxxx\nc:\\path\\to\\your\\project>nuget push YourProject.x.x.nupkg\n\n\nI think that is all to it. Of course there are some other ways as well, like\ncreating spec-file from the dll/exe-file for example, but this is the way I\nthink is most efficient way to do it and will probably be the way I'll create\nNuGet packages in the future. Of course it would be nice if you could specify\neverything in the project so you didn't have to update the spec-file, but that\nseems to be something I'll have to live with.\n\nIn a coming post I'll show an example of how you can use a NuGet package to\nextend the functionality of the Package Manager Console, that is creating your\nown PowerShell commands that gets included in the Console.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:10:25.000Z","updated_at":"2014-03-01T09:10:47.000Z","published_at":"2012-10-25T07:10:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe039","uuid":"bab79e23-6caa-4d74-9ff1-0f78a239709b","title":"Disposible WCF client wrapper","slug":"disposible-wcf-client-wrapper-2","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Ok, so you have implemented your WCF service and is about to implement your service client. As usual you want your client to be disposed as it should so you start writing something like: \\n\\n\\tusing(var client = new SomeServiceType())\\n\\t{\\n\\t\\t// Do stuff with your client\\n\\t}  // Can throw exception here\\n\\nUsually this works perfectly fine, but in the case with WCF this won't work as expected. The reason for this is that Microsoft is sort of \\\"violating\\\" their own definition that a call to `Dispose` should never throw an exception, but in case with WCF that is not true. The call to `Dispose` for the client (which appears when leaving the using block above) might throw an exception when `Close` is called in the `Dispose` function due to some communication issues. Why is this so bad you might ask? The problem is if you get an exception inside of your using block, if you get an exception there it would be \\\"swallowed\\\" by the exception caused in the `Dispose` method. If you want to read about it more you can find some information at the following locations: \\n\\n* [Avoiding Problems with the Using Statement][1]\\n* [Blog discussing the issue][2]\\n\\nOne solution I found on the web is presented [here][3], but I don't like that solution since it is somewhat introducing a new word `Use` instead of `Using` that we are used to. So instead of that solution I wrote a generic wrapper that takes the type it is wrapping, creates a static factory that generates clients of that type and exposes the communication channel as a public property. When creating a new instance of the wrapper the factory is not re-created, instead it uses the old one and opens the channel object. The client wrapper implements the `IDisposable` interface and in the `Dispose` method the wrapper closes the channel properly and \\\"swallows\\\" the exception that might occur when `Abort` on the channel is called (which should never occur). The wrapper code is as follows:\\n\\n    /// <summary>\\n    /// Wraps a service client so you can use \\\"using\\\" without worrying of getting\\n    /// your business exception swalloed. Usage:\\n    /// <example>\\n    /// <code>\\n    /// using(var serviceClient = new ServiceClientWrapper&lt;IServiceContract&gt;)\\n    /// {\\n    /// serviceClient.Channel.YourServiceCall(request);\\n    /// }\\n    /// </code>\\n    /// </example>\\n    /// </summary>\\n    /// <typeparam name=\\\"ServiceType\\\">The type of the ervice type.</typeparam>\\n    public class ServiceClientWrapper<ServiceType> : IDisposable\\n    {\\n        private ServiceType _channel;\\n        /// <summary>\\n        /// Gets the channel.\\n        /// </summary>\\n        /// <value>The channel.</value>\\n        public ServiceType Channel\\n        {\\n            get { return _channel; }\\n        }\\n\\n        private static ChannelFactory<ServiceType> _channelFactory;\\n\\n        /// <summary>\\n        /// Initializes a new instance of the <see cref=\\\"ServiceClientWrapper&lt;ServiceType&gt;\\\"/> class.\\n        /// As a default the endpoint that is used is the one named after the contracts full name.\\n        /// </summary>\\n        public ServiceClientWrapper() : this(typeof(ServiceType).FullName)\\n        { }\\n\\n        /// <summary>\\n        /// Initializes a new instance of the <see cref=\\\"ServiceClientWrapper&lt;ServiceType&gt;\\\"/> class.\\n        /// </summary>\\n        /// <param name=\\\"endpoint\\\">The endpoint.</param>\\n        public ServiceClientWrapper(string endpoint)\\n        {\\n            if (_channelFactory == null)\\n                _channelFactory = new ChannelFactory<ServiceType>(endpoint);\\n            _channel = _channelFactory.CreateChannel();\\n            ((IChannel)_channel).Open();\\n        }\\n\\n        /// <summary>\\n        /// Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources.\\n        /// </summary>\\n        public void Dispose()\\n        {\\n            try\\n            {\\n                ((IChannel)_channel).Close();\\n            }\\n            catch (CommunicationException e)\\n            {\\n                ((IChannel)_channel).Abort();\\n            }\\n            catch (TimeoutException e)\\n            {\\n                ((IChannel)_channel).Abort();\\n            }\\n            catch (Exception e)\\n            {\\n                ((IChannel)_channel).Abort();\\n                // TODO: Insert logging\\n            }\\n        }\\n    }\\n\\nNow when you have your wrapper all you need to do is use the following code:\\n\\n\\tpublic class ClientWrapperUsage\\n\\t{\\n\\t    public static void main(string[] args)\\n\\t    {\\n\\t        using(var clientWrapper = new ServiceClientWrapper<ServiceType>())\\n\\t        {\\n\\t            var response = clientWrapper.Channel.ServiceCall();\\n\\t        }\\n\\t    }\\n\\t}\\n\\nI think the solution is quite ok, but I think we should never have been forced to implement our own wrapper to solve the issue. The code is also availble at github in this [gist][4]\\n\\n[1]:http://msdn.microsoft.com/en-us/library/aa355056.aspx \\\"Avoiding Problems with the Using Statement\\\"\\n[2]:http://social.msdn.microsoft.com/Forums/en-US/wcf/thread/b95b91c7-d498-446c-b38f-ef132989c154 \\\"Blog discussing the problem\\\"\\n[3]:http://old.iserviceoriented.com/blog/post/Indisposable+-+WCF+Gotcha+1.aspx \\\"Alternative solution\\\"\\n[4]:https://gist.github.com/730846 \\\"ServiceClientWrapper gist\\\"\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Ok, so you have implemented your WCF service and is about to implement your service client. As usual you want your client to be disposed as it should so you start writing something like:</p>\n<pre><code>using(var client = new SomeServiceType())\n{\n\t// Do stuff with your client\n}  // Can throw exception here\n</code></pre>\n<p>Usually this works perfectly fine, but in the case with WCF this won't work as expected. The reason for this is that Microsoft is sort of &quot;violating&quot; their own definition that a call to <code>Dispose</code> should never throw an exception, but in case with WCF that is not true. The call to <code>Dispose</code> for the client (which appears when leaving the using block above) might throw an exception when <code>Close</code> is called in the <code>Dispose</code> function due to some communication issues. Why is this so bad you might ask? The problem is if you get an exception inside of your using block, if you get an exception there it would be &quot;swallowed&quot; by the exception caused in the <code>Dispose</code> method. If you want to read about it more you can find some information at the following locations:</p>\n<ul>\n<li><a href=\"http://msdn.microsoft.com/en-us/library/aa355056.aspx\" title=\"Avoiding Problems with the Using Statement\">Avoiding Problems with the Using Statement</a></li>\n<li><a href=\"http://social.msdn.microsoft.com/Forums/en-US/wcf/thread/b95b91c7-d498-446c-b38f-ef132989c154\" title=\"Blog discussing the problem\">Blog discussing the issue</a></li>\n</ul>\n<p>One solution I found on the web is presented <a href=\"http://old.iserviceoriented.com/blog/post/Indisposable+-+WCF+Gotcha+1.aspx\" title=\"Alternative solution\">here</a>, but I don't like that solution since it is somewhat introducing a new word <code>Use</code> instead of <code>Using</code> that we are used to. So instead of that solution I wrote a generic wrapper that takes the type it is wrapping, creates a static factory that generates clients of that type and exposes the communication channel as a public property. When creating a new instance of the wrapper the factory is not re-created, instead it uses the old one and opens the channel object. The client wrapper implements the <code>IDisposable</code> interface and in the <code>Dispose</code> method the wrapper closes the channel properly and &quot;swallows&quot; the exception that might occur when <code>Abort</code> on the channel is called (which should never occur). The wrapper code is as follows:</p>\n<pre><code>/// &lt;summary&gt;\n/// Wraps a service client so you can use &quot;using&quot; without worrying of getting\n/// your business exception swalloed. Usage:\n/// &lt;example&gt;\n/// &lt;code&gt;\n/// using(var serviceClient = new ServiceClientWrapper&amp;lt;IServiceContract&amp;gt;)\n/// {\n/// serviceClient.Channel.YourServiceCall(request);\n/// }\n/// &lt;/code&gt;\n/// &lt;/example&gt;\n/// &lt;/summary&gt;\n/// &lt;typeparam name=&quot;ServiceType&quot;&gt;The type of the ervice type.&lt;/typeparam&gt;\npublic class ServiceClientWrapper&lt;ServiceType&gt; : IDisposable\n{\n    private ServiceType _channel;\n    /// &lt;summary&gt;\n    /// Gets the channel.\n    /// &lt;/summary&gt;\n    /// &lt;value&gt;The channel.&lt;/value&gt;\n    public ServiceType Channel\n    {\n        get { return _channel; }\n    }\n\n    private static ChannelFactory&lt;ServiceType&gt; _channelFactory;\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the &lt;see cref=&quot;ServiceClientWrapper&amp;lt;ServiceType&amp;gt;&quot;/&gt; class.\n    /// As a default the endpoint that is used is the one named after the contracts full name.\n    /// &lt;/summary&gt;\n    public ServiceClientWrapper() : this(typeof(ServiceType).FullName)\n    { }\n\n    /// &lt;summary&gt;\n    /// Initializes a new instance of the &lt;see cref=&quot;ServiceClientWrapper&amp;lt;ServiceType&amp;gt;&quot;/&gt; class.\n    /// &lt;/summary&gt;\n    /// &lt;param name=&quot;endpoint&quot;&gt;The endpoint.&lt;/param&gt;\n    public ServiceClientWrapper(string endpoint)\n    {\n        if (_channelFactory == null)\n            _channelFactory = new ChannelFactory&lt;ServiceType&gt;(endpoint);\n        _channel = _channelFactory.CreateChannel();\n        ((IChannel)_channel).Open();\n    }\n\n    /// &lt;summary&gt;\n    /// Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources.\n    /// &lt;/summary&gt;\n    public void Dispose()\n    {\n        try\n        {\n            ((IChannel)_channel).Close();\n        }\n        catch (CommunicationException e)\n        {\n            ((IChannel)_channel).Abort();\n        }\n        catch (TimeoutException e)\n        {\n            ((IChannel)_channel).Abort();\n        }\n        catch (Exception e)\n        {\n            ((IChannel)_channel).Abort();\n            // TODO: Insert logging\n        }\n    }\n}\n</code></pre>\n<p>Now when you have your wrapper all you need to do is use the following code:</p>\n<pre><code>public class ClientWrapperUsage\n{\n    public static void main(string[] args)\n    {\n        using(var clientWrapper = new ServiceClientWrapper&lt;ServiceType&gt;())\n        {\n            var response = clientWrapper.Channel.ServiceCall();\n        }\n    }\n}\n</code></pre>\n<p>I think the solution is quite ok, but I think we should never have been forced to implement our own wrapper to solve the issue. The code is also availble at github in this <a href=\"https://gist.github.com/730846\" title=\"ServiceClientWrapper gist\">gist</a></p>\n<!--kg-card-end: markdown-->","comment_id":"6","plaintext":"Ok, so you have implemented your WCF service and is about to implement your\nservice client. As usual you want your client to be disposed as it should so you\nstart writing something like:\n\nusing(var client = new SomeServiceType())\n{\n\t// Do stuff with your client\n}  // Can throw exception here\n\n\nUsually this works perfectly fine, but in the case with WCF this won't work as\nexpected. The reason for this is that Microsoft is sort of \"violating\" their own\ndefinition that a call to Dispose should never throw an exception, but in case\nwith WCF that is not true. The call to Dispose for the client (which appears\nwhen leaving the using block above) might throw an exception when Close is\ncalled in the Dispose function due to some communication issues. Why is this so\nbad you might ask? The problem is if you get an exception inside of your using\nblock, if you get an exception there it would be \"swallowed\" by the exception\ncaused in the Dispose method. If you want to read about it more you can find\nsome information at the following locations:\n\n * Avoiding Problems with the Using Statement\n   [http://msdn.microsoft.com/en-us/library/aa355056.aspx]\n * Blog discussing the issue\n   [http://social.msdn.microsoft.com/Forums/en-US/wcf/thread/b95b91c7-d498-446c-b38f-ef132989c154]\n\nOne solution I found on the web is presented here\n[http://old.iserviceoriented.com/blog/post/Indisposable+-+WCF+Gotcha+1.aspx],\nbut I don't like that solution since it is somewhat introducing a new word Use \ninstead of Using that we are used to. So instead of that solution I wrote a\ngeneric wrapper that takes the type it is wrapping, creates a static factory\nthat generates clients of that type and exposes the communication channel as a\npublic property. When creating a new instance of the wrapper the factory is not\nre-created, instead it uses the old one and opens the channel object. The client\nwrapper implements the IDisposable interface and in the Dispose method the\nwrapper closes the channel properly and \"swallows\" the exception that might\noccur when Abort on the channel is called (which should never occur). The\nwrapper code is as follows:\n\n/// <summary>\n/// Wraps a service client so you can use \"using\" without worrying of getting\n/// your business exception swalloed. Usage:\n/// <example>\n/// <code>\n/// using(var serviceClient = new ServiceClientWrapper&lt;IServiceContract&gt;)\n/// {\n/// serviceClient.Channel.YourServiceCall(request);\n/// }\n/// </code>\n/// </example>\n/// </summary>\n/// <typeparam name=\"ServiceType\">The type of the ervice type.</typeparam>\npublic class ServiceClientWrapper<ServiceType> : IDisposable\n{\n    private ServiceType _channel;\n    /// <summary>\n    /// Gets the channel.\n    /// </summary>\n    /// <value>The channel.</value>\n    public ServiceType Channel\n    {\n        get { return _channel; }\n    }\n\n    private static ChannelFactory<ServiceType> _channelFactory;\n\n    /// <summary>\n    /// Initializes a new instance of the <see cref=\"ServiceClientWrapper&lt;ServiceType&gt;\"/> class.\n    /// As a default the endpoint that is used is the one named after the contracts full name.\n    /// </summary>\n    public ServiceClientWrapper() : this(typeof(ServiceType).FullName)\n    { }\n\n    /// <summary>\n    /// Initializes a new instance of the <see cref=\"ServiceClientWrapper&lt;ServiceType&gt;\"/> class.\n    /// </summary>\n    /// <param name=\"endpoint\">The endpoint.</param>\n    public ServiceClientWrapper(string endpoint)\n    {\n        if (_channelFactory == null)\n            _channelFactory = new ChannelFactory<ServiceType>(endpoint);\n        _channel = _channelFactory.CreateChannel();\n        ((IChannel)_channel).Open();\n    }\n\n    /// <summary>\n    /// Performs application-defined tasks associated with freeing, releasing, or resetting unmanaged resources.\n    /// </summary>\n    public void Dispose()\n    {\n        try\n        {\n            ((IChannel)_channel).Close();\n        }\n        catch (CommunicationException e)\n        {\n            ((IChannel)_channel).Abort();\n        }\n        catch (TimeoutException e)\n        {\n            ((IChannel)_channel).Abort();\n        }\n        catch (Exception e)\n        {\n            ((IChannel)_channel).Abort();\n            // TODO: Insert logging\n        }\n    }\n}\n\n\nNow when you have your wrapper all you need to do is use the following code:\n\npublic class ClientWrapperUsage\n{\n    public static void main(string[] args)\n    {\n        using(var clientWrapper = new ServiceClientWrapper<ServiceType>())\n        {\n            var response = clientWrapper.Channel.ServiceCall();\n        }\n    }\n}\n\n\nI think the solution is quite ok, but I think we should never have been forced\nto implement our own wrapper to solve the issue. The code is also availble at\ngithub in this gist [https://gist.github.com/730846]","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:11:48.000Z","updated_at":"2016-01-15T22:19:36.000Z","published_at":"2012-10-25T07:11:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03a","uuid":"48e3bfb3-6f4f-4108-8a7d-9b95af0e5580","title":"Convert DataTable to generic list extension","slug":"convert-datatable-to-generic-list-extension","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The background to the following post is this [question][1] on Stack Overflow and my old [blog post][2] about generic list to DataTable. The question on Stack Overflow is basically asking the opposite of what I wrote in my blog post, but doing the opposite is almost the same code so I figured I just write it down and here it is:\\n\\n    public static class DataTableExtensions\\n    {\\n        public static IList<T> ToList<T>(this DataTable table) where T : new()\\n        {\\n            IList<PropertyInfo> properties = typeof(T).GetProperties().ToList();\\n            IList<T> result = new List<T>();\\n\\n            foreach (var row in table.Rows)\\n            {\\n                var item = CreateItemFromRow<T>((DataRow)row, properties);\\n                result.Add(item);\\n            }\\n\\n            return result;\\n        }\\n\\n        private static T CreateItemFromRow<T>(DataRow row, IList<PropertyInfo> properties) where T : new()\\n        {\\n            T item = new T();\\n            foreach (var property in properties)\\n            {\\n                property.SetValue(item, row[property.Name], null);\\n            }\\n            return item;\\n        }\\n    }\\n\\nYou could probably add some overload to exclude properties, I would in that case go for the signature `static List<T> ToList<T>(this DataTable table, params string[] excludeProperties)`, but I leave that up to you. You might also need some error handling depending on where the code is used.\\n\\n  [1]: http://stackoverflow.com/questions/4104464/convert-datatable-to-generic-list-in-c\\n  [2]: http://blog.tomasjansson.com/2010/10/generic-list-to-datatable/\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The background to the following post is this <a href=\"http://stackoverflow.com/questions/4104464/convert-datatable-to-generic-list-in-c\">question</a> on Stack Overflow and my old <a href=\"http://blog.tomasjansson.com/2010/10/generic-list-to-datatable/\">blog post</a> about generic list to DataTable. The question on Stack Overflow is basically asking the opposite of what I wrote in my blog post, but doing the opposite is almost the same code so I figured I just write it down and here it is:</p>\n<pre><code>public static class DataTableExtensions\n{\n    public static IList&lt;T&gt; ToList&lt;T&gt;(this DataTable table) where T : new()\n    {\n        IList&lt;PropertyInfo&gt; properties = typeof(T).GetProperties().ToList();\n        IList&lt;T&gt; result = new List&lt;T&gt;();\n\n        foreach (var row in table.Rows)\n        {\n            var item = CreateItemFromRow&lt;T&gt;((DataRow)row, properties);\n            result.Add(item);\n        }\n\n        return result;\n    }\n\n    private static T CreateItemFromRow&lt;T&gt;(DataRow row, IList&lt;PropertyInfo&gt; properties) where T : new()\n    {\n        T item = new T();\n        foreach (var property in properties)\n        {\n            property.SetValue(item, row[property.Name], null);\n        }\n        return item;\n    }\n}\n</code></pre>\n<p>You could probably add some overload to exclude properties, I would in that case go for the signature <code>static List&lt;T&gt; ToList&lt;T&gt;(this DataTable table, params string[] excludeProperties)</code>, but I leave that up to you. You might also need some error handling depending on where the code is used.</p>\n<!--kg-card-end: markdown-->","comment_id":"7","plaintext":"The background to the following post is this question\n[http://stackoverflow.com/questions/4104464/convert-datatable-to-generic-list-in-c] \non Stack Overflow and my old blog post\n[http://blog.tomasjansson.com/2010/10/generic-list-to-datatable/] about generic\nlist to DataTable. The question on Stack Overflow is basically asking the\nopposite of what I wrote in my blog post, but doing the opposite is almost the\nsame code so I figured I just write it down and here it is:\n\npublic static class DataTableExtensions\n{\n    public static IList<T> ToList<T>(this DataTable table) where T : new()\n    {\n        IList<PropertyInfo> properties = typeof(T).GetProperties().ToList();\n        IList<T> result = new List<T>();\n\n        foreach (var row in table.Rows)\n        {\n            var item = CreateItemFromRow<T>((DataRow)row, properties);\n            result.Add(item);\n        }\n\n        return result;\n    }\n\n    private static T CreateItemFromRow<T>(DataRow row, IList<PropertyInfo> properties) where T : new()\n    {\n        T item = new T();\n        foreach (var property in properties)\n        {\n            property.SetValue(item, row[property.Name], null);\n        }\n        return item;\n    }\n}\n\n\nYou could probably add some overload to exclude properties, I would in that case\ngo for the signature static List<T> ToList<T>(this DataTable table, params\nstring[] excludeProperties), but I leave that up to you. You might also need\nsome error handling depending on where the code is used.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:13:05.000Z","updated_at":"2014-03-01T09:13:41.000Z","published_at":"2012-10-25T07:13:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03b","uuid":"22a7b44f-a6cd-4ec9-afb1-b7fe6626fcd2","title":"How to check http status code with watin","slug":"how-to-check-http-status-code-with-watin","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I'm about to write my own blog, which will replace this one when it's done. When doing so there is no excuse to not do it as it should be done. So I'm using [SpecFlow](http://specflow.org/) to define my features and [WatiN](http://watin.org/) to drive my browser tests. One thing I found was that there is no direct way to check the response code for request. A colleague of mine has written this [post](http://www.kongsli.net/nblog/2008/07/08/internet-explorer-automationwatin-catching-navigation-error-codes/) that covers how you catch navigation erros. But I would like to catch 200 codes as well so I extended it to allow for this. I don't think it is 100 % accurate, since you can only assume that if you don't get an error it is a 200 code, which is not true all the time. Alos, it only extends the IE browser, but that should be ok for this scenarios. My guess is that any redirect code doesn't cause an error which will not work in my scenario. However, here is the code to do the job.\\n\\nFirst we have the `NavigationObserver` from the blog that will act as an observer of the browser and will catch errors and also when the navigate complete.\\n\\n<script src=\\\"https://gist.github.com/1207490.js?file=NavigationObserver.cs\\\"></script>\\n\\nThe thoughts behind this is that when an error occurs, we catch it in the error handler and set the flag that an error has occured. In the complete event we check if the flag is set, if the flag is set we won't change the status code, and if it isn't checked we assumes that we have an 200 status code. To make it easier to use this observer I have implemented the following extension to the WatiN `IE` browser.\\n\\n<script src=\\\"https://gist.github.com/1207490.js?file=ObservableBrowser.cs\\\"></script>\\n\\nIf you use this custom browser instead of the standard `IE` you should be able to check some status codes. I might do some work with the code to get it to cover more scenarios. \"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I'm about to write my own blog, which will replace this one when it's done. When doing so there is no excuse to not do it as it should be done. So I'm using <a href=\"http://specflow.org/\">SpecFlow</a> to define my features and <a href=\"http://watin.org/\">WatiN</a> to drive my browser tests. One thing I found was that there is no direct way to check the response code for request. A colleague of mine has written this <a href=\"http://www.kongsli.net/nblog/2008/07/08/internet-explorer-automationwatin-catching-navigation-error-codes/\">post</a> that covers how you catch navigation erros. But I would like to catch 200 codes as well so I extended it to allow for this. I don't think it is 100 % accurate, since you can only assume that if you don't get an error it is a 200 code, which is not true all the time. Alos, it only extends the IE browser, but that should be ok for this scenarios. My guess is that any redirect code doesn't cause an error which will not work in my scenario. However, here is the code to do the job.</p>\n<p>First we have the <code>NavigationObserver</code> from the blog that will act as an observer of the browser and will catch errors and also when the navigate complete.</p>\n<script src=\"https://gist.github.com/1207490.js?file=NavigationObserver.cs\"></script>\n<p>The thoughts behind this is that when an error occurs, we catch it in the error handler and set the flag that an error has occured. In the complete event we check if the flag is set, if the flag is set we won't change the status code, and if it isn't checked we assumes that we have an 200 status code. To make it easier to use this observer I have implemented the following extension to the WatiN <code>IE</code> browser.</p>\n<script src=\"https://gist.github.com/1207490.js?file=ObservableBrowser.cs\"></script>\n<p>If you use this custom browser instead of the standard <code>IE</code> you should be able to check some status codes. I might do some work with the code to get it to cover more scenarios.</p>\n<!--kg-card-end: markdown-->","comment_id":"8","plaintext":"I'm about to write my own blog, which will replace this one when it's done. When\ndoing so there is no excuse to not do it as it should be done. So I'm using \nSpecFlow [http://specflow.org/] to define my features and WatiN\n[http://watin.org/] to drive my browser tests. One thing I found was that there\nis no direct way to check the response code for request. A colleague of mine has\nwritten this post\n[http://www.kongsli.net/nblog/2008/07/08/internet-explorer-automationwatin-catching-navigation-error-codes/] \nthat covers how you catch navigation erros. But I would like to catch 200 codes\nas well so I extended it to allow for this. I don't think it is 100 % accurate,\nsince you can only assume that if you don't get an error it is a 200 code, which\nis not true all the time. Alos, it only extends the IE browser, but that should\nbe ok for this scenarios. My guess is that any redirect code doesn't cause an\nerror which will not work in my scenario. However, here is the code to do the\njob.\n\nFirst we have the NavigationObserver from the blog that will act as an observer\nof the browser and will catch errors and also when the navigate complete.\n\nThe thoughts behind this is that when an error occurs, we catch it in the error\nhandler and set the flag that an error has occured. In the complete event we\ncheck if the flag is set, if the flag is set we won't change the status code,\nand if it isn't checked we assumes that we have an 200 status code. To make it\neasier to use this observer I have implemented the following extension to the\nWatiN IE browser.\n\nIf you use this custom browser instead of the standard IE you should be able to\ncheck some status codes. I might do some work with the code to get it to cover\nmore scenarios.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:15:58.000Z","updated_at":"2014-03-01T09:16:19.000Z","published_at":"2012-10-25T07:15:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03c","uuid":"4f3c8d95-9ff3-4831-94ed-94c98da17095","title":"Extending the NuGet Package Manager Console","slug":"extending-the-nuget-package-manager-console","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you read my previous [post](http://blog.tomasjansson.com/2011/04/creating-a-nuget-package/) about how to create a [NuGet](http://www.nuget.org) package this is a follow up on that post. I will in this post, most example code, show you how to extend the Package Manager Console in Visual Studio with a NuGet package. More precisely I will extend the Console with the feature of asking [http://www.classnamer.com](http://www.classnamer.com) to generate a class name for me since sometimes when I develop I run out of imagination and at those points it can be good to get generated class names like `CryptographicUserLogger` :).\\n\\nThe post consist of two parts; the first part shows you the code we will run in order to get the class name and initialize the scripts. So let's get started.\\n\\n### Creating a PowerShell extension script\\nSince this is my first PowerShell ever the code might not be the best code ever, but that is not the point. The first part is to write the actual module that will do all the work. The code for the module ClassNamer.psm1 look like:\\n\\n    function Get-ClassName {\\n        $ie = new-object -com InternetExplorer.Application\\n        $ie.navigate(\\\"http://www.classnamer.com/\\\")\\n        if (!$ie) { Write-Host \\\"variable is null\\\" }\\n        while ($ie.Busy -eq $true) \\n        { \\n            Start-Sleep -Milliseconds 1000; \\n        } \\n\\n        $doc = $ie.Document\\n        if (!$doc) \\n        { \\n            Write-Host \\\"variable is null\\\"\\n            return \\\"SorryCantGiveYouAGenericClass\\\" \\n        }\\n        $answer = $doc.getElementByID(\\\"classname\\\") \\n        return $answer.innerHtml\\n    }\\n\\n    Export-ModuleMember Get-ClassName\\n\\nThere is two part of this code, the function definition and the export command which I guess define which functions that are public from this module. The code is pretty straight forward, but here is a short description of the `Get-ClassName` function:\\n\\n1. Create a IE object\\n2. Browse to get the page\\n3. Sleep while busy (should probably add a max loop counter)\\n4. Get the doc, which is the DOM of the web page\\n5. Query the doc object of the relevant element and extract the value\\n\\nThe next part is to write the script that will get the module imported, init.ps1, to the Package Manager Console. I basically copied the code from Phil Haack's [blog post](http://haacked.com/archive/2011/04/19/writing-a-nuget-package-that-adds-a-command-to-the.aspx) about the same topic. The code looks like: \\n\\n    param($installPath, $toolsPath, $package, $project)\\n\\n    Import-Module (Join-Path $toolsPath ClassNamer.psm1)\\n\\nWhen the above script is run it will import the module ClassNamer.psm1. Now all we need to do is to put both the PowerShell files in a tools folder and in the same folder as where we have tools folder we run the command `nuget spec ClassNamer`. The command will generate a ClassNamer.nuspec file for us with the default metadata. When we run nuget pack on the ClassNamer.nuspec file it will create a package with that contains the tools folder, and when we install the package in Visual Studio the init.ps1 script will be run by NuGet and install our module. If done correctly you will be able to get something like the image below when you've installed the package and run the new `Get-ClassName` command:\\n\\n![ClassNamer screen shot](http://media.tomasjansson.com/2011/05/ClassNamer.png)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>If you read my previous <a href=\"http://blog.tomasjansson.com/2011/04/creating-a-nuget-package/\">post</a> about how to create a <a href=\"http://www.nuget.org\">NuGet</a> package this is a follow up on that post. I will in this post, most example code, show you how to extend the Package Manager Console in Visual Studio with a NuGet package. More precisely I will extend the Console with the feature of asking <a href=\"http://www.classnamer.com\">http://www.classnamer.com</a> to generate a class name for me since sometimes when I develop I run out of imagination and at those points it can be good to get generated class names like <code>CryptographicUserLogger</code> :).</p>\n<p>The post consist of two parts; the first part shows you the code we will run in order to get the class name and initialize the scripts. So let's get started.</p>\n<h3 id=\"creatingapowershellextensionscript\">Creating a PowerShell extension script</h3>\n<p>Since this is my first PowerShell ever the code might not be the best code ever, but that is not the point. The first part is to write the actual module that will do all the work. The code for the module ClassNamer.psm1 look like:</p>\n<pre><code>function Get-ClassName {\n    $ie = new-object -com InternetExplorer.Application\n    $ie.navigate(&quot;http://www.classnamer.com/&quot;)\n    if (!$ie) { Write-Host &quot;variable is null&quot; }\n    while ($ie.Busy -eq $true) \n    { \n        Start-Sleep -Milliseconds 1000; \n    } \n\n    $doc = $ie.Document\n    if (!$doc) \n    { \n        Write-Host &quot;variable is null&quot;\n        return &quot;SorryCantGiveYouAGenericClass&quot; \n    }\n    $answer = $doc.getElementByID(&quot;classname&quot;) \n    return $answer.innerHtml\n}\n\nExport-ModuleMember Get-ClassName\n</code></pre>\n<p>There is two part of this code, the function definition and the export command which I guess define which functions that are public from this module. The code is pretty straight forward, but here is a short description of the <code>Get-ClassName</code> function:</p>\n<ol>\n<li>Create a IE object</li>\n<li>Browse to get the page</li>\n<li>Sleep while busy (should probably add a max loop counter)</li>\n<li>Get the doc, which is the DOM of the web page</li>\n<li>Query the doc object of the relevant element and extract the value</li>\n</ol>\n<p>The next part is to write the script that will get the module imported, init.ps1, to the Package Manager Console. I basically copied the code from Phil Haack's <a href=\"http://haacked.com/archive/2011/04/19/writing-a-nuget-package-that-adds-a-command-to-the.aspx\">blog post</a> about the same topic. The code looks like:</p>\n<pre><code>param($installPath, $toolsPath, $package, $project)\n\nImport-Module (Join-Path $toolsPath ClassNamer.psm1)\n</code></pre>\n<p>When the above script is run it will import the module ClassNamer.psm1. Now all we need to do is to put both the PowerShell files in a tools folder and in the same folder as where we have tools folder we run the command <code>nuget spec ClassNamer</code>. The command will generate a ClassNamer.nuspec file for us with the default metadata. When we run nuget pack on the ClassNamer.nuspec file it will create a package with that contains the tools folder, and when we install the package in Visual Studio the init.ps1 script will be run by NuGet and install our module. If done correctly you will be able to get something like the image below when you've installed the package and run the new <code>Get-ClassName</code> command:</p>\n<p><img src=\"http://media.tomasjansson.com/2011/05/ClassNamer.png\" alt=\"ClassNamer screen shot\"></p>\n<!--kg-card-end: markdown-->","comment_id":"9","plaintext":"If you read my previous post\n[http://blog.tomasjansson.com/2011/04/creating-a-nuget-package/] about how to\ncreate a NuGet [http://www.nuget.org] package this is a follow up on that post.\nI will in this post, most example code, show you how to extend the Package\nManager Console in Visual Studio with a NuGet package. More precisely I will\nextend the Console with the feature of asking http://www.classnamer.com to\ngenerate a class name for me since sometimes when I develop I run out of\nimagination and at those points it can be good to get generated class names like \nCryptographicUserLogger :).\n\nThe post consist of two parts; the first part shows you the code we will run in\norder to get the class name and initialize the scripts. So let's get started.\n\nCreating a PowerShell extension script\nSince this is my first PowerShell ever the code might not be the best code ever,\nbut that is not the point. The first part is to write the actual module that\nwill do all the work. The code for the module ClassNamer.psm1 look like:\n\nfunction Get-ClassName {\n    $ie = new-object -com InternetExplorer.Application\n    $ie.navigate(\"http://www.classnamer.com/\")\n    if (!$ie) { Write-Host \"variable is null\" }\n    while ($ie.Busy -eq $true) \n    { \n        Start-Sleep -Milliseconds 1000; \n    } \n\n    $doc = $ie.Document\n    if (!$doc) \n    { \n        Write-Host \"variable is null\"\n        return \"SorryCantGiveYouAGenericClass\" \n    }\n    $answer = $doc.getElementByID(\"classname\") \n    return $answer.innerHtml\n}\n\nExport-ModuleMember Get-ClassName\n\n\nThere is two part of this code, the function definition and the export command\nwhich I guess define which functions that are public from this module. The code\nis pretty straight forward, but here is a short description of the Get-ClassName \nfunction:\n\n 1. Create a IE object\n 2. Browse to get the page\n 3. Sleep while busy (should probably add a max loop counter)\n 4. Get the doc, which is the DOM of the web page\n 5. Query the doc object of the relevant element and extract the value\n\nThe next part is to write the script that will get the module imported,\ninit.ps1, to the Package Manager Console. I basically copied the code from Phil\nHaack's blog post\n[http://haacked.com/archive/2011/04/19/writing-a-nuget-package-that-adds-a-command-to-the.aspx] \nabout the same topic. The code looks like:\n\nparam($installPath, $toolsPath, $package, $project)\n\nImport-Module (Join-Path $toolsPath ClassNamer.psm1)\n\n\nWhen the above script is run it will import the module ClassNamer.psm1. Now all\nwe need to do is to put both the PowerShell files in a tools folder and in the\nsame folder as where we have tools folder we run the command nuget spec\nClassNamer. The command will generate a ClassNamer.nuspec file for us with the\ndefault metadata. When we run nuget pack on the ClassNamer.nuspec file it will\ncreate a package with that contains the tools folder, and when we install the\npackage in Visual Studio the init.ps1 script will be run by NuGet and install\nour module. If done correctly you will be able to get something like the image\nbelow when you've installed the package and run the new Get-ClassName command:","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:17:43.000Z","updated_at":"2014-03-01T09:18:07.000Z","published_at":"2012-10-25T07:17:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03d","uuid":"a398ed82-eacb-43c3-8463-f07cec26e0c0","title":"Refactor your code - remove those if-else if-else and switch statements","slug":"refactor-your-code-remove-those-if-else-if-else-and-switch-statements","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I'm currently working with some legacy code and came across a really long if-else if-else function. We've all been down that path writing that type of code. A situation where this is pretty common in .Net, and that is where I stumbled on it this time, is when acting on a some kind of command from a grid view for example. The code that I refactored looked something like this:\\n\\n    public void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\\n    {\\n\\t    string commandName = e.CommandName;\\n\\t\\tif(commandName == \\\"command1\\\")\\n\\t\\t{\\n\\t\\t\\t// A lot of code for command 1\\n\\t\\t}\\n\\t\\telse if(commandName == \\\"command2\\\")\\n\\t\\t{\\n\\t\\t\\t// A lot of code for command2\\n\\t\\t}\\n\\t\\telse if(commandName == \\\"command3\\\")\\n\\t\\t{\\n\\t\\t\\t// A lot of code for command3\\n\\t\\t}\\n\\t\\telse if(commandName == \\\"command4\\\")\\n\\t\\t{\\n\\t\\t\\t// A lot of code for command4\\n\\t\\t}\\n    }\\n\\nEvery where were it says \\\"A lot of code for commandx\\\" contained some kind of logic that was pretty hard to understand. So the first step in refactoring this piece of code is actually to learn what those commands are and create separate methods for each method like such: \\n\\n\\tpublic void Command1()\\n\\t{\\n\\t\\t// A lot of code for command1\\n\\t}\\n\\n\\tpublic void Command2()\\n\\t{\\n\\t\\t// A lot of code for command2\\n\\t}\\n\\tpublic void Command3()\\n\\t{\\n\\t\\t// A lot of code for command3\\n\\t}\\n\\tpublic void Command4()\\n\\t{\\n\\t\\t// A lot of code for command4\\n\\t}\\n\\nThe function name must also have a descriptive name and not just `Commandx` as I have in the example here. Now one might think that we could just change our if-else if-else to a switch and call the commands from the switch. But I don't think that is enough. The event handler should just handle the event it should **NOT** be responsible for executing the right command, so what I ended up doing was a dictionary where I mapped the command names to their respective functions and created a function that was responsible for executing the right command given a command string. The final code looked something like: \\n\\n    private Dictionary<string, Action> _commandDictionary;\\n    public Dictionary<string, Action> CommandDictionary\\n    {\\n        get\\n        {\\n            if (_commandDictionary == null)\\n            {\\n                _commandDictionary = new Dictionary<string, Action>();\\n                _commandDictionary.Add(\\\"Command1\\\", Command1);\\n                _commandDictionary.Add(\\\"Command2\\\", Command2);\\n                _commandDictionary.Add(\\\"Command3\\\", Command3);\\n                _commandDictionary.Add(\\\"Command4\\\", Command4);\\n            }\\n            return _commandDictionary;\\n        }\\n    }\\n\\n    public void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\\n    {\\n        string commandToExecute = e.CommandName;\\n        ExecuteCommand(commandToExecute);\\n    }\\n\\n    private void ExecuteCommand(string command)\\n    {\\n        if (CommandDictionary.ContainsKey(command))\\n        {\\n            CommandDictionary[command]();\\n        }\\n        else\\n        {\\n            throw new ArgumentException(\\\"Invalid command: {0}\\\", command);\\n        }\\n    }\\n\\t\\n    public void Command1()\\n    {\\n        // A lot of code for command1\\n    }\\n\\n    public void Command2()\\n    {\\n        // A lot of code for command2\\n    }\\n\\n    public void Command3()\\n    {\\n        // A lot of code for command3\\n    }\\n\\n    public void Command4()\\n    {\\n        // A lot of code for command4\\n    }\\n\\nThe resulting code is not necessarily shorter, but every piece of code has it's own \\\"purpose\\\". The event handler just extract the command that should be executed, the `ExecuteCommand` makes the check if the command is valid and calls the responding command and the command functions do the actual work that is requested. Some advantages you get with this solution are that it is much easier to test each function by itself than it is was before and it is much easier to follow what the code actually does.\\n\\nYou could take this one step further, or if you don't have something like `Action` that I use here in your language of choice, and create an interface called `ICommand` that defines an `Execute` method and wrap all the commands in separate classes that implements that interface. But for now this works fine for me since I only use it in one place and it is not meant to be used anywhere else.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I'm currently working with some legacy code and came across a really long if-else if-else function. We've all been down that path writing that type of code. A situation where this is pretty common in .Net, and that is where I stumbled on it this time, is when acting on a some kind of command from a grid view for example. The code that I refactored looked something like this:</p>\n<pre><code>public void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\n{\n    string commandName = e.CommandName;\n\tif(commandName == &quot;command1&quot;)\n\t{\n\t\t// A lot of code for command 1\n\t}\n\telse if(commandName == &quot;command2&quot;)\n\t{\n\t\t// A lot of code for command2\n\t}\n\telse if(commandName == &quot;command3&quot;)\n\t{\n\t\t// A lot of code for command3\n\t}\n\telse if(commandName == &quot;command4&quot;)\n\t{\n\t\t// A lot of code for command4\n\t}\n}\n</code></pre>\n<p>Every where were it says &quot;A lot of code for commandx&quot; contained some kind of logic that was pretty hard to understand. So the first step in refactoring this piece of code is actually to learn what those commands are and create separate methods for each method like such:</p>\n<pre><code>public void Command1()\n{\n\t// A lot of code for command1\n}\n\npublic void Command2()\n{\n\t// A lot of code for command2\n}\npublic void Command3()\n{\n\t// A lot of code for command3\n}\npublic void Command4()\n{\n\t// A lot of code for command4\n}\n</code></pre>\n<p>The function name must also have a descriptive name and not just <code>Commandx</code> as I have in the example here. Now one might think that we could just change our if-else if-else to a switch and call the commands from the switch. But I don't think that is enough. The event handler should just handle the event it should <strong>NOT</strong> be responsible for executing the right command, so what I ended up doing was a dictionary where I mapped the command names to their respective functions and created a function that was responsible for executing the right command given a command string. The final code looked something like:</p>\n<pre><code>private Dictionary&lt;string, Action&gt; _commandDictionary;\npublic Dictionary&lt;string, Action&gt; CommandDictionary\n{\n    get\n    {\n        if (_commandDictionary == null)\n        {\n            _commandDictionary = new Dictionary&lt;string, Action&gt;();\n            _commandDictionary.Add(&quot;Command1&quot;, Command1);\n            _commandDictionary.Add(&quot;Command2&quot;, Command2);\n            _commandDictionary.Add(&quot;Command3&quot;, Command3);\n            _commandDictionary.Add(&quot;Command4&quot;, Command4);\n        }\n        return _commandDictionary;\n    }\n}\n\npublic void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\n{\n    string commandToExecute = e.CommandName;\n    ExecuteCommand(commandToExecute);\n}\n\nprivate void ExecuteCommand(string command)\n{\n    if (CommandDictionary.ContainsKey(command))\n    {\n        CommandDictionary[command]();\n    }\n    else\n    {\n        throw new ArgumentException(&quot;Invalid command: {0}&quot;, command);\n    }\n}\n\npublic void Command1()\n{\n    // A lot of code for command1\n}\n\npublic void Command2()\n{\n    // A lot of code for command2\n}\n\npublic void Command3()\n{\n    // A lot of code for command3\n}\n\npublic void Command4()\n{\n    // A lot of code for command4\n}\n</code></pre>\n<p>The resulting code is not necessarily shorter, but every piece of code has it's own &quot;purpose&quot;. The event handler just extract the command that should be executed, the <code>ExecuteCommand</code> makes the check if the command is valid and calls the responding command and the command functions do the actual work that is requested. Some advantages you get with this solution are that it is much easier to test each function by itself than it is was before and it is much easier to follow what the code actually does.</p>\n<p>You could take this one step further, or if you don't have something like <code>Action</code> that I use here in your language of choice, and create an interface called <code>ICommand</code> that defines an <code>Execute</code> method and wrap all the commands in separate classes that implements that interface. But for now this works fine for me since I only use it in one place and it is not meant to be used anywhere else.</p>\n<!--kg-card-end: markdown-->","comment_id":"10","plaintext":"I'm currently working with some legacy code and came across a really long\nif-else if-else function. We've all been down that path writing that type of\ncode. A situation where this is pretty common in .Net, and that is where I\nstumbled on it this time, is when acting on a some kind of command from a grid\nview for example. The code that I refactored looked something like this:\n\npublic void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\n{\n    string commandName = e.CommandName;\n\tif(commandName == \"command1\")\n\t{\n\t\t// A lot of code for command 1\n\t}\n\telse if(commandName == \"command2\")\n\t{\n\t\t// A lot of code for command2\n\t}\n\telse if(commandName == \"command3\")\n\t{\n\t\t// A lot of code for command3\n\t}\n\telse if(commandName == \"command4\")\n\t{\n\t\t// A lot of code for command4\n\t}\n}\n\n\nEvery where were it says \"A lot of code for commandx\" contained some kind of\nlogic that was pretty hard to understand. So the first step in refactoring this\npiece of code is actually to learn what those commands are and create separate\nmethods for each method like such:\n\npublic void Command1()\n{\n\t// A lot of code for command1\n}\n\npublic void Command2()\n{\n\t// A lot of code for command2\n}\npublic void Command3()\n{\n\t// A lot of code for command3\n}\npublic void Command4()\n{\n\t// A lot of code for command4\n}\n\n\nThe function name must also have a descriptive name and not just Commandx as I\nhave in the example here. Now one might think that we could just change our\nif-else if-else to a switch and call the commands from the switch. But I don't\nthink that is enough. The event handler should just handle the event it should \nNOT be responsible for executing the right command, so what I ended up doing was\na dictionary where I mapped the command names to their respective functions and\ncreated a function that was responsible for executing the right command given a\ncommand string. The final code looked something like:\n\nprivate Dictionary<string, Action> _commandDictionary;\npublic Dictionary<string, Action> CommandDictionary\n{\n    get\n    {\n        if (_commandDictionary == null)\n        {\n            _commandDictionary = new Dictionary<string, Action>();\n            _commandDictionary.Add(\"Command1\", Command1);\n            _commandDictionary.Add(\"Command2\", Command2);\n            _commandDictionary.Add(\"Command3\", Command3);\n            _commandDictionary.Add(\"Command4\", Command4);\n        }\n        return _commandDictionary;\n    }\n}\n\npublic void gridView_OnRowCommand(object sender, GridViewCommandEventArgs e)\n{\n    string commandToExecute = e.CommandName;\n    ExecuteCommand(commandToExecute);\n}\n\nprivate void ExecuteCommand(string command)\n{\n    if (CommandDictionary.ContainsKey(command))\n    {\n        CommandDictionary[command]();\n    }\n    else\n    {\n        throw new ArgumentException(\"Invalid command: {0}\", command);\n    }\n}\n\npublic void Command1()\n{\n    // A lot of code for command1\n}\n\npublic void Command2()\n{\n    // A lot of code for command2\n}\n\npublic void Command3()\n{\n    // A lot of code for command3\n}\n\npublic void Command4()\n{\n    // A lot of code for command4\n}\n\n\nThe resulting code is not necessarily shorter, but every piece of code has it's\nown \"purpose\". The event handler just extract the command that should be\nexecuted, the ExecuteCommand makes the check if the command is valid and calls\nthe responding command and the command functions do the actual work that is\nrequested. Some advantages you get with this solution are that it is much easier\nto test each function by itself than it is was before and it is much easier to\nfollow what the code actually does.\n\nYou could take this one step further, or if you don't have something like Action \nthat I use here in your language of choice, and create an interface called \nICommand that defines an Execute method and wrap all the commands in separate\nclasses that implements that interface. But for now this works fine for me since\nI only use it in one place and it is not meant to be used anywhere else.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:19:24.000Z","updated_at":"2014-03-01T09:19:51.000Z","published_at":"2012-10-25T07:19:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03e","uuid":"8773c3a6-0277-4c92-b3c4-39cf3ef616fc","title":"Generic list to DataTable","slug":"generic-list-to-datatable","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Ok, I have to admit that I'm sitting on a WebForms project that is using datasets and `DataTable` a lot as `DataSource` to `GridView`'s for example. When using `GridView`'s in asp.net it's pretty nice to have `DataTable` as `DataSource`s since the sorting of data is included in the `DataSource` object, as long as you don't needs paging than you have to take more things into consideration. However, we've decided to implement all the services to send list of objects instead of datasets, which I think is good, but we still want to use `DataTable`'s as `DataSource`. So how do we achieve this? Easy I say, all you need is some basic knowledge in generics and reflection to write an extension for `IList<T>` that creates a `DataTable` based on the generic type `T`. I ended up writing four methods; two public extension methods, one private extension method and one helper method:\\n\\n    public static class ListExtensions\\n    {\\n        public static DataTable ToDataTable<T>(this IList<T> list)\\n        {\\n            IList<PropertyInfo> properties = list.GetPropertiesOfObjectInList();\\n            DataTable resultTable = CreateTable(properties);\\n\\n            foreach(var item in list)\\n            {\\n                var row = CreateRowFromItem<T>(resultTable, item);\\n                resultTable.Rows.Add(row);\\n            }\\n\\n            return resultTable;\\n        }\\n\\n        private static DataTable CreateTable(IList<PropertyInfo> properties)\\n        {\\n            DataTable resultTable = new DataTable();\\n            foreach (var property in properties)\\n            {\\n                resultTable.Columns.Add(property.Name, property.PropertyType);\\n            }\\n            return resultTable;\\n        }\\n\\n        public static IList<PropertyInfo> GetPropertiesOfObjectInList<T>(this IList<T> list)\\n        {\\n            return typeof(T).GetProperties().ToList();\\n        }\\n\\n        private static DataRow CreateRowFromItem<T>(DataTable resultTable, T item)\\n        {\\n            var row = resultTable.NewRow();\\n            var properties = item.GetType().GetProperties().ToList();\\n            foreach (var property in properties)\\n            {\\n                row[property.Name] = property.GetValue(item, null);\\n            }\\n            return row;\\n        }\\n    }\\n\\nWhat they do are the following:\\n\\n* `ToDataTable` takes an `IList<T>` and creates the `DataTable` with data\\n* `CreateTable` creates the actual table from an `IList<T>`\\n* `GetPropertiesOfObjectInList` gets the properties of the object of type `T` (you need that to create the table)\\n* `CreateRowFromItem` takes the resulting table, so you can ask for the structure of a row, and the item you wish to add and creates a `DataRow` object of the item\\n\\nThat's how you spicy up your old WebForms project :). \"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Ok, I have to admit that I'm sitting on a WebForms project that is using datasets and <code>DataTable</code> a lot as <code>DataSource</code> to <code>GridView</code>'s for example. When using <code>GridView</code>'s in asp.net it's pretty nice to have <code>DataTable</code> as <code>DataSource</code>s since the sorting of data is included in the <code>DataSource</code> object, as long as you don't needs paging than you have to take more things into consideration. However, we've decided to implement all the services to send list of objects instead of datasets, which I think is good, but we still want to use <code>DataTable</code>'s as <code>DataSource</code>. So how do we achieve this? Easy I say, all you need is some basic knowledge in generics and reflection to write an extension for <code>IList&lt;T&gt;</code> that creates a <code>DataTable</code> based on the generic type <code>T</code>. I ended up writing four methods; two public extension methods, one private extension method and one helper method:</p>\n<pre><code>public static class ListExtensions\n{\n    public static DataTable ToDataTable&lt;T&gt;(this IList&lt;T&gt; list)\n    {\n        IList&lt;PropertyInfo&gt; properties = list.GetPropertiesOfObjectInList();\n        DataTable resultTable = CreateTable(properties);\n\n        foreach(var item in list)\n        {\n            var row = CreateRowFromItem&lt;T&gt;(resultTable, item);\n            resultTable.Rows.Add(row);\n        }\n\n        return resultTable;\n    }\n\n    private static DataTable CreateTable(IList&lt;PropertyInfo&gt; properties)\n    {\n        DataTable resultTable = new DataTable();\n        foreach (var property in properties)\n        {\n            resultTable.Columns.Add(property.Name, property.PropertyType);\n        }\n        return resultTable;\n    }\n\n    public static IList&lt;PropertyInfo&gt; GetPropertiesOfObjectInList&lt;T&gt;(this IList&lt;T&gt; list)\n    {\n        return typeof(T).GetProperties().ToList();\n    }\n\n    private static DataRow CreateRowFromItem&lt;T&gt;(DataTable resultTable, T item)\n    {\n        var row = resultTable.NewRow();\n        var properties = item.GetType().GetProperties().ToList();\n        foreach (var property in properties)\n        {\n            row[property.Name] = property.GetValue(item, null);\n        }\n        return row;\n    }\n}\n</code></pre>\n<p>What they do are the following:</p>\n<ul>\n<li><code>ToDataTable</code> takes an <code>IList&lt;T&gt;</code> and creates the <code>DataTable</code> with data</li>\n<li><code>CreateTable</code> creates the actual table from an <code>IList&lt;T&gt;</code></li>\n<li><code>GetPropertiesOfObjectInList</code> gets the properties of the object of type <code>T</code> (you need that to create the table)</li>\n<li><code>CreateRowFromItem</code> takes the resulting table, so you can ask for the structure of a row, and the item you wish to add and creates a <code>DataRow</code> object of the item</li>\n</ul>\n<p>That's how you spicy up your old WebForms project :).</p>\n<!--kg-card-end: markdown-->","comment_id":"11","plaintext":"Ok, I have to admit that I'm sitting on a WebForms project that is using\ndatasets and DataTable a lot as DataSource to GridView's for example. When using \nGridView's in asp.net it's pretty nice to have DataTable as DataSources since\nthe sorting of data is included in the DataSource object, as long as you don't\nneeds paging than you have to take more things into consideration. However,\nwe've decided to implement all the services to send list of objects instead of\ndatasets, which I think is good, but we still want to use DataTable's as \nDataSource. So how do we achieve this? Easy I say, all you need is some basic\nknowledge in generics and reflection to write an extension for IList<T> that\ncreates a DataTable based on the generic type T. I ended up writing four\nmethods; two public extension methods, one private extension method and one\nhelper method:\n\npublic static class ListExtensions\n{\n    public static DataTable ToDataTable<T>(this IList<T> list)\n    {\n        IList<PropertyInfo> properties = list.GetPropertiesOfObjectInList();\n        DataTable resultTable = CreateTable(properties);\n\n        foreach(var item in list)\n        {\n            var row = CreateRowFromItem<T>(resultTable, item);\n            resultTable.Rows.Add(row);\n        }\n\n        return resultTable;\n    }\n\n    private static DataTable CreateTable(IList<PropertyInfo> properties)\n    {\n        DataTable resultTable = new DataTable();\n        foreach (var property in properties)\n        {\n            resultTable.Columns.Add(property.Name, property.PropertyType);\n        }\n        return resultTable;\n    }\n\n    public static IList<PropertyInfo> GetPropertiesOfObjectInList<T>(this IList<T> list)\n    {\n        return typeof(T).GetProperties().ToList();\n    }\n\n    private static DataRow CreateRowFromItem<T>(DataTable resultTable, T item)\n    {\n        var row = resultTable.NewRow();\n        var properties = item.GetType().GetProperties().ToList();\n        foreach (var property in properties)\n        {\n            row[property.Name] = property.GetValue(item, null);\n        }\n        return row;\n    }\n}\n\n\nWhat they do are the following:\n\n * ToDataTable takes an IList<T> and creates the DataTable with data\n * CreateTable creates the actual table from an IList<T>\n * GetPropertiesOfObjectInList gets the properties of the object of type T (you\n   need that to create the table)\n * CreateRowFromItem takes the resulting table, so you can ask for the structure\n   of a row, and the item you wish to add and creates a DataRow object of the\n   item\n\nThat's how you spicy up your old WebForms project :).","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:20:37.000Z","updated_at":"2014-03-01T09:20:58.000Z","published_at":"2012-10-25T07:20:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe03f","uuid":"71dd7e8d-7ebf-4820-a7e0-df321db3d2d0","title":"Debugging your Windows Service in Visual Studio","slug":"debugging-your-windows-service-in-visual-studio","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"It could be hard to debug your windows services if you play by the book. If you do play by the book you need to install the windows service and then attach a debugger to the process to be able to debug the service, of course you might need to do that in some cases maybe to make sure that the service acts as it should under the account it is running. However, to debug the actual functionality it is always easier to just hit F5 in Visual Studio and start debugging and here is how you do that. First, create a _Service_ project for your reusable code. The first file you should add is a windows service class, `CustomServiceBase`, that extends the `ServiceBase` class. \\n\\n\\tnamespace Service\\n\\t{\\n\\t\\tpublic class CustomServiceBase : ServiceBase\\n\\t\\t{\\n\\t\\t\\tpublic void StartService(string[] args)\\n\\t\\t\\t{\\n\\t\\t\\t\\tOnStart(args);\\n\\t\\t\\t}\\n\\n\\t\\t\\tpublic void StopService()\\n\\t\\t\\t{\\n\\t\\t\\t\\tOnStop();\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\nThis will be your new `ServiceBase` class instead of `ServiceBase`. Basically all it do is adding two methods that is used later on so we don't have to use reflection to access the `OnStart` and the `OnStop` methods. The next file you need to add to your _Service_ is the generic `ServiceManager` class that will acts as your _runner_. \\n\\n\\tnamespace Service\\n\\t{\\n\\t\\tpublic delegate void ServiceManagerHandler(object sender, EventArgs args);\\n\\n\\t\\tpublic class ServiceManager<T> where T : CustomServiceBase, new() \\n\\t\\t{\\n\\t\\t\\tpublic event ServiceManagerHandler ServiceDefaultModeStarting;\\n\\t\\t\\tpublic event ServiceManagerHandler ServiceInteractiveModeStarting;\\n\\n\\t\\t\\tpublic void Run()\\n\\t\\t\\t{\\n\\t\\t\\t\\tif (!Debugger.IsAttached)\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tStartServiceDefault();\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\telse\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tStartServiceInteractive();\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\n\\t\\t\\tprivate void StartServiceDefault()\\n\\t\\t\\t{\\n\\t\\t\\t\\tif (ServiceDefaultModeStarting != null)\\n\\t\\t\\t\\t\\tServiceDefaultModeStarting(this, null);\\n\\t\\t\\t\\tServiceBase[] ServicesToRun;\\n\\t\\t\\t\\tServicesToRun = new ServiceBase[] \\n\\t\\t\\t\\t\\t{ \\n\\t\\t\\t\\t\\t\\tnew T() \\n\\t\\t\\t\\t\\t};\\n\\t\\t\\t\\tServiceBase.Run(ServicesToRun);\\n\\t\\t\\t}\\n\\n\\t\\t\\tprivate void StartServiceInteractive()\\n\\t\\t\\t{\\n\\t\\t\\t\\tif (ServiceInteractiveModeStarting != null)\\n\\t\\t\\t\\t\\tServiceInteractiveModeStarting(this, null);\\n\\t\\t\\t\\tConsole.Title = Assembly.GetEntryAssembly().FullName;\\n\\t\\t\\t\\tConsole.WriteLine(Assembly.GetEntryAssembly().FullName);\\n\\t\\t\\t\\tT service = new T();\\n\\n\\t\\t\\t\\tConsole.WriteLine();\\n\\t\\t\\t\\ttry\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tConsole.WriteLine();\\n\\t\\t\\t\\t\\tConsole.WriteLine(\\\"Calling StartService()\\\");\\n\\t\\t\\t\\t\\tConsole.WriteLine(\\\"-----------------------------------------------\\\");\\n\\n\\t\\t\\t\\t\\tservice.StartService(null);\\n\\n\\t\\t\\t\\t\\tConsole.WriteLine();\\n\\t\\t\\t\\t\\tConsole.WriteLine(\\\"Service running in foreground, press enter to exit...\\\");\\n\\t\\t\\t\\t\\tConsole.ReadLine();\\n\\n\\t\\t\\t\\t\\tConsole.WriteLine(\\\"Calling StopService()\\\");\\n\\t\\t\\t\\t\\tConsole.WriteLine(\\\"-----------------------------------------------\\\");\\n\\n\\t\\t\\t\\t\\tservice.StopService();\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tcatch (Exception ex)\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\tthrow;\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\nThis is the most simplest version doesn't take any arguments when starting the service. It's straight forward to add that functionality but we don't need it in the project I am in. So what does the `ServiceManager` do? It takes a `CustomServiceBase` type as the generic input, the it checks to see if a debugger is attached or not. What do we achieve with this? If you start it in debug mode in Visual Studio a debugger will be attach and we can take control over the execution, so in that case we call the `StartServiceInteractiveMode` which calls the `StartService` method in the `CustomServiceBase` class and then waits for some input before it calls the `StopService` method. Now you might ask how I know I have a console available? The answer is I don't, I'm using my own convention that the _ServiceImplementation_ project will be console application. To use this new simple library you created above your regular _ServiceImplementation_ project will look something like this:\\n\\n\\tnamespace ServiceImplementation\\n\\t{\\n\\t\\tpublic partial class YourService : CustomServiceBase\\n\\t\\t{\\n\\t\\t\\tpublic void OnStart(string[] args)\\n\\t\\t\\t{\\n\\t\\t\\t\\t// Code to run on start\\n\\t\\t\\t}\\n\\t\\t\\t\\n\\t\\t\\tpublic void OnStop()\\n\\t\\t\\t{\\n\\t\\t\\t\\t// Code to run on stop\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n\\nThis is no different compared to a regular service implementation class. The `Program` class will look something like this\\n\\n\\tnamespace ServiceImplementation\\n\\t{\\n\\t\\tpublic static void Main()\\n\\t\\t{\\n\\t\\t\\tvar serviceManager = new ServiceManager<ConversionService>();\\n\\t\\t\\tserviceManager.ServiceDefaultModeStarting += new ServiceManagerHandler(serviceManager_ServiceDefaultModeStarting);\\n\\t\\t\\tserviceManager.ServiceInteractiveModeStarting += new ServiceManagerHandler(serviceManager_ServiceInteractiveModeStarting);\\n\\t\\t\\tserviceManager.Run();\\n\\t\\t}\\n\\n\\t\\tstatic void serviceManager_ServiceInteractiveModeStarting(object sender, EventArgs args)\\n\\t\\t{\\n\\t\\t\\t// if you want to do something when the service is starting in interactive/debug mode\\n\\t\\t}\\n\\n\\t\\tstatic void serviceManager_ServiceDefaultModeStarting(object sender, EventArgs args)\\n\\t\\t{\\n\\t\\t\\t// if you want to do something when the service is starting in default mode\\n\\t\\t}\\n\\t}\\n\\nYou don't have to implement the two event methods if you don't need to but it could be good in logging purpose. \\n\\n**Note:** Remeber that the _ServiceImplementation_ project should be a console application and nothing else. \\n\\nSo that's basically all you need to make your windows services debuggable unde visual studio. If you have any problem or find it useful let me know.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>It could be hard to debug your windows services if you play by the book. If you do play by the book you need to install the windows service and then attach a debugger to the process to be able to debug the service, of course you might need to do that in some cases maybe to make sure that the service acts as it should under the account it is running. However, to debug the actual functionality it is always easier to just hit F5 in Visual Studio and start debugging and here is how you do that. First, create a <em>Service</em> project for your reusable code. The first file you should add is a windows service class, <code>CustomServiceBase</code>, that extends the <code>ServiceBase</code> class.</p>\n<pre><code>namespace Service\n{\n\tpublic class CustomServiceBase : ServiceBase\n\t{\n\t\tpublic void StartService(string[] args)\n\t\t{\n\t\t\tOnStart(args);\n\t\t}\n\n\t\tpublic void StopService()\n\t\t{\n\t\t\tOnStop();\n\t\t}\n\t}\n}\n</code></pre>\n<p>This will be your new <code>ServiceBase</code> class instead of <code>ServiceBase</code>. Basically all it do is adding two methods that is used later on so we don't have to use reflection to access the <code>OnStart</code> and the <code>OnStop</code> methods. The next file you need to add to your <em>Service</em> is the generic <code>ServiceManager</code> class that will acts as your <em>runner</em>.</p>\n<pre><code>namespace Service\n{\n\tpublic delegate void ServiceManagerHandler(object sender, EventArgs args);\n\n\tpublic class ServiceManager&lt;T&gt; where T : CustomServiceBase, new() \n\t{\n\t\tpublic event ServiceManagerHandler ServiceDefaultModeStarting;\n\t\tpublic event ServiceManagerHandler ServiceInteractiveModeStarting;\n\n\t\tpublic void Run()\n\t\t{\n\t\t\tif (!Debugger.IsAttached)\n\t\t\t{\n\t\t\t\tStartServiceDefault();\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tStartServiceInteractive();\n\t\t\t}\n\t\t}\n\n\t\tprivate void StartServiceDefault()\n\t\t{\n\t\t\tif (ServiceDefaultModeStarting != null)\n\t\t\t\tServiceDefaultModeStarting(this, null);\n\t\t\tServiceBase[] ServicesToRun;\n\t\t\tServicesToRun = new ServiceBase[] \n\t\t\t\t{ \n\t\t\t\t\tnew T() \n\t\t\t\t};\n\t\t\tServiceBase.Run(ServicesToRun);\n\t\t}\n\n\t\tprivate void StartServiceInteractive()\n\t\t{\n\t\t\tif (ServiceInteractiveModeStarting != null)\n\t\t\t\tServiceInteractiveModeStarting(this, null);\n\t\t\tConsole.Title = Assembly.GetEntryAssembly().FullName;\n\t\t\tConsole.WriteLine(Assembly.GetEntryAssembly().FullName);\n\t\t\tT service = new T();\n\n\t\t\tConsole.WriteLine();\n\t\t\ttry\n\t\t\t{\n\t\t\t\tConsole.WriteLine();\n\t\t\t\tConsole.WriteLine(&quot;Calling StartService()&quot;);\n\t\t\t\tConsole.WriteLine(&quot;-----------------------------------------------&quot;);\n\n\t\t\t\tservice.StartService(null);\n\n\t\t\t\tConsole.WriteLine();\n\t\t\t\tConsole.WriteLine(&quot;Service running in foreground, press enter to exit...&quot;);\n\t\t\t\tConsole.ReadLine();\n\n\t\t\t\tConsole.WriteLine(&quot;Calling StopService()&quot;);\n\t\t\t\tConsole.WriteLine(&quot;-----------------------------------------------&quot;);\n\n\t\t\t\tservice.StopService();\n\t\t\t}\n\t\t\tcatch (Exception ex)\n\t\t\t{\n\t\t\t\tthrow;\n\t\t\t}\n\t\t}\n\t}\n}\n</code></pre>\n<p>This is the most simplest version doesn't take any arguments when starting the service. It's straight forward to add that functionality but we don't need it in the project I am in. So what does the <code>ServiceManager</code> do? It takes a <code>CustomServiceBase</code> type as the generic input, the it checks to see if a debugger is attached or not. What do we achieve with this? If you start it in debug mode in Visual Studio a debugger will be attach and we can take control over the execution, so in that case we call the <code>StartServiceInteractiveMode</code> which calls the <code>StartService</code> method in the <code>CustomServiceBase</code> class and then waits for some input before it calls the <code>StopService</code> method. Now you might ask how I know I have a console available? The answer is I don't, I'm using my own convention that the <em>ServiceImplementation</em> project will be console application. To use this new simple library you created above your regular <em>ServiceImplementation</em> project will look something like this:</p>\n<pre><code>namespace ServiceImplementation\n{\n\tpublic partial class YourService : CustomServiceBase\n\t{\n\t\tpublic void OnStart(string[] args)\n\t\t{\n\t\t\t// Code to run on start\n\t\t}\n\t\t\n\t\tpublic void OnStop()\n\t\t{\n\t\t\t// Code to run on stop\n\t\t}\n\t}\n}\n</code></pre>\n<p>This is no different compared to a regular service implementation class. The <code>Program</code> class will look something like this</p>\n<pre><code>namespace ServiceImplementation\n{\n\tpublic static void Main()\n\t{\n\t\tvar serviceManager = new ServiceManager&lt;ConversionService&gt;();\n\t\tserviceManager.ServiceDefaultModeStarting += new ServiceManagerHandler(serviceManager_ServiceDefaultModeStarting);\n\t\tserviceManager.ServiceInteractiveModeStarting += new ServiceManagerHandler(serviceManager_ServiceInteractiveModeStarting);\n\t\tserviceManager.Run();\n\t}\n\n\tstatic void serviceManager_ServiceInteractiveModeStarting(object sender, EventArgs args)\n\t{\n\t\t// if you want to do something when the service is starting in interactive/debug mode\n\t}\n\n\tstatic void serviceManager_ServiceDefaultModeStarting(object sender, EventArgs args)\n\t{\n\t\t// if you want to do something when the service is starting in default mode\n\t}\n}\n</code></pre>\n<p>You don't have to implement the two event methods if you don't need to but it could be good in logging purpose.</p>\n<p><strong>Note:</strong> Remeber that the <em>ServiceImplementation</em> project should be a console application and nothing else.</p>\n<p>So that's basically all you need to make your windows services debuggable unde visual studio. If you have any problem or find it useful let me know.</p>\n<!--kg-card-end: markdown-->","comment_id":"12","plaintext":"It could be hard to debug your windows services if you play by the book. If you\ndo play by the book you need to install the windows service and then attach a\ndebugger to the process to be able to debug the service, of course you might\nneed to do that in some cases maybe to make sure that the service acts as it\nshould under the account it is running. However, to debug the actual\nfunctionality it is always easier to just hit F5 in Visual Studio and start\ndebugging and here is how you do that. First, create a Service project for your\nreusable code. The first file you should add is a windows service class, \nCustomServiceBase, that extends the ServiceBase class.\n\nnamespace Service\n{\n\tpublic class CustomServiceBase : ServiceBase\n\t{\n\t\tpublic void StartService(string[] args)\n\t\t{\n\t\t\tOnStart(args);\n\t\t}\n\n\t\tpublic void StopService()\n\t\t{\n\t\t\tOnStop();\n\t\t}\n\t}\n}\n\n\nThis will be your new ServiceBase class instead of ServiceBase. Basically all it\ndo is adding two methods that is used later on so we don't have to use\nreflection to access the OnStart and the OnStop methods. The next file you need\nto add to your Service is the generic ServiceManager class that will acts as\nyour runner.\n\nnamespace Service\n{\n\tpublic delegate void ServiceManagerHandler(object sender, EventArgs args);\n\n\tpublic class ServiceManager<T> where T : CustomServiceBase, new() \n\t{\n\t\tpublic event ServiceManagerHandler ServiceDefaultModeStarting;\n\t\tpublic event ServiceManagerHandler ServiceInteractiveModeStarting;\n\n\t\tpublic void Run()\n\t\t{\n\t\t\tif (!Debugger.IsAttached)\n\t\t\t{\n\t\t\t\tStartServiceDefault();\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tStartServiceInteractive();\n\t\t\t}\n\t\t}\n\n\t\tprivate void StartServiceDefault()\n\t\t{\n\t\t\tif (ServiceDefaultModeStarting != null)\n\t\t\t\tServiceDefaultModeStarting(this, null);\n\t\t\tServiceBase[] ServicesToRun;\n\t\t\tServicesToRun = new ServiceBase[] \n\t\t\t\t{ \n\t\t\t\t\tnew T() \n\t\t\t\t};\n\t\t\tServiceBase.Run(ServicesToRun);\n\t\t}\n\n\t\tprivate void StartServiceInteractive()\n\t\t{\n\t\t\tif (ServiceInteractiveModeStarting != null)\n\t\t\t\tServiceInteractiveModeStarting(this, null);\n\t\t\tConsole.Title = Assembly.GetEntryAssembly().FullName;\n\t\t\tConsole.WriteLine(Assembly.GetEntryAssembly().FullName);\n\t\t\tT service = new T();\n\n\t\t\tConsole.WriteLine();\n\t\t\ttry\n\t\t\t{\n\t\t\t\tConsole.WriteLine();\n\t\t\t\tConsole.WriteLine(\"Calling StartService()\");\n\t\t\t\tConsole.WriteLine(\"-----------------------------------------------\");\n\n\t\t\t\tservice.StartService(null);\n\n\t\t\t\tConsole.WriteLine();\n\t\t\t\tConsole.WriteLine(\"Service running in foreground, press enter to exit...\");\n\t\t\t\tConsole.ReadLine();\n\n\t\t\t\tConsole.WriteLine(\"Calling StopService()\");\n\t\t\t\tConsole.WriteLine(\"-----------------------------------------------\");\n\n\t\t\t\tservice.StopService();\n\t\t\t}\n\t\t\tcatch (Exception ex)\n\t\t\t{\n\t\t\t\tthrow;\n\t\t\t}\n\t\t}\n\t}\n}\n\n\nThis is the most simplest version doesn't take any arguments when starting the\nservice. It's straight forward to add that functionality but we don't need it in\nthe project I am in. So what does the ServiceManager do? It takes a \nCustomServiceBase type as the generic input, the it checks to see if a debugger\nis attached or not. What do we achieve with this? If you start it in debug mode\nin Visual Studio a debugger will be attach and we can take control over the\nexecution, so in that case we call the StartServiceInteractiveMode which calls\nthe StartService method in the CustomServiceBase class and then waits for some\ninput before it calls the StopService method. Now you might ask how I know I\nhave a console available? The answer is I don't, I'm using my own convention\nthat the ServiceImplementation project will be console application. To use this\nnew simple library you created above your regular ServiceImplementation project\nwill look something like this:\n\nnamespace ServiceImplementation\n{\n\tpublic partial class YourService : CustomServiceBase\n\t{\n\t\tpublic void OnStart(string[] args)\n\t\t{\n\t\t\t// Code to run on start\n\t\t}\n\t\t\n\t\tpublic void OnStop()\n\t\t{\n\t\t\t// Code to run on stop\n\t\t}\n\t}\n}\n\n\nThis is no different compared to a regular service implementation class. The \nProgram class will look something like this\n\nnamespace ServiceImplementation\n{\n\tpublic static void Main()\n\t{\n\t\tvar serviceManager = new ServiceManager<ConversionService>();\n\t\tserviceManager.ServiceDefaultModeStarting += new ServiceManagerHandler(serviceManager_ServiceDefaultModeStarting);\n\t\tserviceManager.ServiceInteractiveModeStarting += new ServiceManagerHandler(serviceManager_ServiceInteractiveModeStarting);\n\t\tserviceManager.Run();\n\t}\n\n\tstatic void serviceManager_ServiceInteractiveModeStarting(object sender, EventArgs args)\n\t{\n\t\t// if you want to do something when the service is starting in interactive/debug mode\n\t}\n\n\tstatic void serviceManager_ServiceDefaultModeStarting(object sender, EventArgs args)\n\t{\n\t\t// if you want to do something when the service is starting in default mode\n\t}\n}\n\n\nYou don't have to implement the two event methods if you don't need to but it\ncould be good in logging purpose.\n\nNote: Remeber that the ServiceImplementation project should be a console\napplication and nothing else.\n\nSo that's basically all you need to make your windows services debuggable unde\nvisual studio. If you have any problem or find it useful let me know.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:22:09.000Z","updated_at":"2014-03-01T09:22:31.000Z","published_at":"2012-10-25T07:22:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe040","uuid":"f0f36898-8068-4b27-9e7a-b1cd21e55767","title":"Generic data mapper for DataReader","slug":"generic-data-mapper-for-datareader","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Isn't it boring to write mappers to map data from a db to objects? Wouldn't it be nice if someone wrote one that is simple to use and works in most cases? I guess so and maybe there are several out there, but I wrote one myself anyway. I know a lot of people think you should use EF, linq to sql, nHibernate or some other ORM... but this is an existing project and I want to improve that as much as I could with the tools at hand without adding new frameworks, I'm just glad we are moving away from `DataSet`s :).\\n\\nThis is sort of an update version of my earlier post [Generic mapper for datareader](http://blog.tomasjansson.com/2010/10/generic-mapper-for-datareader/), but I figured I could do it better.\\n\\nFirst I want to say that the code is not optimized so if you have a really high volume of data access it might be slow, but for most situations this will most likely do just just fine. The implementation focus only on querying where you get a list of objects out from the query. I will be doing this top down so everything hopefully makes sense why I'm doing them.\\n\\nStep 1: I wan't to query a database, and I want to do it in the same way regarding to error handling and logging. So I decided on some query code that look something like this\\n\\n\\tpublic class MyDataAccessClass : BaseDataAccess, IMyDataAccessClass\\n\\t{\\n\\t\\tpublic IList<MyEntity> GetEntities(DateTime date)\\n\\t\\t{\\n\\t\\t\\tvar database = new DataBase(); // 1.\\n\\t\\t\\tvar dbCommand = database.GetStoredProcCommand(\\\"StoredProcName\\\"); // 2.\\n\\t\\t\\tdatabase.AddInParameter(dbCommand, \\\"FromDate\\\", DbType.DateTime, date); // 2.\\n\\t\\t\\tvar result = ExecuteDbCommand<MyEntity>(db, dbCommand, \\\"Property1\\\"); // 3.\\n\\t\\t\\treturn result;\\n\\t\\t}\\n\\t}\\n\\nSo what do we got here?\\n\\n1. We create our database object, could be any one just as the concept is the same. \\n2. We create our command, this is the reason this code is not abstracted since the command we want to execute depends on the result we want and we need the database object to create the command. Also, add parameters to command.\\n3. We got something that will execute the query for us and return a list of objects of the type we specifies. The last parameter (\\\"Property1\\\") is used to define properties that I don't want to map when doing the query.\\n\\nStep 2 will be to write the `ExecuteDbCommand<ReturnType>`. So to do that I implemented an abstract base class that all the data access classes can inherit from. It looks something like this:\\n\\n\\tpublic abstract class DataAccessBase\\n\\t{\\n\\t\\tprotected IList<T> ExecuteDbCommand<T>(Database db, DbCommand command, params string[] excludeProperties) where T : new()\\n\\t\\t{\\n\\t    \\tusing (Tracer myTracer = new Tracer(Constants.TraceLog))\\n\\t    \\t{\\n\\t\\t\\t\\ttry\\n\\t\\t\\t\\t{\\n\\t        \\t\\tvar reader = db.ExecuteReader(command); // 1.\\n\\t        \\t\\tvar mapper = new DataReaderMapper<T>(); // 2.\\n\\t\\t\\t\\t\\tvar result = mapper.MapListExcludeColumns(reader, excludeProperties); // 3. \\n\\t        \\t\\treturn result;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tcatch(Exception ex)\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t// Handle your exception\\n\\t\\t\\t\\t}\\n\\t    \\t}\\n\\t\\t}\\n\\t}\\n\\t\\nOk, now it starts to get interesting. As you can see this takes any type `T` that has a default constructor (by constraint). So what does the code actually do, let me break it down for you:\\n\\n1. Create the list where we want to store the objects we read from the `reader`.\\n2. Execute the databse command.\\n3. Call our mapper to map the reader to a list of objects.\\n\\nNo we are almost done, what we still have left is to implement our magic `DataReaderMapper` which will be our third and final step, it looks something like this:\\n\\n\\tpublic class DataReaderMapper<T> where T : new()\\n\\t{\\n\\t    public IList<T> MapListAll(IDataReader reader)\\n\\t    {\\n\\t        return MapListExcludeColumns(reader);\\n\\t    }\\n\\n\\t    public IList<T> MapListExcludeColumns(IDataReader reader, params string[] excludeColumns)\\n\\t    {\\n\\t        var listOfObjects = new List<T>();\\n\\t        while (reader.Read())\\n\\t        {\\n\\t            listOfObjects.Add(MapRowExclude(reader, excludeColumns));\\n\\t        }\\n\\t        return listOfObjects;\\n\\t    }\\n\\n\\t    public T MapRowExclude(IDataReader reader, params string[] columns)\\n\\t    {\\n\\t        return MapRow(reader, false, columns);\\n\\t    }\\n\\n\\t    public T MapRowInclude(IDataReader reader, params string[] columns)\\n\\t    {\\n\\t        return MapRow(reader, true, columns);\\n\\t    }\\n\\n\\t    public T MapRowAll(IDataReader reader)\\n\\t    {\\n\\t        return MapRow(reader, true, null);\\n\\t    }\\n\\n\\t    private T MapRow(IDataReader reader, bool includeColumns, params string[] columns)\\n\\t    {\\n\\t        T item = new T(); // 1. \\n\\t        var properties = GetPropertiesToMap(includeColumns, columns); // 2. \\n\\t        foreach (var property in properties)\\n\\t        {\\n\\t\\t\\t\\tint ordinal = reader.GetOrdinal(property.Name); // 3. \\n\\t\\t\\t\\tif(!reader.IsDBNull(ordinal)) // 4.\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t// if dbnull the property will get default value, \\n\\t\\t\\t\\t\\t// otherwise try to read the value from reader\\n\\t            \\tproperty.SetValue(item, reader[ordinal], null); // 5.\\n\\t\\t\\t\\t}\\n\\t        }\\n\\t        return item;\\n\\t    }\\n\\n\\t    public IEnumerable<System.Reflection.PropertyInfo> GetPropertiesToMap(bool includeColumns, string[] columns)\\n\\t    {\\n\\n\\t        var properties = typeof(T).GetProperties().Where(y => \\n\\t            (y.PropertyType.Equals(typeof(string)) || \\n\\t            y.PropertyType.Equals(typeof(byte[])) ||\\n\\t            y.PropertyType.IsValueType) && \\n\\t            (columns == null || (columns.Contains(y.Name) == includeColumns)));\\n\\t        return properties;\\n\\t    }\\n\\t}\\n\\nI have removed all the comments to shorten the post :). As you can see there are a bunch of overloads and variations, like including/excluding properties etc. If we start with the `MapListExcludeColumns`, which is the reason we ended up here. The basic thoughts are to create a list and an item for each read of the reader. When we add an item we call the `MapRowExclude` method, which calls the private method `MapRow`... and that is where all the magic is and of course I will walk you through it:\\n\\n1. Create the item that we will return.\\n2. Get the properties that we are going to map, if you look at the method it calls the method only returns value type properties, strings and byte arrays. That is because I want to keep it simple and assumes we have simple data objects. Also, it only returns those properties we want to have included.\\n3. Get the ordinal from the reader by the property name.\\n4. We only want to map if the reader actually has a value for that ordinal.\\n5. If it is not null set value using the `PropertyInfo` we have at this point.\\n\\n**Note that as of now it works only by the convention that the column names in the reader is the same as of the properties in the target object**. That is something I will fix by adding a mapping concept using a simple dictionary, but that is something I will do later on.\\n\\nI think this it, that's very easy to use mapper. As of now I think I am good. I will make a real project of this and it up on github when I get a hang of github :). If you find any errors or have any improvements suggestions please tell. \"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Isn't it boring to write mappers to map data from a db to objects? Wouldn't it be nice if someone wrote one that is simple to use and works in most cases? I guess so and maybe there are several out there, but I wrote one myself anyway. I know a lot of people think you should use EF, linq to sql, nHibernate or some other ORM... but this is an existing project and I want to improve that as much as I could with the tools at hand without adding new frameworks, I'm just glad we are moving away from <code>DataSet</code>s :).</p>\n<p>This is sort of an update version of my earlier post <a href=\"http://blog.tomasjansson.com/2010/10/generic-mapper-for-datareader/\">Generic mapper for datareader</a>, but I figured I could do it better.</p>\n<p>First I want to say that the code is not optimized so if you have a really high volume of data access it might be slow, but for most situations this will most likely do just just fine. The implementation focus only on querying where you get a list of objects out from the query. I will be doing this top down so everything hopefully makes sense why I'm doing them.</p>\n<p>Step 1: I wan't to query a database, and I want to do it in the same way regarding to error handling and logging. So I decided on some query code that look something like this</p>\n<pre><code>public class MyDataAccessClass : BaseDataAccess, IMyDataAccessClass\n{\n\tpublic IList&lt;MyEntity&gt; GetEntities(DateTime date)\n\t{\n\t\tvar database = new DataBase(); // 1.\n\t\tvar dbCommand = database.GetStoredProcCommand(&quot;StoredProcName&quot;); // 2.\n\t\tdatabase.AddInParameter(dbCommand, &quot;FromDate&quot;, DbType.DateTime, date); // 2.\n\t\tvar result = ExecuteDbCommand&lt;MyEntity&gt;(db, dbCommand, &quot;Property1&quot;); // 3.\n\t\treturn result;\n\t}\n}\n</code></pre>\n<p>So what do we got here?</p>\n<ol>\n<li>We create our database object, could be any one just as the concept is the same.</li>\n<li>We create our command, this is the reason this code is not abstracted since the command we want to execute depends on the result we want and we need the database object to create the command. Also, add parameters to command.</li>\n<li>We got something that will execute the query for us and return a list of objects of the type we specifies. The last parameter (&quot;Property1&quot;) is used to define properties that I don't want to map when doing the query.</li>\n</ol>\n<p>Step 2 will be to write the <code>ExecuteDbCommand&lt;ReturnType&gt;</code>. So to do that I implemented an abstract base class that all the data access classes can inherit from. It looks something like this:</p>\n<pre><code>public abstract class DataAccessBase\n{\n\tprotected IList&lt;T&gt; ExecuteDbCommand&lt;T&gt;(Database db, DbCommand command, params string[] excludeProperties) where T : new()\n\t{\n    \tusing (Tracer myTracer = new Tracer(Constants.TraceLog))\n    \t{\n\t\t\ttry\n\t\t\t{\n        \t\tvar reader = db.ExecuteReader(command); // 1.\n        \t\tvar mapper = new DataReaderMapper&lt;T&gt;(); // 2.\n\t\t\t\tvar result = mapper.MapListExcludeColumns(reader, excludeProperties); // 3. \n        \t\treturn result;\n\t\t\t}\n\t\t\tcatch(Exception ex)\n\t\t\t{\n\t\t\t\t// Handle your exception\n\t\t\t}\n    \t}\n\t}\n}\n</code></pre>\n<p>Ok, now it starts to get interesting. As you can see this takes any type <code>T</code> that has a default constructor (by constraint). So what does the code actually do, let me break it down for you:</p>\n<ol>\n<li>Create the list where we want to store the objects we read from the <code>reader</code>.</li>\n<li>Execute the databse command.</li>\n<li>Call our mapper to map the reader to a list of objects.</li>\n</ol>\n<p>No we are almost done, what we still have left is to implement our magic <code>DataReaderMapper</code> which will be our third and final step, it looks something like this:</p>\n<pre><code>public class DataReaderMapper&lt;T&gt; where T : new()\n{\n    public IList&lt;T&gt; MapListAll(IDataReader reader)\n    {\n        return MapListExcludeColumns(reader);\n    }\n\n    public IList&lt;T&gt; MapListExcludeColumns(IDataReader reader, params string[] excludeColumns)\n    {\n        var listOfObjects = new List&lt;T&gt;();\n        while (reader.Read())\n        {\n            listOfObjects.Add(MapRowExclude(reader, excludeColumns));\n        }\n        return listOfObjects;\n    }\n\n    public T MapRowExclude(IDataReader reader, params string[] columns)\n    {\n        return MapRow(reader, false, columns);\n    }\n\n    public T MapRowInclude(IDataReader reader, params string[] columns)\n    {\n        return MapRow(reader, true, columns);\n    }\n\n    public T MapRowAll(IDataReader reader)\n    {\n        return MapRow(reader, true, null);\n    }\n\n    private T MapRow(IDataReader reader, bool includeColumns, params string[] columns)\n    {\n        T item = new T(); // 1. \n        var properties = GetPropertiesToMap(includeColumns, columns); // 2. \n        foreach (var property in properties)\n        {\n\t\t\tint ordinal = reader.GetOrdinal(property.Name); // 3. \n\t\t\tif(!reader.IsDBNull(ordinal)) // 4.\n\t\t\t{\n\t\t\t\t// if dbnull the property will get default value, \n\t\t\t\t// otherwise try to read the value from reader\n            \tproperty.SetValue(item, reader[ordinal], null); // 5.\n\t\t\t}\n        }\n        return item;\n    }\n\n    public IEnumerable&lt;System.Reflection.PropertyInfo&gt; GetPropertiesToMap(bool includeColumns, string[] columns)\n    {\n\n        var properties = typeof(T).GetProperties().Where(y =&gt; \n            (y.PropertyType.Equals(typeof(string)) || \n            y.PropertyType.Equals(typeof(byte[])) ||\n            y.PropertyType.IsValueType) &amp;&amp; \n            (columns == null || (columns.Contains(y.Name) == includeColumns)));\n        return properties;\n    }\n}\n</code></pre>\n<p>I have removed all the comments to shorten the post :). As you can see there are a bunch of overloads and variations, like including/excluding properties etc. If we start with the <code>MapListExcludeColumns</code>, which is the reason we ended up here. The basic thoughts are to create a list and an item for each read of the reader. When we add an item we call the <code>MapRowExclude</code> method, which calls the private method <code>MapRow</code>... and that is where all the magic is and of course I will walk you through it:</p>\n<ol>\n<li>Create the item that we will return.</li>\n<li>Get the properties that we are going to map, if you look at the method it calls the method only returns value type properties, strings and byte arrays. That is because I want to keep it simple and assumes we have simple data objects. Also, it only returns those properties we want to have included.</li>\n<li>Get the ordinal from the reader by the property name.</li>\n<li>We only want to map if the reader actually has a value for that ordinal.</li>\n<li>If it is not null set value using the <code>PropertyInfo</code> we have at this point.</li>\n</ol>\n<p><strong>Note that as of now it works only by the convention that the column names in the reader is the same as of the properties in the target object</strong>. That is something I will fix by adding a mapping concept using a simple dictionary, but that is something I will do later on.</p>\n<p>I think this it, that's very easy to use mapper. As of now I think I am good. I will make a real project of this and it up on github when I get a hang of github :). If you find any errors or have any improvements suggestions please tell.</p>\n<!--kg-card-end: markdown-->","comment_id":"13","plaintext":"Isn't it boring to write mappers to map data from a db to objects? Wouldn't it\nbe nice if someone wrote one that is simple to use and works in most cases? I\nguess so and maybe there are several out there, but I wrote one myself anyway. I\nknow a lot of people think you should use EF, linq to sql, nHibernate or some\nother ORM... but this is an existing project and I want to improve that as much\nas I could with the tools at hand without adding new frameworks, I'm just glad\nwe are moving away from DataSets :).\n\nThis is sort of an update version of my earlier post Generic mapper for\ndatareader [http://blog.tomasjansson.com/2010/10/generic-mapper-for-datareader/]\n, but I figured I could do it better.\n\nFirst I want to say that the code is not optimized so if you have a really high\nvolume of data access it might be slow, but for most situations this will most\nlikely do just just fine. The implementation focus only on querying where you\nget a list of objects out from the query. I will be doing this top down so\neverything hopefully makes sense why I'm doing them.\n\nStep 1: I wan't to query a database, and I want to do it in the same way\nregarding to error handling and logging. So I decided on some query code that\nlook something like this\n\npublic class MyDataAccessClass : BaseDataAccess, IMyDataAccessClass\n{\n\tpublic IList<MyEntity> GetEntities(DateTime date)\n\t{\n\t\tvar database = new DataBase(); // 1.\n\t\tvar dbCommand = database.GetStoredProcCommand(\"StoredProcName\"); // 2.\n\t\tdatabase.AddInParameter(dbCommand, \"FromDate\", DbType.DateTime, date); // 2.\n\t\tvar result = ExecuteDbCommand<MyEntity>(db, dbCommand, \"Property1\"); // 3.\n\t\treturn result;\n\t}\n}\n\n\nSo what do we got here?\n\n 1. We create our database object, could be any one just as the concept is the\n    same.\n 2. We create our command, this is the reason this code is not abstracted since\n    the command we want to execute depends on the result we want and we need the\n    database object to create the command. Also, add parameters to command.\n 3. We got something that will execute the query for us and return a list of\n    objects of the type we specifies. The last parameter (\"Property1\") is used\n    to define properties that I don't want to map when doing the query.\n\nStep 2 will be to write the ExecuteDbCommand<ReturnType>. So to do that I\nimplemented an abstract base class that all the data access classes can inherit\nfrom. It looks something like this:\n\npublic abstract class DataAccessBase\n{\n\tprotected IList<T> ExecuteDbCommand<T>(Database db, DbCommand command, params string[] excludeProperties) where T : new()\n\t{\n    \tusing (Tracer myTracer = new Tracer(Constants.TraceLog))\n    \t{\n\t\t\ttry\n\t\t\t{\n        \t\tvar reader = db.ExecuteReader(command); // 1.\n        \t\tvar mapper = new DataReaderMapper<T>(); // 2.\n\t\t\t\tvar result = mapper.MapListExcludeColumns(reader, excludeProperties); // 3. \n        \t\treturn result;\n\t\t\t}\n\t\t\tcatch(Exception ex)\n\t\t\t{\n\t\t\t\t// Handle your exception\n\t\t\t}\n    \t}\n\t}\n}\n\n\nOk, now it starts to get interesting. As you can see this takes any type T that\nhas a default constructor (by constraint). So what does the code actually do,\nlet me break it down for you:\n\n 1. Create the list where we want to store the objects we read from the reader.\n 2. Execute the databse command.\n 3. Call our mapper to map the reader to a list of objects.\n\nNo we are almost done, what we still have left is to implement our magic \nDataReaderMapper which will be our third and final step, it looks something like\nthis:\n\npublic class DataReaderMapper<T> where T : new()\n{\n    public IList<T> MapListAll(IDataReader reader)\n    {\n        return MapListExcludeColumns(reader);\n    }\n\n    public IList<T> MapListExcludeColumns(IDataReader reader, params string[] excludeColumns)\n    {\n        var listOfObjects = new List<T>();\n        while (reader.Read())\n        {\n            listOfObjects.Add(MapRowExclude(reader, excludeColumns));\n        }\n        return listOfObjects;\n    }\n\n    public T MapRowExclude(IDataReader reader, params string[] columns)\n    {\n        return MapRow(reader, false, columns);\n    }\n\n    public T MapRowInclude(IDataReader reader, params string[] columns)\n    {\n        return MapRow(reader, true, columns);\n    }\n\n    public T MapRowAll(IDataReader reader)\n    {\n        return MapRow(reader, true, null);\n    }\n\n    private T MapRow(IDataReader reader, bool includeColumns, params string[] columns)\n    {\n        T item = new T(); // 1. \n        var properties = GetPropertiesToMap(includeColumns, columns); // 2. \n        foreach (var property in properties)\n        {\n\t\t\tint ordinal = reader.GetOrdinal(property.Name); // 3. \n\t\t\tif(!reader.IsDBNull(ordinal)) // 4.\n\t\t\t{\n\t\t\t\t// if dbnull the property will get default value, \n\t\t\t\t// otherwise try to read the value from reader\n            \tproperty.SetValue(item, reader[ordinal], null); // 5.\n\t\t\t}\n        }\n        return item;\n    }\n\n    public IEnumerable<System.Reflection.PropertyInfo> GetPropertiesToMap(bool includeColumns, string[] columns)\n    {\n\n        var properties = typeof(T).GetProperties().Where(y => \n            (y.PropertyType.Equals(typeof(string)) || \n            y.PropertyType.Equals(typeof(byte[])) ||\n            y.PropertyType.IsValueType) && \n            (columns == null || (columns.Contains(y.Name) == includeColumns)));\n        return properties;\n    }\n}\n\n\nI have removed all the comments to shorten the post :). As you can see there are\na bunch of overloads and variations, like including/excluding properties etc. If\nwe start with the MapListExcludeColumns, which is the reason we ended up here.\nThe basic thoughts are to create a list and an item for each read of the reader.\nWhen we add an item we call the MapRowExclude method, which calls the private\nmethod MapRow... and that is where all the magic is and of course I will walk\nyou through it:\n\n 1. Create the item that we will return.\n 2. Get the properties that we are going to map, if you look at the method it\n    calls the method only returns value type properties, strings and byte\n    arrays. That is because I want to keep it simple and assumes we have simple\n    data objects. Also, it only returns those properties we want to have\n    included.\n 3. Get the ordinal from the reader by the property name.\n 4. We only want to map if the reader actually has a value for that ordinal.\n 5. If it is not null set value using the PropertyInfo we have at this point.\n\nNote that as of now it works only by the convention that the column names in the\nreader is the same as of the properties in the target object. That is something\nI will fix by adding a mapping concept using a simple dictionary, but that is\nsomething I will do later on.\n\nI think this it, that's very easy to use mapper. As of now I think I am good. I\nwill make a real project of this and it up on github when I get a hang of github\n:). If you find any errors or have any improvements suggestions please tell.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:24:14.000Z","updated_at":"2014-03-01T09:24:35.000Z","published_at":"2012-10-25T07:24:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe041","uuid":"ce63ca82-8f0b-499d-98ed-1d61806109c5","title":"Split column in sql (t-sql)","slug":"split-column-in-sql-t-sql","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Ok, first of all you should never be in a situation where you need to split columns in a database if you ask me, that is, if you have done your job well. A sql database consists of tables and tables consists of columns so why join columns together and separate the values with ';'? Enough about that. The problem I got served was that there was such a column, comma separated, in the database and now I was asked to split that column into multiple columns, so how do you do that? There is actually two things you need to do:\\n\\n1. Split your existing column\\n2. Insert the new data in the table\\n<p></p>\\nTo split the column I used the following code that I found on [stackoverflow] [1]:\\n\\n    CREATE FUNCTION dbo.Split (@sep char(1), @s varchar(512))\\n    RETURNS table\\n    AS\\n    RETURN (\\n        WITH Pieces(pn, start, stop) AS (\\n          SELECT 1, 1, CHARINDEX(@sep, @s)\\n          UNION ALL\\n          SELECT pn + 1, stop + 1, CHARINDEX(@sep, @s, stop + 1)\\n          FROM Pieces\\n          WHERE stop > 0\\n        )\\n    SELECT pn,\\n    SUBSTRING(@s, start, CASE WHEN stop > 0 THEN stop-start ELSE 512 END) AS s\\n    FROM Pieces\\n    )\\n\\nThe function takes the string you want to split and the character you want to split on and returns a table, for example would the following input string: \\\"value1;value2\\\" be:\\n<table>\\n   <tr>\\n      <td>1</td>\\n      <td>value1</td>\\n   </tr>\\n   <tr>\\n      <td>2</td>\\n      <td>value2</td>\\n   </tr>\\n</table>\\n\\nThe next step is to get these values back in the table and to do that you need to do the following things:\\n\\n1. Create a cursor that loops over the ids in your table\\n2. For each id do a split, using the previous function, on the column that should be split and insert in a **temporary table** --> a lot of null values in the table (it's ok)\\n3. Group the temporary table and take the maximum value for each new column (the null values will disappear)\\n4. Update the original table with the new values.\\n\\nIn code it looks like this:\\n\\n\\t-- Create temporary table for holding the new columns together with the respective id\\n\\tCREATE TABLE #NewColumnsTable \\n\\t(\\n\\t         TableId int,\\n\\t         NewColumn1 varchar(2),\\n\\t         NewColumn2 varchar(7)\\n\\t);\\n\\n\\t-- Temporary variable used in loop\\n\\tDECLARE @TableId int;\\n\\n\\t-- Create cursor over all the ids in the table and open cursor\\n\\tDECLARE tableIdCursor CURSOR FOR\\n\\tSELECT TableId FROM YourTable;\\n\\topen tableIdCursor;\\n\\n\\t-- Initial fetch\\n\\tfetch next from tableIdCursor into @TableId;\\n\\n\\t-- Loop while we get a result from fetch\\n\\tWhile @@FETCH_STATUS = 0\\n\\tBEGIN\\n\\t         -- Update temporary table with splitted values\\n\\t         insert into #NewColumnsTable\\n\\t         SELECT @TableId as TableId\\n\\t         , CASE epb.pn\\n\\t                 when 1 then epb.s\\n\\t                 else null\\n\\t           end AS NewColumn1\\n\\t         , CASE epb.pn\\n\\t                 when 2 then epb.s\\n\\t                 else null\\n\\t           end AS NewColumn2\\n\\t         FROM  pgsa.split(';', \\n\\t                          (SELECT Column2Split FROM YourTable yt2 Where yt2.TableId = @TableId)) epb;\\n\\n\\t         -- Fetch the new besvarelseid\\n\\t         fetch next from tableIdCursor into @TableId;\\n\\tEND\\n\\n\\t-- Close and deallocate cursor since it is no longer in use\\n\\tclose tableIdCursor;\\n\\tdeallocate tableIdCursor;\\n\\tGO\\n\\n\\t-- Group table to get one row of values per id\\n\\tSELECT * INTO #GroupedNewColumnsTable\\n\\tFROM (SELECT TableId\\n\\t         , MAX(NewColumn1) as NewColumn1\\n\\t         , MAX(NewColumn2) as NewColumn2\\n\\tFROM #NewColumnsTable\\n\\tGROUP BY TableId) as t;\\n\\tGO\\n\\n\\t-- Update table with the values for the new columns\\n\\tUPDATE YourTable\\n\\tSET NewColumn1 = bu.NewColumn1,\\n\\t         NewColumn2 = bu.NewColumn2\\n\\tFROM YourTable b\\n\\tINNER JOIN #GroupedNewColumnsTable bu\\n\\tON b.TableId = bu.TableId;\\n\\tGO\\n\\n[1]: http://stackoverflow.com/questions/314824/t-sql-opposite-to-string-concatenation-how-to-split-string-into-multiple-recor\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Ok, first of all you should never be in a situation where you need to split columns in a database if you ask me, that is, if you have done your job well. A sql database consists of tables and tables consists of columns so why join columns together and separate the values with ';'? Enough about that. The problem I got served was that there was such a column, comma separated, in the database and now I was asked to split that column into multiple columns, so how do you do that? There is actually two things you need to do:</p>\n<ol>\n<li>Split your existing column</li>\n<li>Insert the new data in the table</li>\n</ol>\n<p></p>\nTo split the column I used the following code that I found on [stackoverflow] [1]:\n<pre><code>CREATE FUNCTION dbo.Split (@sep char(1), @s varchar(512))\nRETURNS table\nAS\nRETURN (\n    WITH Pieces(pn, start, stop) AS (\n      SELECT 1, 1, CHARINDEX(@sep, @s)\n      UNION ALL\n      SELECT pn + 1, stop + 1, CHARINDEX(@sep, @s, stop + 1)\n      FROM Pieces\n      WHERE stop &gt; 0\n    )\nSELECT pn,\nSUBSTRING(@s, start, CASE WHEN stop &gt; 0 THEN stop-start ELSE 512 END) AS s\nFROM Pieces\n)\n</code></pre>\n<p>The function takes the string you want to split and the character you want to split on and returns a table, for example would the following input string: &quot;value1;value2&quot; be:</p>\n<table>\n   <tr>\n      <td>1</td>\n      <td>value1</td>\n   </tr>\n   <tr>\n      <td>2</td>\n      <td>value2</td>\n   </tr>\n</table>\n<p>The next step is to get these values back in the table and to do that you need to do the following things:</p>\n<ol>\n<li>Create a cursor that loops over the ids in your table</li>\n<li>For each id do a split, using the previous function, on the column that should be split and insert in a <strong>temporary table</strong> --&gt; a lot of null values in the table (it's ok)</li>\n<li>Group the temporary table and take the maximum value for each new column (the null values will disappear)</li>\n<li>Update the original table with the new values.</li>\n</ol>\n<p>In code it looks like this:</p>\n<pre><code>-- Create temporary table for holding the new columns together with the respective id\nCREATE TABLE #NewColumnsTable \n(\n         TableId int,\n         NewColumn1 varchar(2),\n         NewColumn2 varchar(7)\n);\n\n-- Temporary variable used in loop\nDECLARE @TableId int;\n\n-- Create cursor over all the ids in the table and open cursor\nDECLARE tableIdCursor CURSOR FOR\nSELECT TableId FROM YourTable;\nopen tableIdCursor;\n\n-- Initial fetch\nfetch next from tableIdCursor into @TableId;\n\n-- Loop while we get a result from fetch\nWhile @@FETCH_STATUS = 0\nBEGIN\n         -- Update temporary table with splitted values\n         insert into #NewColumnsTable\n         SELECT @TableId as TableId\n         , CASE epb.pn\n                 when 1 then epb.s\n                 else null\n           end AS NewColumn1\n         , CASE epb.pn\n                 when 2 then epb.s\n                 else null\n           end AS NewColumn2\n         FROM  pgsa.split(';', \n                          (SELECT Column2Split FROM YourTable yt2 Where yt2.TableId = @TableId)) epb;\n\n         -- Fetch the new besvarelseid\n         fetch next from tableIdCursor into @TableId;\nEND\n\n-- Close and deallocate cursor since it is no longer in use\nclose tableIdCursor;\ndeallocate tableIdCursor;\nGO\n\n-- Group table to get one row of values per id\nSELECT * INTO #GroupedNewColumnsTable\nFROM (SELECT TableId\n         , MAX(NewColumn1) as NewColumn1\n         , MAX(NewColumn2) as NewColumn2\nFROM #NewColumnsTable\nGROUP BY TableId) as t;\nGO\n\n-- Update table with the values for the new columns\nUPDATE YourTable\nSET NewColumn1 = bu.NewColumn1,\n         NewColumn2 = bu.NewColumn2\nFROM YourTable b\nINNER JOIN #GroupedNewColumnsTable bu\nON b.TableId = bu.TableId;\nGO\n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"14","plaintext":"Ok, first of all you should never be in a situation where you need to split\ncolumns in a database if you ask me, that is, if you have done your job well. A\nsql database consists of tables and tables consists of columns so why join\ncolumns together and separate the values with ';'? Enough about that. The\nproblem I got served was that there was such a column, comma separated, in the\ndatabase and now I was asked to split that column into multiple columns, so how\ndo you do that? There is actually two things you need to do:\n\n 1. Split your existing column\n 2. Insert the new data in the table\n\n\n\nTo split the column I used the following code that I found on [stackoverflow]\n[1]:CREATE FUNCTION dbo.Split (@sep char(1), @s varchar(512))\nRETURNS table\nAS\nRETURN (\n    WITH Pieces(pn, start, stop) AS (\n      SELECT 1, 1, CHARINDEX(@sep, @s)\n      UNION ALL\n      SELECT pn + 1, stop + 1, CHARINDEX(@sep, @s, stop + 1)\n      FROM Pieces\n      WHERE stop > 0\n    )\nSELECT pn,\nSUBSTRING(@s, start, CASE WHEN stop > 0 THEN stop-start ELSE 512 END) AS s\nFROM Pieces\n)\n\n\nThe function takes the string you want to split and the character you want to\nsplit on and returns a table, for example would the following input string:\n\"value1;value2\" be:\n\n1 value1 2 value2 The next step is to get these values back in the table and to\ndo that you need to do the following things:\n\n 1. Create a cursor that loops over the ids in your table\n 2. For each id do a split, using the previous function, on the column that\n    should be split and insert in a temporary table --> a lot of null values in\n    the table (it's ok)\n 3. Group the temporary table and take the maximum value for each new column\n    (the null values will disappear)\n 4. Update the original table with the new values.\n\nIn code it looks like this:\n\n-- Create temporary table for holding the new columns together with the respective id\nCREATE TABLE #NewColumnsTable \n(\n         TableId int,\n         NewColumn1 varchar(2),\n         NewColumn2 varchar(7)\n);\n\n-- Temporary variable used in loop\nDECLARE @TableId int;\n\n-- Create cursor over all the ids in the table and open cursor\nDECLARE tableIdCursor CURSOR FOR\nSELECT TableId FROM YourTable;\nopen tableIdCursor;\n\n-- Initial fetch\nfetch next from tableIdCursor into @TableId;\n\n-- Loop while we get a result from fetch\nWhile @@FETCH_STATUS = 0\nBEGIN\n         -- Update temporary table with splitted values\n         insert into #NewColumnsTable\n         SELECT @TableId as TableId\n         , CASE epb.pn\n                 when 1 then epb.s\n                 else null\n           end AS NewColumn1\n         , CASE epb.pn\n                 when 2 then epb.s\n                 else null\n           end AS NewColumn2\n         FROM  pgsa.split(';', \n                          (SELECT Column2Split FROM YourTable yt2 Where yt2.TableId = @TableId)) epb;\n\n         -- Fetch the new besvarelseid\n         fetch next from tableIdCursor into @TableId;\nEND\n\n-- Close and deallocate cursor since it is no longer in use\nclose tableIdCursor;\ndeallocate tableIdCursor;\nGO\n\n-- Group table to get one row of values per id\nSELECT * INTO #GroupedNewColumnsTable\nFROM (SELECT TableId\n         , MAX(NewColumn1) as NewColumn1\n         , MAX(NewColumn2) as NewColumn2\nFROM #NewColumnsTable\nGROUP BY TableId) as t;\nGO\n\n-- Update table with the values for the new columns\nUPDATE YourTable\nSET NewColumn1 = bu.NewColumn1,\n         NewColumn2 = bu.NewColumn2\nFROM YourTable b\nINNER JOIN #GroupedNewColumnsTable bu\nON b.TableId = bu.TableId;\nGO","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:25:38.000Z","updated_at":"2014-03-01T09:26:08.000Z","published_at":"2012-10-25T07:25:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe042","uuid":"5a6c3f64-cc93-4246-988e-5e0f4af4bac3","title":"Automatic creation of repository when using gitolite","slug":"automatic_creation_of_repository_when_using_gitolite","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The last couple of days I've been playing around with git and setup my own git server using first gitosis and then gitolite. When playing around I created a lot of different test repositories to make sure everything work. To stop repeating myself I created three minor bash functions to make it easier to add repositories in gitocline. The script I ended up with are the following:\\n\\n    ar() {\\n        if [ \\\"$1\\\" == \\\"\\\" ] ; then echo \\\"[E] One arg is needed!\\\"; return 1; \\n\\telse\\n            cd ~/<some path to your>/gitolite-admin/conf\\n            echo -e \\\"\\\" >> gitolite.conf\\n            echo -e \\\"repo ${1}\\\" >> gitolite.conf\\n            echo -e \\\"RW+ = <your gitolite username>\\\" >> gitolite.conf\\n            cd ..\\n            git commit -am \\\"Added repo ${1}\\\"\\n            git push\\n        fi \\t\\n    }\\n\\n    sr() {\\n        if [ \\\"$1\\\" == \\\"\\\" ] ; then echo \\\"[E] One arg is needed!\\\"; return 1; \\n        else\\n            mkdir <path to where you have your repositories>/${1}\\n            cd <path to where you have your repositories>/${1}\\n            git init\\n            git remote add origin gitolite@e2n.no:${1}\\n            touch README\\n            git add .\\n            git commit -am \\\"Initial commit\\\"\\n            git push origin master\\n        fi\\n    }\\n\\n    cr() {\\n        if [ \\\"$1\\\" == \\\"\\\" ] ; then echo \\\"[E] One arg is needed!\\\"; return 1; \\n        else\\n            ar ${1}\\n            sr ${1}\\n        fi\\n    }\\n\\t\\t\\nIt is somewhat static code, but I'm not that likely to change where I have the folders on my local computer anyway. I should probably mention that these three functions are stored in the .bashrc file and that I assumes you have checked out the gitolite-admin repository. Let us walk through the three functions.\\n\\n - ar (short for add repository): will navigate to the configuration folder for gitolite and add the repository plus read and write access for a user name (should probably use a group instead). It will also commit the changes and push them to the server to create the repository.\\n - sr (short for set up repository): will first create a folder for the repository, then initialize git in that folder, add a remote to your new repository and make an initial commit and push.\\n - cr (short for create repository): will execute both the above functions.\\n\\nThis will allow us to create a repository and set it up on our local machine by running\\n\\n    $ cr repository_name\\n\\nThat's all.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The last couple of days I've been playing around with git and setup my own git server using first gitosis and then gitolite. When playing around I created a lot of different test repositories to make sure everything work. To stop repeating myself I created three minor bash functions to make it easier to add repositories in gitocline. The script I ended up with are the following:</p>\n<pre><code>ar() {\n    if [ &quot;$1&quot; == &quot;&quot; ] ; then echo &quot;[E] One arg is needed!&quot;; return 1; \nelse\n        cd ~/&lt;some path to your&gt;/gitolite-admin/conf\n        echo -e &quot;&quot; &gt;&gt; gitolite.conf\n        echo -e &quot;repo ${1}&quot; &gt;&gt; gitolite.conf\n        echo -e &quot;RW+ = &lt;your gitolite username&gt;&quot; &gt;&gt; gitolite.conf\n        cd ..\n        git commit -am &quot;Added repo ${1}&quot;\n        git push\n    fi \t\n}\n\nsr() {\n    if [ &quot;$1&quot; == &quot;&quot; ] ; then echo &quot;[E] One arg is needed!&quot;; return 1; \n    else\n        mkdir &lt;path to where you have your repositories&gt;/${1}\n        cd &lt;path to where you have your repositories&gt;/${1}\n        git init\n        git remote add origin gitolite@e2n.no:${1}\n        touch README\n        git add .\n        git commit -am &quot;Initial commit&quot;\n        git push origin master\n    fi\n}\n\ncr() {\n    if [ &quot;$1&quot; == &quot;&quot; ] ; then echo &quot;[E] One arg is needed!&quot;; return 1; \n    else\n        ar ${1}\n        sr ${1}\n    fi\n}\n</code></pre>\n<p>It is somewhat static code, but I'm not that likely to change where I have the folders on my local computer anyway. I should probably mention that these three functions are stored in the .bashrc file and that I assumes you have checked out the gitolite-admin repository. Let us walk through the three functions.</p>\n<ul>\n<li>ar (short for add repository): will navigate to the configuration folder for gitolite and add the repository plus read and write access for a user name (should probably use a group instead). It will also commit the changes and push them to the server to create the repository.</li>\n<li>sr (short for set up repository): will first create a folder for the repository, then initialize git in that folder, add a remote to your new repository and make an initial commit and push.</li>\n<li>cr (short for create repository): will execute both the above functions.</li>\n</ul>\n<p>This will allow us to create a repository and set it up on our local machine by running</p>\n<pre><code>$ cr repository_name\n</code></pre>\n<p>That's all.</p>\n<!--kg-card-end: markdown-->","comment_id":"15","plaintext":"The last couple of days I've been playing around with git and setup my own git\nserver using first gitosis and then gitolite. When playing around I created a\nlot of different test repositories to make sure everything work. To stop\nrepeating myself I created three minor bash functions to make it easier to add\nrepositories in gitocline. The script I ended up with are the following:\n\nar() {\n    if [ \"$1\" == \"\" ] ; then echo \"[E] One arg is needed!\"; return 1; \nelse\n        cd ~/<some path to your>/gitolite-admin/conf\n        echo -e \"\" >> gitolite.conf\n        echo -e \"repo ${1}\" >> gitolite.conf\n        echo -e \"RW+ = <your gitolite username>\" >> gitolite.conf\n        cd ..\n        git commit -am \"Added repo ${1}\"\n        git push\n    fi \t\n}\n\nsr() {\n    if [ \"$1\" == \"\" ] ; then echo \"[E] One arg is needed!\"; return 1; \n    else\n        mkdir <path to where you have your repositories>/${1}\n        cd <path to where you have your repositories>/${1}\n        git init\n        git remote add origin gitolite@e2n.no:${1}\n        touch README\n        git add .\n        git commit -am \"Initial commit\"\n        git push origin master\n    fi\n}\n\ncr() {\n    if [ \"$1\" == \"\" ] ; then echo \"[E] One arg is needed!\"; return 1; \n    else\n        ar ${1}\n        sr ${1}\n    fi\n}\n\n\nIt is somewhat static code, but I'm not that likely to change where I have the\nfolders on my local computer anyway. I should probably mention that these three\nfunctions are stored in the .bashrc file and that I assumes you have checked out\nthe gitolite-admin repository. Let us walk through the three functions.\n\n * ar (short for add repository): will navigate to the configuration folder for\n   gitolite and add the repository plus read and write access for a user name\n   (should probably use a group instead). It will also commit the changes and\n   push them to the server to create the repository.\n * sr (short for set up repository): will first create a folder for the\n   repository, then initialize git in that folder, add a remote to your new\n   repository and make an initial commit and push.\n * cr (short for create repository): will execute both the above functions.\n\nThis will allow us to create a repository and set it up on our local machine by\nrunning\n\n$ cr repository_name\n\n\nThat's all.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:26:55.000Z","updated_at":"2014-03-01T09:27:22.000Z","published_at":"2012-10-25T07:26:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe043","uuid":"d4e5fbb4-47b9-4807-b1a7-733fa7810359","title":"Linq tip of the day - Aggregate","slug":"linq-tip-of-the-day-aggregate","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"One thing that I have noticed is that people tend to not be aware of the [`Aggregate`](http://msdn.microsoft.com/en-us/library/system.linq.enumerable.aggregate.aspx) function that exist for linq. If you learn how to use the `Aggregate` function it will be useful for you in many scenarios. One scenario where I have found it useful many times is in logging scenarios, especially when you want to log some output from a list or array as the example below:\\n\\n    var list = new string[]{\\\"These\\\", \\\"values\\\", \\\"need\\\", \\\"to\\\", \\\"be\\\", \\\"logged\\\", \\\"in\\\", \\\"reverse\\\"};\\n    var output = list.Aggregate((aggregate, next) => next + \\\" \\\" + aggregate);\\n    Console.WriteLine(output); //prints \\\"reverse in logged be to need values These\\\"\\n\\nAnother useful scenario is when you want to do some mathematical calculations on a series of numbers with a starting condition, like a checking account in the example below:\\n\\n    var currentBalance = 100000;\\n    var withdrawals = new int[] { 1000, 2034, 16243, 2423, 44234 };\\n    var updatedBalance = withdrawals.Aggregate(currentBalance, (aggregate, next) => currentBalance - next);\\n    Console.WriteLine(updatedBalance);\\n\\nThe following is a little bit more complicated but I think it really shows the strength of the `Aggregate` function. Let say you have some kind of rule engine that validates some kind of object. Now you what to run all these rules against your object, and at the same time update the object so it stores a collection of violated rules. I think that a realistict scenario and the following code show how you can solve the validation using `Aggregate`.\\n\\n    class Program\\n    {\\n        static void Main(string[] args)\\n        {\\n            var rules = new List<Rule> { new RemoveARule(), new RemoveXRule(), new RemoveZRule() };\\n            var input = new MySuperSecretEntity();\\n            var output = rules.Aggregate(input, (aggregate, rule) => rule.Validate(aggregate));\\n            // input.ViolatedRules now contain a set of violated rules\\n            var violatedRules = input.ViolatedRules.Aggregate(\\\"\\\", (aggregate, next) => aggregate + \\\", \\\" + next.ToString());\\n\\n            Console.WriteLine(violatedRules); // Prints the type of the two violated rules\\n            Console.ReadLine();\\n        }\\n    }\\n\\n    class MySuperSecretEntity\\n    {\\n        public string DoILookFine = \\\"This is the string that will be validated against a set of rules!!!\\\";\\n        public List<Rule> ViolatedRules { get; set; }\\n\\n        public MySuperSecretEntity()\\n        {\\n            ViolatedRules = new List<Rule>();\\n        }\\n    }\\n\\n    abstract class Rule\\n    {\\n        protected string MustContainLetter { get; set; }\\n\\n        public MySuperSecretEntity Validate(MySuperSecretEntity input)\\n        {\\n            var isValid = input.DoILookFine.Contains(MustContainLetter);\\n            if (!isValid) input.ViolatedRules.Add(this);\\n            return input;\\n        }\\n    }\\n\\n    class RemoveARule : Rule\\n    {\\n        public RemoveARule()\\n        {\\n            MustContainLetter = \\\"a\\\";\\n        }\\n    }\\n\\n    class RemoveXRule : Rule\\n    {\\n        public RemoveXRule()\\n        {\\n            MustContainLetter = \\\"x\\\";\\n        }\\n    }\\n\\n    class RemoveZRule : Rule\\n    {\\n        public RemoveZRule()\\n        {\\n            MustContainLetter = \\\"z\\\";\\n        }\\n    }\\n\\nThat was my .Net/linq tip of the day.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>One thing that I have noticed is that people tend to not be aware of the <a href=\"http://msdn.microsoft.com/en-us/library/system.linq.enumerable.aggregate.aspx\"><code>Aggregate</code></a> function that exist for linq. If you learn how to use the <code>Aggregate</code> function it will be useful for you in many scenarios. One scenario where I have found it useful many times is in logging scenarios, especially when you want to log some output from a list or array as the example below:</p>\n<pre><code>var list = new string[]{&quot;These&quot;, &quot;values&quot;, &quot;need&quot;, &quot;to&quot;, &quot;be&quot;, &quot;logged&quot;, &quot;in&quot;, &quot;reverse&quot;};\nvar output = list.Aggregate((aggregate, next) =&gt; next + &quot; &quot; + aggregate);\nConsole.WriteLine(output); //prints &quot;reverse in logged be to need values These&quot;\n</code></pre>\n<p>Another useful scenario is when you want to do some mathematical calculations on a series of numbers with a starting condition, like a checking account in the example below:</p>\n<pre><code>var currentBalance = 100000;\nvar withdrawals = new int[] { 1000, 2034, 16243, 2423, 44234 };\nvar updatedBalance = withdrawals.Aggregate(currentBalance, (aggregate, next) =&gt; currentBalance - next);\nConsole.WriteLine(updatedBalance);\n</code></pre>\n<p>The following is a little bit more complicated but I think it really shows the strength of the <code>Aggregate</code> function. Let say you have some kind of rule engine that validates some kind of object. Now you what to run all these rules against your object, and at the same time update the object so it stores a collection of violated rules. I think that a realistict scenario and the following code show how you can solve the validation using <code>Aggregate</code>.</p>\n<pre><code>class Program\n{\n    static void Main(string[] args)\n    {\n        var rules = new List&lt;Rule&gt; { new RemoveARule(), new RemoveXRule(), new RemoveZRule() };\n        var input = new MySuperSecretEntity();\n        var output = rules.Aggregate(input, (aggregate, rule) =&gt; rule.Validate(aggregate));\n        // input.ViolatedRules now contain a set of violated rules\n        var violatedRules = input.ViolatedRules.Aggregate(&quot;&quot;, (aggregate, next) =&gt; aggregate + &quot;, &quot; + next.ToString());\n\n        Console.WriteLine(violatedRules); // Prints the type of the two violated rules\n        Console.ReadLine();\n    }\n}\n\nclass MySuperSecretEntity\n{\n    public string DoILookFine = &quot;This is the string that will be validated against a set of rules!!!&quot;;\n    public List&lt;Rule&gt; ViolatedRules { get; set; }\n\n    public MySuperSecretEntity()\n    {\n        ViolatedRules = new List&lt;Rule&gt;();\n    }\n}\n\nabstract class Rule\n{\n    protected string MustContainLetter { get; set; }\n\n    public MySuperSecretEntity Validate(MySuperSecretEntity input)\n    {\n        var isValid = input.DoILookFine.Contains(MustContainLetter);\n        if (!isValid) input.ViolatedRules.Add(this);\n        return input;\n    }\n}\n\nclass RemoveARule : Rule\n{\n    public RemoveARule()\n    {\n        MustContainLetter = &quot;a&quot;;\n    }\n}\n\nclass RemoveXRule : Rule\n{\n    public RemoveXRule()\n    {\n        MustContainLetter = &quot;x&quot;;\n    }\n}\n\nclass RemoveZRule : Rule\n{\n    public RemoveZRule()\n    {\n        MustContainLetter = &quot;z&quot;;\n    }\n}\n</code></pre>\n<p>That was my .Net/linq tip of the day.</p>\n<!--kg-card-end: markdown-->","comment_id":"16","plaintext":"One thing that I have noticed is that people tend to not be aware of the \nAggregate\n[http://msdn.microsoft.com/en-us/library/system.linq.enumerable.aggregate.aspx] \nfunction that exist for linq. If you learn how to use the Aggregate function it\nwill be useful for you in many scenarios. One scenario where I have found it\nuseful many times is in logging scenarios, especially when you want to log some\noutput from a list or array as the example below:\n\nvar list = new string[]{\"These\", \"values\", \"need\", \"to\", \"be\", \"logged\", \"in\", \"reverse\"};\nvar output = list.Aggregate((aggregate, next) => next + \" \" + aggregate);\nConsole.WriteLine(output); //prints \"reverse in logged be to need values These\"\n\n\nAnother useful scenario is when you want to do some mathematical calculations on\na series of numbers with a starting condition, like a checking account in the\nexample below:\n\nvar currentBalance = 100000;\nvar withdrawals = new int[] { 1000, 2034, 16243, 2423, 44234 };\nvar updatedBalance = withdrawals.Aggregate(currentBalance, (aggregate, next) => currentBalance - next);\nConsole.WriteLine(updatedBalance);\n\n\nThe following is a little bit more complicated but I think it really shows the\nstrength of the Aggregate function. Let say you have some kind of rule engine\nthat validates some kind of object. Now you what to run all these rules against\nyour object, and at the same time update the object so it stores a collection of\nviolated rules. I think that a realistict scenario and the following code show\nhow you can solve the validation using Aggregate.\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        var rules = new List<Rule> { new RemoveARule(), new RemoveXRule(), new RemoveZRule() };\n        var input = new MySuperSecretEntity();\n        var output = rules.Aggregate(input, (aggregate, rule) => rule.Validate(aggregate));\n        // input.ViolatedRules now contain a set of violated rules\n        var violatedRules = input.ViolatedRules.Aggregate(\"\", (aggregate, next) => aggregate + \", \" + next.ToString());\n\n        Console.WriteLine(violatedRules); // Prints the type of the two violated rules\n        Console.ReadLine();\n    }\n}\n\nclass MySuperSecretEntity\n{\n    public string DoILookFine = \"This is the string that will be validated against a set of rules!!!\";\n    public List<Rule> ViolatedRules { get; set; }\n\n    public MySuperSecretEntity()\n    {\n        ViolatedRules = new List<Rule>();\n    }\n}\n\nabstract class Rule\n{\n    protected string MustContainLetter { get; set; }\n\n    public MySuperSecretEntity Validate(MySuperSecretEntity input)\n    {\n        var isValid = input.DoILookFine.Contains(MustContainLetter);\n        if (!isValid) input.ViolatedRules.Add(this);\n        return input;\n    }\n}\n\nclass RemoveARule : Rule\n{\n    public RemoveARule()\n    {\n        MustContainLetter = \"a\";\n    }\n}\n\nclass RemoveXRule : Rule\n{\n    public RemoveXRule()\n    {\n        MustContainLetter = \"x\";\n    }\n}\n\nclass RemoveZRule : Rule\n{\n    public RemoveZRule()\n    {\n        MustContainLetter = \"z\";\n    }\n}\n\n\nThat was my .Net/linq tip of the day.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:28:00.000Z","updated_at":"2014-03-01T09:28:23.000Z","published_at":"2012-10-25T07:28:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe044","uuid":"824e8d15-156d-48e5-879d-f893bf364d62","title":"How to start IIS Express with PowerShell","slug":"how-to-start-iis-express-with-powershell","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I had a presentation about [SignalR](http://signalr.net/) last week at [BEKK](http://www.bekk.no). In the presentation I wanted to show what a scale out scenario could look like, and to do that I need at least two instances of a web application. Using the script below I was able to fire up two instances of the same application on different ports in IIS Express.\\n\\n\\tparam( \\n\\t\\t[string] $port = $(throw \\\"Port is required.\\\")\\n\\t)\\n\\t$iisExpressExe = '\\\"c:\\\\Program Files (x86)\\\\IIS Express\\\\iisexpress.exe\\\"'\\n\\t$path = (Resolve-path .)\\n\\tWrite-Host $path\\n\\tWrite-host \\\"Starting site on port: $port\\\"\\n\\t$params = \\\"/port:$port /path:$path\\\"\\n\\t$command = \\\"$iisExpressExe $params\\\"\\n\\tcmd /c start cmd /k \\\"$command\\\"\\n\\tStart-Sleep -m 1000\\n\\tWrite-Host \\\"Site started\\\"\\n\\nThat was all I wanted to share today. Hope someone find it useful.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I had a presentation about <a href=\"http://signalr.net/\">SignalR</a> last week at <a href=\"http://www.bekk.no\">BEKK</a>. In the presentation I wanted to show what a scale out scenario could look like, and to do that I need at least two instances of a web application. Using the script below I was able to fire up two instances of the same application on different ports in IIS Express.</p>\n<pre><code>param( \n\t[string] $port = $(throw &quot;Port is required.&quot;)\n)\n$iisExpressExe = '&quot;c:\\Program Files (x86)\\IIS Express\\iisexpress.exe&quot;'\n$path = (Resolve-path .)\nWrite-Host $path\nWrite-host &quot;Starting site on port: $port&quot;\n$params = &quot;/port:$port /path:$path&quot;\n$command = &quot;$iisExpressExe $params&quot;\ncmd /c start cmd /k &quot;$command&quot;\nStart-Sleep -m 1000\nWrite-Host &quot;Site started&quot;\n</code></pre>\n<p>That was all I wanted to share today. Hope someone find it useful.</p>\n<!--kg-card-end: markdown-->","comment_id":"17","plaintext":"I had a presentation about SignalR [http://signalr.net/] last week at BEKK\n[http://www.bekk.no]. In the presentation I wanted to show what a scale out\nscenario could look like, and to do that I need at least two instances of a web\napplication. Using the script below I was able to fire up two instances of the\nsame application on different ports in IIS Express.\n\nparam( \n\t[string] $port = $(throw \"Port is required.\")\n)\n$iisExpressExe = '\"c:\\Program Files (x86)\\IIS Express\\iisexpress.exe\"'\n$path = (Resolve-path .)\nWrite-Host $path\nWrite-host \"Starting site on port: $port\"\n$params = \"/port:$port /path:$path\"\n$command = \"$iisExpressExe $params\"\ncmd /c start cmd /k \"$command\"\nStart-Sleep -m 1000\nWrite-Host \"Site started\"\n\n\nThat was all I wanted to share today. Hope someone find it useful.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:29:04.000Z","updated_at":"2014-03-01T09:29:31.000Z","published_at":"2012-11-23T09:29:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe045","uuid":"a925bc45-a663-4790-b376-7f80bdd3e552","title":"NServiceBus course summary","slug":"nservicebus-course-summary","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Last week I've been attending the course [Enterprise Development with NServiceBus](http://www.programutvikling.no/kurskalenderoversikt.aspx?id=1528011) authored by [Udi Dahan](http://www.udidahan.com/) and held by [Andreas Öhlund](http://andreasohlund.net/) at [Programutvikling](http://www.programutvikling.no/). The course was well structured and the course material, which you get a copy of, is really great. Andreas made all of the topics easy to understand, disregarding the complexity of the topics. This is a course I highly recommend to everyone that wants to learn NServiceBus and messaging in general. Along the way you will also learn some DDD and CQRS.\\n\\n## Key takeaways\\nThere were a lot of really good takeaways from this course, but the one I think was the most useful one is the [store and forward](http://en.wikipedia.org/wiki/Store_and_forward) pattern. Store and forward is a fundamental pattern to build highly scalable distributed messaging system which NServiceBus is built up on. The other two fundamental patterns, I consider NServiceBus to have three fundamental patterns, are one-way messaging and queues.\\n\\n### Queues\\nTo store messages NServiceBus uses queues. The default queue is MSMQ, but other queues are supported as well.\\n\\n### One-way messaging\\nWhen NServiceBus services communicate with each other they are sending messages to each other by putting them in each others queues. Since the message is put on the queue the service won't get a direct response, or more exactly, it will not get a response at all. If a response is to be extected that has to be done through a new one-way message back to the originator.\\n\\n### Store and forward\\nThis is a pattern based on queues and one-way messaging and will make the services highly scalable. This is best explained by an example. Imagine a distributed system that has an order service and an billing service. When a customer makes an order the order service will process it and if it is ok it will send an order accepted message to the billing service. In the process of sending the order accepted message the order will first store the message in it's own queue and when it succeed to connect to the billing service it will send the message. This allows the system as a whole to keep accepting orders even though the billing service is down, which is really good from a business perspective.\\n\\n## General comments\\n \\n * NServiceBus is 100 % transactional, that's why it uses RavendDB (as default) and not mongoDB for example to store its own state. That means when a message is processed NServiceBus will make sure it is processed otherwise it will be returned to the queue and NServiceBus will try process the message later. Also, if you interact with a sql server or some other service that supports transaction that could be used while processing a message.\\n * The [saga](http://www.udidahan.com/2009/04/20/saga-persistence-and-event-driven-architectures/) implementation in NServiceBus is really powerful and might for example help you [kill your batch jobs](http://skillsmatter.com/podcast/home/death-batch-job).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Last week I've been attending the course <a href=\"http://www.programutvikling.no/kurskalenderoversikt.aspx?id=1528011\">Enterprise Development with NServiceBus</a> authored by <a href=\"http://www.udidahan.com/\">Udi Dahan</a> and held by <a href=\"http://andreasohlund.net/\">Andreas Öhlund</a> at <a href=\"http://www.programutvikling.no/\">Programutvikling</a>. The course was well structured and the course material, which you get a copy of, is really great. Andreas made all of the topics easy to understand, disregarding the complexity of the topics. This is a course I highly recommend to everyone that wants to learn NServiceBus and messaging in general. Along the way you will also learn some DDD and CQRS.</p>\n<h2 id=\"keytakeaways\">Key takeaways</h2>\n<p>There were a lot of really good takeaways from this course, but the one I think was the most useful one is the <a href=\"http://en.wikipedia.org/wiki/Store_and_forward\">store and forward</a> pattern. Store and forward is a fundamental pattern to build highly scalable distributed messaging system which NServiceBus is built up on. The other two fundamental patterns, I consider NServiceBus to have three fundamental patterns, are one-way messaging and queues.</p>\n<h3 id=\"queues\">Queues</h3>\n<p>To store messages NServiceBus uses queues. The default queue is MSMQ, but other queues are supported as well.</p>\n<h3 id=\"onewaymessaging\">One-way messaging</h3>\n<p>When NServiceBus services communicate with each other they are sending messages to each other by putting them in each others queues. Since the message is put on the queue the service won't get a direct response, or more exactly, it will not get a response at all. If a response is to be extected that has to be done through a new one-way message back to the originator.</p>\n<h3 id=\"storeandforward\">Store and forward</h3>\n<p>This is a pattern based on queues and one-way messaging and will make the services highly scalable. This is best explained by an example. Imagine a distributed system that has an order service and an billing service. When a customer makes an order the order service will process it and if it is ok it will send an order accepted message to the billing service. In the process of sending the order accepted message the order will first store the message in it's own queue and when it succeed to connect to the billing service it will send the message. This allows the system as a whole to keep accepting orders even though the billing service is down, which is really good from a business perspective.</p>\n<h2 id=\"generalcomments\">General comments</h2>\n<ul>\n<li>NServiceBus is 100 % transactional, that's why it uses RavendDB (as default) and not mongoDB for example to store its own state. That means when a message is processed NServiceBus will make sure it is processed otherwise it will be returned to the queue and NServiceBus will try process the message later. Also, if you interact with a sql server or some other service that supports transaction that could be used while processing a message.</li>\n<li>The <a href=\"http://www.udidahan.com/2009/04/20/saga-persistence-and-event-driven-architectures/\">saga</a> implementation in NServiceBus is really powerful and might for example help you <a href=\"http://skillsmatter.com/podcast/home/death-batch-job\">kill your batch jobs</a>.</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"18","plaintext":"Last week I've been attending the course Enterprise Development with NServiceBus\n[http://www.programutvikling.no/kurskalenderoversikt.aspx?id=1528011] authored\nby Udi Dahan [http://www.udidahan.com/] and held by Andreas Öhlund\n[http://andreasohlund.net/] at Programutvikling\n[http://www.programutvikling.no/]. The course was well structured and the course\nmaterial, which you get a copy of, is really great. Andreas made all of the\ntopics easy to understand, disregarding the complexity of the topics. This is a\ncourse I highly recommend to everyone that wants to learn NServiceBus and\nmessaging in general. Along the way you will also learn some DDD and CQRS.\n\nKey takeaways\nThere were a lot of really good takeaways from this course, but the one I think\nwas the most useful one is the store and forward\n[http://en.wikipedia.org/wiki/Store_and_forward] pattern. Store and forward is a\nfundamental pattern to build highly scalable distributed messaging system which\nNServiceBus is built up on. The other two fundamental patterns, I consider\nNServiceBus to have three fundamental patterns, are one-way messaging and\nqueues.\n\nQueues\nTo store messages NServiceBus uses queues. The default queue is MSMQ, but other\nqueues are supported as well.\n\nOne-way messaging\nWhen NServiceBus services communicate with each other they are sending messages\nto each other by putting them in each others queues. Since the message is put on\nthe queue the service won't get a direct response, or more exactly, it will not\nget a response at all. If a response is to be extected that has to be done\nthrough a new one-way message back to the originator.\n\nStore and forward\nThis is a pattern based on queues and one-way messaging and will make the\nservices highly scalable. This is best explained by an example. Imagine a\ndistributed system that has an order service and an billing service. When a\ncustomer makes an order the order service will process it and if it is ok it\nwill send an order accepted message to the billing service. In the process of\nsending the order accepted message the order will first store the message in\nit's own queue and when it succeed to connect to the billing service it will\nsend the message. This allows the system as a whole to keep accepting orders\neven though the billing service is down, which is really good from a business\nperspective.\n\nGeneral comments\n * NServiceBus is 100 % transactional, that's why it uses RavendDB (as default)\n   and not mongoDB for example to store its own state. That means when a message\n   is processed NServiceBus will make sure it is processed otherwise it will be\n   returned to the queue and NServiceBus will try process the message later.\n   Also, if you interact with a sql server or some other service that supports\n   transaction that could be used while processing a message.\n * The saga\n   [http://www.udidahan.com/2009/04/20/saga-persistence-and-event-driven-architectures/] \n   implementation in NServiceBus is really powerful and might for example help\n   you kill your batch jobs\n   [http://skillsmatter.com/podcast/home/death-batch-job].","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:30:19.000Z","updated_at":"2014-03-01T09:30:38.000Z","published_at":"2012-12-03T09:30:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe046","uuid":"3df0f888-e426-49b9-aed6-252df01896d7","title":"Parsing json with PowerShell and Json.NET","slug":"parsing-json-with-powershell-and-json-net","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I've been working lately on a set of scripts to automate setup of computers (I will write more about those later). With setup I mean from a web-developers perspective, so it includes setting up IIS, adding users to computer, changing hosts-file and changing environment-variables. An important part of this is how to specify variables to the scripts and I choose json. This short post will show you the module I implemented to parse a json file with the result of an `HashTable`. So let's get started. I will just show you the code and then explain it under.\\n\\n    function ParseItem($jsonItem) {\\n        if($jsonItem.Type -eq \\\"Array\\\") {\\n            return ParseJsonArray($jsonItem)\\n        }\\n        elseif($jsonItem.Type -eq \\\"Object\\\") {\\n            return ParseJsonObject($jsonItem)\\n        }\\n        else {\\n            return $jsonItem.ToString()\\n        }\\n    }\\n\\n    function ParseJsonObject($jsonObj) {\\n        $result = @{}\\n        $jsonObj.Keys | ForEach-Object {\\n            $key = $_\\n            $item = $jsonObj[$key]\\n            $parsedItem = ParseItem $item\\n            $result.Add($key,$parsedItem)\\n        }\\n        return $result\\n    }\\n\\n    function ParseJsonArray($jsonArray) {\\n        $result = @()\\n        $jsonArray | ForEach-Object {\\n            $parsedItem = ParseItem $_\\n            $result += $parsedItem\\n        }\\n        return $result\\n    }\\n\\n    function ParseJsonString($json) {\\n        $config = [Newtonsoft.Json.Linq.JObject]::Parse($json)\\n        return ParseJsonObject($config)\\n    }\\n\\n    function ParseJsonFile($fileName) {\\n        $json = (Get-Content $FileName | Out-String)\\n        return ParseJsonString $json\\n    }\\n\\n    Export-ModuleMember ParseJsonFile, ParseJsonString\\n\\nSo let's start from the bottom of the file. I export the module members `ParseJSonFile` and `ParseJsonString`, those are the entry points for the script. `ParseJsonFile` is a function that just reads the json from a file and then calls `ParseJsonString` and it is in that function the magic starts. If you look at the first row in `ParseJsonString` I call an external assembly, and that is the Json.NET assembly. To call it you have to load the assembly first. You could, as I have done here, use a module manifest for the module and add the assembly to the required assembly list. If you don't want to do so you could also load the assembly before exuting a method from the module using this line:\\n\\n    [Reflection.Assembly]::LoadFile(\\\"path-to\\\\Newtonsoft.Json.dll”)\\n\\nWhen I've parsed ths json string I make one assumption, and that is that what you have in the json is an object, that is the only assumption I had since it fits my requirements. The technique I use for the rest of the script is recursion, so if you have a really long json-file you might get an exception, but I don't think that would be a problem. The `ParseJsonObject` creates a new `HashTable` and then loops over all the keys in the object from Json.NET. For each value I call the most interesting function in the script, the `ParseItem` function. This method looks at the value and then do three different things depending on the type of the value. If it is an object it recursively calls the `ParseJsonObject` function, if it is an array it calls the `ParseJsonArray` function and if it is neither of those it takes the string value and returns the value. The last method is the `ParseJsonArray` and it works the same way as the `ParseJsonObject` excepts it's traversing an array instead of an object.\\n\\nA small example of what it looks like against one json-file I have:\\n\\n\\n![Json parsing example][1]\\n\\n  [1]: https://ykaqzw.bay.livefilestore.com/y1pA2PfyvbBJb1RByqI4tyndvpi3P-wV8gcE_cYBahljq8H7jQfzmvB7QBo5h3HKLAStoH0dB2Whp_qkmT6UDirnkPQmq9yOnli/JsonParseExample.png?psid=1 \\\"Json parsing example\\\"\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I've been working lately on a set of scripts to automate setup of computers (I will write more about those later). With setup I mean from a web-developers perspective, so it includes setting up IIS, adding users to computer, changing hosts-file and changing environment-variables. An important part of this is how to specify variables to the scripts and I choose json. This short post will show you the module I implemented to parse a json file with the result of an <code>HashTable</code>. So let's get started. I will just show you the code and then explain it under.</p>\n<pre><code>function ParseItem($jsonItem) {\n    if($jsonItem.Type -eq &quot;Array&quot;) {\n        return ParseJsonArray($jsonItem)\n    }\n    elseif($jsonItem.Type -eq &quot;Object&quot;) {\n        return ParseJsonObject($jsonItem)\n    }\n    else {\n        return $jsonItem.ToString()\n    }\n}\n\nfunction ParseJsonObject($jsonObj) {\n    $result = @{}\n    $jsonObj.Keys | ForEach-Object {\n        $key = $_\n        $item = $jsonObj[$key]\n        $parsedItem = ParseItem $item\n        $result.Add($key,$parsedItem)\n    }\n    return $result\n}\n\nfunction ParseJsonArray($jsonArray) {\n    $result = @()\n    $jsonArray | ForEach-Object {\n        $parsedItem = ParseItem $_\n        $result += $parsedItem\n    }\n    return $result\n}\n\nfunction ParseJsonString($json) {\n    $config = [Newtonsoft.Json.Linq.JObject]::Parse($json)\n    return ParseJsonObject($config)\n}\n\nfunction ParseJsonFile($fileName) {\n    $json = (Get-Content $FileName | Out-String)\n    return ParseJsonString $json\n}\n\nExport-ModuleMember ParseJsonFile, ParseJsonString\n</code></pre>\n<p>So let's start from the bottom of the file. I export the module members <code>ParseJSonFile</code> and <code>ParseJsonString</code>, those are the entry points for the script. <code>ParseJsonFile</code> is a function that just reads the json from a file and then calls <code>ParseJsonString</code> and it is in that function the magic starts. If you look at the first row in <code>ParseJsonString</code> I call an external assembly, and that is the Json.NET assembly. To call it you have to load the assembly first. You could, as I have done here, use a module manifest for the module and add the assembly to the required assembly list. If you don't want to do so you could also load the assembly before exuting a method from the module using this line:</p>\n<pre><code>[Reflection.Assembly]::LoadFile(&quot;path-to\\Newtonsoft.Json.dll”)\n</code></pre>\n<p>When I've parsed ths json string I make one assumption, and that is that what you have in the json is an object, that is the only assumption I had since it fits my requirements. The technique I use for the rest of the script is recursion, so if you have a really long json-file you might get an exception, but I don't think that would be a problem. The <code>ParseJsonObject</code> creates a new <code>HashTable</code> and then loops over all the keys in the object from Json.NET. For each value I call the most interesting function in the script, the <code>ParseItem</code> function. This method looks at the value and then do three different things depending on the type of the value. If it is an object it recursively calls the <code>ParseJsonObject</code> function, if it is an array it calls the <code>ParseJsonArray</code> function and if it is neither of those it takes the string value and returns the value. The last method is the <code>ParseJsonArray</code> and it works the same way as the <code>ParseJsonObject</code> excepts it's traversing an array instead of an object.</p>\n<p>A small example of what it looks like against one json-file I have:</p>\n<p><img src=\"https://ykaqzw.bay.livefilestore.com/y1pA2PfyvbBJb1RByqI4tyndvpi3P-wV8gcE_cYBahljq8H7jQfzmvB7QBo5h3HKLAStoH0dB2Whp_qkmT6UDirnkPQmq9yOnli/JsonParseExample.png?psid=1\" alt=\"Json parsing example\" title=\"Json parsing example\"></p>\n<!--kg-card-end: markdown-->","comment_id":"19","plaintext":"I've been working lately on a set of scripts to automate setup of computers (I\nwill write more about those later). With setup I mean from a web-developers\nperspective, so it includes setting up IIS, adding users to computer, changing\nhosts-file and changing environment-variables. An important part of this is how\nto specify variables to the scripts and I choose json. This short post will show\nyou the module I implemented to parse a json file with the result of an \nHashTable. So let's get started. I will just show you the code and then explain\nit under.\n\nfunction ParseItem($jsonItem) {\n    if($jsonItem.Type -eq \"Array\") {\n        return ParseJsonArray($jsonItem)\n    }\n    elseif($jsonItem.Type -eq \"Object\") {\n        return ParseJsonObject($jsonItem)\n    }\n    else {\n        return $jsonItem.ToString()\n    }\n}\n\nfunction ParseJsonObject($jsonObj) {\n    $result = @{}\n    $jsonObj.Keys | ForEach-Object {\n        $key = $_\n        $item = $jsonObj[$key]\n        $parsedItem = ParseItem $item\n        $result.Add($key,$parsedItem)\n    }\n    return $result\n}\n\nfunction ParseJsonArray($jsonArray) {\n    $result = @()\n    $jsonArray | ForEach-Object {\n        $parsedItem = ParseItem $_\n        $result += $parsedItem\n    }\n    return $result\n}\n\nfunction ParseJsonString($json) {\n    $config = [Newtonsoft.Json.Linq.JObject]::Parse($json)\n    return ParseJsonObject($config)\n}\n\nfunction ParseJsonFile($fileName) {\n    $json = (Get-Content $FileName | Out-String)\n    return ParseJsonString $json\n}\n\nExport-ModuleMember ParseJsonFile, ParseJsonString\n\n\nSo let's start from the bottom of the file. I export the module members \nParseJSonFile and ParseJsonString, those are the entry points for the script. \nParseJsonFile is a function that just reads the json from a file and then calls \nParseJsonString and it is in that function the magic starts. If you look at the\nfirst row in ParseJsonString I call an external assembly, and that is the\nJson.NET assembly. To call it you have to load the assembly first. You could, as\nI have done here, use a module manifest for the module and add the assembly to\nthe required assembly list. If you don't want to do so you could also load the\nassembly before exuting a method from the module using this line:\n\n[Reflection.Assembly]::LoadFile(\"path-to\\Newtonsoft.Json.dll”)\n\n\nWhen I've parsed ths json string I make one assumption, and that is that what\nyou have in the json is an object, that is the only assumption I had since it\nfits my requirements. The technique I use for the rest of the script is\nrecursion, so if you have a really long json-file you might get an exception,\nbut I don't think that would be a problem. The ParseJsonObject creates a new \nHashTable and then loops over all the keys in the object from Json.NET. For each\nvalue I call the most interesting function in the script, the ParseItem \nfunction. This method looks at the value and then do three different things\ndepending on the type of the value. If it is an object it recursively calls the \nParseJsonObject function, if it is an array it calls the ParseJsonArray function\nand if it is neither of those it takes the string value and returns the value.\nThe last method is the ParseJsonArray and it works the same way as the \nParseJsonObject excepts it's traversing an array instead of an object.\n\nA small example of what it looks like against one json-file I have:","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:31:54.000Z","updated_at":"2014-03-01T09:32:05.000Z","published_at":"2013-01-17T09:31:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe047","uuid":"7a0d436d-e992-45fa-9878-97a109ee570c","title":"State or intent? The missing discussion of NoSQL","slug":"state-or-intent-the-missing-discussion-of-nosql","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The NoSQL movement has been going on for quite a while now and that I think is a good thing. I really think that different storage mechanisms are good for different situations and we should be better to take advantage of the possibilities we have today. However, there is one discussion that is not as loud as NoSQL vs. SQL/Relational database, and that is the discussion of what we are storing in our database. I think that in many scenarios where the actual application is a little more complex and when you don't know how the application will evolve you would be better off with an event store and event sourcing.\\n\\nHave you ever been in a situation where you wish that the you had the full log of what had happened in the application so you could refactor the data store? I have and I have also seen multiple attempts to solve this problem with a regular relational database where the state of the application is stored, and next to each table is an extra table that stores the changes to each column. Doesn't this seem a little bit redundant?\\n\\n## What is event sourcing?\\nEvent sourcing is a way to use events to build up your domain objects instead of just capturing the state they are in right now. The events are usually stored in an event store which is a storage for your events as the name implies. When you build an event store you are totally free to choose whatever storage mechanism you like, it doesn't matter if it is a relational database, file or a NoSQL database. The basics behind an event store is pretty straight forward and to implement it in a SQL database you would need one table with three columns; one column for the id, one column for the version of the object and one column for the serialized data.\\n\\n### How does event sourcing work?\\nThere is always one thing you need when using event sourcing, and that is an id to the aggregate created the event. The id is a must to read the events from the event store and re-apply them to your aggregate. You might also need to store the version of the aggregate the events belongs so you know in which order to re-apply the events as well to detect conflicts.\\n\\nI've tried to simplify how it works in the illustration below. First the client ask the `OrderRepository` for an aggregate. If an aggregate is found the client make an action, in this case it adds an `OrderLine`. The aggregate is responsible to make sure that the action can be applied to the aggregate and this validation should be done before the resulting events are stored.\\n\\n![Simple event sourcing][1]\\n\\nThe central part which I didn't mention above is that when the client ask a repository for an aggregate all the historical events for that aggregate are read and applied to the aggregate. When applying these events no validation should be made since the validation should have been done before the event is stored, once an event is stored it is part of the history and it is nothing that can change that fact. An event should never be deleted, if you have a scenario where you have found a bug you should do some compensating action to resolve that bug.\\n\\n### When is event sourcing a good alternative?\\nI think event sourcing fits best with a DDD-application using CQRS, but you could use it in other situations as well. It is often mentioned together with CQRS, but you don't have to use event sourcing with CQRS even though I would recommend it. DDD, CQRS and event sourcing are three different things and should be treated such, but they do fit well together. I want go into the details of DDD or CQRS but I will make use of some of the terms from DDD in lack of other words to explain things.\\n\\n## Why use event sourcing?\\nThere are multiple reasons why you might want to use event sourcing, but I will only cover the functionality perspective here.\\n\\n### You store the intention rather than the state of your application\\nWhen you use event sourcing the information stored is so much richer than when you only store the state as you normally do. For every state change you also store why the state changed. There are several things this enabled you to do like:\\n\\n * Create reports later on things that wasn't specified when the project started\\n * Replay events to track down bugs\\n * Refactor object structure without thinking about the database\\n\\n### It drives you to better communication with the business\\nI think using event sourcing will help you focus ond the \\\"why\\\" instead of \\\"what\\\" something is, and that is something I think is really important. You will have a complete history of why something has a state rather than just the state of something, this will enable you to have a deeped understanding of the domain and make it easier to talk to the business.\\n\\nThe event that you are using should have name that reflects the domain which you are modelling and they should also be in past tense. A business person should be able to read the events and from those events tell you which state an object should be or should not be in. \\n\\n### Improved testing through black box testing\\nThe testing part with event sourcing is where it really shines. Mocking an event store during testing is really simple and when you have done so you could do some really black box testing. What do I mean with that?I basically mean that you can see on your application the same way as the business sees it and it is their problem you are trying to solve. This makes it really suitable for writing great acceptance tests like this  made up example:\\n\\n> Given a user with account 12334 has made a deposit of 400\\n\\n> and has enabled credit on the account\\n\\n> When a withdrawal on 500 is made\\n\\n> Then the account is withdrawn 500\\n\\n> and the user is notified that the account is overdrawn\\n\\nImplementing the test for this specification is not hard when using event sourcing. First you add three events as a pre-condition; `AccountCreated`, `DepositMade` and `CreditEnabled`. After you have set up the test you execute the action: `WithdrawMoney`. To check if the test has passed you check if the two events `MoneyWithdrawn` and `AccountOverdrawn` has been created. This doesn't only check for things that happened, with this approach you can also check that you don't have any unwanted side effects. Checking for unwanted side effects is almost impossible when using a standard relational database, all you most of the time do is expecting the implementation not to have side effects.\\n\\n### No impedance mismatch\\nSine all you need to do read an object from an event store is reading all the events for the object and applying them to the object. The methods for applying the events is something you should have done anyway if you have a really clean design, this might just look a little bit different. You don't need to worry about how the objects will serialize down to the database and what the relations should like etc. If you are really \\\"advanced\\\" you could implement the applying of events by convention, but I personally like to do that part explicitly.\\n\\n## Why is it consider more complex to use event sourcing?\\nI personally don't think using event sourcing is more complex if it is used in the right type of project. With that I mean a project that has some kind of complexity and not just a simple CRUD application. I even think that using event sourcing might drive your development to a place where you focus on what the application is supposed to do rather than what is the state of the application. As stated earlier, state is something that is derived from what has happened.\\n\\nTo be successful with event sourcing you almost always need a separate data store that stores the actual data you want to show for the user. A lot of people sees this as an additional complexity but all it really is it separation of concerns. I don't think you will right more code doing this separation, instead you will have the code better aligned for their specific purpose.\\n\\n## Resources:\\nIf you want to see some code I have played around with a simple framework which you can find here: [TJ.CQRS](https://github.com/mastoj/TJ.CQRS)\\n\\nMy framework is very inspired by a similar implementation by [Mark Nijhof](http://cre8ivethought.com/blog) which you can find here: [Fohjin](https://github.com/MarkNijhof/Fohjin). He has also gathered several of his blog posts about CQRS and put them in a book [CQRS - The Example](https://leanpub.com/cqrs)\\n\\n[Martin Fowler](http://martinfowler.com/) has also written a great post about event sourcing at his [blog](http://martinfowler.com/eaaDev/EventSourcing.html).\\n\\n\\n  [1]: https://public.bn1.livefilestore.com/y1pwK56vcHOuKJy8WVO1nS0PE7ksgfB5xbshOlmouXipAuAsFkCoaAtH0KDMZ_r6-xGDp9zDXhD8Q9GIHwGMibHbw/EventSourcing.png?psid=1\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The NoSQL movement has been going on for quite a while now and that I think is a good thing. I really think that different storage mechanisms are good for different situations and we should be better to take advantage of the possibilities we have today. However, there is one discussion that is not as loud as NoSQL vs. SQL/Relational database, and that is the discussion of what we are storing in our database. I think that in many scenarios where the actual application is a little more complex and when you don't know how the application will evolve you would be better off with an event store and event sourcing.</p>\n<p>Have you ever been in a situation where you wish that the you had the full log of what had happened in the application so you could refactor the data store? I have and I have also seen multiple attempts to solve this problem with a regular relational database where the state of the application is stored, and next to each table is an extra table that stores the changes to each column. Doesn't this seem a little bit redundant?</p>\n<h2 id=\"whatiseventsourcing\">What is event sourcing?</h2>\n<p>Event sourcing is a way to use events to build up your domain objects instead of just capturing the state they are in right now. The events are usually stored in an event store which is a storage for your events as the name implies. When you build an event store you are totally free to choose whatever storage mechanism you like, it doesn't matter if it is a relational database, file or a NoSQL database. The basics behind an event store is pretty straight forward and to implement it in a SQL database you would need one table with three columns; one column for the id, one column for the version of the object and one column for the serialized data.</p>\n<h3 id=\"howdoeseventsourcingwork\">How does event sourcing work?</h3>\n<p>There is always one thing you need when using event sourcing, and that is an id to the aggregate created the event. The id is a must to read the events from the event store and re-apply them to your aggregate. You might also need to store the version of the aggregate the events belongs so you know in which order to re-apply the events as well to detect conflicts.</p>\n<p>I've tried to simplify how it works in the illustration below. First the client ask the <code>OrderRepository</code> for an aggregate. If an aggregate is found the client make an action, in this case it adds an <code>OrderLine</code>. The aggregate is responsible to make sure that the action can be applied to the aggregate and this validation should be done before the resulting events are stored.</p>\n<p><img src=\"https://public.bn1.livefilestore.com/y1pwK56vcHOuKJy8WVO1nS0PE7ksgfB5xbshOlmouXipAuAsFkCoaAtH0KDMZ_r6-xGDp9zDXhD8Q9GIHwGMibHbw/EventSourcing.png?psid=1\" alt=\"Simple event sourcing\"></p>\n<p>The central part which I didn't mention above is that when the client ask a repository for an aggregate all the historical events for that aggregate are read and applied to the aggregate. When applying these events no validation should be made since the validation should have been done before the event is stored, once an event is stored it is part of the history and it is nothing that can change that fact. An event should never be deleted, if you have a scenario where you have found a bug you should do some compensating action to resolve that bug.</p>\n<h3 id=\"wheniseventsourcingagoodalternative\">When is event sourcing a good alternative?</h3>\n<p>I think event sourcing fits best with a DDD-application using CQRS, but you could use it in other situations as well. It is often mentioned together with CQRS, but you don't have to use event sourcing with CQRS even though I would recommend it. DDD, CQRS and event sourcing are three different things and should be treated such, but they do fit well together. I want go into the details of DDD or CQRS but I will make use of some of the terms from DDD in lack of other words to explain things.</p>\n<h2 id=\"whyuseeventsourcing\">Why use event sourcing?</h2>\n<p>There are multiple reasons why you might want to use event sourcing, but I will only cover the functionality perspective here.</p>\n<h3 id=\"youstoretheintentionratherthanthestateofyourapplication\">You store the intention rather than the state of your application</h3>\n<p>When you use event sourcing the information stored is so much richer than when you only store the state as you normally do. For every state change you also store why the state changed. There are several things this enabled you to do like:</p>\n<ul>\n<li>Create reports later on things that wasn't specified when the project started</li>\n<li>Replay events to track down bugs</li>\n<li>Refactor object structure without thinking about the database</li>\n</ul>\n<h3 id=\"itdrivesyoutobettercommunicationwiththebusiness\">It drives you to better communication with the business</h3>\n<p>I think using event sourcing will help you focus ond the &quot;why&quot; instead of &quot;what&quot; something is, and that is something I think is really important. You will have a complete history of why something has a state rather than just the state of something, this will enable you to have a deeped understanding of the domain and make it easier to talk to the business.</p>\n<p>The event that you are using should have name that reflects the domain which you are modelling and they should also be in past tense. A business person should be able to read the events and from those events tell you which state an object should be or should not be in.</p>\n<h3 id=\"improvedtestingthroughblackboxtesting\">Improved testing through black box testing</h3>\n<p>The testing part with event sourcing is where it really shines. Mocking an event store during testing is really simple and when you have done so you could do some really black box testing. What do I mean with that?I basically mean that you can see on your application the same way as the business sees it and it is their problem you are trying to solve. This makes it really suitable for writing great acceptance tests like this  made up example:</p>\n<blockquote>\n<p>Given a user with account 12334 has made a deposit of 400</p>\n</blockquote>\n<blockquote>\n<p>and has enabled credit on the account</p>\n</blockquote>\n<blockquote>\n<p>When a withdrawal on 500 is made</p>\n</blockquote>\n<blockquote>\n<p>Then the account is withdrawn 500</p>\n</blockquote>\n<blockquote>\n<p>and the user is notified that the account is overdrawn</p>\n</blockquote>\n<p>Implementing the test for this specification is not hard when using event sourcing. First you add three events as a pre-condition; <code>AccountCreated</code>, <code>DepositMade</code> and <code>CreditEnabled</code>. After you have set up the test you execute the action: <code>WithdrawMoney</code>. To check if the test has passed you check if the two events <code>MoneyWithdrawn</code> and <code>AccountOverdrawn</code> has been created. This doesn't only check for things that happened, with this approach you can also check that you don't have any unwanted side effects. Checking for unwanted side effects is almost impossible when using a standard relational database, all you most of the time do is expecting the implementation not to have side effects.</p>\n<h3 id=\"noimpedancemismatch\">No impedance mismatch</h3>\n<p>Sine all you need to do read an object from an event store is reading all the events for the object and applying them to the object. The methods for applying the events is something you should have done anyway if you have a really clean design, this might just look a little bit different. You don't need to worry about how the objects will serialize down to the database and what the relations should like etc. If you are really &quot;advanced&quot; you could implement the applying of events by convention, but I personally like to do that part explicitly.</p>\n<h2 id=\"whyisitconsidermorecomplextouseeventsourcing\">Why is it consider more complex to use event sourcing?</h2>\n<p>I personally don't think using event sourcing is more complex if it is used in the right type of project. With that I mean a project that has some kind of complexity and not just a simple CRUD application. I even think that using event sourcing might drive your development to a place where you focus on what the application is supposed to do rather than what is the state of the application. As stated earlier, state is something that is derived from what has happened.</p>\n<p>To be successful with event sourcing you almost always need a separate data store that stores the actual data you want to show for the user. A lot of people sees this as an additional complexity but all it really is it separation of concerns. I don't think you will right more code doing this separation, instead you will have the code better aligned for their specific purpose.</p>\n<h2 id=\"resources\">Resources:</h2>\n<p>If you want to see some code I have played around with a simple framework which you can find here: <a href=\"https://github.com/mastoj/TJ.CQRS\">TJ.CQRS</a></p>\n<p>My framework is very inspired by a similar implementation by <a href=\"http://cre8ivethought.com/blog\">Mark Nijhof</a> which you can find here: <a href=\"https://github.com/MarkNijhof/Fohjin\">Fohjin</a>. He has also gathered several of his blog posts about CQRS and put them in a book <a href=\"https://leanpub.com/cqrs\">CQRS - The Example</a></p>\n<p><a href=\"http://martinfowler.com/\">Martin Fowler</a> has also written a great post about event sourcing at his <a href=\"http://martinfowler.com/eaaDev/EventSourcing.html\">blog</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"20","plaintext":"The NoSQL movement has been going on for quite a while now and that I think is a\ngood thing. I really think that different storage mechanisms are good for\ndifferent situations and we should be better to take advantage of the\npossibilities we have today. However, there is one discussion that is not as\nloud as NoSQL vs. SQL/Relational database, and that is the discussion of what we\nare storing in our database. I think that in many scenarios where the actual\napplication is a little more complex and when you don't know how the application\nwill evolve you would be better off with an event store and event sourcing.\n\nHave you ever been in a situation where you wish that the you had the full log\nof what had happened in the application so you could refactor the data store? I\nhave and I have also seen multiple attempts to solve this problem with a regular\nrelational database where the state of the application is stored, and next to\neach table is an extra table that stores the changes to each column. Doesn't\nthis seem a little bit redundant?\n\nWhat is event sourcing?\nEvent sourcing is a way to use events to build up your domain objects instead of\njust capturing the state they are in right now. The events are usually stored in\nan event store which is a storage for your events as the name implies. When you\nbuild an event store you are totally free to choose whatever storage mechanism\nyou like, it doesn't matter if it is a relational database, file or a NoSQL\ndatabase. The basics behind an event store is pretty straight forward and to\nimplement it in a SQL database you would need one table with three columns; one\ncolumn for the id, one column for the version of the object and one column for\nthe serialized data.\n\nHow does event sourcing work?\nThere is always one thing you need when using event sourcing, and that is an id\nto the aggregate created the event. The id is a must to read the events from the\nevent store and re-apply them to your aggregate. You might also need to store\nthe version of the aggregate the events belongs so you know in which order to\nre-apply the events as well to detect conflicts.\n\nI've tried to simplify how it works in the illustration below. First the client\nask the OrderRepository for an aggregate. If an aggregate is found the client\nmake an action, in this case it adds an OrderLine. The aggregate is responsible\nto make sure that the action can be applied to the aggregate and this validation\nshould be done before the resulting events are stored.\n\n\n\nThe central part which I didn't mention above is that when the client ask a\nrepository for an aggregate all the historical events for that aggregate are\nread and applied to the aggregate. When applying these events no validation\nshould be made since the validation should have been done before the event is\nstored, once an event is stored it is part of the history and it is nothing that\ncan change that fact. An event should never be deleted, if you have a scenario\nwhere you have found a bug you should do some compensating action to resolve\nthat bug.\n\nWhen is event sourcing a good alternative?\nI think event sourcing fits best with a DDD-application using CQRS, but you\ncould use it in other situations as well. It is often mentioned together with\nCQRS, but you don't have to use event sourcing with CQRS even though I would\nrecommend it. DDD, CQRS and event sourcing are three different things and should\nbe treated such, but they do fit well together. I want go into the details of\nDDD or CQRS but I will make use of some of the terms from DDD in lack of other\nwords to explain things.\n\nWhy use event sourcing?\nThere are multiple reasons why you might want to use event sourcing, but I will\nonly cover the functionality perspective here.\n\nYou store the intention rather than the state of your application\nWhen you use event sourcing the information stored is so much richer than when\nyou only store the state as you normally do. For every state change you also\nstore why the state changed. There are several things this enabled you to do\nlike:\n\n * Create reports later on things that wasn't specified when the project started\n * Replay events to track down bugs\n * Refactor object structure without thinking about the database\n\nIt drives you to better communication with the business\nI think using event sourcing will help you focus ond the \"why\" instead of \"what\"\nsomething is, and that is something I think is really important. You will have a\ncomplete history of why something has a state rather than just the state of\nsomething, this will enable you to have a deeped understanding of the domain and\nmake it easier to talk to the business.\n\nThe event that you are using should have name that reflects the domain which you\nare modelling and they should also be in past tense. A business person should be\nable to read the events and from those events tell you which state an object\nshould be or should not be in.\n\nImproved testing through black box testing\nThe testing part with event sourcing is where it really shines. Mocking an event\nstore during testing is really simple and when you have done so you could do\nsome really black box testing. What do I mean with that?I basically mean that\nyou can see on your application the same way as the business sees it and it is\ntheir problem you are trying to solve. This makes it really suitable for writing\ngreat acceptance tests like this made up example:\n\n> Given a user with account 12334 has made a deposit of 400\n\n\n> and has enabled credit on the account\n\n\n> When a withdrawal on 500 is made\n\n\n> Then the account is withdrawn 500\n\n\n> and the user is notified that the account is overdrawn\n\n\nImplementing the test for this specification is not hard when using event\nsourcing. First you add three events as a pre-condition; AccountCreated, \nDepositMade and CreditEnabled. After you have set up the test you execute the\naction: WithdrawMoney. To check if the test has passed you check if the two\nevents MoneyWithdrawn and AccountOverdrawn has been created. This doesn't only\ncheck for things that happened, with this approach you can also check that you\ndon't have any unwanted side effects. Checking for unwanted side effects is\nalmost impossible when using a standard relational database, all you most of the\ntime do is expecting the implementation not to have side effects.\n\nNo impedance mismatch\nSine all you need to do read an object from an event store is reading all the\nevents for the object and applying them to the object. The methods for applying\nthe events is something you should have done anyway if you have a really clean\ndesign, this might just look a little bit different. You don't need to worry\nabout how the objects will serialize down to the database and what the relations\nshould like etc. If you are really \"advanced\" you could implement the applying\nof events by convention, but I personally like to do that part explicitly.\n\nWhy is it consider more complex to use event sourcing?\nI personally don't think using event sourcing is more complex if it is used in\nthe right type of project. With that I mean a project that has some kind of\ncomplexity and not just a simple CRUD application. I even think that using event\nsourcing might drive your development to a place where you focus on what the\napplication is supposed to do rather than what is the state of the application.\nAs stated earlier, state is something that is derived from what has happened.\n\nTo be successful with event sourcing you almost always need a separate data\nstore that stores the actual data you want to show for the user. A lot of people\nsees this as an additional complexity but all it really is it separation of\nconcerns. I don't think you will right more code doing this separation, instead\nyou will have the code better aligned for their specific purpose.\n\nResources:\nIf you want to see some code I have played around with a simple framework which\nyou can find here: TJ.CQRS [https://github.com/mastoj/TJ.CQRS]\n\nMy framework is very inspired by a similar implementation by Mark Nijhof\n[http://cre8ivethought.com/blog] which you can find here: Fohjin\n[https://github.com/MarkNijhof/Fohjin]. He has also gathered several of his blog\nposts about CQRS and put them in a book CQRS - The Example\n[https://leanpub.com/cqrs]\n\nMartin Fowler [http://martinfowler.com/] has also written a great post about\nevent sourcing at his blog [http://martinfowler.com/eaaDev/EventSourcing.html].","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:33:16.000Z","updated_at":"2014-03-09T17:28:59.000Z","published_at":"2013-04-20T07:33:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe048","uuid":"de6e350a-8af7-4e2d-8c2e-6cb86debe6d7","title":"Git Cheat Sheet","slug":"git-cheat-sheet","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"## Branching\\n * Create branch\\n\\n    `git checkout -b <branch name>`\\n\\n * Delete local branch\\n\\n    `git branch -d <branch name>`\\n\\n * Delete remote branch\\n\\n    `git push origin :<branch name>`\\n\\n * Synchronize your local branches with remote branches\\n\\n    `git fetch -p|--prune`\\n\\n## Getting changes\\n * Simple get changes from remote\\n\\n    `git fetch <remote name>`\\n\\n * Simple get changes and merge into your repository\\n\\n    `git pull [<remote name> [<remote branch>]]`\\n\\n## Working with remotes\\n * Add remote\\n\\n    `git remote add <name> <url>`\\n\\n * Delete remote\\n\\n    `git remote rm <name>`\\n\\n * Rename\\n\\n    `git remote rename <old name> <new name>`\\n\\n * Listing of remote branches\\n\\n    `git remote show <remote name>`\\n\\n * Checkout and track remote branch\\n\\n    `git checkout --track <remote name>/<remote branch>`\\n\\n## Pushing the code\\n * Initial push (to set the uplink)\\n\\n    `git push -u <remote> branch`\\n\\n * If you have the upling set\\n\\n    `git push` \\n\\n## Showing the changes\\n * Show changes\\n\\n    `git log`\\n\\n * Show changes with graph\\n\\n    `git log --graph`\\n\\n * Show changes for all branches (with graph)\\n\\n    `git log --graph --all`\\n\\n * Show one line per commit\\n\\n    `git log --pretty=oneline`\\n\\n   `pretty` has the following options: oneline, short, medium, full, fuller, email, raw. \\n\\n## Handling merge conflicts\\n * Take source file\\n\\n    `git checkout --theirs <file>`\\n\\n * Keep your file\\n\\n    `git checkout --ours <file>`\\n\\n## Diffing between branches\\n * Regular diff with master\\n\\n    `git diff master..<branch name>`\\n\\n * Get the name of the files that has changed\\n\\n    `git diff --name-status master..<branch name>`\\n\\n## Tagging\\n * Creating an annotated tag (recommended)\\n\\n    `git tag -a <tag name> -m '<message>'`\\n\\n * Creating an annotated tag from a checkin\\n\\n    `git tag -a <tag name> -m '<message>' <commit checksum>`\\n\\n * Listing all tags\\n\\n    `git tag`\\n\\n * Listing a defined list of tag\\n\\n    `git tag -l 'v1.4.*'`\\n\\n * Show tag info\\n\\n    `git show <tag name>`\\n\\n * Checkout from tag\\n\\n    `git checkout <tag name>`\\n\\n * Sharing one tag\\n\\n    `git push origin <tag name>`\\n\\n * Sharing all new tags\\n\\n    `git push origin --tags`\\n\\n\\n## Sources\\n * [Github workflow for submitting pull requests](https://www.openshift.com/wiki/github-workflow-for-submitting-pull-requests)\\n * [Git rebasing](http://git-scm.com/book/en/Git-Branching-Rebasing)\\n * [Another blog with tips](http://mislav.uniqpath.com/2010/07/git-tips/)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><h2 id=\"branching\">Branching</h2>\n<ul>\n<li>\n<p>Create branch</p>\n<p><code>git checkout -b &lt;branch name&gt;</code></p>\n</li>\n<li>\n<p>Delete local branch</p>\n<p><code>git branch -d &lt;branch name&gt;</code></p>\n</li>\n<li>\n<p>Delete remote branch</p>\n<p><code>git push origin :&lt;branch name&gt;</code></p>\n</li>\n<li>\n<p>Synchronize your local branches with remote branches</p>\n<p><code>git fetch -p|--prune</code></p>\n</li>\n</ul>\n<h2 id=\"gettingchanges\">Getting changes</h2>\n<ul>\n<li>\n<p>Simple get changes from remote</p>\n<p><code>git fetch &lt;remote name&gt;</code></p>\n</li>\n<li>\n<p>Simple get changes and merge into your repository</p>\n<p><code>git pull [&lt;remote name&gt; [&lt;remote branch&gt;]]</code></p>\n</li>\n</ul>\n<h2 id=\"workingwithremotes\">Working with remotes</h2>\n<ul>\n<li>\n<p>Add remote</p>\n<p><code>git remote add &lt;name&gt; &lt;url&gt;</code></p>\n</li>\n<li>\n<p>Delete remote</p>\n<p><code>git remote rm &lt;name&gt;</code></p>\n</li>\n<li>\n<p>Rename</p>\n<p><code>git remote rename &lt;old name&gt; &lt;new name&gt;</code></p>\n</li>\n<li>\n<p>Listing of remote branches</p>\n<p><code>git remote show &lt;remote name&gt;</code></p>\n</li>\n<li>\n<p>Checkout and track remote branch</p>\n<p><code>git checkout --track &lt;remote name&gt;/&lt;remote branch&gt;</code></p>\n</li>\n</ul>\n<h2 id=\"pushingthecode\">Pushing the code</h2>\n<ul>\n<li>\n<p>Initial push (to set the uplink)</p>\n<p><code>git push -u &lt;remote&gt; branch</code></p>\n</li>\n<li>\n<p>If you have the upling set</p>\n<p><code>git push</code></p>\n</li>\n</ul>\n<h2 id=\"showingthechanges\">Showing the changes</h2>\n<ul>\n<li>\n<p>Show changes</p>\n<p><code>git log</code></p>\n</li>\n<li>\n<p>Show changes with graph</p>\n<p><code>git log --graph</code></p>\n</li>\n<li>\n<p>Show changes for all branches (with graph)</p>\n<p><code>git log --graph --all</code></p>\n</li>\n<li>\n<p>Show one line per commit</p>\n<p><code>git log --pretty=oneline</code></p>\n<p><code>pretty</code> has the following options: oneline, short, medium, full, fuller, email, raw.</p>\n</li>\n</ul>\n<h2 id=\"handlingmergeconflicts\">Handling merge conflicts</h2>\n<ul>\n<li>\n<p>Take source file</p>\n<p><code>git checkout --theirs &lt;file&gt;</code></p>\n</li>\n<li>\n<p>Keep your file</p>\n<p><code>git checkout --ours &lt;file&gt;</code></p>\n</li>\n</ul>\n<h2 id=\"diffingbetweenbranches\">Diffing between branches</h2>\n<ul>\n<li>\n<p>Regular diff with master</p>\n<p><code>git diff master..&lt;branch name&gt;</code></p>\n</li>\n<li>\n<p>Get the name of the files that has changed</p>\n<p><code>git diff --name-status master..&lt;branch name&gt;</code></p>\n</li>\n</ul>\n<h2 id=\"tagging\">Tagging</h2>\n<ul>\n<li>\n<p>Creating an annotated tag (recommended)</p>\n<p><code>git tag -a &lt;tag name&gt; -m '&lt;message&gt;'</code></p>\n</li>\n<li>\n<p>Creating an annotated tag from a checkin</p>\n<p><code>git tag -a &lt;tag name&gt; -m '&lt;message&gt;' &lt;commit checksum&gt;</code></p>\n</li>\n<li>\n<p>Listing all tags</p>\n<p><code>git tag</code></p>\n</li>\n<li>\n<p>Listing a defined list of tag</p>\n<p><code>git tag -l 'v1.4.*'</code></p>\n</li>\n<li>\n<p>Show tag info</p>\n<p><code>git show &lt;tag name&gt;</code></p>\n</li>\n<li>\n<p>Checkout from tag</p>\n<p><code>git checkout &lt;tag name&gt;</code></p>\n</li>\n<li>\n<p>Sharing one tag</p>\n<p><code>git push origin &lt;tag name&gt;</code></p>\n</li>\n<li>\n<p>Sharing all new tags</p>\n<p><code>git push origin --tags</code></p>\n</li>\n</ul>\n<h2 id=\"sources\">Sources</h2>\n<ul>\n<li><a href=\"https://www.openshift.com/wiki/github-workflow-for-submitting-pull-requests\">Github workflow for submitting pull requests</a></li>\n<li><a href=\"http://git-scm.com/book/en/Git-Branching-Rebasing\">Git rebasing</a></li>\n<li><a href=\"http://mislav.uniqpath.com/2010/07/git-tips/\">Another blog with tips</a></li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"21","plaintext":"Branching\n * Create branch\n   \n   git checkout -b <branch name>\n   \n   \n * Delete local branch\n   \n   git branch -d <branch name>\n   \n   \n * Delete remote branch\n   \n   git push origin :<branch name>\n   \n   \n * Synchronize your local branches with remote branches\n   \n   git fetch -p|--prune\n   \n   \n\nGetting changes\n * Simple get changes from remote\n   \n   git fetch <remote name>\n   \n   \n * Simple get changes and merge into your repository\n   \n   git pull [<remote name> [<remote branch>]]\n   \n   \n\nWorking with remotes\n * Add remote\n   \n   git remote add <name> <url>\n   \n   \n * Delete remote\n   \n   git remote rm <name>\n   \n   \n * Rename\n   \n   git remote rename <old name> <new name>\n   \n   \n * Listing of remote branches\n   \n   git remote show <remote name>\n   \n   \n * Checkout and track remote branch\n   \n   git checkout --track <remote name>/<remote branch>\n   \n   \n\nPushing the code\n * Initial push (to set the uplink)\n   \n   git push -u <remote> branch\n   \n   \n * If you have the upling set\n   \n   git push\n   \n   \n\nShowing the changes\n * Show changes\n   \n   git log\n   \n   \n * Show changes with graph\n   \n   git log --graph\n   \n   \n * Show changes for all branches (with graph)\n   \n   git log --graph --all\n   \n   \n * Show one line per commit\n   \n   git log --pretty=oneline\n   \n   pretty has the following options: oneline, short, medium, full, fuller,\n   email, raw.\n   \n   \n\nHandling merge conflicts\n * Take source file\n   \n   git checkout --theirs <file>\n   \n   \n * Keep your file\n   \n   git checkout --ours <file>\n   \n   \n\nDiffing between branches\n * Regular diff with master\n   \n   git diff master..<branch name>\n   \n   \n * Get the name of the files that has changed\n   \n   git diff --name-status master..<branch name>\n   \n   \n\nTagging\n * Creating an annotated tag (recommended)\n   \n   git tag -a <tag name> -m '<message>'\n   \n   \n * Creating an annotated tag from a checkin\n   \n   git tag -a <tag name> -m '<message>' <commit checksum>\n   \n   \n * Listing all tags\n   \n   git tag\n   \n   \n * Listing a defined list of tag\n   \n   git tag -l 'v1.4.*'\n   \n   \n * Show tag info\n   \n   git show <tag name>\n   \n   \n * Checkout from tag\n   \n   git checkout <tag name>\n   \n   \n * Sharing one tag\n   \n   git push origin <tag name>\n   \n   \n * Sharing all new tags\n   \n   git push origin --tags\n   \n   \n\nSources\n * Github workflow for submitting pull requests\n   [https://www.openshift.com/wiki/github-workflow-for-submitting-pull-requests]\n * Git rebasing [http://git-scm.com/book/en/Git-Branching-Rebasing]\n * Another blog with tips [http://mislav.uniqpath.com/2010/07/git-tips/]","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:35:43.000Z","updated_at":"2014-03-01T09:36:17.000Z","published_at":"2013-04-23T07:35:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe049","uuid":"e0aefc9d-832b-43d8-ae6f-5cbe3f850a8e","title":"Configuring Windows Azure Access Control Service","slug":"configuring-windows-azure-active-directory-access-control","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"When implementing an application I think it is nice if you can focus your development solving the problems the application should solve and almost nothing else. Most of the time authentication isn't really part of what your application should solve so if you can let someone else handling that part for you, and it would be really great if it were someone you trust.\\n\\nConfiguring Windows Azure Control Service, also known as Windows Azure Active Directory Control or ACS as I will call it through out this post, is not that hard to do as long as you know how to do it. However, a lot of the documentation out there is outdated (things move fast in this space) so I thought I would write this... which will most likely be outdated in a couple of days or so :). The walkthrough will be made using the latest version of .NET, that is, .NET 4.5 as of the moment.\\n\\n## How it works\\nWhen you authenticate against an application that uses Azure ACS it is some kind of table tennis game that goes on behind the curtains to identify you. This is a good thing because all you need to have in mind as a developer is the Security Token Service or STS. The STS in our case is Acure ACS and it is then responsible for coordinating everything with the other STS out there as google and microsoft live.\\n\\nThe table tennis going is illustrated below:\\n![ACS authentication flow][1]\\n\\n 1. The client to access some kind of resource from the application, also often mentioned as a Relying Party (RP).\\n 2. The application answer with a not authorize request and the clients is redirected to the access control site.\\n 3. At the access control site the client are presented with some different options that can be used to authenticate, as google or Microsoft Live for example.\\n 4. The client choose one of the identity providers and are redirected to the site where it authenticate authenticate.\\n 5. The client gets a token from the identity provider identifying the client.\\n 6. The client presents the token to the ACS and the ACS gives another token that should be used to authenticate the client.\\n 7. The client shows the token to the application and the authentication process is done.\\n\\n\\nThere are some trust that has to be established for this to work. First the application have to trust the access control and the access control has to trust the identity providers. In azure you can configure the urls of the application, that is, establishing the trust between the application and acs. You can also define which identity providers to use, that is you define that you trust them.\\n\\n## Configuring azure\\n\\n 1. Create the access control namespace\\n![Create namespace][2]\\n\\n 2. Choose your namespace and click \\\"manage\\\", this will take you to the management portal for access control\\n![Go to navigate][3]\\n\\n 3. Choose the identity providers you want to support\\n![enter image description here][4]\\n\\n 4. Add the relying party, which is basically which url you want to connect to the access control. Enter data in the `Realm` and `Return URL`\\n![Relying party configuration][5]\\n\\n 5. Click to edit the rule group that was created for your relying party and click generate to ![enter image description here][6]\\n\\n 6. The following three images shows you how to get to the management key which you will need later\\n![Management service][7]\\n![Management service symmetric key][8]\\n![Management service getting the key][9]\\n\\nThat's all you need to do in azure. Copy or write down the key that you located in the last step so you can use it later.\\n\\n## Configuring your application\\nBefore starting Visual Studio install the [Identity and Access Tool](http://visualstudiogallery.msdn.microsoft.com/e21bf653-dfe1-4d81-b3d3-795cb104066e) which will give you some help configuring your application to use the Azure ACS. To demonstrate how it works I will guide you through how to require authentication and list the claims received from the authentication.\\n\\n 1. Create an empty web application.\\n 2. Add a reference to `System.IdentityModel.Services`, which you will have if are running .Net 4.5.\\n 3. Right click on the web project and click the `Identity and Access...` option which is now available since you installed the Identity and Access Tool.\\n ![Identity and Access option][10]\\n 4. Check the \\\"Use the Windows Azure Access Control Service\\\" and click \\\"(Configure...)\\\"\\n ![Configure Access Control][11]\\n 5. In the \\\"Configure ACS namespace\\\" enter the name of your acs namespace, which you created in the first step configuring Azure, and enter the management key you retrieved in the last step in the field for the key.\\n![Configure ACS namespace][12]\\n\\n 6. In the `system.web` section in the web.config add the following\\n\\n    <authorization>\\n      <deny users=\\\"?\\\" />\\n    </authorization>\\n\\nAfter the steps above you have an updated web.config that enables you to use the ACS to authenticate but the application is still empty so we will implement a simple application that list all the claims retrieved when you have authenticated against Azure ACS in two simple steps:\\n\\n 1. So first add a `HomeController` with the following simple `Index` method:\\n\\n    public class HomeController : Controller\\n    {\\n        public ActionResult Index()\\n        {\\n            return View();\\n        }\\n    }\\n\\n 2. Add the `Index` view:\\n\\n        @using System.Security.Claims\\n        @using System.Threading\\n        @{\\n            var claimsIdentity = Thread.CurrentPrincipal.Identity as ClaimsIdentity;\\n        }\\n        <h2>Amazing claims:</h2>\\n        <ul>\\n        @foreach (var claim in claimsIdentity.Claims)\\n        {\\n            <li>\\n                <span class=\\\"key\\\">Type: @claim.Type</span>, \\n                <span class=\\\"value\\\">Value: @claim.Value</span>\\n            </li>  \\n        }\\n        </ul>\\n\\n\\nNow hit F5 and hopefully you will be presented a view telling you to authenticate with google or Microsoft Live. After you have authenticated all the claims retreived are listed on the page.\\n\\n## Getting the application to run on Azure\\nThere are three things you need to do to get the application:\\n\\n 1. Add a relying party for your application.\\n 2. Add the following web.config transform to change the realm:\\n\\n        <system.identityModel>\\n            <identityConfiguration>\\n                <audienceUris>\\n                    <add value=\\\"http://yourapp/\\\" xdt:Transform=\\\"Replace\\\"/>\\n                </audienceUris>\\n            </identityConfiguration>\\n        </system.identityModel>\\n        <system.identityModel.services>\\n            <federationConfiguration>\\n                <wsFederation realm=\\\"http://yourapp/\\\" xdt:Transform=\\\"SetAttributes(realm)\\\" />\\n            </federationConfiguration>\\n        </system.identityModel.services>\\n\\n 3. Create a Windows Azure Clour Service project and deploy your application.\\n\\nThat was all for now and hope the description is correct.\\n\\n  [1]: https://vpm6ug.blu.livefilestore.com/y1pVRlI1m7jv1E0xMc1tH8QWUXNAquFCa8jQB1f3061QzmGPOoLtqJtfQUSK3VdTIIdyFhEjJG1TTZkjiSDDFqCIED_7uqEBMR7/ACS_flow.png?psid=1\\n  [2]: https://qbtmcq.blu.livefilestore.com/y1pEN61EJrZ-ljnAdi9ThVij98kFuNZ2mJgYc5zJppJPtQmQFWf9byp_aZgYHHN3heJHHhx2lHODpVNciyRcIr0H-sUbRLvxGI4/ACS_Azure1.png?psid=1 \\\"Create namespace\\\"\\n  [3]: https://qbtmcq.blu.livefilestore.com/y1p2LEuZwCar7Vu3XOaMipciEMx0-cCap6ha4EneKtHgI1ENjRVtCGrI2yABg0ar-pxPOxgCFJ-9c1Y4OoGevIc8p-lWW4Rjg8_/ACS_Azure2.png?psid=1 \\\"Go to navigate\\\"\\n  [4]: https://qbtmcq.blu.livefilestore.com/y1p3VfAUpOvTuflmORgwLs3z8j3iWHsn_hAokLt76OkDvzxFcACoy6-nnALqDKwzGjJZHe3rmuZyFH_OBDkv5zYlKfz4tKv91eJ/ACS_Azure3.png?psid=1 \\\"Choose identity providers\\\"\\n  [5]: https://qbtmcq.blu.livefilestore.com/y1pnNN6uqnWDjY1bmPZGi-KB0fXLKJsT0GkK8f0Q2O8qnNpTrfkiNEdrnKM4n8QJ2X70tqWHB9L8DEsOpIBuhqLBW_Odb03_Rbe/ACS_Azure4.png?psid=1 \\\"Relying party\\\"\\n  [6]: https://qbtmcq.blu.livefilestore.com/y1pog5hQp31PaFGYQ7TetjLQelLR65BoWStuq3_JhG0uBZ7O9HHrkTyVFMb3XcJRBGvzdVYYsyiAbx5D_7zWJJxWFWvDla170hj/ACS_Azure5.png?psid=1 \\\"Rule group configuration\\\"\\n  [7]: https://qbtmcq.blu.livefilestore.com/y1pAROgKMCyjpGHSoQqROIZ0Lh6vUF3ABJirpvEgeWaCG93X8qdrEkbNYnGG5_rRsfFzNfzsHS8v2JlNE-c9eSLLTssCYYKu6Si/ACS_Azure6.png?psid=1\\n  [8]: https://qbtmcq.blu.livefilestore.com/y1pAbToQkhuTS5Rn-uEKAx-cdZkAaJRPQPZ5Jqn_KIqCOL-LBa76uY3qamo6T0u2aI66h_YXT4Xrw1LeoaTRcGYYHrgOKAFswbb/ACS_Azure7.png?psid=1 \\\"Management service symmetric key\\\"\\n  [9]: https://qbtmcq.blu.livefilestore.com/y1phuCt4lLPSWVGw8W_C6vWa889UmJzQZS-89e9Wi3t25yN_RHJtiKz000CD-80J_kWdWUXqjVRDCbIfp7TfC6cRYMY93TyGpN0/ACS_Azure8.png?psid=1 \\\"Edit Management service\\\"\\n  [10]: https://qbtmcq.blu.livefilestore.com/y1p8fMIToSsRCx9rzZKYG24MHyhwySNZQXceY-1VepSbs8tyvYIq1XxTbNFggBjHJr2sPWNPVzSMH45TK11WwXUV59AAcLjR9sj/ACS_VS1.png?psid=1 \\\"Identity and Access option\\\"\\n  [11]: https://qbtmcq.blu.livefilestore.com/y1pm7SOOPUBoMQi-mnayetcgY1meCH_uKRD-7Sr0NRRdtb2yUutdWvVqOis_bt1-la5bhK4DFtjOAM6bsLpcKrozWnT41V2Glqu/ACS_VS2.png?psid=1 \\\"Configure Access Control\\\"\\n  [12]: https://qbtmcq.blu.livefilestore.com/y1p84U3Fe8bE03Hkl0UddGeSOTLJiK8t9Uz3Ik_Weu5cDa6HTGxnxtN_wMychIct_UwcblUet8QdF7uKlhKjVABVVZg-2U1xM3D/ACS_VS3.png?psid=1 \\\"Configure ACS namespace\\\"\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>When implementing an application I think it is nice if you can focus your development solving the problems the application should solve and almost nothing else. Most of the time authentication isn't really part of what your application should solve so if you can let someone else handling that part for you, and it would be really great if it were someone you trust.</p>\n<p>Configuring Windows Azure Control Service, also known as Windows Azure Active Directory Control or ACS as I will call it through out this post, is not that hard to do as long as you know how to do it. However, a lot of the documentation out there is outdated (things move fast in this space) so I thought I would write this... which will most likely be outdated in a couple of days or so :). The walkthrough will be made using the latest version of .NET, that is, .NET 4.5 as of the moment.</p>\n<h2 id=\"howitworks\">How it works</h2>\n<p>When you authenticate against an application that uses Azure ACS it is some kind of table tennis game that goes on behind the curtains to identify you. This is a good thing because all you need to have in mind as a developer is the Security Token Service or STS. The STS in our case is Acure ACS and it is then responsible for coordinating everything with the other STS out there as google and microsoft live.</p>\n<p>The table tennis going is illustrated below:<br>\n<img src=\"https://vpm6ug.blu.livefilestore.com/y1pVRlI1m7jv1E0xMc1tH8QWUXNAquFCa8jQB1f3061QzmGPOoLtqJtfQUSK3VdTIIdyFhEjJG1TTZkjiSDDFqCIED_7uqEBMR7/ACS_flow.png?psid=1\" alt=\"ACS authentication flow\"></p>\n<ol>\n<li>The client to access some kind of resource from the application, also often mentioned as a Relying Party (RP).</li>\n<li>The application answer with a not authorize request and the clients is redirected to the access control site.</li>\n<li>At the access control site the client are presented with some different options that can be used to authenticate, as google or Microsoft Live for example.</li>\n<li>The client choose one of the identity providers and are redirected to the site where it authenticate authenticate.</li>\n<li>The client gets a token from the identity provider identifying the client.</li>\n<li>The client presents the token to the ACS and the ACS gives another token that should be used to authenticate the client.</li>\n<li>The client shows the token to the application and the authentication process is done.</li>\n</ol>\n<p>There are some trust that has to be established for this to work. First the application have to trust the access control and the access control has to trust the identity providers. In azure you can configure the urls of the application, that is, establishing the trust between the application and acs. You can also define which identity providers to use, that is you define that you trust them.</p>\n<h2 id=\"configuringazure\">Configuring azure</h2>\n<ol>\n<li>\n<p>Create the access control namespace<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1pEN61EJrZ-ljnAdi9ThVij98kFuNZ2mJgYc5zJppJPtQmQFWf9byp_aZgYHHN3heJHHhx2lHODpVNciyRcIr0H-sUbRLvxGI4/ACS_Azure1.png?psid=1\" alt=\"Create namespace\" title=\"Create namespace\"></p>\n</li>\n<li>\n<p>Choose your namespace and click &quot;manage&quot;, this will take you to the management portal for access control<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1p2LEuZwCar7Vu3XOaMipciEMx0-cCap6ha4EneKtHgI1ENjRVtCGrI2yABg0ar-pxPOxgCFJ-9c1Y4OoGevIc8p-lWW4Rjg8_/ACS_Azure2.png?psid=1\" alt=\"Go to navigate\" title=\"Go to navigate\"></p>\n</li>\n<li>\n<p>Choose the identity providers you want to support<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1p3VfAUpOvTuflmORgwLs3z8j3iWHsn_hAokLt76OkDvzxFcACoy6-nnALqDKwzGjJZHe3rmuZyFH_OBDkv5zYlKfz4tKv91eJ/ACS_Azure3.png?psid=1\" alt=\"enter image description here\" title=\"Choose identity providers\"></p>\n</li>\n<li>\n<p>Add the relying party, which is basically which url you want to connect to the access control. Enter data in the <code>Realm</code> and <code>Return URL</code><br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1pnNN6uqnWDjY1bmPZGi-KB0fXLKJsT0GkK8f0Q2O8qnNpTrfkiNEdrnKM4n8QJ2X70tqWHB9L8DEsOpIBuhqLBW_Odb03_Rbe/ACS_Azure4.png?psid=1\" alt=\"Relying party configuration\" title=\"Relying party\"></p>\n</li>\n<li>\n<p>Click to edit the rule group that was created for your relying party and click generate to <img src=\"https://qbtmcq.blu.livefilestore.com/y1pog5hQp31PaFGYQ7TetjLQelLR65BoWStuq3_JhG0uBZ7O9HHrkTyVFMb3XcJRBGvzdVYYsyiAbx5D_7zWJJxWFWvDla170hj/ACS_Azure5.png?psid=1\" alt=\"enter image description here\" title=\"Rule group configuration\"></p>\n</li>\n<li>\n<p>The following three images shows you how to get to the management key which you will need later<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1pAROgKMCyjpGHSoQqROIZ0Lh6vUF3ABJirpvEgeWaCG93X8qdrEkbNYnGG5_rRsfFzNfzsHS8v2JlNE-c9eSLLTssCYYKu6Si/ACS_Azure6.png?psid=1\" alt=\"Management service\"><br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1pAbToQkhuTS5Rn-uEKAx-cdZkAaJRPQPZ5Jqn_KIqCOL-LBa76uY3qamo6T0u2aI66h_YXT4Xrw1LeoaTRcGYYHrgOKAFswbb/ACS_Azure7.png?psid=1\" alt=\"Management service symmetric key\" title=\"Management service symmetric key\"><br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1phuCt4lLPSWVGw8W_C6vWa889UmJzQZS-89e9Wi3t25yN_RHJtiKz000CD-80J_kWdWUXqjVRDCbIfp7TfC6cRYMY93TyGpN0/ACS_Azure8.png?psid=1\" alt=\"Management service getting the key\" title=\"Edit Management service\"></p>\n</li>\n</ol>\n<p>That's all you need to do in azure. Copy or write down the key that you located in the last step so you can use it later.</p>\n<h2 id=\"configuringyourapplication\">Configuring your application</h2>\n<p>Before starting Visual Studio install the <a href=\"http://visualstudiogallery.msdn.microsoft.com/e21bf653-dfe1-4d81-b3d3-795cb104066e\">Identity and Access Tool</a> which will give you some help configuring your application to use the Azure ACS. To demonstrate how it works I will guide you through how to require authentication and list the claims received from the authentication.</p>\n<ol>\n<li>\n<p>Create an empty web application.</p>\n</li>\n<li>\n<p>Add a reference to <code>System.IdentityModel.Services</code>, which you will have if are running .Net 4.5.</p>\n</li>\n<li>\n<p>Right click on the web project and click the <code>Identity and Access...</code> option which is now available since you installed the Identity and Access Tool.<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1p8fMIToSsRCx9rzZKYG24MHyhwySNZQXceY-1VepSbs8tyvYIq1XxTbNFggBjHJr2sPWNPVzSMH45TK11WwXUV59AAcLjR9sj/ACS_VS1.png?psid=1\" alt=\"Identity and Access option\" title=\"Identity and Access option\"></p>\n</li>\n<li>\n<p>Check the &quot;Use the Windows Azure Access Control Service&quot; and click &quot;(Configure...)&quot;<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1pm7SOOPUBoMQi-mnayetcgY1meCH_uKRD-7Sr0NRRdtb2yUutdWvVqOis_bt1-la5bhK4DFtjOAM6bsLpcKrozWnT41V2Glqu/ACS_VS2.png?psid=1\" alt=\"Configure Access Control\" title=\"Configure Access Control\"></p>\n</li>\n<li>\n<p>In the &quot;Configure ACS namespace&quot; enter the name of your acs namespace, which you created in the first step configuring Azure, and enter the management key you retrieved in the last step in the field for the key.<br>\n<img src=\"https://qbtmcq.blu.livefilestore.com/y1p84U3Fe8bE03Hkl0UddGeSOTLJiK8t9Uz3Ik_Weu5cDa6HTGxnxtN_wMychIct_UwcblUet8QdF7uKlhKjVABVVZg-2U1xM3D/ACS_VS3.png?psid=1\" alt=\"Configure ACS namespace\" title=\"Configure ACS namespace\"></p>\n</li>\n<li>\n<p>In the <code>system.web</code> section in the web.config add the following</p>\n<authorization>\n  <deny users=\"?\" />\n</authorization>\n</li>\n</ol>\n<p>After the steps above you have an updated web.config that enables you to use the ACS to authenticate but the application is still empty so we will implement a simple application that list all the claims retrieved when you have authenticated against Azure ACS in two simple steps:</p>\n<ol>\n<li>\n<p>So first add a <code>HomeController</code> with the following simple <code>Index</code> method:</p>\n<p>public class HomeController : Controller<br>\n{<br>\npublic ActionResult Index()<br>\n{<br>\nreturn View();<br>\n}<br>\n}</p>\n</li>\n<li>\n<p>Add the <code>Index</code> view:</p>\n<pre><code>@using System.Security.Claims\n@using System.Threading\n@{\n    var claimsIdentity = Thread.CurrentPrincipal.Identity as ClaimsIdentity;\n}\n&lt;h2&gt;Amazing claims:&lt;/h2&gt;\n&lt;ul&gt;\n@foreach (var claim in claimsIdentity.Claims)\n{\n    &lt;li&gt;\n        &lt;span class=&quot;key&quot;&gt;Type: @claim.Type&lt;/span&gt;, \n        &lt;span class=&quot;value&quot;&gt;Value: @claim.Value&lt;/span&gt;\n    &lt;/li&gt;  \n}\n&lt;/ul&gt;\n</code></pre>\n</li>\n</ol>\n<p>Now hit F5 and hopefully you will be presented a view telling you to authenticate with google or Microsoft Live. After you have authenticated all the claims retreived are listed on the page.</p>\n<h2 id=\"gettingtheapplicationtorunonazure\">Getting the application to run on Azure</h2>\n<p>There are three things you need to do to get the application:</p>\n<ol>\n<li>\n<p>Add a relying party for your application.</p>\n</li>\n<li>\n<p>Add the following web.config transform to change the realm:</p>\n<pre><code>&lt;system.identityModel&gt;\n    &lt;identityConfiguration&gt;\n        &lt;audienceUris&gt;\n            &lt;add value=&quot;http://yourapp/&quot; xdt:Transform=&quot;Replace&quot;/&gt;\n        &lt;/audienceUris&gt;\n    &lt;/identityConfiguration&gt;\n&lt;/system.identityModel&gt;\n&lt;system.identityModel.services&gt;\n    &lt;federationConfiguration&gt;\n        &lt;wsFederation realm=&quot;http://yourapp/&quot; xdt:Transform=&quot;SetAttributes(realm)&quot; /&gt;\n    &lt;/federationConfiguration&gt;\n&lt;/system.identityModel.services&gt;\n</code></pre>\n</li>\n<li>\n<p>Create a Windows Azure Clour Service project and deploy your application.</p>\n</li>\n</ol>\n<p>That was all for now and hope the description is correct.</p>\n<!--kg-card-end: markdown-->","comment_id":"22","plaintext":"When implementing an application I think it is nice if you can focus your\ndevelopment solving the problems the application should solve and almost nothing\nelse. Most of the time authentication isn't really part of what your application\nshould solve so if you can let someone else handling that part for you, and it\nwould be really great if it were someone you trust.\n\nConfiguring Windows Azure Control Service, also known as Windows Azure Active\nDirectory Control or ACS as I will call it through out this post, is not that\nhard to do as long as you know how to do it. However, a lot of the documentation\nout there is outdated (things move fast in this space) so I thought I would\nwrite this... which will most likely be outdated in a couple of days or so :).\nThe walkthrough will be made using the latest version of .NET, that is, .NET 4.5\nas of the moment.\n\nHow it works\nWhen you authenticate against an application that uses Azure ACS it is some kind\nof table tennis game that goes on behind the curtains to identify you. This is a\ngood thing because all you need to have in mind as a developer is the Security\nToken Service or STS. The STS in our case is Acure ACS and it is then\nresponsible for coordinating everything with the other STS out there as google\nand microsoft live.\n\nThe table tennis going is illustrated below:\n\n\n 1. The client to access some kind of resource from the application, also often\n    mentioned as a Relying Party (RP).\n 2. The application answer with a not authorize request and the clients is\n    redirected to the access control site.\n 3. At the access control site the client are presented with some different\n    options that can be used to authenticate, as google or Microsoft Live for\n    example.\n 4. The client choose one of the identity providers and are redirected to the\n    site where it authenticate authenticate.\n 5. The client gets a token from the identity provider identifying the client.\n 6. The client presents the token to the ACS and the ACS gives another token\n    that should be used to authenticate the client.\n 7. The client shows the token to the application and the authentication process\n    is done.\n\nThere are some trust that has to be established for this to work. First the\napplication have to trust the access control and the access control has to trust\nthe identity providers. In azure you can configure the urls of the application,\nthat is, establishing the trust between the application and acs. You can also\ndefine which identity providers to use, that is you define that you trust them.\n\nConfiguring azure\n 1. Create the access control namespace\n    \n    \n    \n 2. Choose your namespace and click \"manage\", this will take you to the\n    management portal for access control\n    \n    \n    \n 3. Choose the identity providers you want to support\n    \n    \n    \n 4. Add the relying party, which is basically which url you want to connect to\n    the access control. Enter data in the Realm and Return URL\n    \n    \n    \n 5. Click to edit the rule group that was created for your relying party and\n    click generate to \n    \n    \n 6. The following three images shows you how to get to the management key which\n    you will need later\n    \n    \n    \n    \n    \n\nThat's all you need to do in azure. Copy or write down the key that you located\nin the last step so you can use it later.\n\nConfiguring your application\nBefore starting Visual Studio install the Identity and Access Tool\n[http://visualstudiogallery.msdn.microsoft.com/e21bf653-dfe1-4d81-b3d3-795cb104066e] \nwhich will give you some help configuring your application to use the Azure ACS.\nTo demonstrate how it works I will guide you through how to require\nauthentication and list the claims received from the authentication.\n\n 1. Create an empty web application.\n    \n    \n 2. Add a reference to System.IdentityModel.Services, which you will have if are\n    running .Net 4.5.\n    \n    \n 3. Right click on the web project and click the Identity and Access... option\n    which is now available since you installed the Identity and Access Tool.\n    \n    \n    \n 4. Check the \"Use the Windows Azure Access Control Service\" and click\n    \"(Configure...)\"\n    \n    \n    \n 5. In the \"Configure ACS namespace\" enter the name of your acs namespace, which\n    you created in the first step configuring Azure, and enter the management\n    key you retrieved in the last step in the field for the key.\n    \n    \n    \n 6. In the system.web section in the web.config add the following\n    \n    \n\nAfter the steps above you have an updated web.config that enables you to use the\nACS to authenticate but the application is still empty so we will implement a\nsimple application that list all the claims retrieved when you have\nauthenticated against Azure ACS in two simple steps:\n\n 1. So first add a HomeController with the following simple Index method:\n    \n    public class HomeController : Controller\n    {\n    public ActionResult Index()\n    {\n    return View();\n    }\n    }\n    \n    \n 2. Add the Index view:\n    \n    @using System.Security.Claims\n    @using System.Threading\n    @{\n        var claimsIdentity = Thread.CurrentPrincipal.Identity as ClaimsIdentity;\n    }\n    <h2>Amazing claims:</h2>\n    <ul>\n    @foreach (var claim in claimsIdentity.Claims)\n    {\n        <li>\n            <span class=\"key\">Type: @claim.Type</span>, \n            <span class=\"value\">Value: @claim.Value</span>\n        </li>  \n    }\n    </ul>\n    \n    \n    \n\nNow hit F5 and hopefully you will be presented a view telling you to\nauthenticate with google or Microsoft Live. After you have authenticated all the\nclaims retreived are listed on the page.\n\nGetting the application to run on Azure\nThere are three things you need to do to get the application:\n\n 1. Add a relying party for your application.\n    \n    \n 2. Add the following web.config transform to change the realm:\n    \n    <system.identityModel>\n        <identityConfiguration>\n            <audienceUris>\n                <add value=\"http://yourapp/\" xdt:Transform=\"Replace\"/>\n            </audienceUris>\n        </identityConfiguration>\n    </system.identityModel>\n    <system.identityModel.services>\n        <federationConfiguration>\n            <wsFederation realm=\"http://yourapp/\" xdt:Transform=\"SetAttributes(realm)\" />\n        </federationConfiguration>\n    </system.identityModel.services>\n    \n    \n    \n 3. Create a Windows Azure Clour Service project and deploy your application.\n    \n    \n\nThat was all for now and hope the description is correct.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:37:25.000Z","updated_at":"2014-03-01T09:37:34.000Z","published_at":"2013-04-28T07:37:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe04a","uuid":"a8bad3e5-bc5a-46bf-8c03-0a4b08db1f3d","title":"Effective continuous deployment with TeamCity, Octopus Deploy and PowerShell","slug":"effective-continuous-deployment-with-teamcity-and-octopus-deploy","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I've been a solid user of [TeamCity](http://www.jetbrains.com/teamcity/) for the last 3-4 years and started using [Octopus Deploy](http://octopusdeploy.com/) for about a year ago. Since then I have set up multiple configurations at several different sites and thought I would share my way of setting up an effective continuous deployment environment with the world.\\n\\nI will cover the basics as well as how you can configure TeamCity to create a new build configuration with just a few clicks. Before we get there we'll have to start at where we want to arrive and work our way from there. I will not cover how you install TeamCity and Octopus since that is already covered on their sites.\\n\\n## UPDATE\\nI've gathered the Power Shell script and created a NuGet package to make it easier to use. The NuGet package can be found [here](https://nuget.org/packages/OctoWebSetup/) and if you're interested in the code it is up on [github](https://github.com/mastoj/OctoWebSetup).\\n\\n## Setting up your web project\\nThe application I want to deploy is a standard ASP.NET MVC 4 application, I have enabled nuget package restore and the only package I've added is the [OctoPack](http://nuget.org/packages/OctoPack) to make it easier to build the deploy package that is needed by Octopus. I like to enable the nuget package restore before I install the OctoPack package since how it is installed in the projects differs if you have nuget package restore enabled or not.  \\n\\nWhen you have installed OctoPack all you need to do to build a deploy package is set the build parameter `RunOctoPack` to `true`, but I will come back to this later when setting up the build. We have some more things to cover first.\\n\\n### Creating the nuspec file\\nIf you aren't that familiar with [NuGet](http://nuget.codeplex.com/) package manager yet are really recommend you to get familiar with it. The format of the deployment packages used by Octopus has the same format as NuGet so you can just use NuGet to create the packages. To define some metadata and such we need a nuspec-file, which is a specification file for a nuget package. Creating such a file for a .NET-project is dead simple, you just run `nuget spec YourProject.csproj` which will create a `YourProject.nuspec` file. Include the file in your web project and change the parameters. The one I'm using looks something like this: \\n\\n    <?xml version=\\\"1.0\\\"?>\\n    <package >\\n        <metadata>\\n            <id>DemoProject.Web</id>\\n            <version>1.0.0</version>\\n            <title>DemoProject.Web</title>\\n            <authors>Tomas Jansson</authors>\\n            <owners>Tomas Jansson</owners>\\n            <requireLicenseAcceptance>false</requireLicenseAcceptance>\\n            <description>This is a sample project used for demoing Octopus and TeamCity</description>\\n            <releaseNotes>Some cool text goes here.</releaseNotes>\\n            <copyright>Copyright 2013</copyright>\\n            <tags>Octopus TeamCity</tags>\\n        </metadata>\\n    </package>\\n\\nDon't bother about the version number since that is something we will override anyway. \\n\\n### Adding the deployment scripts\\nOk, this is something that you don't have to do, but I have included it to show you that isn't that hard to program IIS and with PowerShell and get as much as possible automated. \\n\\nBefore I describe the scripts let set the stage. When Octopus run a deployment it goes through three phases in a successful deploy; pre deploy, deploy and post deploy. For each of these phases you can have a script running if it has the correct name; `PreDeploy.ps1`, `Deploy.ps1` or `PostDeploy.ps1` respectively. So the goal we have is in the pre deploy phase check if the web site or application exists and create it if it doesn't. With creating the application I mean basically do everything as creating application pool, creating site, creating application and set the bindings for the site. I would also be able to configure this as simple as possible and since the language is PowerShell I think [dot sourcing](http://technet.microsoft.com/en-us/library/ee176949.aspx#ECAA) of a configuration file is suitable. I have also decided to split the actual script that do the actual configuration in two scripts, one that contains all the functions manipulating IIS and one that uses the previous script to run through the configuration.\\n\\nThe first script is a sample configuration script `Dev.Config.ps1`:\\n\\n    $config = @{\\n        ApplicationPools = @(\\n            @{\\n                Name = \\\"DemoSiteAppPool\\\";\\n                FrameworkVersion = \\\"v4.0\\\"\\n            },\\n            @{\\n                Name = \\\"DemoSiteAppAppPool\\\";\\n                FrameworkVersion = \\\"v4.0\\\"\\n            });\\n        Site = @{\\n            Name = \\\"DemoSite\\\";\\n            SiteRoot = \\\"c:\\\\tmp\\\";\\n            AppPoolName = \\\"DemoSiteAppPool\\\";\\n            Port = 88\\n            Bindings = @(\\n                @{Port= 88; HostName= \\\"*\\\"}, \\n                @{Port= 89; HostName= \\\"DemoApp\\\"}\\n            );\\n            Application = @{\\n                Name = \\\"DemoApp\\\";\\n                AppPoolName = \\\"DemoSiteAppAppPool\\\";\\n                ApplicationRoot = \\\"c:\\\\tmp\\\"\\n            }\\n        };\\n    }\\n\\nIf you dot source that file you will get a `$config` object that has two properties; `ApplicationPools` and `Site`. Reading through this file I think it is pretty clear what I want it to configure so let's go through it from the top. First I define that I want two application pools, this is something that might be useful if you are running multiple apps under one site. After that I have one site defined and it should be set up on port 88, which is used when setting up the site. For the site I have also defined bindings which can override the original settings since this is something that you might want to change at a later time. The last part is the application and it is nested under the site.\\n\\nNow when the config is defined let's look at the `IISConfiguration.ps1` file that contains all the helper functions for configuring IIS: \\n\\n    Import-Module WebAdministration\\n\\n    $appPoolsPath = \\\"IIS:\\\\AppPools\\\"\\n    $iisSitesPath = \\\"iis:\\\\sites\\\"\\n\\n    function Write-Info ($message) {\\n        Write-Host \\\"Info:\\\" $message\\n    }\\n\\n    function Write-Error ($message) {\\n        Write-Host \\\"Error:\\\" $message\\n    }\\n\\n    function GuardAgainstNull($value, $message) {\\n        if($value -eq $null) {\\n            Write-Error $message\\n            exit 1\\n        }    \\n    }\\n\\n    function IISObjectExists($objectName) {\\n        return Test-Path $objectName\\n    }\\n\\n    function WebAppExists($appName) {\\n        if($appName.ToLower().StartsWith(\\\"iis\\\")) {\\n            return IISObjectExists $appName\\n        } else {\\n            return IISObjectExists \\\"IIS:\\\\Sites\\\\$appName\\\"\\n        }\\n    }\\n\\n    function WebSiteExists($siteName) {\\n        return WebAppExists $siteName \\n    }\\n\\n    function AppPoolExists($appPoolName) {\\n        return IISObjectExists \\\"$appPoolsPath\\\\$appPoolName\\\"\\n    }\\n\\n    function GetIfNull($value, $default) {\\n        if ($value -eq $null) { $default } else { $value }\\n    }\\n\\n    function CreateApplicationPool($appPoolName, $appPoolFrameworkVersion, $appPoolIdentityType, $userName, $password) {\\n        $appPoolFrameworkVersion = GetIfNull $appPoolFrameworkVersion \\\"v4.0\\\"\\n        $appPoolIdentityType = GetIfNull $appPoolIdentityType \\\"ApplicationPoolIdentity\\\"\\n        if($appPoolIdentityType -eq \\\"SpecificUser\\\") {\\n            GuardAgainstNull $userName \\\"userName and password must be set when using SpecificUser\\\"\\n            GuardAgainstNull $password \\\"userName and password must be set when using SpecificUser\\\"\\n        }\\n        \\n        if(AppPoolExists $appPoolName) {\\n            Write-Info \\\"Application pool already exists\\\"\\n        } else {\\n            Write-Info \\\"Creating application pool: $appPoolName\\\"\\n            $appPoolFullPath = \\\"$appPoolsPath\\\\$appPoolName\\\"\\n            $appPool = new-item $appPoolFullPath\\n            if($appPoolIdentityType -ne \\\"SpecificUser\\\") {\\n                Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=\\\"$appPoolIdentityType\\\"}\\n            }\\n            else {\\n                Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=\\\"$appPoolIdentityType\\\"; username=\\\"$userName\\\"; password=\\\"$password\\\"}\\n            }\\n            Set-ItemProperty $appPoolFullPath managedRuntimeVersion \\\"$appPoolFrameworkVersion\\\"\\n            Write-Info \\\"Application pool created\\\"\\n        }\\n    }\\n\\n    function GetNextSiteId {\\n        (dir $iisSitesPath | foreach {$_.id} | sort -Descending | select -first 1) + 1\\n    }\\n\\n    function CreateSite($siteName, $siteRoot, $appPoolName, $port) {\\n        $port = GetIfNull $port 80\\n        GuardAgainstNull $siteName \\\"siteName mest be set\\\"\\n        GuardAgainstNull $siteRoot \\\"siteRoot must be set\\\"\\n        GuardAgainstNull $appPoolName \\\"appPoolName must be set when creating a site\\\"\\n        if(WebSiteExists $siteName) {\\n            Write-Info \\\"Site $siteName already exists\\\"\\n        } else {\\n            Write-Info \\\"Creating site\\\"\\n            if (!(Test-Path $siteRoot)) {\\n                Write-Info \\\"Site root does not exist, creating...\\\"\\n                [void](new-item $siteRoot -itemType directory)\\n            }\\n\\n            $id = GetNextSiteId\\n            $sitePath = \\\"$iisSitesPath\\\\$siteName\\\"\\n            new-item $sitePath -bindings @{protocol=\\\"http\\\";bindingInformation=\\\"*:${port}:*\\\"} -id $id -physicalPath $siteRoot\\n            Set-ItemProperty $sitePath -name applicationPool -value \\\"$appPoolName\\\"\\n            Write-Info \\\"Site created, starting site\\\"\\n            Start-Website $siteName\\n        }\\n    }\\n\\n    function CreateApplication($siteName, $applicationName, $applicationRoot, $appPoolName) {\\n        GuardAgainstNull $siteName \\\"siteName mest be set\\\"\\n        GuardAgainstNull $applicationRoot \\\"applicationRoot must be set\\\"\\n        GuardAgainstNull $applicationName \\\"applicationName must be set\\\"\\n        GuardAgainstNull $appPoolName \\\"appPoolName must be set\\\"\\n        $applicationIISPath = ($iisSitesPath + \\\"\\\\\\\" + $siteName + \\\"\\\\\\\" + $applicationName)\\n        if(WebAppExists $applicationIISPath) {\\n            Write-Info \\\"Application $siteName\\\\$applicationName already exists\\\"\\n        }\\n        else {\\n            Write-Info \\\"Application does not exist, creating...\\\"\\n            New-Item $applicationIISPath -physicalPath \\\"$applicationRoot\\\" -type Application\\n            Set-ItemProperty $applicationIISPath -name applicationPool -value \\\"$appPoolName\\\"\\n            Write-Info \\\"Application Created\\\" \\n        }\\n    }\\n\\n    function GetHostNamesForSite($siteName) {\\n        return $site.bindings.Collection | %{$_.bindingInformation.Split(\\\":\\\")[2]}\\n    }\\n\\n    function ClearBindings($siteName) {\\n        Clear-ItemProperty \\\"$iisSitesPath\\\\$siteName\\\" -Name bindings\\n    }\\n\\n    function AddBindings($siteName, $bindings) {\\n        ForEach($binding in $bindings) {\\n            $port = $binding.Port\\n            $hostName = $binding.HostName\\n            New-WebBinding -Name $siteName -HostHeader $hostName -Port $port -Protocol \\\"http\\\"\\n        }\\n    }\\n\\n    #@(@{Port= 83; HostName= \\\"tomas\\\"}, @{Port= 84; HostName= \\\"tomas\\\"})\\n    function SetBindings($siteName, $bindings) {\\n        Write-Info \\\"Bindings will be deleted and added again\\\"\\n        Write-Info \\\"SiteName: $siteName\\\"\\n        Write-Info \\\"Bindings: $bindings\\\"\\n        if($bindings -ne $null) {\\n            Write-Info \\\"Deleting bindings\\\"\\n            ClearBindings $siteName\\n            Write-Info \\\"Adding bindings\\\"\\n            AddBindings $siteName $bindings\\n        }\\n    }\\n\\n    function CreateAppPools($appPoolsConfig) {\\n        Foreach($appPoolConfig in $appPoolsConfig) {\\n            $appPoolName = $appPoolConfig.Name\\n            $appPoolFrameworkVersion = $appPoolConfig.FrameworkVersion\\n            $appPoolIdentityType = $appPoolConfig.AppPoolIdentityType\\n            $userName = $appPoolConfig.UserName\\n            $password = $appPoolConfig.Password\\n            CreateApplicationPool $appPoolName $appPoolFrameworkVersion $appPoolIdentityType $userName $password\\n        }\\n    }\\n\\n    function CreateSiteFromConfig($siteConfig) {\\n        $siteName = $siteConfig.Name\\n        $siteRoot = $siteConfig.SiteRoot\\n        $appPoolName = $siteConfig.AppPoolName\\n        $port = $siteConfig.Port\\n        CreateSite $siteName $siteRoot $appPoolName $port\\n        \\n        if($siteConfig.Bindings) {\\n            SetBindings $siteName $siteConfig.Bindings\\n        }\\n        if($siteConfig.Application) {\\n            $applicationName = $siteConfig.Application.Name\\n            $applicationRoot = $siteConfig.Application.ApplicationRoot\\n            $appPoolName = $siteConfig.Application.AppPoolName\\n            CreateApplication $siteName $applicationName $applicationRoot $appPoolName\\n        }\\n    }\\n    \\nI won't go through this file in detail, one thing that is worth mentioning though is that this could be a module instead of a simple script file but this is the way I decided to do it. If someone has a really good reason to why I should make it a module let me know. The functions that is most use full are `CreateAppPools` and `CreateSiteFromConfig` since those are the functions that will be called from execution script which is up next.\\n\\nThe purpose of the `DoConfig.ps1` script is just a separation of concerns. \\n\\n    $ErrorActionPreference = \\\"Stop\\\"\\n\\n    function Get-ScriptDirectory\\n    {\\n        Split-Path $script:MyInvocation.MyCommand.Path\\n    }\\n\\n    function Get-OctopusWebSiteNameFromConfig($conf) {\\n        if($conf.Site) {\\n            if($conf.Site.Application) {\\n                return $conf.Site.Name + \\\"/\\\" + $conf.Site.Application.Name\\n            }\\n            return $conf.Site.Name\\n        }\\n        Write-Error \\\"Configuration is missing site\\\"\\n        exit 1\\n    }\\n\\n    if($configFile -eq $null) {\\n        $configFile = \\\"Local.Config.ps1\\\"\\n    }\\n\\n    $configFilePath = (Get-ScriptDirectory) + \\\"\\\\$configFile\\\"\\n\\n    $IISConfigurationScriptPath = (Get-ScriptDirectory) + \\\"\\\\IISConfiguration.ps1\\\"\\n    . $IISConfigurationScriptPath\\n\\n    . $configFilePath\\n\\n    CreateAppPools $config.ApplicationPools\\n    CreateSiteFromConfig $config.Site\\n\\n    Set-OctopusVariable -Name \\\"OctopusWebSiteName\\\" -Value (Get-OctopusWebSiteNameFromConfig $config)\\n\\nWhen looking at the script you might wonder where the variable `$configFile` comes from? The answer to this is it dependes :). You can set it yourself before running the script manually, but the purpose of this script is that it should be run by Octopus and in that context we will define the variable `$configFile` in Octopus so you can have different configuration files for different environments. This script uses both the script above and it also sets the Octopus variable `OctopusWebSiteName` on the last row which is really important. Setting that Octopus internal variable in the pre deploy phase means that it is set in the deploy phase and here Octopus will use it to set the path of the application you are deploying to point to the right folder.\\n\\nThe last and smallest script we will look at is `PreDeploy.ps`\\n\\n    function Get-ScriptDirectory\\n    {\\n        Split-Path $script:MyInvocation.MyCommand.Path\\n    }\\n\\n    $doConfigPath = (Get-ScriptDirectory) + \\\"\\\\DeployScripts\\\\\\\" + \\\"DoConfig.ps1\\\"\\n    . $doConfigPath\\n\\nAll this script does is calling `DoConfig.ps1`, the reason to why I've split the files this way is because you might want to do more things under pre deploy and this make it easier to follow what you are actually doing on a higher level.\\n\\nThe demo application I'm using looks like this:\\n\\n![Solution overview][1]\\n    \\n## Configuring Octopus\\nWe have actually covered the hardes part now with all that ground work. A solid foundation is always important :). I want show you in detail how to set up and configure Octopus, I'll just show how to set up the project. \\n\\n### Creating the deployment step\\nClick on steps for your project in Octopus deploy and then \\\"Add step\\\". The step you should add is a \\\"Deploy a NuGet package\\\". Name your step and and choose the repository where you will publish your package and enter the name of your deploy package, the id from the nuspec-file, in the \\\"NuGet package\\\" field. Select the role you want to deploy to and use default on the rest.\\n\\nWhen you have added your deployment step you also need to add one variable to your variables list. You need to add a variable `configFile` and set the name to the name of the configuration file. My sample looks like this:\\n\\n![Variables overview][2]\\n\\nWhen you have set up Octopus it is a good time to try everything out. First we need to build the deployment package: \\n\\n    MSBuild /t:Rebuild /p:RunOctoPack=true /p:OctopackPackageVersion=1.1.0 /p:PackageVersion=1.1.0 /p:OctoPackPublishPackageToFileShare=C:\\\\Packages .\\\\DemoProject.Web.sln\\n\\nThe command above will build the solution, create the deployment NuGet package and copy the package to C:\\\\Packages which I have set up as a NuGet repository in NuGet.\\n\\nThe next thing is to go back to Octopus and create a release and then try to deploy it. This should create application pools for you, setting up the site, application and the bindings for the application.\\n    \\n## Configuring TeamCity\\nSo far we have a nice deployment setup, but we want to take it a little bit further. For many of the customers I've been at we've had multiple projects and almost all of them have been web application. The setup in TeamCity I'll walk you through is useful if you are building and deploying multiple applications and want to make it easy to get started with a new application. \\n\\nI'll use the assumption that you are lucky enough that you have a default github root that can be used for all your projects. Also, the user that runs the TeamCity build agent and server are running as a specific user that you have generated an ssh key for that is uploaded to github for easier access. For local experimentation you could use your own account.\\n\\nTo make things easier I'll use the [Octopus plugin for TeamCity](http://octopusdeploy.com/documentation/integration/teamcity). I have also enabled the NuGet server that is included in TeamCity so that all the deploy packages I build is published automatically to the NuGet server provided by TeamCity.\\n\\nCreating template and separating things as I do here is not necessary if you only have one project, but it is really helpful if you are handling multiple projects and configurations. Also, this is not a guide to how should do it, this is a description of how I have set things up and what works for me.\\n\\n### Creating the templates\\nThe first template we will create is the one building a solution and creating the release packages. I call the template `BuildWebAndReleasePackage`. The settings you need to specify are the following:\\n\\n * General Settings\\n   - Build number format: `%conf.Version%.{0}`\\n * Version Control Settings\\n   - Create a new git VCS root with the following settings:\\n     - Fetch URL: `git@github.com:<your github user>/%conf.GitProjectName%.git`\\n     - Authentication Method: Default Private Key (this requires that you have configured the server to run as a user with access to the repo and configured ssh)\\n     - Check \\\"Make this VCS root available to all of the projects\\\"\\n     - You can later go back and experiment with the branch specification, but I will use the default for this now.\\n  * Add one Visual Studio build step with the following settings\\n    - Solution file path: `%conf.PathToSolution%`\\n    - Check \\\"Run OctoPack\\\" which you have if you installed the TeamCity plugin\\n    - OctoPack package version: `%conf.OctopusPackageVersion%`\\n  * Build Triggering\\n    - Add a new VCS Trigger, default settings is fine\\n  * Add an AssemblyInfo patcher under \\\"Additional Build Features\\\"\\n    - Assembly version format: `%build.number%` \\n  * Build parameters\\n    - Add one Environment variable `PackageVersion`: `%build.number%`\\n\\nThe second template we need is the one that trigger the deployment, I call it ´DeployWithOctopus`. Use the following settings:\\n\\n  * General Settings\\n    - Build number format: %conf.BuildNumber% (we will set this with a dependency)\\n  * Don't add any VCS settings\\n  * Add a \\\"OctopusDeploy: Release\\\" build step\\n    - Octopus URL: `<url to your Octopus server>`\\n    - API key: `<api key to a user on your Octopus server>`\\n    - Project: `%conf.OctoProjectName%`\\n    - Release number: `%build.number%`\\n    - Deploy to: `%conf.DeployEnvironment%`\\n    - Check \\\"Wait for deployment to complete\\\"\\n\\nI usually create a \\\"dummy\\\" project called something like \\\"Templates project\\\" where I put all the templates so the are accessible from every other project and that also makes it possible to share things like the Octopus api key between projects so I don't have to enter that for each of the projects I have.    \\n    \\n### Using the templates\\nNow when we have the templates we need to create our configurations using the templates. Let's start with the 'BuildWebAndReleasePackage'. Navigate to the template and locate the button \\\"Create build configuration\\\". Fill in the form you are presented with something like:\\n\\n![Creating build configuration][3]\\n\\nThe `conf.GitProjectName` is only the name of your git repo if you configured your VCS root correctly. The `conf.OctopusPackageVersion` is using the build number that is generated by `conf.Version` together with the incremental id of the build. When you have created the configuration move it to the project it belongs and the first configuration is all set.\\n\\nThe next configuration to create is the release configuration, which is of course based on the `DeployWithOctopus` template. There are some more stuff to configure here but it's not that complicated. First you create a build configuration from the template. Create a build from the template specifying values for all the fields except `conf.BuildNumber`. When it is created you move it to the same projects as the previous configuration. The next thing is to set up a snapshot dependency between the configurations, this sort of means that this configuration will use the same checkout as the previous configuration. To create this dependency go to \\\"Dependencies\\\" and choose \\\"Add new snapshot dependency\\\" (you can probably use artifact dependency as well but I choose snapshot). Here you find the configuration choose that one. The next thing we need is to set a trigger for the configuration so go to \\\"Build Triggers\\\" and choose \\\"Add new trigger\\\". Choose \\\"Finish Build Trigger\\\" and select the other configuration as well as check \\\"Trigger after successful build only\\\" checkbox. Now go back to the parameters and click the %conf.BuildNumber% and as value you start writing \\\"%dep\\\" which will give you a list of things starting with that. Choose the one that resembles `%dep.bt4.build.number%`, you can have a different number then the 4 I have depending on the state of your TeamCity server.\\n\\n## Summary\\nThis a setup, or variants of it, in scenarios where I have to handle more than just one project. It allows me to streamline how we do deployment and also formalize the procedure. In a way I use TeamCity and Octopus as the living documentation of how we get things from our code repository to the production environment. You can make the templates even more complex, like make the configuration just listen to certain branches for example. The way we have it in my current project is that we have a development template that build from every possible branch except master, but it doesn't create a deployment. If you want a deployment you have to trigger it manually so we don't flood the test environments for each push to every single branch. Then we have a release template that only listens to checkins to the master branch. When there is a checkin at the master branchwe build a release and deploy it to the test environment so we can verify it works. We have also added things like automatic release note generation between two master branch checkins for that template, it's a simple generation that creates a line for each commit and generates a markdown file with the links to the commits and appends it to the release so it shows up with the release in Octopus. Really nice and I might write about it later, I think this is enough for now. \\n\\nI hope someone find this useful, please comment if something is wrong or could be done better in another way. \\n\\n  [1]: https://public.blu.livefilestore.com/y2pBq3DbzywLNaGYrCsAaqTE17orjnSdJswnl2BQdy5QDayuLn_Wb5BsrpAIA0fnMFObB_1A9uDpbO7y4zfamCmI9e2GuvPD2okoJi93sfdGgg/OctoTeam01.PNG?psid=1\\n  [2]: https://public.blu.livefilestore.com/y2p1TwfqnKIWqATT7cvFvAXDwLx7thEIBh2nJs44r3PmGtgt25-Wk0Jb0Tcob8Yb9pB9HjdIK3dtum0BpmYwNUB5PnOXitIYfdjrS9SIiIjwPo/OctoTeam02.PNG?psid=1\\n  [3]: https://public.blu.livefilestore.com/y2pTXYbmVwxIofOD1-LA1ZRaADNbkffUICg8Uz3EYcV2Z7Q2ilo1fULCrs-JQs83a3AslVHmXKPI2PCMjFMdr2q7skDf0WYwhJpORtxPDPMKFc/OctoTeam03.PNG?psid=1\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I've been a solid user of <a href=\"http://www.jetbrains.com/teamcity/\">TeamCity</a> for the last 3-4 years and started using <a href=\"http://octopusdeploy.com/\">Octopus Deploy</a> for about a year ago. Since then I have set up multiple configurations at several different sites and thought I would share my way of setting up an effective continuous deployment environment with the world.</p>\n<p>I will cover the basics as well as how you can configure TeamCity to create a new build configuration with just a few clicks. Before we get there we'll have to start at where we want to arrive and work our way from there. I will not cover how you install TeamCity and Octopus since that is already covered on their sites.</p>\n<h2 id=\"update\">UPDATE</h2>\n<p>I've gathered the Power Shell script and created a NuGet package to make it easier to use. The NuGet package can be found <a href=\"https://nuget.org/packages/OctoWebSetup/\">here</a> and if you're interested in the code it is up on <a href=\"https://github.com/mastoj/OctoWebSetup\">github</a>.</p>\n<h2 id=\"settingupyourwebproject\">Setting up your web project</h2>\n<p>The application I want to deploy is a standard ASP.NET MVC 4 application, I have enabled nuget package restore and the only package I've added is the <a href=\"http://nuget.org/packages/OctoPack\">OctoPack</a> to make it easier to build the deploy package that is needed by Octopus. I like to enable the nuget package restore before I install the OctoPack package since how it is installed in the projects differs if you have nuget package restore enabled or not.</p>\n<p>When you have installed OctoPack all you need to do to build a deploy package is set the build parameter <code>RunOctoPack</code> to <code>true</code>, but I will come back to this later when setting up the build. We have some more things to cover first.</p>\n<h3 id=\"creatingthenuspecfile\">Creating the nuspec file</h3>\n<p>If you aren't that familiar with <a href=\"http://nuget.codeplex.com/\">NuGet</a> package manager yet are really recommend you to get familiar with it. The format of the deployment packages used by Octopus has the same format as NuGet so you can just use NuGet to create the packages. To define some metadata and such we need a nuspec-file, which is a specification file for a nuget package. Creating such a file for a .NET-project is dead simple, you just run <code>nuget spec YourProject.csproj</code> which will create a <code>YourProject.nuspec</code> file. Include the file in your web project and change the parameters. The one I'm using looks something like this:</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;package &gt;\n    &lt;metadata&gt;\n        &lt;id&gt;DemoProject.Web&lt;/id&gt;\n        &lt;version&gt;1.0.0&lt;/version&gt;\n        &lt;title&gt;DemoProject.Web&lt;/title&gt;\n        &lt;authors&gt;Tomas Jansson&lt;/authors&gt;\n        &lt;owners&gt;Tomas Jansson&lt;/owners&gt;\n        &lt;requireLicenseAcceptance&gt;false&lt;/requireLicenseAcceptance&gt;\n        &lt;description&gt;This is a sample project used for demoing Octopus and TeamCity&lt;/description&gt;\n        &lt;releaseNotes&gt;Some cool text goes here.&lt;/releaseNotes&gt;\n        &lt;copyright&gt;Copyright 2013&lt;/copyright&gt;\n        &lt;tags&gt;Octopus TeamCity&lt;/tags&gt;\n    &lt;/metadata&gt;\n&lt;/package&gt;\n</code></pre>\n<p>Don't bother about the version number since that is something we will override anyway.</p>\n<h3 id=\"addingthedeploymentscripts\">Adding the deployment scripts</h3>\n<p>Ok, this is something that you don't have to do, but I have included it to show you that isn't that hard to program IIS and with PowerShell and get as much as possible automated.</p>\n<p>Before I describe the scripts let set the stage. When Octopus run a deployment it goes through three phases in a successful deploy; pre deploy, deploy and post deploy. For each of these phases you can have a script running if it has the correct name; <code>PreDeploy.ps1</code>, <code>Deploy.ps1</code> or <code>PostDeploy.ps1</code> respectively. So the goal we have is in the pre deploy phase check if the web site or application exists and create it if it doesn't. With creating the application I mean basically do everything as creating application pool, creating site, creating application and set the bindings for the site. I would also be able to configure this as simple as possible and since the language is PowerShell I think <a href=\"http://technet.microsoft.com/en-us/library/ee176949.aspx#ECAA\">dot sourcing</a> of a configuration file is suitable. I have also decided to split the actual script that do the actual configuration in two scripts, one that contains all the functions manipulating IIS and one that uses the previous script to run through the configuration.</p>\n<p>The first script is a sample configuration script <code>Dev.Config.ps1</code>:</p>\n<pre><code>$config = @{\n    ApplicationPools = @(\n        @{\n            Name = &quot;DemoSiteAppPool&quot;;\n            FrameworkVersion = &quot;v4.0&quot;\n        },\n        @{\n            Name = &quot;DemoSiteAppAppPool&quot;;\n            FrameworkVersion = &quot;v4.0&quot;\n        });\n    Site = @{\n        Name = &quot;DemoSite&quot;;\n        SiteRoot = &quot;c:\\tmp&quot;;\n        AppPoolName = &quot;DemoSiteAppPool&quot;;\n        Port = 88\n        Bindings = @(\n            @{Port= 88; HostName= &quot;*&quot;}, \n            @{Port= 89; HostName= &quot;DemoApp&quot;}\n        );\n        Application = @{\n            Name = &quot;DemoApp&quot;;\n            AppPoolName = &quot;DemoSiteAppAppPool&quot;;\n            ApplicationRoot = &quot;c:\\tmp&quot;\n        }\n    };\n}\n</code></pre>\n<p>If you dot source that file you will get a <code>$config</code> object that has two properties; <code>ApplicationPools</code> and <code>Site</code>. Reading through this file I think it is pretty clear what I want it to configure so let's go through it from the top. First I define that I want two application pools, this is something that might be useful if you are running multiple apps under one site. After that I have one site defined and it should be set up on port 88, which is used when setting up the site. For the site I have also defined bindings which can override the original settings since this is something that you might want to change at a later time. The last part is the application and it is nested under the site.</p>\n<p>Now when the config is defined let's look at the <code>IISConfiguration.ps1</code> file that contains all the helper functions for configuring IIS:</p>\n<pre><code>Import-Module WebAdministration\n\n$appPoolsPath = &quot;IIS:\\AppPools&quot;\n$iisSitesPath = &quot;iis:\\sites&quot;\n\nfunction Write-Info ($message) {\n    Write-Host &quot;Info:&quot; $message\n}\n\nfunction Write-Error ($message) {\n    Write-Host &quot;Error:&quot; $message\n}\n\nfunction GuardAgainstNull($value, $message) {\n    if($value -eq $null) {\n        Write-Error $message\n        exit 1\n    }    \n}\n\nfunction IISObjectExists($objectName) {\n    return Test-Path $objectName\n}\n\nfunction WebAppExists($appName) {\n    if($appName.ToLower().StartsWith(&quot;iis&quot;)) {\n        return IISObjectExists $appName\n    } else {\n        return IISObjectExists &quot;IIS:\\Sites\\$appName&quot;\n    }\n}\n\nfunction WebSiteExists($siteName) {\n    return WebAppExists $siteName \n}\n\nfunction AppPoolExists($appPoolName) {\n    return IISObjectExists &quot;$appPoolsPath\\$appPoolName&quot;\n}\n\nfunction GetIfNull($value, $default) {\n    if ($value -eq $null) { $default } else { $value }\n}\n\nfunction CreateApplicationPool($appPoolName, $appPoolFrameworkVersion, $appPoolIdentityType, $userName, $password) {\n    $appPoolFrameworkVersion = GetIfNull $appPoolFrameworkVersion &quot;v4.0&quot;\n    $appPoolIdentityType = GetIfNull $appPoolIdentityType &quot;ApplicationPoolIdentity&quot;\n    if($appPoolIdentityType -eq &quot;SpecificUser&quot;) {\n        GuardAgainstNull $userName &quot;userName and password must be set when using SpecificUser&quot;\n        GuardAgainstNull $password &quot;userName and password must be set when using SpecificUser&quot;\n    }\n    \n    if(AppPoolExists $appPoolName) {\n        Write-Info &quot;Application pool already exists&quot;\n    } else {\n        Write-Info &quot;Creating application pool: $appPoolName&quot;\n        $appPoolFullPath = &quot;$appPoolsPath\\$appPoolName&quot;\n        $appPool = new-item $appPoolFullPath\n        if($appPoolIdentityType -ne &quot;SpecificUser&quot;) {\n            Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=&quot;$appPoolIdentityType&quot;}\n        }\n        else {\n            Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=&quot;$appPoolIdentityType&quot;; username=&quot;$userName&quot;; password=&quot;$password&quot;}\n        }\n        Set-ItemProperty $appPoolFullPath managedRuntimeVersion &quot;$appPoolFrameworkVersion&quot;\n        Write-Info &quot;Application pool created&quot;\n    }\n}\n\nfunction GetNextSiteId {\n    (dir $iisSitesPath | foreach {$_.id} | sort -Descending | select -first 1) + 1\n}\n\nfunction CreateSite($siteName, $siteRoot, $appPoolName, $port) {\n    $port = GetIfNull $port 80\n    GuardAgainstNull $siteName &quot;siteName mest be set&quot;\n    GuardAgainstNull $siteRoot &quot;siteRoot must be set&quot;\n    GuardAgainstNull $appPoolName &quot;appPoolName must be set when creating a site&quot;\n    if(WebSiteExists $siteName) {\n        Write-Info &quot;Site $siteName already exists&quot;\n    } else {\n        Write-Info &quot;Creating site&quot;\n        if (!(Test-Path $siteRoot)) {\n            Write-Info &quot;Site root does not exist, creating...&quot;\n            [void](new-item $siteRoot -itemType directory)\n        }\n\n        $id = GetNextSiteId\n        $sitePath = &quot;$iisSitesPath\\$siteName&quot;\n        new-item $sitePath -bindings @{protocol=&quot;http&quot;;bindingInformation=&quot;*:${port}:*&quot;} -id $id -physicalPath $siteRoot\n        Set-ItemProperty $sitePath -name applicationPool -value &quot;$appPoolName&quot;\n        Write-Info &quot;Site created, starting site&quot;\n        Start-Website $siteName\n    }\n}\n\nfunction CreateApplication($siteName, $applicationName, $applicationRoot, $appPoolName) {\n    GuardAgainstNull $siteName &quot;siteName mest be set&quot;\n    GuardAgainstNull $applicationRoot &quot;applicationRoot must be set&quot;\n    GuardAgainstNull $applicationName &quot;applicationName must be set&quot;\n    GuardAgainstNull $appPoolName &quot;appPoolName must be set&quot;\n    $applicationIISPath = ($iisSitesPath + &quot;\\&quot; + $siteName + &quot;\\&quot; + $applicationName)\n    if(WebAppExists $applicationIISPath) {\n        Write-Info &quot;Application $siteName\\$applicationName already exists&quot;\n    }\n    else {\n        Write-Info &quot;Application does not exist, creating...&quot;\n        New-Item $applicationIISPath -physicalPath &quot;$applicationRoot&quot; -type Application\n        Set-ItemProperty $applicationIISPath -name applicationPool -value &quot;$appPoolName&quot;\n        Write-Info &quot;Application Created&quot; \n    }\n}\n\nfunction GetHostNamesForSite($siteName) {\n    return $site.bindings.Collection | %{$_.bindingInformation.Split(&quot;:&quot;)[2]}\n}\n\nfunction ClearBindings($siteName) {\n    Clear-ItemProperty &quot;$iisSitesPath\\$siteName&quot; -Name bindings\n}\n\nfunction AddBindings($siteName, $bindings) {\n    ForEach($binding in $bindings) {\n        $port = $binding.Port\n        $hostName = $binding.HostName\n        New-WebBinding -Name $siteName -HostHeader $hostName -Port $port -Protocol &quot;http&quot;\n    }\n}\n\n#@(@{Port= 83; HostName= &quot;tomas&quot;}, @{Port= 84; HostName= &quot;tomas&quot;})\nfunction SetBindings($siteName, $bindings) {\n    Write-Info &quot;Bindings will be deleted and added again&quot;\n    Write-Info &quot;SiteName: $siteName&quot;\n    Write-Info &quot;Bindings: $bindings&quot;\n    if($bindings -ne $null) {\n        Write-Info &quot;Deleting bindings&quot;\n        ClearBindings $siteName\n        Write-Info &quot;Adding bindings&quot;\n        AddBindings $siteName $bindings\n    }\n}\n\nfunction CreateAppPools($appPoolsConfig) {\n    Foreach($appPoolConfig in $appPoolsConfig) {\n        $appPoolName = $appPoolConfig.Name\n        $appPoolFrameworkVersion = $appPoolConfig.FrameworkVersion\n        $appPoolIdentityType = $appPoolConfig.AppPoolIdentityType\n        $userName = $appPoolConfig.UserName\n        $password = $appPoolConfig.Password\n        CreateApplicationPool $appPoolName $appPoolFrameworkVersion $appPoolIdentityType $userName $password\n    }\n}\n\nfunction CreateSiteFromConfig($siteConfig) {\n    $siteName = $siteConfig.Name\n    $siteRoot = $siteConfig.SiteRoot\n    $appPoolName = $siteConfig.AppPoolName\n    $port = $siteConfig.Port\n    CreateSite $siteName $siteRoot $appPoolName $port\n    \n    if($siteConfig.Bindings) {\n        SetBindings $siteName $siteConfig.Bindings\n    }\n    if($siteConfig.Application) {\n        $applicationName = $siteConfig.Application.Name\n        $applicationRoot = $siteConfig.Application.ApplicationRoot\n        $appPoolName = $siteConfig.Application.AppPoolName\n        CreateApplication $siteName $applicationName $applicationRoot $appPoolName\n    }\n}\n</code></pre>\n<p>I won't go through this file in detail, one thing that is worth mentioning though is that this could be a module instead of a simple script file but this is the way I decided to do it. If someone has a really good reason to why I should make it a module let me know. The functions that is most use full are <code>CreateAppPools</code> and <code>CreateSiteFromConfig</code> since those are the functions that will be called from execution script which is up next.</p>\n<p>The purpose of the <code>DoConfig.ps1</code> script is just a separation of concerns.</p>\n<pre><code>$ErrorActionPreference = &quot;Stop&quot;\n\nfunction Get-ScriptDirectory\n{\n    Split-Path $script:MyInvocation.MyCommand.Path\n}\n\nfunction Get-OctopusWebSiteNameFromConfig($conf) {\n    if($conf.Site) {\n        if($conf.Site.Application) {\n            return $conf.Site.Name + &quot;/&quot; + $conf.Site.Application.Name\n        }\n        return $conf.Site.Name\n    }\n    Write-Error &quot;Configuration is missing site&quot;\n    exit 1\n}\n\nif($configFile -eq $null) {\n    $configFile = &quot;Local.Config.ps1&quot;\n}\n\n$configFilePath = (Get-ScriptDirectory) + &quot;\\$configFile&quot;\n\n$IISConfigurationScriptPath = (Get-ScriptDirectory) + &quot;\\IISConfiguration.ps1&quot;\n. $IISConfigurationScriptPath\n\n. $configFilePath\n\nCreateAppPools $config.ApplicationPools\nCreateSiteFromConfig $config.Site\n\nSet-OctopusVariable -Name &quot;OctopusWebSiteName&quot; -Value (Get-OctopusWebSiteNameFromConfig $config)\n</code></pre>\n<p>When looking at the script you might wonder where the variable <code>$configFile</code> comes from? The answer to this is it dependes :). You can set it yourself before running the script manually, but the purpose of this script is that it should be run by Octopus and in that context we will define the variable <code>$configFile</code> in Octopus so you can have different configuration files for different environments. This script uses both the script above and it also sets the Octopus variable <code>OctopusWebSiteName</code> on the last row which is really important. Setting that Octopus internal variable in the pre deploy phase means that it is set in the deploy phase and here Octopus will use it to set the path of the application you are deploying to point to the right folder.</p>\n<p>The last and smallest script we will look at is <code>PreDeploy.ps</code></p>\n<pre><code>function Get-ScriptDirectory\n{\n    Split-Path $script:MyInvocation.MyCommand.Path\n}\n\n$doConfigPath = (Get-ScriptDirectory) + &quot;\\DeployScripts\\&quot; + &quot;DoConfig.ps1&quot;\n. $doConfigPath\n</code></pre>\n<p>All this script does is calling <code>DoConfig.ps1</code>, the reason to why I've split the files this way is because you might want to do more things under pre deploy and this make it easier to follow what you are actually doing on a higher level.</p>\n<p>The demo application I'm using looks like this:</p>\n<p><img src=\"https://public.blu.livefilestore.com/y2pBq3DbzywLNaGYrCsAaqTE17orjnSdJswnl2BQdy5QDayuLn_Wb5BsrpAIA0fnMFObB_1A9uDpbO7y4zfamCmI9e2GuvPD2okoJi93sfdGgg/OctoTeam01.PNG?psid=1\" alt=\"Solution overview\"></p>\n<h2 id=\"configuringoctopus\">Configuring Octopus</h2>\n<p>We have actually covered the hardes part now with all that ground work. A solid foundation is always important :). I want show you in detail how to set up and configure Octopus, I'll just show how to set up the project.</p>\n<h3 id=\"creatingthedeploymentstep\">Creating the deployment step</h3>\n<p>Click on steps for your project in Octopus deploy and then &quot;Add step&quot;. The step you should add is a &quot;Deploy a NuGet package&quot;. Name your step and and choose the repository where you will publish your package and enter the name of your deploy package, the id from the nuspec-file, in the &quot;NuGet package&quot; field. Select the role you want to deploy to and use default on the rest.</p>\n<p>When you have added your deployment step you also need to add one variable to your variables list. You need to add a variable <code>configFile</code> and set the name to the name of the configuration file. My sample looks like this:</p>\n<p><img src=\"https://public.blu.livefilestore.com/y2p1TwfqnKIWqATT7cvFvAXDwLx7thEIBh2nJs44r3PmGtgt25-Wk0Jb0Tcob8Yb9pB9HjdIK3dtum0BpmYwNUB5PnOXitIYfdjrS9SIiIjwPo/OctoTeam02.PNG?psid=1\" alt=\"Variables overview\"></p>\n<p>When you have set up Octopus it is a good time to try everything out. First we need to build the deployment package:</p>\n<pre><code>MSBuild /t:Rebuild /p:RunOctoPack=true /p:OctopackPackageVersion=1.1.0 /p:PackageVersion=1.1.0 /p:OctoPackPublishPackageToFileShare=C:\\Packages .\\DemoProject.Web.sln\n</code></pre>\n<p>The command above will build the solution, create the deployment NuGet package and copy the package to C:\\Packages which I have set up as a NuGet repository in NuGet.</p>\n<p>The next thing is to go back to Octopus and create a release and then try to deploy it. This should create application pools for you, setting up the site, application and the bindings for the application.</p>\n<h2 id=\"configuringteamcity\">Configuring TeamCity</h2>\n<p>So far we have a nice deployment setup, but we want to take it a little bit further. For many of the customers I've been at we've had multiple projects and almost all of them have been web application. The setup in TeamCity I'll walk you through is useful if you are building and deploying multiple applications and want to make it easy to get started with a new application.</p>\n<p>I'll use the assumption that you are lucky enough that you have a default github root that can be used for all your projects. Also, the user that runs the TeamCity build agent and server are running as a specific user that you have generated an ssh key for that is uploaded to github for easier access. For local experimentation you could use your own account.</p>\n<p>To make things easier I'll use the <a href=\"http://octopusdeploy.com/documentation/integration/teamcity\">Octopus plugin for TeamCity</a>. I have also enabled the NuGet server that is included in TeamCity so that all the deploy packages I build is published automatically to the NuGet server provided by TeamCity.</p>\n<p>Creating template and separating things as I do here is not necessary if you only have one project, but it is really helpful if you are handling multiple projects and configurations. Also, this is not a guide to how should do it, this is a description of how I have set things up and what works for me.</p>\n<h3 id=\"creatingthetemplates\">Creating the templates</h3>\n<p>The first template we will create is the one building a solution and creating the release packages. I call the template <code>BuildWebAndReleasePackage</code>. The settings you need to specify are the following:</p>\n<ul>\n<li>General Settings\n<ul>\n<li>Build number format: <code>%conf.Version%.{0}</code></li>\n</ul>\n</li>\n<li>Version Control Settings\n<ul>\n<li>Create a new git VCS root with the following settings:\n<ul>\n<li>Fetch URL: <code>git@github.com:&lt;your github user&gt;/%conf.GitProjectName%.git</code></li>\n<li>Authentication Method: Default Private Key (this requires that you have configured the server to run as a user with access to the repo and configured ssh)</li>\n<li>Check &quot;Make this VCS root available to all of the projects&quot;</li>\n<li>You can later go back and experiment with the branch specification, but I will use the default for this now.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Add one Visual Studio build step with the following settings\n<ul>\n<li>Solution file path: <code>%conf.PathToSolution%</code></li>\n<li>Check &quot;Run OctoPack&quot; which you have if you installed the TeamCity plugin</li>\n<li>OctoPack package version: <code>%conf.OctopusPackageVersion%</code></li>\n</ul>\n</li>\n<li>Build Triggering\n<ul>\n<li>Add a new VCS Trigger, default settings is fine</li>\n</ul>\n</li>\n<li>Add an AssemblyInfo patcher under &quot;Additional Build Features&quot;\n<ul>\n<li>Assembly version format: <code>%build.number%</code></li>\n</ul>\n</li>\n<li>Build parameters\n<ul>\n<li>Add one Environment variable <code>PackageVersion</code>: <code>%build.number%</code></li>\n</ul>\n</li>\n</ul>\n<p>The second template we need is the one that trigger the deployment, I call it ´DeployWithOctopus`. Use the following settings:</p>\n<ul>\n<li>General Settings\n<ul>\n<li>Build number format: %conf.BuildNumber% (we will set this with a dependency)</li>\n</ul>\n</li>\n<li>Don't add any VCS settings</li>\n<li>Add a &quot;OctopusDeploy: Release&quot; build step\n<ul>\n<li>Octopus URL: <code>&lt;url to your Octopus server&gt;</code></li>\n<li>API key: <code>&lt;api key to a user on your Octopus server&gt;</code></li>\n<li>Project: <code>%conf.OctoProjectName%</code></li>\n<li>Release number: <code>%build.number%</code></li>\n<li>Deploy to: <code>%conf.DeployEnvironment%</code></li>\n<li>Check &quot;Wait for deployment to complete&quot;</li>\n</ul>\n</li>\n</ul>\n<p>I usually create a &quot;dummy&quot; project called something like &quot;Templates project&quot; where I put all the templates so the are accessible from every other project and that also makes it possible to share things like the Octopus api key between projects so I don't have to enter that for each of the projects I have.</p>\n<h3 id=\"usingthetemplates\">Using the templates</h3>\n<p>Now when we have the templates we need to create our configurations using the templates. Let's start with the 'BuildWebAndReleasePackage'. Navigate to the template and locate the button &quot;Create build configuration&quot;. Fill in the form you are presented with something like:</p>\n<p><img src=\"https://public.blu.livefilestore.com/y2pTXYbmVwxIofOD1-LA1ZRaADNbkffUICg8Uz3EYcV2Z7Q2ilo1fULCrs-JQs83a3AslVHmXKPI2PCMjFMdr2q7skDf0WYwhJpORtxPDPMKFc/OctoTeam03.PNG?psid=1\" alt=\"Creating build configuration\"></p>\n<p>The <code>conf.GitProjectName</code> is only the name of your git repo if you configured your VCS root correctly. The <code>conf.OctopusPackageVersion</code> is using the build number that is generated by <code>conf.Version</code> together with the incremental id of the build. When you have created the configuration move it to the project it belongs and the first configuration is all set.</p>\n<p>The next configuration to create is the release configuration, which is of course based on the <code>DeployWithOctopus</code> template. There are some more stuff to configure here but it's not that complicated. First you create a build configuration from the template. Create a build from the template specifying values for all the fields except <code>conf.BuildNumber</code>. When it is created you move it to the same projects as the previous configuration. The next thing is to set up a snapshot dependency between the configurations, this sort of means that this configuration will use the same checkout as the previous configuration. To create this dependency go to &quot;Dependencies&quot; and choose &quot;Add new snapshot dependency&quot; (you can probably use artifact dependency as well but I choose snapshot). Here you find the configuration choose that one. The next thing we need is to set a trigger for the configuration so go to &quot;Build Triggers&quot; and choose &quot;Add new trigger&quot;. Choose &quot;Finish Build Trigger&quot; and select the other configuration as well as check &quot;Trigger after successful build only&quot; checkbox. Now go back to the parameters and click the %conf.BuildNumber% and as value you start writing &quot;%dep&quot; which will give you a list of things starting with that. Choose the one that resembles <code>%dep.bt4.build.number%</code>, you can have a different number then the 4 I have depending on the state of your TeamCity server.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This a setup, or variants of it, in scenarios where I have to handle more than just one project. It allows me to streamline how we do deployment and also formalize the procedure. In a way I use TeamCity and Octopus as the living documentation of how we get things from our code repository to the production environment. You can make the templates even more complex, like make the configuration just listen to certain branches for example. The way we have it in my current project is that we have a development template that build from every possible branch except master, but it doesn't create a deployment. If you want a deployment you have to trigger it manually so we don't flood the test environments for each push to every single branch. Then we have a release template that only listens to checkins to the master branch. When there is a checkin at the master branchwe build a release and deploy it to the test environment so we can verify it works. We have also added things like automatic release note generation between two master branch checkins for that template, it's a simple generation that creates a line for each commit and generates a markdown file with the links to the commits and appends it to the release so it shows up with the release in Octopus. Really nice and I might write about it later, I think this is enough for now.</p>\n<p>I hope someone find this useful, please comment if something is wrong or could be done better in another way.</p>\n<!--kg-card-end: markdown-->","comment_id":"23","plaintext":"I've been a solid user of TeamCity [http://www.jetbrains.com/teamcity/] for the\nlast 3-4 years and started using Octopus Deploy [http://octopusdeploy.com/] for\nabout a year ago. Since then I have set up multiple configurations at several\ndifferent sites and thought I would share my way of setting up an effective\ncontinuous deployment environment with the world.\n\nI will cover the basics as well as how you can configure TeamCity to create a\nnew build configuration with just a few clicks. Before we get there we'll have\nto start at where we want to arrive and work our way from there. I will not\ncover how you install TeamCity and Octopus since that is already covered on\ntheir sites.\n\nUPDATE\nI've gathered the Power Shell script and created a NuGet package to make it\neasier to use. The NuGet package can be found here\n[https://nuget.org/packages/OctoWebSetup/] and if you're interested in the code\nit is up on github [https://github.com/mastoj/OctoWebSetup].\n\nSetting up your web project\nThe application I want to deploy is a standard ASP.NET MVC 4 application, I have\nenabled nuget package restore and the only package I've added is the OctoPack\n[http://nuget.org/packages/OctoPack] to make it easier to build the deploy\npackage that is needed by Octopus. I like to enable the nuget package restore\nbefore I install the OctoPack package since how it is installed in the projects\ndiffers if you have nuget package restore enabled or not.\n\nWhen you have installed OctoPack all you need to do to build a deploy package is\nset the build parameter RunOctoPack to true, but I will come back to this later\nwhen setting up the build. We have some more things to cover first.\n\nCreating the nuspec file\nIf you aren't that familiar with NuGet [http://nuget.codeplex.com/] package\nmanager yet are really recommend you to get familiar with it. The format of the\ndeployment packages used by Octopus has the same format as NuGet so you can just\nuse NuGet to create the packages. To define some metadata and such we need a\nnuspec-file, which is a specification file for a nuget package. Creating such a\nfile for a .NET-project is dead simple, you just run nuget spec\nYourProject.csproj which will create a YourProject.nuspec file. Include the file\nin your web project and change the parameters. The one I'm using looks something\nlike this:\n\n<?xml version=\"1.0\"?>\n<package >\n    <metadata>\n        <id>DemoProject.Web</id>\n        <version>1.0.0</version>\n        <title>DemoProject.Web</title>\n        <authors>Tomas Jansson</authors>\n        <owners>Tomas Jansson</owners>\n        <requireLicenseAcceptance>false</requireLicenseAcceptance>\n        <description>This is a sample project used for demoing Octopus and TeamCity</description>\n        <releaseNotes>Some cool text goes here.</releaseNotes>\n        <copyright>Copyright 2013</copyright>\n        <tags>Octopus TeamCity</tags>\n    </metadata>\n</package>\n\n\nDon't bother about the version number since that is something we will override\nanyway.\n\nAdding the deployment scripts\nOk, this is something that you don't have to do, but I have included it to show\nyou that isn't that hard to program IIS and with PowerShell and get as much as\npossible automated.\n\nBefore I describe the scripts let set the stage. When Octopus run a deployment\nit goes through three phases in a successful deploy; pre deploy, deploy and post\ndeploy. For each of these phases you can have a script running if it has the\ncorrect name; PreDeploy.ps1, Deploy.ps1 or PostDeploy.ps1 respectively. So the\ngoal we have is in the pre deploy phase check if the web site or application\nexists and create it if it doesn't. With creating the application I mean\nbasically do everything as creating application pool, creating site, creating\napplication and set the bindings for the site. I would also be able to configure\nthis as simple as possible and since the language is PowerShell I think dot\nsourcing [http://technet.microsoft.com/en-us/library/ee176949.aspx#ECAA] of a\nconfiguration file is suitable. I have also decided to split the actual script\nthat do the actual configuration in two scripts, one that contains all the\nfunctions manipulating IIS and one that uses the previous script to run through\nthe configuration.\n\nThe first script is a sample configuration script Dev.Config.ps1:\n\n$config = @{\n    ApplicationPools = @(\n        @{\n            Name = \"DemoSiteAppPool\";\n            FrameworkVersion = \"v4.0\"\n        },\n        @{\n            Name = \"DemoSiteAppAppPool\";\n            FrameworkVersion = \"v4.0\"\n        });\n    Site = @{\n        Name = \"DemoSite\";\n        SiteRoot = \"c:\\tmp\";\n        AppPoolName = \"DemoSiteAppPool\";\n        Port = 88\n        Bindings = @(\n            @{Port= 88; HostName= \"*\"}, \n            @{Port= 89; HostName= \"DemoApp\"}\n        );\n        Application = @{\n            Name = \"DemoApp\";\n            AppPoolName = \"DemoSiteAppAppPool\";\n            ApplicationRoot = \"c:\\tmp\"\n        }\n    };\n}\n\n\nIf you dot source that file you will get a $config object that has two\nproperties; ApplicationPools and Site. Reading through this file I think it is\npretty clear what I want it to configure so let's go through it from the top.\nFirst I define that I want two application pools, this is something that might\nbe useful if you are running multiple apps under one site. After that I have one\nsite defined and it should be set up on port 88, which is used when setting up\nthe site. For the site I have also defined bindings which can override the\noriginal settings since this is something that you might want to change at a\nlater time. The last part is the application and it is nested under the site.\n\nNow when the config is defined let's look at the IISConfiguration.ps1 file that\ncontains all the helper functions for configuring IIS:\n\nImport-Module WebAdministration\n\n$appPoolsPath = \"IIS:\\AppPools\"\n$iisSitesPath = \"iis:\\sites\"\n\nfunction Write-Info ($message) {\n    Write-Host \"Info:\" $message\n}\n\nfunction Write-Error ($message) {\n    Write-Host \"Error:\" $message\n}\n\nfunction GuardAgainstNull($value, $message) {\n    if($value -eq $null) {\n        Write-Error $message\n        exit 1\n    }    \n}\n\nfunction IISObjectExists($objectName) {\n    return Test-Path $objectName\n}\n\nfunction WebAppExists($appName) {\n    if($appName.ToLower().StartsWith(\"iis\")) {\n        return IISObjectExists $appName\n    } else {\n        return IISObjectExists \"IIS:\\Sites\\$appName\"\n    }\n}\n\nfunction WebSiteExists($siteName) {\n    return WebAppExists $siteName \n}\n\nfunction AppPoolExists($appPoolName) {\n    return IISObjectExists \"$appPoolsPath\\$appPoolName\"\n}\n\nfunction GetIfNull($value, $default) {\n    if ($value -eq $null) { $default } else { $value }\n}\n\nfunction CreateApplicationPool($appPoolName, $appPoolFrameworkVersion, $appPoolIdentityType, $userName, $password) {\n    $appPoolFrameworkVersion = GetIfNull $appPoolFrameworkVersion \"v4.0\"\n    $appPoolIdentityType = GetIfNull $appPoolIdentityType \"ApplicationPoolIdentity\"\n    if($appPoolIdentityType -eq \"SpecificUser\") {\n        GuardAgainstNull $userName \"userName and password must be set when using SpecificUser\"\n        GuardAgainstNull $password \"userName and password must be set when using SpecificUser\"\n    }\n    \n    if(AppPoolExists $appPoolName) {\n        Write-Info \"Application pool already exists\"\n    } else {\n        Write-Info \"Creating application pool: $appPoolName\"\n        $appPoolFullPath = \"$appPoolsPath\\$appPoolName\"\n        $appPool = new-item $appPoolFullPath\n        if($appPoolIdentityType -ne \"SpecificUser\") {\n            Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=\"$appPoolIdentityType\"}\n        }\n        else {\n            Set-ItemProperty $appPoolFullPath -name processModel -value @{identitytype=\"$appPoolIdentityType\"; username=\"$userName\"; password=\"$password\"}\n        }\n        Set-ItemProperty $appPoolFullPath managedRuntimeVersion \"$appPoolFrameworkVersion\"\n        Write-Info \"Application pool created\"\n    }\n}\n\nfunction GetNextSiteId {\n    (dir $iisSitesPath | foreach {$_.id} | sort -Descending | select -first 1) + 1\n}\n\nfunction CreateSite($siteName, $siteRoot, $appPoolName, $port) {\n    $port = GetIfNull $port 80\n    GuardAgainstNull $siteName \"siteName mest be set\"\n    GuardAgainstNull $siteRoot \"siteRoot must be set\"\n    GuardAgainstNull $appPoolName \"appPoolName must be set when creating a site\"\n    if(WebSiteExists $siteName) {\n        Write-Info \"Site $siteName already exists\"\n    } else {\n        Write-Info \"Creating site\"\n        if (!(Test-Path $siteRoot)) {\n            Write-Info \"Site root does not exist, creating...\"\n            [void](new-item $siteRoot -itemType directory)\n        }\n\n        $id = GetNextSiteId\n        $sitePath = \"$iisSitesPath\\$siteName\"\n        new-item $sitePath -bindings @{protocol=\"http\";bindingInformation=\"*:${port}:*\"} -id $id -physicalPath $siteRoot\n        Set-ItemProperty $sitePath -name applicationPool -value \"$appPoolName\"\n        Write-Info \"Site created, starting site\"\n        Start-Website $siteName\n    }\n}\n\nfunction CreateApplication($siteName, $applicationName, $applicationRoot, $appPoolName) {\n    GuardAgainstNull $siteName \"siteName mest be set\"\n    GuardAgainstNull $applicationRoot \"applicationRoot must be set\"\n    GuardAgainstNull $applicationName \"applicationName must be set\"\n    GuardAgainstNull $appPoolName \"appPoolName must be set\"\n    $applicationIISPath = ($iisSitesPath + \"\\\" + $siteName + \"\\\" + $applicationName)\n    if(WebAppExists $applicationIISPath) {\n        Write-Info \"Application $siteName\\$applicationName already exists\"\n    }\n    else {\n        Write-Info \"Application does not exist, creating...\"\n        New-Item $applicationIISPath -physicalPath \"$applicationRoot\" -type Application\n        Set-ItemProperty $applicationIISPath -name applicationPool -value \"$appPoolName\"\n        Write-Info \"Application Created\" \n    }\n}\n\nfunction GetHostNamesForSite($siteName) {\n    return $site.bindings.Collection | %{$_.bindingInformation.Split(\":\")[2]}\n}\n\nfunction ClearBindings($siteName) {\n    Clear-ItemProperty \"$iisSitesPath\\$siteName\" -Name bindings\n}\n\nfunction AddBindings($siteName, $bindings) {\n    ForEach($binding in $bindings) {\n        $port = $binding.Port\n        $hostName = $binding.HostName\n        New-WebBinding -Name $siteName -HostHeader $hostName -Port $port -Protocol \"http\"\n    }\n}\n\n#@(@{Port= 83; HostName= \"tomas\"}, @{Port= 84; HostName= \"tomas\"})\nfunction SetBindings($siteName, $bindings) {\n    Write-Info \"Bindings will be deleted and added again\"\n    Write-Info \"SiteName: $siteName\"\n    Write-Info \"Bindings: $bindings\"\n    if($bindings -ne $null) {\n        Write-Info \"Deleting bindings\"\n        ClearBindings $siteName\n        Write-Info \"Adding bindings\"\n        AddBindings $siteName $bindings\n    }\n}\n\nfunction CreateAppPools($appPoolsConfig) {\n    Foreach($appPoolConfig in $appPoolsConfig) {\n        $appPoolName = $appPoolConfig.Name\n        $appPoolFrameworkVersion = $appPoolConfig.FrameworkVersion\n        $appPoolIdentityType = $appPoolConfig.AppPoolIdentityType\n        $userName = $appPoolConfig.UserName\n        $password = $appPoolConfig.Password\n        CreateApplicationPool $appPoolName $appPoolFrameworkVersion $appPoolIdentityType $userName $password\n    }\n}\n\nfunction CreateSiteFromConfig($siteConfig) {\n    $siteName = $siteConfig.Name\n    $siteRoot = $siteConfig.SiteRoot\n    $appPoolName = $siteConfig.AppPoolName\n    $port = $siteConfig.Port\n    CreateSite $siteName $siteRoot $appPoolName $port\n    \n    if($siteConfig.Bindings) {\n        SetBindings $siteName $siteConfig.Bindings\n    }\n    if($siteConfig.Application) {\n        $applicationName = $siteConfig.Application.Name\n        $applicationRoot = $siteConfig.Application.ApplicationRoot\n        $appPoolName = $siteConfig.Application.AppPoolName\n        CreateApplication $siteName $applicationName $applicationRoot $appPoolName\n    }\n}\n\n\nI won't go through this file in detail, one thing that is worth mentioning\nthough is that this could be a module instead of a simple script file but this\nis the way I decided to do it. If someone has a really good reason to why I\nshould make it a module let me know. The functions that is most use full are \nCreateAppPools and CreateSiteFromConfig since those are the functions that will\nbe called from execution script which is up next.\n\nThe purpose of the DoConfig.ps1 script is just a separation of concerns.\n\n$ErrorActionPreference = \"Stop\"\n\nfunction Get-ScriptDirectory\n{\n    Split-Path $script:MyInvocation.MyCommand.Path\n}\n\nfunction Get-OctopusWebSiteNameFromConfig($conf) {\n    if($conf.Site) {\n        if($conf.Site.Application) {\n            return $conf.Site.Name + \"/\" + $conf.Site.Application.Name\n        }\n        return $conf.Site.Name\n    }\n    Write-Error \"Configuration is missing site\"\n    exit 1\n}\n\nif($configFile -eq $null) {\n    $configFile = \"Local.Config.ps1\"\n}\n\n$configFilePath = (Get-ScriptDirectory) + \"\\$configFile\"\n\n$IISConfigurationScriptPath = (Get-ScriptDirectory) + \"\\IISConfiguration.ps1\"\n. $IISConfigurationScriptPath\n\n. $configFilePath\n\nCreateAppPools $config.ApplicationPools\nCreateSiteFromConfig $config.Site\n\nSet-OctopusVariable -Name \"OctopusWebSiteName\" -Value (Get-OctopusWebSiteNameFromConfig $config)\n\n\nWhen looking at the script you might wonder where the variable $configFile comes\nfrom? The answer to this is it dependes :). You can set it yourself before\nrunning the script manually, but the purpose of this script is that it should be\nrun by Octopus and in that context we will define the variable $configFile in\nOctopus so you can have different configuration files for different\nenvironments. This script uses both the script above and it also sets the\nOctopus variable OctopusWebSiteName on the last row which is really important.\nSetting that Octopus internal variable in the pre deploy phase means that it is\nset in the deploy phase and here Octopus will use it to set the path of the\napplication you are deploying to point to the right folder.\n\nThe last and smallest script we will look at is PreDeploy.ps\n\nfunction Get-ScriptDirectory\n{\n    Split-Path $script:MyInvocation.MyCommand.Path\n}\n\n$doConfigPath = (Get-ScriptDirectory) + \"\\DeployScripts\\\" + \"DoConfig.ps1\"\n. $doConfigPath\n\n\nAll this script does is calling DoConfig.ps1, the reason to why I've split the\nfiles this way is because you might want to do more things under pre deploy and\nthis make it easier to follow what you are actually doing on a higher level.\n\nThe demo application I'm using looks like this:\n\n\n\nConfiguring Octopus\nWe have actually covered the hardes part now with all that ground work. A solid\nfoundation is always important :). I want show you in detail how to set up and\nconfigure Octopus, I'll just show how to set up the project.\n\nCreating the deployment step\nClick on steps for your project in Octopus deploy and then \"Add step\". The step\nyou should add is a \"Deploy a NuGet package\". Name your step and and choose the\nrepository where you will publish your package and enter the name of your deploy\npackage, the id from the nuspec-file, in the \"NuGet package\" field. Select the\nrole you want to deploy to and use default on the rest.\n\nWhen you have added your deployment step you also need to add one variable to\nyour variables list. You need to add a variable configFile and set the name to\nthe name of the configuration file. My sample looks like this:\n\n\n\nWhen you have set up Octopus it is a good time to try everything out. First we\nneed to build the deployment package:\n\nMSBuild /t:Rebuild /p:RunOctoPack=true /p:OctopackPackageVersion=1.1.0 /p:PackageVersion=1.1.0 /p:OctoPackPublishPackageToFileShare=C:\\Packages .\\DemoProject.Web.sln\n\n\nThe command above will build the solution, create the deployment NuGet package\nand copy the package to C:\\Packages which I have set up as a NuGet repository in\nNuGet.\n\nThe next thing is to go back to Octopus and create a release and then try to\ndeploy it. This should create application pools for you, setting up the site,\napplication and the bindings for the application.\n\nConfiguring TeamCity\nSo far we have a nice deployment setup, but we want to take it a little bit\nfurther. For many of the customers I've been at we've had multiple projects and\nalmost all of them have been web application. The setup in TeamCity I'll walk\nyou through is useful if you are building and deploying multiple applications\nand want to make it easy to get started with a new application.\n\nI'll use the assumption that you are lucky enough that you have a default github\nroot that can be used for all your projects. Also, the user that runs the\nTeamCity build agent and server are running as a specific user that you have\ngenerated an ssh key for that is uploaded to github for easier access. For local\nexperimentation you could use your own account.\n\nTo make things easier I'll use the Octopus plugin for TeamCity\n[http://octopusdeploy.com/documentation/integration/teamcity]. I have also\nenabled the NuGet server that is included in TeamCity so that all the deploy\npackages I build is published automatically to the NuGet server provided by\nTeamCity.\n\nCreating template and separating things as I do here is not necessary if you\nonly have one project, but it is really helpful if you are handling multiple\nprojects and configurations. Also, this is not a guide to how should do it, this\nis a description of how I have set things up and what works for me.\n\nCreating the templates\nThe first template we will create is the one building a solution and creating\nthe release packages. I call the template BuildWebAndReleasePackage. The\nsettings you need to specify are the following:\n\n * General Settings * Build number format: %conf.Version%.{0}\n   \n   \n * Version Control Settings * Create a new git VCS root with the following\n      settings: * Fetch URL: git@github.com:<your github user>/%conf.GitProjectName%.git\n       * Authentication Method: Default Private Key (this requires that\n         you have configured the server to run as a user with access to the repo\n         and configured ssh)\n       * Check \"Make this VCS root available to all of the projects\"\n       * You can later go back and experiment with the branch\n         specification, but I will use the default for this now.\n      \n      \n   \n   \n * Add one Visual Studio build step with the following settings * Solution file\n      path: %conf.PathToSolution%\n    * Check \"Run\n      OctoPack\" which you have if you installed the TeamCity plugin\n    * OctoPack\n      package version: %conf.OctopusPackageVersion%\n   \n   \n * Build Triggering * Add a new VCS Trigger, default settings is fine\n   \n   \n * Add an AssemblyInfo patcher under \"Additional Build Features\" * Assembly\n      version format: %build.number%\n   \n   \n * Build parameters * Add one Environment variable PackageVersion: %build.number%\n   \n   \n\nThe second template we need is the one that trigger the deployment, I call it\n´DeployWithOctopus`. Use the following settings:\n\n * General Settings * Build number format: %conf.BuildNumber% (we will set this\n      with a dependency)\n   \n   \n * Don't add any VCS settings\n * Add a \"OctopusDeploy: Release\" build step * Octopus URL: <url to your Octopus server>\n    * API key: <api key to a user on your Octopus server>\n    * Project: %conf.OctoProjectName%\n    * Release number: %build.number%\n    * Deploy to: %conf.DeployEnvironment%\n    * Check \"Wait for deployment to\n      complete\"\n   \n   \n\nI usually create a \"dummy\" project called something like \"Templates project\"\nwhere I put all the templates so the are accessible from every other project and\nthat also makes it possible to share things like the Octopus api key between\nprojects so I don't have to enter that for each of the projects I have.\n\nUsing the templates\nNow when we have the templates we need to create our configurations using the\ntemplates. Let's start with the 'BuildWebAndReleasePackage'. Navigate to the\ntemplate and locate the button \"Create build configuration\". Fill in the form\nyou are presented with something like:\n\n\n\nThe conf.GitProjectName is only the name of your git repo if you configured your\nVCS root correctly. The conf.OctopusPackageVersion is using the build number\nthat is generated by conf.Version together with the incremental id of the build.\nWhen you have created the configuration move it to the project it belongs and\nthe first configuration is all set.\n\nThe next configuration to create is the release configuration, which is of\ncourse based on the DeployWithOctopus template. There are some more stuff to\nconfigure here but it's not that complicated. First you create a build\nconfiguration from the template. Create a build from the template specifying\nvalues for all the fields except conf.BuildNumber. When it is created you move\nit to the same projects as the previous configuration. The next thing is to set\nup a snapshot dependency between the configurations, this sort of means that\nthis configuration will use the same checkout as the previous configuration. To\ncreate this dependency go to \"Dependencies\" and choose \"Add new snapshot\ndependency\" (you can probably use artifact dependency as well but I choose\nsnapshot). Here you find the configuration choose that one. The next thing we\nneed is to set a trigger for the configuration so go to \"Build Triggers\" and\nchoose \"Add new trigger\". Choose \"Finish Build Trigger\" and select the other\nconfiguration as well as check \"Trigger after successful build only\" checkbox.\nNow go back to the parameters and click the %conf.BuildNumber% and as value you\nstart writing \"%dep\" which will give you a list of things starting with that.\nChoose the one that resembles %dep.bt4.build.number%, you can have a different\nnumber then the 4 I have depending on the state of your TeamCity server.\n\nSummary\nThis a setup, or variants of it, in scenarios where I have to handle more than\njust one project. It allows me to streamline how we do deployment and also\nformalize the procedure. In a way I use TeamCity and Octopus as the living\ndocumentation of how we get things from our code repository to the production\nenvironment. You can make the templates even more complex, like make the\nconfiguration just listen to certain branches for example. The way we have it in\nmy current project is that we have a development template that build from every\npossible branch except master, but it doesn't create a deployment. If you want a\ndeployment you have to trigger it manually so we don't flood the test\nenvironments for each push to every single branch. Then we have a release\ntemplate that only listens to checkins to the master branch. When there is a\ncheckin at the master branchwe build a release and deploy it to the test\nenvironment so we can verify it works. We have also added things like automatic\nrelease note generation between two master branch checkins for that template,\nit's a simple generation that creates a line for each commit and generates a\nmarkdown file with the links to the commits and appends it to the release so it\nshows up with the release in Octopus. Really nice and I might write about it\nlater, I think this is enough for now.\n\nI hope someone find this useful, please comment if something is wrong or could\nbe done better in another way.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:38:15.000Z","updated_at":"2014-06-24T20:32:41.000Z","published_at":"2013-05-15T07:38:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe04b","uuid":"463f7c11-c251-4d7b-b2be-84b96fefc9f9","title":"I will be speaking at NDC London 2013","slug":"i_will_be_speaking_at_ndc_london_2013","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"![enter image description here][1]\\n\\nMy presentation about continuous deployment with [Octopus Deploy](http://www.octopusdeploy.com/) and [TeamCity](http://www.jetbrains.com/teamcity/) was accepted to [NDC London](http://ndc-london.com/). During the presentation I will show you how to configure your tools to be able to create a new web application and get it deployed in roughly 10 minutes.\\n\\n\\n  [1]: https://wcsjxa.blu.livefilestore.com/y2pvp0Fl3YkTdwo4PW1TSiNbaQFlnfz9kr8kkC27utxeonhV-CRjb6K5D0KDYIgFG00b8BvFL8p5Tx4hjmYGBD0iq5lP6Hhq94HDo099N7N1Yc/NDCBanner.jpg?psid=1\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p><img src=\"https://wcsjxa.blu.livefilestore.com/y2pvp0Fl3YkTdwo4PW1TSiNbaQFlnfz9kr8kkC27utxeonhV-CRjb6K5D0KDYIgFG00b8BvFL8p5Tx4hjmYGBD0iq5lP6Hhq94HDo099N7N1Yc/NDCBanner.jpg?psid=1\" alt=\"enter image description here\"></p>\n<p>My presentation about continuous deployment with <a href=\"http://www.octopusdeploy.com/\">Octopus Deploy</a> and <a href=\"http://www.jetbrains.com/teamcity/\">TeamCity</a> was accepted to <a href=\"http://ndc-london.com/\">NDC London</a>. During the presentation I will show you how to configure your tools to be able to create a new web application and get it deployed in roughly 10 minutes.</p>\n<!--kg-card-end: markdown-->","comment_id":"24","plaintext":"\n\nMy presentation about continuous deployment with Octopus Deploy\n[http://www.octopusdeploy.com/] and TeamCity\n[http://www.jetbrains.com/teamcity/] was accepted to NDC London\n[http://ndc-london.com/]. During the presentation I will show you how to\nconfigure your tools to be able to create a new web application and get it\ndeployed in roughly 10 minutes.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:40:09.000Z","updated_at":"2014-03-01T09:40:38.000Z","published_at":"2013-10-09T07:40:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe04c","uuid":"61034bda-199c-4d89-8b57-edd4b593a1e8","title":"Hello Owin!","slug":"hello-owin","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I'm planning to write a couple of short posts describing how to get started with Owin and how Owin works. This first will show you how to create the simplest self hosted \\\"Hello World\\\" application.\\n\\nLet's get started\\nWhen creating a self hosted Owin application you don't have to create the project from the web application project template in Visual Studio, or you really shouldn't since the whole point with self hosting is that you control the hosting. So in Visual Studio create a standard console application. The next thing you need to do is installing four [NuGet packages](http://www.nuget.org); Owin, Microsoft.Owin, Microsoft.Owin.Hosting and Microsoft.Owin.Host.HttpListener. There is a dependency from Microsoft.Owin.Hosting to Owin and Microsoft.Owin so in reality you onle need to run the two following commands in the package manager console:\\n\\n    install-package Microsoft.Owin.Hosting\\n\\tinstall-package Microsoft.Owin.Hosting.HttpListener\\n\\t\\nThe Microsoft.* packages comes with a lot of helper functionality that will make it easier to get started. \\n\\nThe next thing to do is to write the application, let's get right to the code:\\n\\n    using System;\\n    using Microsoft.Owin.Hosting;\\n    using Owin;\\n\\n    namespace HelloOwin\\n    {\\n\\n        class Program\\n        {\\n            static void Main(string[] args)\\n            {\\n                using(WebApp.Start<Startup>(url: \\\"http://localhost:9765/\\\"))\\n                {\\n                    Console.WriteLine(\\\"Appication is running\\\");\\n                    Console.ReadLine();\\n                }\\n            }\\n        }\\n\\t}\\n\\t\\nAs you can see this is a regular console application with the common `Main` method. When you run this application the `WebApp.Start` method will start an application of type `Startup` with the url ´http://localhost:9765´. The next thing to do is to write the actual application, that is, the `Startup` class.\\n\\n    using Owin;\\n\\n    namespace HelloOwin\\n    {\\n        public class Startup\\n        {\\n            public void Configuration(IAppBuilder app)\\n            {\\n                app.Run(context =>\\n                {\\n                    var task = context.Response.WriteAsync(\\\"Hello world!\\\");\\n                    return task;\\n                });\\n            }\\n        }\\n    }\\n\\t\\nBy convention the `Configuration` method, that has the same signature as the one in the example, is run to configure the application. The `app.Run` method takes a `Func` as parameter that should take the Owin context as a parameter. The `Func` must return a `Task`.\\n\\nIn the next post I will cover middleware and how you can setup multiple handlers for a request.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I'm planning to write a couple of short posts describing how to get started with Owin and how Owin works. This first will show you how to create the simplest self hosted &quot;Hello World&quot; application.</p>\n<p>Let's get started<br>\nWhen creating a self hosted Owin application you don't have to create the project from the web application project template in Visual Studio, or you really shouldn't since the whole point with self hosting is that you control the hosting. So in Visual Studio create a standard console application. The next thing you need to do is installing four <a href=\"http://www.nuget.org\">NuGet packages</a>; Owin, Microsoft.Owin, Microsoft.Owin.Hosting and Microsoft.Owin.Host.HttpListener. There is a dependency from Microsoft.Owin.Hosting to Owin and Microsoft.Owin so in reality you onle need to run the two following commands in the package manager console:</p>\n<pre><code>install-package Microsoft.Owin.Hosting\ninstall-package Microsoft.Owin.Hosting.HttpListener\n</code></pre>\n<p>The Microsoft.* packages comes with a lot of helper functionality that will make it easier to get started.</p>\n<p>The next thing to do is to write the application, let's get right to the code:</p>\n<pre><code>using System;\nusing Microsoft.Owin.Hosting;\nusing Owin;\n\nnamespace HelloOwin\n{\n\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            using(WebApp.Start&lt;Startup&gt;(url: &quot;http://localhost:9765/&quot;))\n            {\n                Console.WriteLine(&quot;Appication is running&quot;);\n                Console.ReadLine();\n            }\n        }\n    }\n}\n</code></pre>\n<p>As you can see this is a regular console application with the common <code>Main</code> method. When you run this application the <code>WebApp.Start</code> method will start an application of type <code>Startup</code> with the url ´http://localhost:9765´. The next thing to do is to write the actual application, that is, the <code>Startup</code> class.</p>\n<pre><code>using Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Run(context =&gt;\n            {\n                var task = context.Response.WriteAsync(&quot;Hello world!&quot;);\n                return task;\n            });\n        }\n    }\n}\n</code></pre>\n<p>By convention the <code>Configuration</code> method, that has the same signature as the one in the example, is run to configure the application. The <code>app.Run</code> method takes a <code>Func</code> as parameter that should take the Owin context as a parameter. The <code>Func</code> must return a <code>Task</code>.</p>\n<p>In the next post I will cover middleware and how you can setup multiple handlers for a request.</p>\n<!--kg-card-end: markdown-->","comment_id":"25","plaintext":"I'm planning to write a couple of short posts describing how to get started with\nOwin and how Owin works. This first will show you how to create the simplest\nself hosted \"Hello World\" application.\n\nLet's get started\nWhen creating a self hosted Owin application you don't have to create the\nproject from the web application project template in Visual Studio, or you\nreally shouldn't since the whole point with self hosting is that you control the\nhosting. So in Visual Studio create a standard console application. The next\nthing you need to do is installing four NuGet packages [http://www.nuget.org];\nOwin, Microsoft.Owin, Microsoft.Owin.Hosting and\nMicrosoft.Owin.Host.HttpListener. There is a dependency from\nMicrosoft.Owin.Hosting to Owin and Microsoft.Owin so in reality you onle need to\nrun the two following commands in the package manager console:\n\ninstall-package Microsoft.Owin.Hosting\ninstall-package Microsoft.Owin.Hosting.HttpListener\n\n\nThe Microsoft.* packages comes with a lot of helper functionality that will make\nit easier to get started.\n\nThe next thing to do is to write the application, let's get right to the code:\n\nusing System;\nusing Microsoft.Owin.Hosting;\nusing Owin;\n\nnamespace HelloOwin\n{\n\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            using(WebApp.Start<Startup>(url: \"http://localhost:9765/\"))\n            {\n                Console.WriteLine(\"Appication is running\");\n                Console.ReadLine();\n            }\n        }\n    }\n}\n\n\nAs you can see this is a regular console application with the common Main \nmethod. When you run this application the WebApp.Start method will start an\napplication of type Startup with the url ´http://localhost:9765´. The next thing\nto do is to write the actual application, that is, the Startup class.\n\nusing Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Run(context =>\n            {\n                var task = context.Response.WriteAsync(\"Hello world!\");\n                return task;\n            });\n        }\n    }\n}\n\n\nBy convention the Configuration method, that has the same signature as the one\nin the example, is run to configure the application. The app.Run method takes a \nFunc as parameter that should take the Owin context as a parameter. The Func \nmust return a Task.\n\nIn the next post I will cover middleware and how you can setup multiple handlers\nfor a request.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:41:31.000Z","updated_at":"2014-03-01T09:41:45.000Z","published_at":"2013-11-11T09:41:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe04d","uuid":"4b5a13cd-d18d-4023-86b7-f7c186ef47f0","title":"Owin middleware","slug":"owin-middleware","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Owin middleware\\n\\nIn the last [post](http://blog.tomasjansson.com/hello-owin) I showed you how to create a simple \\\"Hello World\\\" application, the next step is to take advantage of the Owin pipeline.\\n\\nI'm going to use the application from the [last post](http://blog.tomasjansson.com/hello-owin) as a starting point for this blog post and add a example (stupid) authentication middleware, but first I will make a short recap of the previous post and also show you a really simple middleware. Let's get started.\\n\\nAs in the previous post I'm using a self hosted solution using the HttpListener from the [Katana Project](http://katanaproject.codeplex.com/), which is a open source project initiated from Microsoft's to aid development of OWIN-based web applications. I will go into more detail in different kinds of hosting in a later post, but as of now it will do with the self hosting solution from the Katana Project. The basic of this applications has two files, \\\"Program.cs\\\":\\n\\n    using System;\\n    using Microsoft.Owin.Hosting;\\n\\n    namespace HelloOwin\\n    {\\n        internal class Program\\n        {\\n            private static void Main(string[] args)\\n            {\\n                using (WebApp.Start<Startup>(url: \\\"http://localhost:1337/\\\"))\\n                {\\n                    Console.WriteLine(\\\"Application is running\\\");\\n                    Console.ReadLine();\\n                }\\n            }\\n        }\\n    }\\n\\nand \\\"Startup.cs\\\":\\n\\n    using Owin;\\n\\n    namespace HelloOwin\\n    {\\n        public class Startup\\n        {\\n            public void Configuration(IAppBuilder app)\\n            {\\n                app.Run(context =>\\n                {\\n                    var task = context.Response.WriteAsync(\\\"Hello world! \\\" + context.Request.Path);\\n                    return task;\\n                });\\n            }\\n        }\\n    }\\n\\nRunning this application will print \\\"Hello world!\\\" and the requested paths. The way middleware works is that it got a reference to the next step in the pipeline and when it is done with its own processing it just calls the next step. If you really think about it middleware and application is actually the same thing, the difference is that an application just ignores to call the next step in the pipeline and therefor this is where the pipeline ends. Every call to the next middleware returns a task, which allows every middleware to execute some functionality when that task is finished making it possible to process the request on the way back again, this processing is done in the reverse order of the middleware definition. A simple illustration of middleware is seen below:\\n\\n![Owin pipeline][1]\\n\\nLet's get started with our first simple middleware. Let's say that we want to add some text to every start and end of the response stream, that's a simple thing to do with a middleware and it also captures most of the concepts involving middleware. To do so let's modify the \\\"Startup.cs\\\"\\n\\n    using Owin;\\n\\n    namespace HelloOwin\\n    {\\n        public class Startup\\n        {\\n            public void Configuration(IAppBuilder app)\\n            {\\n                app.Use((context, next) =>\\n                {\\n                    context.Response.WriteAsync(\\\"PIMPING IT! \\\");\\n                    return next().ContinueWith(task =>\\n                    {\\n                        context.Response.WriteAsync(\\\" DONE PIMPING IT! \\\");\\n                    });\\n                });\\n                app.Use((context, next) =>\\n                {\\n                    context.Response.WriteAsync(\\\"PIMPING IT MORE! \\\");\\n                    return next().ContinueWith(task =>\\n                    {\\n                        context.Response.WriteAsync(\\\" DONE PIMPING IT MORE! \\\");\\n                    });\\n                });\\n\\n                app.Run(context =>\\n                {\\n                    var task = context.Response.WriteAsync(\\\"Hello world! \\\" + context.Request.Path);\\n                    return task;\\n                });\\n            }\\n        }\\n    }\\n\\t\\nIn the example above I've added to simple and easy to understand middlewares, both are actually doing the same thing but this will show you in which order the middlewares get executed on each request. But first we'll focus on one of them: \\n\\n\\tapp.Use((context, next) =>\\n\\t{\\n\\t\\tcontext.Response.WriteAsync(\\\"PIMPING IT! \\\");\\n\\t\\treturn next().ContinueWith(task =>\\n\\t\\t{\\n\\t\\t\\tcontext.Response.WriteAsync(\\\" DONE PIMPING IT! \\\");\\n\\t\\t});\\n\\t});\\n\\nFirst of, this is also using the `Use` extension method defined in the Owin assembly from the Katana Project. The `Use` method takes a `Func<IOwinContext, Task, Task>` as a parameter. Breaking it down gives us; an environment `IOwinContext` which is a wrapper over a `IDictionary<string, object>` that contains all the data in the request and response. The first `Task` is the next step in the pipeline and the last `Task` is just the return type of the `Func`. The simple example above will first print \\\"PIMPING IT!\\\" before the actual response and after the response it will add \\\"DONE PIMPING IT!\\\". The `next()` returns a new `Task` which is from the next step in the pipeline, and to that `Task` we can attach a function to get executed when finished. That is exactly what we are doing with the call to `ContinueWith`.\\n\\nPutting both those middlewares to work at the same time will give us a response like:\\n\\n    PIMPING IT! PIMPING IT MORE! Hello world! / DONE PIMPING IT MORE! DONE PIMPINT IT!\\n\\t\\nAs you see the order in which the middlewares got executed are reversed on the way back.\\n\\n## Stepping it up a notch\\n\\nTo show some more of the features and how to interact with the Owin environment I'll show you a simple and stupid authentication middleware. The goal of the middleware is to reject all request to a given url, and redirect to a login url if you try to access it without being authenticated. After visiting the login url you'll get automatically authenticated and a \\\"user cookie\\\" is set to show that your authenticated.\\n\\n > **NOTE:** this is not a demo of how to write a good authentication middleware, it is a deeper demo to middleware than the first demo \\n \\n When writing more advanced middleware it most likely will be more common to encapsulate the middleware in a class instead of a `Func` the you pass the call to `Use`. An example of that could look something like:\\n \\n\\tpublic class Startup\\n\\t{\\n\\t\\tpublic void Configuration(IAppBuilder app)\\n\\t\\t{\\n\\t\\t\\tvar options = new StupidAuthOptions() { LoginPath = \\\"/login\\\", SecurePath = \\\"/secure\\\" };\\n\\t\\t\\tapp.Use(typeof(StupidAuth), options);\\n\\n\\t\\t\\tapp.Run(context =>\\n\\t\\t\\t{\\n\\t\\t\\t\\tvar task = context.Response.WriteAsync(\\\"Hello world! \\\" + context.Request.Path);\\n\\t\\t\\t\\treturn task;\\n\\t\\t\\t});\\n\\t\\t}\\n\\t}\\n\\nIn the configuration code above I first define a set of options that I want to pass to my middleware, these options will be passed to the constructor when creating the middleware later on. So on the next line I'm configuring the `app` what the type of my middleware is and that I want to pass the options parameter when it is instantiated. The `StupidAuthOptions` is a really simple POCO: \\n\\n    public class StupidAuthOptions\\n    {\\n        public string SecurePath { get; set; }\\n        public string LoginPath { get; set; }\\n    }\\n\\t\\nI will now show you the actual middleware implementation step by step. The first part is the constructor and the `Invoke` method: \\n\\n\\tusing System;\\n\\tusing System.Collections.Generic;\\n\\tusing System.IO;\\n\\tusing System.Linq;\\n\\tusing System.Threading.Tasks;\\n\\n\\tnamespace HelloOwin\\n\\t{\\n\\t\\tpublic class StupidAuth\\n\\t\\t{\\n\\t\\t\\tprivate Func<IDictionary<string, object>, Task> _next;\\n\\t\\t\\tprivate StupidAuthOptions _options;\\n\\n\\t\\t\\tpublic StupidAuth(Func<IDictionary<string, object>, Task> next, StupidAuthOptions options)\\n\\t\\t\\t{\\n\\t\\t\\t\\t_next = next;\\n\\t\\t\\t\\t_options = options;\\n\\t\\t\\t}\\n\\n\\t\\t\\tpublic Task Invoke(IDictionary<string, object> environment)\\n\\t\\t\\t{\\n\\t\\t\\t\\t....\\n\\t\\t\\t}\\n\\t\\t\\t....\\n\\t\\t}\\n\\t}\\n\\nA lot of the code is using conventions, and you see some of them in the example above. As you see the middleware doesn't implement any interface, but that is something you could add your self. The constructor takes as the first argument a `Func` which basically is the next step in pipeline. Calling this `Func` is executing the next step, but you always have the opportunity to not call the pipeline and making your middleware the final step. The `Invoke` method takes in the environment, as a `IDictionary`, with all relevant information for the request. If you think about it the signature of the `Invoke` method basically is the same as `Func<IDictionary<string, object>, Task>`. The return of a `Task` makes it easy to chain middleware and also execute a block of code on the way as I showed earlier.\\n\\nI won't go into detail in the rest of the code, but I'll at least show it to you: \\n\\n    public class StupidAuth\\n    {\\n        private Func<IDictionary<string, object>, Task> _next;\\n        private StupidAuthOptions _options;\\n        private Dictionary<string, Func<IDictionary<string, object>, Task>> _requestDispatcher; \\n\\n        public StupidAuth(Func<IDictionary<string, object>, Task> next, StupidAuthOptions options)\\n        {\\n            _next = next;\\n            _options = options;\\n            _requestDispatcher = new Dictionary<string, Func<IDictionary<string, object>, Task>>()\\n            {\\n                {_options.LoginPath, LoginHandler},\\n                {_options.SecurePath, SecureHandler}\\n            };\\n        }\\n\\n        private Task SecureHandler(IDictionary<string, object> environment)\\n        {\\n            if (!IsAuthenticated(environment))\\n            {\\n                return Redirect(environment, _options.LoginPath);\\n            }\\n            WriteToResponseStream(environment, \\\"You are watching super secret stuff! \\\");\\n            return _next(environment);\\n        }\\n\\n        private Task LoginHandler(IDictionary<string, object> environment)\\n        {\\n            if (IsAuthenticated(environment))\\n            {\\n                return WriteToResponseStream(environment, \\\"Logged in!\\\");\\n            }\\n            AddCookie(environment, \\\"user\\\", \\\"john doe\\\");\\n            return WriteToResponseStream(environment, \\\"Logging in!\\\");\\n        }\\n\\n        public Task Invoke(IDictionary<string, object> environment)\\n        {\\n            var path = environment[\\\"owin.RequestPath\\\"] as string;\\n            Func<IDictionary<string, object>, Task> handler;\\n            if (_requestDispatcher.TryGetValue(path, out handler))\\n            {\\n                return handler(environment);\\n            }\\n            return _next(environment);\\n        }\\n\\n        private Task WriteToResponseStream(IDictionary<string, object> environment, string message)\\n        {\\n            var response = environment[\\\"owin.ResponseBody\\\"] as Stream;\\n            var streamWriter = new StreamWriter(response);\\n            return Task.Factory.StartNew(() =>\\n            {\\n                streamWriter.Write(message);\\n                streamWriter.Dispose();\\n            });\\n        }\\n\\n        private bool IsAuthenticated(IDictionary<string, object> environment)\\n        {\\n            var requestHeaders = environment[\\\"owin.RequestHeaders\\\"] as IDictionary<string, string[]>;\\n            if (requestHeaders.ContainsKey(\\\"Cookie\\\"))\\n            {\\n                var cookies = requestHeaders[\\\"Cookie\\\"];\\n                var parsedCookie = ParseCookies(cookies[0]);\\n                return parsedCookie.ContainsKey(\\\"user\\\");                \\n            }\\n            return false;\\n        }\\n\\n        private IDictionary<string, string> ParseCookies(string cookies)\\n        {\\n            return cookies.Split(';')\\n                .Select(y => Uri.UnescapeDataString(y.Trim()).Split('='))\\n                .ToDictionary(y => y[0], y => y[1]);\\n        }\\n\\n        private void AddCookie(IDictionary<string, object> environment, string key, string value)\\n        {\\n            var setCookieValue = string.Concat(\\n                Uri.EscapeDataString(key),\\n                \\\"=\\\",\\n                value);\\n\\n            SetResponseHeader(environment, \\\"Set-Cookie\\\", setCookieValue);\\n        }\\n\\n        private Task Redirect(IDictionary<string, object> environment, string loginPath)\\n        {\\n            SetResponseHeader(environment, \\\"Location\\\", loginPath);\\n            environment[\\\"owin.ResponseStatusCode\\\"] = 302;\\n            var tcs = new TaskCompletionSource<object>();\\n            tcs.SetResult(null);\\n            return tcs.Task;\\n        }\\n\\n        private void SetResponseHeader(IDictionary<string, object> environment, string key, string value)\\n        {\\n            var headers = environment[\\\"owin.ResponseHeaders\\\"] as IDictionary<string, string[]>;\\n            headers = headers ?? new Dictionary<string, string[]>();\\n            headers.Add(key, new [] {value});\\n        }\\n    }\\n\\nThe way I choose to structure the code is to keep a mapping of the path's this middleware will handle in a dictionary pointing to different handlers, and than use the `Invoke` method to find the correct dispatcher and handle the request. The rest of the code is just helper methods for setting headers or creating a redirect response. I thought I would show it since might help you get started writing more advanced middleware than this stupid authentication handler.\\n\\n## Summary\\nWriting middleware is something that I think we will be doing a lot when we have changed our mind of thinking. As I showed you with some simple examples it could be really simple to write a piece of middleware, but I think you also realize that it could be really hard depending on what you are trying to solve. One question I didn't answer here is why you would like to use middleware? Writing good middleware will allow you to reuse them over and over again, you could stream line how you do things in your organization for example. Maybe you have a special setup of static files or you want all your web applications to use exactly the same authorization mechanism. I really look forward to see what kind of middleware that will pop up in the ecosystem.\\n\\n\\n  [1]: https://qbtmcq.dm2302.livefilestore.com/y2pCgbiQedecsgVAzavyDd0MHMy5oIqWUayyuWMX2u119bfjXdyuXqiom8p90qhUA9rsOsVUqT8rQOz5GT4ZCilMIgAUmhSkEWfcB_MAig2uNA/SimpleMiddlewarePipeline.png?psid=1\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Owin middleware</p>\n<p>In the last <a href=\"http://blog.tomasjansson.com/hello-owin\">post</a> I showed you how to create a simple &quot;Hello World&quot; application, the next step is to take advantage of the Owin pipeline.</p>\n<p>I'm going to use the application from the <a href=\"http://blog.tomasjansson.com/hello-owin\">last post</a> as a starting point for this blog post and add a example (stupid) authentication middleware, but first I will make a short recap of the previous post and also show you a really simple middleware. Let's get started.</p>\n<p>As in the previous post I'm using a self hosted solution using the HttpListener from the <a href=\"http://katanaproject.codeplex.com/\">Katana Project</a>, which is a open source project initiated from Microsoft's to aid development of OWIN-based web applications. I will go into more detail in different kinds of hosting in a later post, but as of now it will do with the self hosting solution from the Katana Project. The basic of this applications has two files, &quot;Program.cs&quot;:</p>\n<pre><code>using System;\nusing Microsoft.Owin.Hosting;\n\nnamespace HelloOwin\n{\n    internal class Program\n    {\n        private static void Main(string[] args)\n        {\n            using (WebApp.Start&lt;Startup&gt;(url: &quot;http://localhost:1337/&quot;))\n            {\n                Console.WriteLine(&quot;Application is running&quot;);\n                Console.ReadLine();\n            }\n        }\n    }\n}\n</code></pre>\n<p>and &quot;Startup.cs&quot;:</p>\n<pre><code>using Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Run(context =&gt;\n            {\n                var task = context.Response.WriteAsync(&quot;Hello world! &quot; + context.Request.Path);\n                return task;\n            });\n        }\n    }\n}\n</code></pre>\n<p>Running this application will print &quot;Hello world!&quot; and the requested paths. The way middleware works is that it got a reference to the next step in the pipeline and when it is done with its own processing it just calls the next step. If you really think about it middleware and application is actually the same thing, the difference is that an application just ignores to call the next step in the pipeline and therefor this is where the pipeline ends. Every call to the next middleware returns a task, which allows every middleware to execute some functionality when that task is finished making it possible to process the request on the way back again, this processing is done in the reverse order of the middleware definition. A simple illustration of middleware is seen below:</p>\n<p><img src=\"https://qbtmcq.dm2302.livefilestore.com/y2pCgbiQedecsgVAzavyDd0MHMy5oIqWUayyuWMX2u119bfjXdyuXqiom8p90qhUA9rsOsVUqT8rQOz5GT4ZCilMIgAUmhSkEWfcB_MAig2uNA/SimpleMiddlewarePipeline.png?psid=1\" alt=\"Owin pipeline\"></p>\n<p>Let's get started with our first simple middleware. Let's say that we want to add some text to every start and end of the response stream, that's a simple thing to do with a middleware and it also captures most of the concepts involving middleware. To do so let's modify the &quot;Startup.cs&quot;</p>\n<pre><code>using Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Use((context, next) =&gt;\n            {\n                context.Response.WriteAsync(&quot;PIMPING IT! &quot;);\n                return next().ContinueWith(task =&gt;\n                {\n                    context.Response.WriteAsync(&quot; DONE PIMPING IT! &quot;);\n                });\n            });\n            app.Use((context, next) =&gt;\n            {\n                context.Response.WriteAsync(&quot;PIMPING IT MORE! &quot;);\n                return next().ContinueWith(task =&gt;\n                {\n                    context.Response.WriteAsync(&quot; DONE PIMPING IT MORE! &quot;);\n                });\n            });\n\n            app.Run(context =&gt;\n            {\n                var task = context.Response.WriteAsync(&quot;Hello world! &quot; + context.Request.Path);\n                return task;\n            });\n        }\n    }\n}\n</code></pre>\n<p>In the example above I've added to simple and easy to understand middlewares, both are actually doing the same thing but this will show you in which order the middlewares get executed on each request. But first we'll focus on one of them:</p>\n<pre><code>app.Use((context, next) =&gt;\n{\n\tcontext.Response.WriteAsync(&quot;PIMPING IT! &quot;);\n\treturn next().ContinueWith(task =&gt;\n\t{\n\t\tcontext.Response.WriteAsync(&quot; DONE PIMPING IT! &quot;);\n\t});\n});\n</code></pre>\n<p>First of, this is also using the <code>Use</code> extension method defined in the Owin assembly from the Katana Project. The <code>Use</code> method takes a <code>Func&lt;IOwinContext, Task, Task&gt;</code> as a parameter. Breaking it down gives us; an environment <code>IOwinContext</code> which is a wrapper over a <code>IDictionary&lt;string, object&gt;</code> that contains all the data in the request and response. The first <code>Task</code> is the next step in the pipeline and the last <code>Task</code> is just the return type of the <code>Func</code>. The simple example above will first print &quot;PIMPING IT!&quot; before the actual response and after the response it will add &quot;DONE PIMPING IT!&quot;. The <code>next()</code> returns a new <code>Task</code> which is from the next step in the pipeline, and to that <code>Task</code> we can attach a function to get executed when finished. That is exactly what we are doing with the call to <code>ContinueWith</code>.</p>\n<p>Putting both those middlewares to work at the same time will give us a response like:</p>\n<pre><code>PIMPING IT! PIMPING IT MORE! Hello world! / DONE PIMPING IT MORE! DONE PIMPINT IT!\n</code></pre>\n<p>As you see the order in which the middlewares got executed are reversed on the way back.</p>\n<h2 id=\"steppingitupanotch\">Stepping it up a notch</h2>\n<p>To show some more of the features and how to interact with the Owin environment I'll show you a simple and stupid authentication middleware. The goal of the middleware is to reject all request to a given url, and redirect to a login url if you try to access it without being authenticated. After visiting the login url you'll get automatically authenticated and a &quot;user cookie&quot; is set to show that your authenticated.</p>\n<blockquote>\n<p><strong>NOTE:</strong> this is not a demo of how to write a good authentication middleware, it is a deeper demo to middleware than the first demo</p>\n</blockquote>\n<p>When writing more advanced middleware it most likely will be more common to encapsulate the middleware in a class instead of a <code>Func</code> the you pass the call to <code>Use</code>. An example of that could look something like:</p>\n<pre><code>public class Startup\n{\n\tpublic void Configuration(IAppBuilder app)\n\t{\n\t\tvar options = new StupidAuthOptions() { LoginPath = &quot;/login&quot;, SecurePath = &quot;/secure&quot; };\n\t\tapp.Use(typeof(StupidAuth), options);\n\n\t\tapp.Run(context =&gt;\n\t\t{\n\t\t\tvar task = context.Response.WriteAsync(&quot;Hello world! &quot; + context.Request.Path);\n\t\t\treturn task;\n\t\t});\n\t}\n}\n</code></pre>\n<p>In the configuration code above I first define a set of options that I want to pass to my middleware, these options will be passed to the constructor when creating the middleware later on. So on the next line I'm configuring the <code>app</code> what the type of my middleware is and that I want to pass the options parameter when it is instantiated. The <code>StupidAuthOptions</code> is a really simple POCO:</p>\n<pre><code>public class StupidAuthOptions\n{\n    public string SecurePath { get; set; }\n    public string LoginPath { get; set; }\n}\n</code></pre>\n<p>I will now show you the actual middleware implementation step by step. The first part is the constructor and the <code>Invoke</code> method:</p>\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System.Threading.Tasks;\n\nnamespace HelloOwin\n{\n\tpublic class StupidAuth\n\t{\n\t\tprivate Func&lt;IDictionary&lt;string, object&gt;, Task&gt; _next;\n\t\tprivate StupidAuthOptions _options;\n\n\t\tpublic StupidAuth(Func&lt;IDictionary&lt;string, object&gt;, Task&gt; next, StupidAuthOptions options)\n\t\t{\n\t\t\t_next = next;\n\t\t\t_options = options;\n\t\t}\n\n\t\tpublic Task Invoke(IDictionary&lt;string, object&gt; environment)\n\t\t{\n\t\t\t....\n\t\t}\n\t\t....\n\t}\n}\n</code></pre>\n<p>A lot of the code is using conventions, and you see some of them in the example above. As you see the middleware doesn't implement any interface, but that is something you could add your self. The constructor takes as the first argument a <code>Func</code> which basically is the next step in pipeline. Calling this <code>Func</code> is executing the next step, but you always have the opportunity to not call the pipeline and making your middleware the final step. The <code>Invoke</code> method takes in the environment, as a <code>IDictionary</code>, with all relevant information for the request. If you think about it the signature of the <code>Invoke</code> method basically is the same as <code>Func&lt;IDictionary&lt;string, object&gt;, Task&gt;</code>. The return of a <code>Task</code> makes it easy to chain middleware and also execute a block of code on the way as I showed earlier.</p>\n<p>I won't go into detail in the rest of the code, but I'll at least show it to you:</p>\n<pre><code>public class StupidAuth\n{\n    private Func&lt;IDictionary&lt;string, object&gt;, Task&gt; _next;\n    private StupidAuthOptions _options;\n    private Dictionary&lt;string, Func&lt;IDictionary&lt;string, object&gt;, Task&gt;&gt; _requestDispatcher; \n\n    public StupidAuth(Func&lt;IDictionary&lt;string, object&gt;, Task&gt; next, StupidAuthOptions options)\n    {\n        _next = next;\n        _options = options;\n        _requestDispatcher = new Dictionary&lt;string, Func&lt;IDictionary&lt;string, object&gt;, Task&gt;&gt;()\n        {\n            {_options.LoginPath, LoginHandler},\n            {_options.SecurePath, SecureHandler}\n        };\n    }\n\n    private Task SecureHandler(IDictionary&lt;string, object&gt; environment)\n    {\n        if (!IsAuthenticated(environment))\n        {\n            return Redirect(environment, _options.LoginPath);\n        }\n        WriteToResponseStream(environment, &quot;You are watching super secret stuff! &quot;);\n        return _next(environment);\n    }\n\n    private Task LoginHandler(IDictionary&lt;string, object&gt; environment)\n    {\n        if (IsAuthenticated(environment))\n        {\n            return WriteToResponseStream(environment, &quot;Logged in!&quot;);\n        }\n        AddCookie(environment, &quot;user&quot;, &quot;john doe&quot;);\n        return WriteToResponseStream(environment, &quot;Logging in!&quot;);\n    }\n\n    public Task Invoke(IDictionary&lt;string, object&gt; environment)\n    {\n        var path = environment[&quot;owin.RequestPath&quot;] as string;\n        Func&lt;IDictionary&lt;string, object&gt;, Task&gt; handler;\n        if (_requestDispatcher.TryGetValue(path, out handler))\n        {\n            return handler(environment);\n        }\n        return _next(environment);\n    }\n\n    private Task WriteToResponseStream(IDictionary&lt;string, object&gt; environment, string message)\n    {\n        var response = environment[&quot;owin.ResponseBody&quot;] as Stream;\n        var streamWriter = new StreamWriter(response);\n        return Task.Factory.StartNew(() =&gt;\n        {\n            streamWriter.Write(message);\n            streamWriter.Dispose();\n        });\n    }\n\n    private bool IsAuthenticated(IDictionary&lt;string, object&gt; environment)\n    {\n        var requestHeaders = environment[&quot;owin.RequestHeaders&quot;] as IDictionary&lt;string, string[]&gt;;\n        if (requestHeaders.ContainsKey(&quot;Cookie&quot;))\n        {\n            var cookies = requestHeaders[&quot;Cookie&quot;];\n            var parsedCookie = ParseCookies(cookies[0]);\n            return parsedCookie.ContainsKey(&quot;user&quot;);                \n        }\n        return false;\n    }\n\n    private IDictionary&lt;string, string&gt; ParseCookies(string cookies)\n    {\n        return cookies.Split(';')\n            .Select(y =&gt; Uri.UnescapeDataString(y.Trim()).Split('='))\n            .ToDictionary(y =&gt; y[0], y =&gt; y[1]);\n    }\n\n    private void AddCookie(IDictionary&lt;string, object&gt; environment, string key, string value)\n    {\n        var setCookieValue = string.Concat(\n            Uri.EscapeDataString(key),\n            &quot;=&quot;,\n            value);\n\n        SetResponseHeader(environment, &quot;Set-Cookie&quot;, setCookieValue);\n    }\n\n    private Task Redirect(IDictionary&lt;string, object&gt; environment, string loginPath)\n    {\n        SetResponseHeader(environment, &quot;Location&quot;, loginPath);\n        environment[&quot;owin.ResponseStatusCode&quot;] = 302;\n        var tcs = new TaskCompletionSource&lt;object&gt;();\n        tcs.SetResult(null);\n        return tcs.Task;\n    }\n\n    private void SetResponseHeader(IDictionary&lt;string, object&gt; environment, string key, string value)\n    {\n        var headers = environment[&quot;owin.ResponseHeaders&quot;] as IDictionary&lt;string, string[]&gt;;\n        headers = headers ?? new Dictionary&lt;string, string[]&gt;();\n        headers.Add(key, new [] {value});\n    }\n}\n</code></pre>\n<p>The way I choose to structure the code is to keep a mapping of the path's this middleware will handle in a dictionary pointing to different handlers, and than use the <code>Invoke</code> method to find the correct dispatcher and handle the request. The rest of the code is just helper methods for setting headers or creating a redirect response. I thought I would show it since might help you get started writing more advanced middleware than this stupid authentication handler.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>Writing middleware is something that I think we will be doing a lot when we have changed our mind of thinking. As I showed you with some simple examples it could be really simple to write a piece of middleware, but I think you also realize that it could be really hard depending on what you are trying to solve. One question I didn't answer here is why you would like to use middleware? Writing good middleware will allow you to reuse them over and over again, you could stream line how you do things in your organization for example. Maybe you have a special setup of static files or you want all your web applications to use exactly the same authorization mechanism. I really look forward to see what kind of middleware that will pop up in the ecosystem.</p>\n<!--kg-card-end: markdown-->","comment_id":"26","plaintext":"Owin middleware\n\nIn the last post [http://blog.tomasjansson.com/hello-owin] I showed you how to\ncreate a simple \"Hello World\" application, the next step is to take advantage of\nthe Owin pipeline.\n\nI'm going to use the application from the last post\n[http://blog.tomasjansson.com/hello-owin] as a starting point for this blog post\nand add a example (stupid) authentication middleware, but first I will make a\nshort recap of the previous post and also show you a really simple middleware.\nLet's get started.\n\nAs in the previous post I'm using a self hosted solution using the HttpListener\nfrom the Katana Project [http://katanaproject.codeplex.com/], which is a open\nsource project initiated from Microsoft's to aid development of OWIN-based web\napplications. I will go into more detail in different kinds of hosting in a\nlater post, but as of now it will do with the self hosting solution from the\nKatana Project. The basic of this applications has two files, \"Program.cs\":\n\nusing System;\nusing Microsoft.Owin.Hosting;\n\nnamespace HelloOwin\n{\n    internal class Program\n    {\n        private static void Main(string[] args)\n        {\n            using (WebApp.Start<Startup>(url: \"http://localhost:1337/\"))\n            {\n                Console.WriteLine(\"Application is running\");\n                Console.ReadLine();\n            }\n        }\n    }\n}\n\n\nand \"Startup.cs\":\n\nusing Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Run(context =>\n            {\n                var task = context.Response.WriteAsync(\"Hello world! \" + context.Request.Path);\n                return task;\n            });\n        }\n    }\n}\n\n\nRunning this application will print \"Hello world!\" and the requested paths. The\nway middleware works is that it got a reference to the next step in the pipeline\nand when it is done with its own processing it just calls the next step. If you\nreally think about it middleware and application is actually the same thing, the\ndifference is that an application just ignores to call the next step in the\npipeline and therefor this is where the pipeline ends. Every call to the next\nmiddleware returns a task, which allows every middleware to execute some\nfunctionality when that task is finished making it possible to process the\nrequest on the way back again, this processing is done in the reverse order of\nthe middleware definition. A simple illustration of middleware is seen below:\n\n\n\nLet's get started with our first simple middleware. Let's say that we want to\nadd some text to every start and end of the response stream, that's a simple\nthing to do with a middleware and it also captures most of the concepts\ninvolving middleware. To do so let's modify the \"Startup.cs\"\n\nusing Owin;\n\nnamespace HelloOwin\n{\n    public class Startup\n    {\n        public void Configuration(IAppBuilder app)\n        {\n            app.Use((context, next) =>\n            {\n                context.Response.WriteAsync(\"PIMPING IT! \");\n                return next().ContinueWith(task =>\n                {\n                    context.Response.WriteAsync(\" DONE PIMPING IT! \");\n                });\n            });\n            app.Use((context, next) =>\n            {\n                context.Response.WriteAsync(\"PIMPING IT MORE! \");\n                return next().ContinueWith(task =>\n                {\n                    context.Response.WriteAsync(\" DONE PIMPING IT MORE! \");\n                });\n            });\n\n            app.Run(context =>\n            {\n                var task = context.Response.WriteAsync(\"Hello world! \" + context.Request.Path);\n                return task;\n            });\n        }\n    }\n}\n\n\nIn the example above I've added to simple and easy to understand middlewares,\nboth are actually doing the same thing but this will show you in which order the\nmiddlewares get executed on each request. But first we'll focus on one of them:\n\napp.Use((context, next) =>\n{\n\tcontext.Response.WriteAsync(\"PIMPING IT! \");\n\treturn next().ContinueWith(task =>\n\t{\n\t\tcontext.Response.WriteAsync(\" DONE PIMPING IT! \");\n\t});\n});\n\n\nFirst of, this is also using the Use extension method defined in the Owin\nassembly from the Katana Project. The Use method takes a Func<IOwinContext,\nTask, Task> as a parameter. Breaking it down gives us; an environment \nIOwinContext which is a wrapper over a IDictionary<string, object> that contains\nall the data in the request and response. The first Task is the next step in the\npipeline and the last Task is just the return type of the Func. The simple\nexample above will first print \"PIMPING IT!\" before the actual response and\nafter the response it will add \"DONE PIMPING IT!\". The next() returns a new Task \nwhich is from the next step in the pipeline, and to that Task we can attach a\nfunction to get executed when finished. That is exactly what we are doing with\nthe call to ContinueWith.\n\nPutting both those middlewares to work at the same time will give us a response\nlike:\n\nPIMPING IT! PIMPING IT MORE! Hello world! / DONE PIMPING IT MORE! DONE PIMPINT IT!\n\n\nAs you see the order in which the middlewares got executed are reversed on the\nway back.\n\nStepping it up a notch\nTo show some more of the features and how to interact with the Owin environment\nI'll show you a simple and stupid authentication middleware. The goal of the\nmiddleware is to reject all request to a given url, and redirect to a login url\nif you try to access it without being authenticated. After visiting the login\nurl you'll get automatically authenticated and a \"user cookie\" is set to show\nthat your authenticated.\n\n> NOTE: this is not a demo of how to write a good authentication middleware, it is\na deeper demo to middleware than the first demo\n\n\nWhen writing more advanced middleware it most likely will be more common to\nencapsulate the middleware in a class instead of a Func the you pass the call to \nUse. An example of that could look something like:\n\npublic class Startup\n{\n\tpublic void Configuration(IAppBuilder app)\n\t{\n\t\tvar options = new StupidAuthOptions() { LoginPath = \"/login\", SecurePath = \"/secure\" };\n\t\tapp.Use(typeof(StupidAuth), options);\n\n\t\tapp.Run(context =>\n\t\t{\n\t\t\tvar task = context.Response.WriteAsync(\"Hello world! \" + context.Request.Path);\n\t\t\treturn task;\n\t\t});\n\t}\n}\n\n\nIn the configuration code above I first define a set of options that I want to\npass to my middleware, these options will be passed to the constructor when\ncreating the middleware later on. So on the next line I'm configuring the app \nwhat the type of my middleware is and that I want to pass the options parameter\nwhen it is instantiated. The StupidAuthOptions is a really simple POCO:\n\npublic class StupidAuthOptions\n{\n    public string SecurePath { get; set; }\n    public string LoginPath { get; set; }\n}\n\n\nI will now show you the actual middleware implementation step by step. The first\npart is the constructor and the Invoke method:\n\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Linq;\nusing System.Threading.Tasks;\n\nnamespace HelloOwin\n{\n\tpublic class StupidAuth\n\t{\n\t\tprivate Func<IDictionary<string, object>, Task> _next;\n\t\tprivate StupidAuthOptions _options;\n\n\t\tpublic StupidAuth(Func<IDictionary<string, object>, Task> next, StupidAuthOptions options)\n\t\t{\n\t\t\t_next = next;\n\t\t\t_options = options;\n\t\t}\n\n\t\tpublic Task Invoke(IDictionary<string, object> environment)\n\t\t{\n\t\t\t....\n\t\t}\n\t\t....\n\t}\n}\n\n\nA lot of the code is using conventions, and you see some of them in the example\nabove. As you see the middleware doesn't implement any interface, but that is\nsomething you could add your self. The constructor takes as the first argument a \nFunc which basically is the next step in pipeline. Calling this Func is\nexecuting the next step, but you always have the opportunity to not call the\npipeline and making your middleware the final step. The Invoke method takes in\nthe environment, as a IDictionary, with all relevant information for the\nrequest. If you think about it the signature of the Invoke method basically is\nthe same as Func<IDictionary<string, object>, Task>. The return of a Task makes\nit easy to chain middleware and also execute a block of code on the way as I\nshowed earlier.\n\nI won't go into detail in the rest of the code, but I'll at least show it to\nyou:\n\npublic class StupidAuth\n{\n    private Func<IDictionary<string, object>, Task> _next;\n    private StupidAuthOptions _options;\n    private Dictionary<string, Func<IDictionary<string, object>, Task>> _requestDispatcher; \n\n    public StupidAuth(Func<IDictionary<string, object>, Task> next, StupidAuthOptions options)\n    {\n        _next = next;\n        _options = options;\n        _requestDispatcher = new Dictionary<string, Func<IDictionary<string, object>, Task>>()\n        {\n            {_options.LoginPath, LoginHandler},\n            {_options.SecurePath, SecureHandler}\n        };\n    }\n\n    private Task SecureHandler(IDictionary<string, object> environment)\n    {\n        if (!IsAuthenticated(environment))\n        {\n            return Redirect(environment, _options.LoginPath);\n        }\n        WriteToResponseStream(environment, \"You are watching super secret stuff! \");\n        return _next(environment);\n    }\n\n    private Task LoginHandler(IDictionary<string, object> environment)\n    {\n        if (IsAuthenticated(environment))\n        {\n            return WriteToResponseStream(environment, \"Logged in!\");\n        }\n        AddCookie(environment, \"user\", \"john doe\");\n        return WriteToResponseStream(environment, \"Logging in!\");\n    }\n\n    public Task Invoke(IDictionary<string, object> environment)\n    {\n        var path = environment[\"owin.RequestPath\"] as string;\n        Func<IDictionary<string, object>, Task> handler;\n        if (_requestDispatcher.TryGetValue(path, out handler))\n        {\n            return handler(environment);\n        }\n        return _next(environment);\n    }\n\n    private Task WriteToResponseStream(IDictionary<string, object> environment, string message)\n    {\n        var response = environment[\"owin.ResponseBody\"] as Stream;\n        var streamWriter = new StreamWriter(response);\n        return Task.Factory.StartNew(() =>\n        {\n            streamWriter.Write(message);\n            streamWriter.Dispose();\n        });\n    }\n\n    private bool IsAuthenticated(IDictionary<string, object> environment)\n    {\n        var requestHeaders = environment[\"owin.RequestHeaders\"] as IDictionary<string, string[]>;\n        if (requestHeaders.ContainsKey(\"Cookie\"))\n        {\n            var cookies = requestHeaders[\"Cookie\"];\n            var parsedCookie = ParseCookies(cookies[0]);\n            return parsedCookie.ContainsKey(\"user\");                \n        }\n        return false;\n    }\n\n    private IDictionary<string, string> ParseCookies(string cookies)\n    {\n        return cookies.Split(';')\n            .Select(y => Uri.UnescapeDataString(y.Trim()).Split('='))\n            .ToDictionary(y => y[0], y => y[1]);\n    }\n\n    private void AddCookie(IDictionary<string, object> environment, string key, string value)\n    {\n        var setCookieValue = string.Concat(\n            Uri.EscapeDataString(key),\n            \"=\",\n            value);\n\n        SetResponseHeader(environment, \"Set-Cookie\", setCookieValue);\n    }\n\n    private Task Redirect(IDictionary<string, object> environment, string loginPath)\n    {\n        SetResponseHeader(environment, \"Location\", loginPath);\n        environment[\"owin.ResponseStatusCode\"] = 302;\n        var tcs = new TaskCompletionSource<object>();\n        tcs.SetResult(null);\n        return tcs.Task;\n    }\n\n    private void SetResponseHeader(IDictionary<string, object> environment, string key, string value)\n    {\n        var headers = environment[\"owin.ResponseHeaders\"] as IDictionary<string, string[]>;\n        headers = headers ?? new Dictionary<string, string[]>();\n        headers.Add(key, new [] {value});\n    }\n}\n\n\nThe way I choose to structure the code is to keep a mapping of the path's this\nmiddleware will handle in a dictionary pointing to different handlers, and than\nuse the Invoke method to find the correct dispatcher and handle the request. The\nrest of the code is just helper methods for setting headers or creating a\nredirect response. I thought I would show it since might help you get started\nwriting more advanced middleware than this stupid authentication handler.\n\nSummary\nWriting middleware is something that I think we will be doing a lot when we have\nchanged our mind of thinking. As I showed you with some simple examples it could\nbe really simple to write a piece of middleware, but I think you also realize\nthat it could be really hard depending on what you are trying to solve. One\nquestion I didn't answer here is why you would like to use middleware? Writing\ngood middleware will allow you to reuse them over and over again, you could\nstream line how you do things in your organization for example. Maybe you have a\nspecial setup of static files or you want all your web applications to use\nexactly the same authorization mechanism. I really look forward to see what kind\nof middleware that will pop up in the ecosystem.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-01T09:42:17.000Z","updated_at":"2014-07-07T13:11:23.000Z","published_at":"2013-11-18T09:42:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe04e","uuid":"36f9e544-034f-472e-b43d-c3b2608d3402","title":"About me","slug":"aboutme","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Should Tomas write this in third person or should I write this as myself, I never know. I'll try to write it as myself. I'm a software developer that know there are tons of stuff I don't know. I started programming at high school in the mid/late 90's and the interest in creating software has increased a lot since. After high school I went straight to Chalmers University of Technology in Gothenburg were I got my M.Sc. in Computer Engineering, with a specialization towards \\\"Applied mathematics\\\" and \\\"Algorithms\\\". \\n\\nAfter my univeristy I thought I wanted to work for a major consultancy firm, so I joined Accenture and started doing some ABAP coding in a SAP project. It didn't take me long to realize that that kind of enterprise software isn't something I believe in. I truly believe that you can deliver software better, cheaper and at the same time have more fun than you do in those kind of SAP projects. After the six months I spent at Accenture I went to a smaller consultancy firm, Netlight Consulting, where I did programming in java, perl, javascript, html and finally C#/.NET for various customers. \\n\\n.NET has been my main focus since 2007 or 2008, and I've been working hard with community work and self study to learn my way around. The work has paid of and I'm now an Microsoft MVP, board leader of NNUG Oslo and have spoken at multiple conferences and user groups the last couple of years.\\n\\nEven though my focus area is and has been .NET the last couple of years I do study other languages, mainly functional like erlang and haskell. I'm a true believer that we will build better software using functional languages, like F# in .NET, than we do in OO languages so that is one of my focus areas as of the moment.\\n\\nAnd that I think finish of my little introduction about myself.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Should Tomas write this in third person or should I write this as myself, I never know. I'll try to write it as myself. I'm a software developer that know there are tons of stuff I don't know. I started programming at high school in the mid/late 90's and the interest in creating software has increased a lot since. After high school I went straight to Chalmers University of Technology in Gothenburg were I got my M.Sc. in Computer Engineering, with a specialization towards &quot;Applied mathematics&quot; and &quot;Algorithms&quot;.</p>\n<p>After my univeristy I thought I wanted to work for a major consultancy firm, so I joined Accenture and started doing some ABAP coding in a SAP project. It didn't take me long to realize that that kind of enterprise software isn't something I believe in. I truly believe that you can deliver software better, cheaper and at the same time have more fun than you do in those kind of SAP projects. After the six months I spent at Accenture I went to a smaller consultancy firm, Netlight Consulting, where I did programming in java, perl, javascript, html and finally C#/.NET for various customers.</p>\n<p>.NET has been my main focus since 2007 or 2008, and I've been working hard with community work and self study to learn my way around. The work has paid of and I'm now an Microsoft MVP, board leader of NNUG Oslo and have spoken at multiple conferences and user groups the last couple of years.</p>\n<p>Even though my focus area is and has been .NET the last couple of years I do study other languages, mainly functional like erlang and haskell. I'm a true believer that we will build better software using functional languages, like F# in .NET, than we do in OO languages so that is one of my focus areas as of the moment.</p>\n<p>And that I think finish of my little introduction about myself.</p>\n<!--kg-card-end: markdown-->","comment_id":"27","plaintext":"Should Tomas write this in third person or should I write this as myself, I\nnever know. I'll try to write it as myself. I'm a software developer that know\nthere are tons of stuff I don't know. I started programming at high school in\nthe mid/late 90's and the interest in creating software has increased a lot\nsince. After high school I went straight to Chalmers University of Technology in\nGothenburg were I got my M.Sc. in Computer Engineering, with a specialization\ntowards \"Applied mathematics\" and \"Algorithms\".\n\nAfter my univeristy I thought I wanted to work for a major consultancy firm, so\nI joined Accenture and started doing some ABAP coding in a SAP project. It\ndidn't take me long to realize that that kind of enterprise software isn't\nsomething I believe in. I truly believe that you can deliver software better,\ncheaper and at the same time have more fun than you do in those kind of SAP\nprojects. After the six months I spent at Accenture I went to a smaller\nconsultancy firm, Netlight Consulting, where I did programming in java, perl,\njavascript, html and finally C#/.NET for various customers.\n\n.NET has been my main focus since 2007 or 2008, and I've been working hard with\ncommunity work and self study to learn my way around. The work has paid of and\nI'm now an Microsoft MVP, board leader of NNUG Oslo and have spoken at multiple\nconferences and user groups the last couple of years.\n\nEven though my focus area is and has been .NET the last couple of years I do\nstudy other languages, mainly functional like erlang and haskell. I'm a true\nbeliever that we will build better software using functional languages, like F#\nin .NET, than we do in OO languages so that is one of my focus areas as of the\nmoment.\n\nAnd that I think finish of my little introduction about myself.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-03-03T20:22:12.000Z","updated_at":"2015-01-13T19:08:42.000Z","published_at":"2014-03-03T20:22:12.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","send_email_when_published":0},{"id":"5e382b29f81c630018abe04f","uuid":"f4693957-0d29-4fe8-9a8e-9580d90c525c","title":"CQRS the simple way with eventstore and elasticsearch: Project structure","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/ \\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * Project structure\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Solution structure\\nThis introduction post will walk you through the project I would like to see in a project and a short comment about the intention with the project.\\n\\nThe projects that will be present are the one in the following screen shot:\\n![Project structure](/content/images/2014/Jun/Project-structure-1.PNG)\\nThe rest of this post will give you a brief introduction to each of these projects and what I think should put into them. Note that there isn't such a thing as a generic solution to every problem so for your specific problem it might be good to have a different structure.\\n\\n##Contracts\\nThe contracts project will contain all the commands that will go into the system and events that will come out as a result of those commands. All the building blocks for putting together those commands and events will also be in this project.\\n\\nTo implement this I thing it is really beneficial to use F# instead of C#. The reason for this is that it compact way of defining new types that are value types, that is, you can compare them by \\\"value\\\" instead of by reference. These types will also be immutable by default which make it easier to write code with less side effects, and also easier to test things (will be covered in a later post)\\n\\n##Domain\\nThis is where all the logic related to the domain should go. It's not more complex that that, exactly what will go into this project will be covered in later posts.\\n\\n##Infrastructure\\nIn the infrastructure we put everything that specify how we store the things we do, that is, connect to the eventstore. This will also contain the implementation of the base aggregate class that all our aggregates should implement to make life easier. It also contains the interfaces for what an event and command is.\\n\\n##Service\\nThis project is the most difficult to define since it depends so much on the application you are building. With that said it will contain the code that will react on events and update the read models/views in my example. You could also have things that take a long time in this project.  \\n\\n##Web\\nStraight forward, this is where all the code that is facing the user should be. It could the web api, html or both. For my example I'll use [Simple.Web](https://github.com/markrendle/Simple.Web).\\n\\n##Search\\nSince I'm going to store all the views and readmodels in elasticsearch I've a dedicated project to query that model. This project will also contains the actual DTOs that I'm storing in elasticsearch.\\n\\n##Domain tests\\nQuite straight forward, this will contain all the tests against the domain. You could of course have tests against the other parts as well if you like, but for me that is not that necessary since the domain will use all the other parts, like infrastructure, so if I get the domain work the other parts has to work as well.\\n\\nThis finish the first post in an unknown number of posts. Hopefully I finish a couple of more this week, but the weather is nice so I don't know if I have time.\\n\\nThe next part is [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li>Project structure</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"solutionstructure\">Solution structure</h1>\n<p>This introduction post will walk you through the project I would like to see in a project and a short comment about the intention with the project.</p>\n<p>The projects that will be present are the one in the following screen shot:<br>\n<img src=\"/content/images/2014/Jun/Project-structure-1.PNG\" alt=\"Project structure\"><br>\nThe rest of this post will give you a brief introduction to each of these projects and what I think should put into them. Note that there isn't such a thing as a generic solution to every problem so for your specific problem it might be good to have a different structure.</p>\n<h2 id=\"contracts\">Contracts</h2>\n<p>The contracts project will contain all the commands that will go into the system and events that will come out as a result of those commands. All the building blocks for putting together those commands and events will also be in this project.</p>\n<p>To implement this I thing it is really beneficial to use F# instead of C#. The reason for this is that it compact way of defining new types that are value types, that is, you can compare them by &quot;value&quot; instead of by reference. These types will also be immutable by default which make it easier to write code with less side effects, and also easier to test things (will be covered in a later post)</p>\n<h2 id=\"domain\">Domain</h2>\n<p>This is where all the logic related to the domain should go. It's not more complex that that, exactly what will go into this project will be covered in later posts.</p>\n<h2 id=\"infrastructure\">Infrastructure</h2>\n<p>In the infrastructure we put everything that specify how we store the things we do, that is, connect to the eventstore. This will also contain the implementation of the base aggregate class that all our aggregates should implement to make life easier. It also contains the interfaces for what an event and command is.</p>\n<h2 id=\"service\">Service</h2>\n<p>This project is the most difficult to define since it depends so much on the application you are building. With that said it will contain the code that will react on events and update the read models/views in my example. You could also have things that take a long time in this project.</p>\n<h2 id=\"web\">Web</h2>\n<p>Straight forward, this is where all the code that is facing the user should be. It could the web api, html or both. For my example I'll use <a href=\"https://github.com/markrendle/Simple.Web\">Simple.Web</a>.</p>\n<h2 id=\"search\">Search</h2>\n<p>Since I'm going to store all the views and readmodels in elasticsearch I've a dedicated project to query that model. This project will also contains the actual DTOs that I'm storing in elasticsearch.</p>\n<h2 id=\"domaintests\">Domain tests</h2>\n<p>Quite straight forward, this will contain all the tests against the domain. You could of course have tests against the other parts as well if you like, but for me that is not that necessary since the domain will use all the other parts, like infrastructure, so if I get the domain work the other parts has to work as well.</p>\n<p>This finish the first post in an unknown number of posts. Hopefully I finish a couple of more this week, but the weather is nice so I don't know if I have time.</p>\n<p>The next part is <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"28","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nSolution structure\nThis introduction post will walk you through the project I would like to see in\na project and a short comment about the intention with the project.\n\nThe projects that will be present are the one in the following screen shot:\n\nThe rest of this post will give you a brief introduction to each of these\nprojects and what I think should put into them. Note that there isn't such a\nthing as a generic solution to every problem so for your specific problem it\nmight be good to have a different structure.\n\nContracts\nThe contracts project will contain all the commands that will go into the system\nand events that will come out as a result of those commands. All the building\nblocks for putting together those commands and events will also be in this\nproject.\n\nTo implement this I thing it is really beneficial to use F# instead of C#. The\nreason for this is that it compact way of defining new types that are value\ntypes, that is, you can compare them by \"value\" instead of by reference. These\ntypes will also be immutable by default which make it easier to write code with\nless side effects, and also easier to test things (will be covered in a later\npost)\n\nDomain\nThis is where all the logic related to the domain should go. It's not more\ncomplex that that, exactly what will go into this project will be covered in\nlater posts.\n\nInfrastructure\nIn the infrastructure we put everything that specify how we store the things we\ndo, that is, connect to the eventstore. This will also contain the\nimplementation of the base aggregate class that all our aggregates should\nimplement to make life easier. It also contains the interfaces for what an event\nand command is.\n\nService\nThis project is the most difficult to define since it depends so much on the\napplication you are building. With that said it will contain the code that will\nreact on events and update the read models/views in my example. You could also\nhave things that take a long time in this project.\n\nWeb\nStraight forward, this is where all the code that is facing the user should be.\nIt could the web api, html or both. For my example I'll use Simple.Web\n[https://github.com/markrendle/Simple.Web].\n\nSearch\nSince I'm going to store all the views and readmodels in elasticsearch I've a\ndedicated project to query that model. This project will also contains the\nactual DTOs that I'm storing in elasticsearch.\n\nDomain tests\nQuite straight forward, this will contain all the tests against the domain. You\ncould of course have tests against the other parts as well if you like, but for\nme that is not that necessary since the domain will use all the other parts,\nlike infrastructure, so if I get the domain work the other parts has to work as\nwell.\n\nThis finish the first post in an unknown number of posts. Hopefully I finish a\ncouple of more this week, but the weather is nice so I don't know if I have\ntime.\n\nThe next part is Infrastructure\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-24T11:37:50.000Z","updated_at":"2014-07-16T06:11:35.000Z","published_at":"2014-06-24T19:29:12.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe050","uuid":"d0e08d85-3ed4-4dff-9e11-e72021bced47","title":"Ending discussion to my blog series about CQRS and event sourcing","slug":"ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In the last couple of [posts](http://blog.tomasjansson.com/tag/cqrsshop/) I've walked you through how to get started writing an application with eventstore, elasticsearch and finally neo4j. The code is on [github](https://github.com/mastoj/CQRSShop). I'm quite sure that implementing an application with multiple views without using event sourcing will be much harder. There are a couple of reasons why and that is what I thought I would start this discussion with.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * Ending discussion\\n\\n#Storing state is like a low quality mp3 audio file\\nWhat I mean with this is that when you store the state of an application you are loosing data, the intention is lost when you move from one state to another. If you compare that to storing the events where you are only storing the intention of the action, but if you know the intention you can derive the state you want. \\\"The state you want\\\" is the key part here, what if you want to derive a different state later on? That is almost impossible (depending on the complexity of the view) if you are using a regular state database like sql or a document store, but when using event sourcing you're free to do so at any point in time since you have a complete log of the system. Of course you can achieve the same with say a sql database by creating shadow tables where you have a complete log of tranisitions of states, but at that point you're basically trying to implement an eventstore without realizing it.\\n\\n#Creating views with event sourcing is the same as updating them\\nWhen you are creating the views in an event sourced system the view is created the same way at any point in time, you process the events and update the view. This makes it really easy recreate the views if you find that something is wrong, you just fix the code that creates the views and than process all the events again.\\n\\nIf you have an application based on a state database plus using events to update views like a graph database you need to maintain two different processes to update a view. On process it the one you run to recreate a view, you have to have that in place if you need to create the views from an existing database. You also need a process to handle the events to update the views, that is, you have two set of codes to maintain instead of just one.\\n\\n#What do I do if the logic that created an event is wrong?\\nLet's say that the code that raises the event indicating that a customer should be a preferred customer has a bug. How do we fix all the customer that now has the wrong state? In that case I think you should first fix the bug, then find which of the customers that could possibly have been affected by the bug an run a counter action on those customers. Doing that you have that the history is preserved and the bug is fixed. Another way you can solve this to is that you expose the logic that calculate if a customer as a service, so it does the calculation upon request, but then you wouldn't get the event telling you that the thing has happened. The important part is that such a request doesn't change anything, it should just get the data without side effects. I believe that in the long run the first example is better and will keep things simpler, but as you see, there is more than one way to solve the issue. The key point here is that you will be able to fix it since you have the complete history of the system.\\n\\nIf you have the same issue in a \\\"traditional\\\" system, when you have fixed the issue you will loose the history of the actual error which might be really bad depending on the domain.\\n\\n#Eventstore as a service bus\\nIn a system we are building at a current project we are not using this model, and I think it starts to be more complex than it have to. If you use the eventstore to store the changes of the system you could basically use it as your \\\"service bus\\\" at the same time. This is exactly what we did with the example application where the service is just listening to changes to the eventstore and then updates accordingly. So if you use eventstore you could end up with less technology, that is, it can be used for your main storage as well as the service bus for the system.\\n\\n#You just want to try something new\\nNo, the thing is that I want to use the right tool for the right work. In the domain model where I'm actually doing things I want to store the changes, and derive the state from the changes. Then I want to use other database for my views since eventstore isn't optimal to query against, of course you can do it but asking after relations is much simpler in a graph database. The same thing applies for searching, you can enable full text searching in a sql database, but it is better to use a tool like elasticsearch which have that as its primary focus. If you need tabular data you can have a projection of the events into a sql database which is greate for tabular data.\\n\\nSo the right tool for the right work is what I'm going for. The samething applied for the programming language I choose in the example project. I used F# for the contracts, some types and events since it is really powerful expressing those types of constructs. I also think that you could probably save a lot if rewriting everything using eventstore since all I'm doing in the domain model is really functional. Applying events is a left fold. Executing an command is a function call with a derived state and command as parameter.\\n\\n#Storing history\\nIn many system the customer want to have an audit log. I've seen, and implemented myself, many systems where you have two tables of everything that you want history on. This is hard to maintain and you are still missing data if you don't add a column of the extra history table that indicate why that is a history row. Also, querying against this type of data can be problematic depending on the structure of the data. \\n\\n#Changing a database is hard when storing state\\nIf you have an application where you store the state it is hard to make changes to this since then you know you have to go over all the code in all the layers and change the structure of that code, and also write some migration script. I you are using event sourcing you just change the event and you are done, as long as you don't change the name of an event or a property of the event, then you have to do some migration of the events. I think it is less common that an event might change than a database changes, since event are smaller and more concise so they are harder to model wrong ones you talked to the business.\\n\\nChanging the view database in a event sources system is easy. You just drop the database, update the logic and re-applies all the event once again.\\n\\n#Give it a try!\\nI really believe that change is good, just have a look at evolution. We know how and can build system as we allways have, but that doesn't mean we can't do it better. If you try this out for real I think you'll end up as a happier developer, have better code with less bugs, understand the business better, react to business needs faster, be able to write better near real time systems (if that is something you want) and learn a ton of new things at the same time. \\n\\nThis is not harder just using a sql database, it is just different. Working with sql is something many of us developer has learned in college and/or at the university. We have also been working with it for multiple years. This give us a false perception that it is much easier to work with sql, I say it isn't. It is different! Different doesn't mean it is harder, just that you might just know how it works just yet, but it isn't that hard to learn if you really give it a try and that is what I ask of you. Give this a try. If I can write an application and 10 blog post about it just 3 evenings you can learn how to do it as well.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>In the last couple of <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">posts</a> I've walked you through how to get started writing an application with eventstore, elasticsearch and finally neo4j. The code is on <a href=\"https://github.com/mastoj/CQRSShop\">github</a>. I'm quite sure that implementing an application with multiple views without using event sourcing will be much harder. There are a couple of reasons why and that is what I thought I would start this discussion with.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li>Ending discussion</li>\n</ul>\n<h1 id=\"storingstateislikealowqualitymp3audiofile\">Storing state is like a low quality mp3 audio file</h1>\n<p>What I mean with this is that when you store the state of an application you are loosing data, the intention is lost when you move from one state to another. If you compare that to storing the events where you are only storing the intention of the action, but if you know the intention you can derive the state you want. &quot;The state you want&quot; is the key part here, what if you want to derive a different state later on? That is almost impossible (depending on the complexity of the view) if you are using a regular state database like sql or a document store, but when using event sourcing you're free to do so at any point in time since you have a complete log of the system. Of course you can achieve the same with say a sql database by creating shadow tables where you have a complete log of tranisitions of states, but at that point you're basically trying to implement an eventstore without realizing it.</p>\n<h1 id=\"creatingviewswitheventsourcingisthesameasupdatingthem\">Creating views with event sourcing is the same as updating them</h1>\n<p>When you are creating the views in an event sourced system the view is created the same way at any point in time, you process the events and update the view. This makes it really easy recreate the views if you find that something is wrong, you just fix the code that creates the views and than process all the events again.</p>\n<p>If you have an application based on a state database plus using events to update views like a graph database you need to maintain two different processes to update a view. On process it the one you run to recreate a view, you have to have that in place if you need to create the views from an existing database. You also need a process to handle the events to update the views, that is, you have two set of codes to maintain instead of just one.</p>\n<h1 id=\"whatdoidoifthelogicthatcreatedaneventiswrong\">What do I do if the logic that created an event is wrong?</h1>\n<p>Let's say that the code that raises the event indicating that a customer should be a preferred customer has a bug. How do we fix all the customer that now has the wrong state? In that case I think you should first fix the bug, then find which of the customers that could possibly have been affected by the bug an run a counter action on those customers. Doing that you have that the history is preserved and the bug is fixed. Another way you can solve this to is that you expose the logic that calculate if a customer as a service, so it does the calculation upon request, but then you wouldn't get the event telling you that the thing has happened. The important part is that such a request doesn't change anything, it should just get the data without side effects. I believe that in the long run the first example is better and will keep things simpler, but as you see, there is more than one way to solve the issue. The key point here is that you will be able to fix it since you have the complete history of the system.</p>\n<p>If you have the same issue in a &quot;traditional&quot; system, when you have fixed the issue you will loose the history of the actual error which might be really bad depending on the domain.</p>\n<h1 id=\"eventstoreasaservicebus\">Eventstore as a service bus</h1>\n<p>In a system we are building at a current project we are not using this model, and I think it starts to be more complex than it have to. If you use the eventstore to store the changes of the system you could basically use it as your &quot;service bus&quot; at the same time. This is exactly what we did with the example application where the service is just listening to changes to the eventstore and then updates accordingly. So if you use eventstore you could end up with less technology, that is, it can be used for your main storage as well as the service bus for the system.</p>\n<h1 id=\"youjustwanttotrysomethingnew\">You just want to try something new</h1>\n<p>No, the thing is that I want to use the right tool for the right work. In the domain model where I'm actually doing things I want to store the changes, and derive the state from the changes. Then I want to use other database for my views since eventstore isn't optimal to query against, of course you can do it but asking after relations is much simpler in a graph database. The same thing applies for searching, you can enable full text searching in a sql database, but it is better to use a tool like elasticsearch which have that as its primary focus. If you need tabular data you can have a projection of the events into a sql database which is greate for tabular data.</p>\n<p>So the right tool for the right work is what I'm going for. The samething applied for the programming language I choose in the example project. I used F# for the contracts, some types and events since it is really powerful expressing those types of constructs. I also think that you could probably save a lot if rewriting everything using eventstore since all I'm doing in the domain model is really functional. Applying events is a left fold. Executing an command is a function call with a derived state and command as parameter.</p>\n<h1 id=\"storinghistory\">Storing history</h1>\n<p>In many system the customer want to have an audit log. I've seen, and implemented myself, many systems where you have two tables of everything that you want history on. This is hard to maintain and you are still missing data if you don't add a column of the extra history table that indicate why that is a history row. Also, querying against this type of data can be problematic depending on the structure of the data.</p>\n<h1 id=\"changingadatabaseishardwhenstoringstate\">Changing a database is hard when storing state</h1>\n<p>If you have an application where you store the state it is hard to make changes to this since then you know you have to go over all the code in all the layers and change the structure of that code, and also write some migration script. I you are using event sourcing you just change the event and you are done, as long as you don't change the name of an event or a property of the event, then you have to do some migration of the events. I think it is less common that an event might change than a database changes, since event are smaller and more concise so they are harder to model wrong ones you talked to the business.</p>\n<p>Changing the view database in a event sources system is easy. You just drop the database, update the logic and re-applies all the event once again.</p>\n<h1 id=\"giveitatry\">Give it a try!</h1>\n<p>I really believe that change is good, just have a look at evolution. We know how and can build system as we allways have, but that doesn't mean we can't do it better. If you try this out for real I think you'll end up as a happier developer, have better code with less bugs, understand the business better, react to business needs faster, be able to write better near real time systems (if that is something you want) and learn a ton of new things at the same time.</p>\n<p>This is not harder just using a sql database, it is just different. Working with sql is something many of us developer has learned in college and/or at the university. We have also been working with it for multiple years. This give us a false perception that it is much easier to work with sql, I say it isn't. It is different! Different doesn't mean it is harder, just that you might just know how it works just yet, but it isn't that hard to learn if you really give it a try and that is what I ask of you. Give this a try. If I can write an application and 10 blog post about it just 3 evenings you can learn how to do it as well.</p>\n<!--kg-card-end: markdown-->","comment_id":"29","plaintext":"In the last couple of posts [http://blog.tomasjansson.com/tag/cqrsshop/] I've\nwalked you through how to get started writing an application with eventstore,\nelasticsearch and finally neo4j. The code is on github\n[https://github.com/mastoj/CQRSShop]. I'm quite sure that implementing an\napplication with multiple views without using event sourcing will be much\nharder. There are a couple of reasons why and that is what I thought I would\nstart this discussion with.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n\nStoring state is like a low quality mp3 audio file\nWhat I mean with this is that when you store the state of an application you are\nloosing data, the intention is lost when you move from one state to another. If\nyou compare that to storing the events where you are only storing the intention\nof the action, but if you know the intention you can derive the state you want.\n\"The state you want\" is the key part here, what if you want to derive a\ndifferent state later on? That is almost impossible (depending on the complexity\nof the view) if you are using a regular state database like sql or a document\nstore, but when using event sourcing you're free to do so at any point in time\nsince you have a complete log of the system. Of course you can achieve the same\nwith say a sql database by creating shadow tables where you have a complete log\nof tranisitions of states, but at that point you're basically trying to\nimplement an eventstore without realizing it.\n\nCreating views with event sourcing is the same as updating them\nWhen you are creating the views in an event sourced system the view is created\nthe same way at any point in time, you process the events and update the view.\nThis makes it really easy recreate the views if you find that something is\nwrong, you just fix the code that creates the views and than process all the\nevents again.\n\nIf you have an application based on a state database plus using events to update\nviews like a graph database you need to maintain two different processes to\nupdate a view. On process it the one you run to recreate a view, you have to\nhave that in place if you need to create the views from an existing database.\nYou also need a process to handle the events to update the views, that is, you\nhave two set of codes to maintain instead of just one.\n\nWhat do I do if the logic that created an event is wrong?\nLet's say that the code that raises the event indicating that a customer should\nbe a preferred customer has a bug. How do we fix all the customer that now has\nthe wrong state? In that case I think you should first fix the bug, then find\nwhich of the customers that could possibly have been affected by the bug an run\na counter action on those customers. Doing that you have that the history is\npreserved and the bug is fixed. Another way you can solve this to is that you\nexpose the logic that calculate if a customer as a service, so it does the\ncalculation upon request, but then you wouldn't get the event telling you that\nthe thing has happened. The important part is that such a request doesn't change\nanything, it should just get the data without side effects. I believe that in\nthe long run the first example is better and will keep things simpler, but as\nyou see, there is more than one way to solve the issue. The key point here is\nthat you will be able to fix it since you have the complete history of the\nsystem.\n\nIf you have the same issue in a \"traditional\" system, when you have fixed the\nissue you will loose the history of the actual error which might be really bad\ndepending on the domain.\n\nEventstore as a service bus\nIn a system we are building at a current project we are not using this model,\nand I think it starts to be more complex than it have to. If you use the\neventstore to store the changes of the system you could basically use it as your\n\"service bus\" at the same time. This is exactly what we did with the example\napplication where the service is just listening to changes to the eventstore and\nthen updates accordingly. So if you use eventstore you could end up with less\ntechnology, that is, it can be used for your main storage as well as the service\nbus for the system.\n\nYou just want to try something new\nNo, the thing is that I want to use the right tool for the right work. In the\ndomain model where I'm actually doing things I want to store the changes, and\nderive the state from the changes. Then I want to use other database for my\nviews since eventstore isn't optimal to query against, of course you can do it\nbut asking after relations is much simpler in a graph database. The same thing\napplies for searching, you can enable full text searching in a sql database, but\nit is better to use a tool like elasticsearch which have that as its primary\nfocus. If you need tabular data you can have a projection of the events into a\nsql database which is greate for tabular data.\n\nSo the right tool for the right work is what I'm going for. The samething\napplied for the programming language I choose in the example project. I used F#\nfor the contracts, some types and events since it is really powerful expressing\nthose types of constructs. I also think that you could probably save a lot if\nrewriting everything using eventstore since all I'm doing in the domain model is\nreally functional. Applying events is a left fold. Executing an command is a\nfunction call with a derived state and command as parameter.\n\nStoring history\nIn many system the customer want to have an audit log. I've seen, and\nimplemented myself, many systems where you have two tables of everything that\nyou want history on. This is hard to maintain and you are still missing data if\nyou don't add a column of the extra history table that indicate why that is a\nhistory row. Also, querying against this type of data can be problematic\ndepending on the structure of the data.\n\nChanging a database is hard when storing state\nIf you have an application where you store the state it is hard to make changes\nto this since then you know you have to go over all the code in all the layers\nand change the structure of that code, and also write some migration script. I\nyou are using event sourcing you just change the event and you are done, as long\nas you don't change the name of an event or a property of the event, then you\nhave to do some migration of the events. I think it is less common that an event\nmight change than a database changes, since event are smaller and more concise\nso they are harder to model wrong ones you talked to the business.\n\nChanging the view database in a event sources system is easy. You just drop the\ndatabase, update the logic and re-applies all the event once again.\n\nGive it a try!\nI really believe that change is good, just have a look at evolution. We know how\nand can build system as we allways have, but that doesn't mean we can't do it\nbetter. If you try this out for real I think you'll end up as a happier\ndeveloper, have better code with less bugs, understand the business better,\nreact to business needs faster, be able to write better near real time systems\n(if that is something you want) and learn a ton of new things at the same time.\n\nThis is not harder just using a sql database, it is just different. Working with\nsql is something many of us developer has learned in college and/or at the\nuniversity. We have also been working with it for multiple years. This give us a\nfalse perception that it is much easier to work with sql, I say it isn't. It is\ndifferent! Different doesn't mean it is harder, just that you might just know\nhow it works just yet, but it isn't that hard to learn if you really give it a\ntry and that is what I ask of you. Give this a try. If I can write an\napplication and 10 blog post about it just 3 evenings you can learn how to do it\nas well.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-24T11:41:33.000Z","updated_at":"2014-08-18T08:09:51.000Z","published_at":"2014-06-28T17:48:30.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe051","uuid":"48250401-e146-4464-84aa-4ddff538b5a4","title":"CQRS the simple way with eventstore and elasticsearch: Infrastructure","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * Infrastructure\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Let's start in the deep end\\nWriting the infrastructure code is most likely the most technical complex code in this project, other parts might be complex due to the actual domain. The repositories and code I'll show here is more complex than the average SQL repository, but the good thing is that we only need one and just one repository for all the aggregates in our domain. The code to the infrastructure project is located on this url: https://github.com/mastoj/CQRSShop/tree/master/src/CQRSShop.Infrastructure\\n\\n##The interfaces\\nThere are a couple of interfaces that I thought I will cover first, this interfaces are quite straightforwad so I won't go into great details (the post will most likely be quite long either way).\\n \\n * `ICommand` - marker interface for our commands. This can make it possible to find command handlers through reflection (but I like to do that explicitly)\\n * `IEvent` - interface for all our events, which should have an id on them so we know which aggregate that was the cause for an event\\n * `IAggregate` - defines the basic functionality for an aggregate, the base implementation will be covered in more detail later in this post \\n * `IDomainRepository` - interface for what we expect from a domain repository, there will be one repository for eventstore as well as a simple in-memory one used for testing\\n\\n##The command dispatcher\\nThe purpose for the command dispatcher is to route the command to the registered handler, execute the handler and then save the result. Registration of ther to route the message will be done from the actual domain, so this is just the infrastructure part. The implementation is listed below, with comments after the code.\\n\\n    public class CommandDispatcher\\n    {\\n        private Dictionary<Type, Func<object, IAggregate>> _routes;\\n        private IDomainRepository _domainRepository;\\n        private readonly IEnumerable<Action<object>> _postExecutionPipe;\\n        private readonly IEnumerable<Action<ICommand>> _preExecutionPipe;\\n\\n        public CommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\\n        {\\n            _domainRepository = domainRepository;\\n            _postExecutionPipe = postExecutionPipe;\\n            _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\\n            _routes = new Dictionary<Type, Func<object, IAggregate>>();\\n        }\\n\\n        public void RegisterHandler<TCommand>(Func<TCommand, IAggregate> handle) where TCommand : class, ICommand\\n        {\\n            _routes.Add(typeof(TCommand), o => handle(o as TCommand));\\n        }\\n\\n        public void ExecuteCommand(ICommand command)\\n        {\\n            var commandType = command.GetType();\\n\\n            RunPreExecutionPipe(command);\\n            if (!_routes.ContainsKey(commandType))\\n            {\\n                throw new ApplicationException(\\\"Missing handler for \\\" + commandType.Name);\\n            }\\n            var aggregate = _routes[commandType](command);\\n            var savedEvents = _domainRepository.Save(aggregate);\\n            RunPostExecutionPipe(savedEvents);\\n        }\\n\\n        private void RunPostExecutionPipe(IEnumerable<object> savedEvents)\\n        {\\n            foreach (var savedEvent in savedEvents)\\n            {\\n                foreach (var action in _postExecutionPipe)\\n                {\\n                    action(savedEvent);\\n                }\\n            }\\n        }\\n\\n        private void RunPreExecutionPipe(ICommand command)\\n        {\\n            foreach (var action in _preExecutionPipe)\\n            {\\n                action(command);\\n            }\\n        }\\n    }\\n    \\nThere are three parameters for the constructor, one is the domain repository since that is needed to save the events after a command finished. The `preExecutionPipe` and `postExecutionPipe` is not mandatory to use, but I show here how you can easily inject methods before and after the execution of a command. This could be things like logging or some security check.\\n\\nThere are two public methods; `RegisterHandler` that is called to register one handler that a command should be routed to, and then there is the `ExecuteCommand` that does the actual routing and execution of a command.\\n\\n##The base repository\\nThere are some logic that is common for the repository for eventstore as well as for the in-memory repository. See below for the code and comments below the code. \\n\\n    public abstract class DomainRepositoryBase : IDomainRepository\\n    {\\n        public abstract IEnumerable<IEvent> Save<TAggregate>(TAggregate aggregate) where TAggregate : IAggregate;\\n        public abstract TResult GetById<TResult>(Guid id) where TResult : IAggregate, new();\\n\\n        protected int CalculateExpectedVersion(IAggregate aggregate, List<IEvent> events)\\n        {\\n            var expectedVersion = aggregate.Version - events.Count;\\n            return expectedVersion;\\n        }\\n\\n        protected TResult BuildAggregate<TResult>(IEnumerable<IEvent> events) where TResult : IAggregate, new()\\n        {\\n            var result = new TResult();\\n            foreach (var @event in events)\\n            {\\n                result.ApplyEvent(@event);\\n            }\\n            return result;\\n        }\\n    }\\n\\nTwo methods here, one that is used to calculate the version of an aggregate, this is important so you don't try to save events when you have the wrong version. The second method, `BuildAggregate` is used to build up an aggregate for a series of events.\\n\\n##The repositories\\nMy eventstore repository is based on the implementation here: http://geteventstore.com/blog/20130220/getting-started-part-2-implementing-the-commondomain-repository-interface/, so I won't go into details of how it works. If you understand that blog post you understand my code as well since it is a simplified version.\\n\\nThe in-memory won't be covered since it is not mandatory to have, but if you're interested feel free to check it out on github.\\n\\n##The base aggregate\\nThe last thing that will be cover is probably one of the most important classes. One thing that is important to realize before looking to the code is that when working with aggregates in this model the transition of state is separated from the logic that defines if the transition is valid. With that said the code looks like this: \\n\\n    public class AggregateBase : IAggregate\\n    {\\n        public int Version\\n        {\\n            get\\n            {\\n                return _version;\\n            }\\n            protected set\\n            {\\n                _version = value;\\n            }\\n        }\\n\\n        public Guid Id { get; protected set; }\\n\\n        private List<IEvent> _uncommitedEvents = new List<IEvent>();\\n        private Dictionary<Type, Action<IEvent>> _routes = new Dictionary<Type, Action<IEvent>>();\\n        private int _version = -1;\\n\\n        public void RaiseEvent(IEvent @event)\\n        {\\n            ApplyEvent(@event);\\n            _uncommitedEvents.Add(@event);\\n        }\\n\\n        protected void RegisterTransition<T>(Action<T> transition) where T : class\\n        {\\n            _routes.Add(typeof(T), o => transition(o as T));\\n        }\\n\\n        public void ApplyEvent(IEvent @event)\\n        {\\n            var eventType = @event.GetType();\\n            if (_routes.ContainsKey(eventType))\\n            {\\n                _routes[eventType](@event);\\n            }\\n            Version++;\\n        }\\n\\n        public IEnumerable<IEvent> UncommitedEvents()\\n        {\\n            return _uncommitedEvents;\\n        }\\n\\n        public void ClearUncommitedEvents()\\n        {\\n            _uncommitedEvents.Clear();\\n        }\\n    }\\n\\nThere are two base properties for all aggregates; `Id` and `Version`. I think those properties are sort of self explaining, the id tells you what id the aggregate has since an aggregate is something that has an id compared to a value object. The version is an important property on an aggregate when doing things with event sourcing. The `RaiseEvent` method is called in the aggregates when logic has passed to do a transition. `RegisterTransition` is used a simple helper method to register transition that should be applied in the case of an event. `ApplyEvent` is called when an event should be applied to the aggregate, this will make a call to the registered transition method for the aggregate and change the state of the aggregate. The last two methods are the `UncommitedEvents` and `ClearUncommitedEvents` which are used to get all the changes caused by a command before saving them, and after save the events are cleared.\\n\\nThat finish the second post, please feel free to comment. The next part in the series is [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/).\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li>Infrastructure</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"letsstartinthedeepend\">Let's start in the deep end</h1>\n<p>Writing the infrastructure code is most likely the most technical complex code in this project, other parts might be complex due to the actual domain. The repositories and code I'll show here is more complex than the average SQL repository, but the good thing is that we only need one and just one repository for all the aggregates in our domain. The code to the infrastructure project is located on this url: <a href=\"https://github.com/mastoj/CQRSShop/tree/master/src/CQRSShop.Infrastructure\">https://github.com/mastoj/CQRSShop/tree/master/src/CQRSShop.Infrastructure</a></p>\n<h2 id=\"theinterfaces\">The interfaces</h2>\n<p>There are a couple of interfaces that I thought I will cover first, this interfaces are quite straightforwad so I won't go into great details (the post will most likely be quite long either way).</p>\n<ul>\n<li><code>ICommand</code> - marker interface for our commands. This can make it possible to find command handlers through reflection (but I like to do that explicitly)</li>\n<li><code>IEvent</code> - interface for all our events, which should have an id on them so we know which aggregate that was the cause for an event</li>\n<li><code>IAggregate</code> - defines the basic functionality for an aggregate, the base implementation will be covered in more detail later in this post</li>\n<li><code>IDomainRepository</code> - interface for what we expect from a domain repository, there will be one repository for eventstore as well as a simple in-memory one used for testing</li>\n</ul>\n<h2 id=\"thecommanddispatcher\">The command dispatcher</h2>\n<p>The purpose for the command dispatcher is to route the command to the registered handler, execute the handler and then save the result. Registration of ther to route the message will be done from the actual domain, so this is just the infrastructure part. The implementation is listed below, with comments after the code.</p>\n<pre><code>public class CommandDispatcher\n{\n    private Dictionary&lt;Type, Func&lt;object, IAggregate&gt;&gt; _routes;\n    private IDomainRepository _domainRepository;\n    private readonly IEnumerable&lt;Action&lt;object&gt;&gt; _postExecutionPipe;\n    private readonly IEnumerable&lt;Action&lt;ICommand&gt;&gt; _preExecutionPipe;\n\n    public CommandDispatcher(IDomainRepository domainRepository, IEnumerable&lt;Action&lt;ICommand&gt;&gt; preExecutionPipe, IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe)\n    {\n        _domainRepository = domainRepository;\n        _postExecutionPipe = postExecutionPipe;\n        _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty&lt;Action&lt;ICommand&gt;&gt;();\n        _routes = new Dictionary&lt;Type, Func&lt;object, IAggregate&gt;&gt;();\n    }\n\n    public void RegisterHandler&lt;TCommand&gt;(Func&lt;TCommand, IAggregate&gt; handle) where TCommand : class, ICommand\n    {\n        _routes.Add(typeof(TCommand), o =&gt; handle(o as TCommand));\n    }\n\n    public void ExecuteCommand(ICommand command)\n    {\n        var commandType = command.GetType();\n\n        RunPreExecutionPipe(command);\n        if (!_routes.ContainsKey(commandType))\n        {\n            throw new ApplicationException(&quot;Missing handler for &quot; + commandType.Name);\n        }\n        var aggregate = _routes[commandType](command);\n        var savedEvents = _domainRepository.Save(aggregate);\n        RunPostExecutionPipe(savedEvents);\n    }\n\n    private void RunPostExecutionPipe(IEnumerable&lt;object&gt; savedEvents)\n    {\n        foreach (var savedEvent in savedEvents)\n        {\n            foreach (var action in _postExecutionPipe)\n            {\n                action(savedEvent);\n            }\n        }\n    }\n\n    private void RunPreExecutionPipe(ICommand command)\n    {\n        foreach (var action in _preExecutionPipe)\n        {\n            action(command);\n        }\n    }\n}\n</code></pre>\n<p>There are three parameters for the constructor, one is the domain repository since that is needed to save the events after a command finished. The <code>preExecutionPipe</code> and <code>postExecutionPipe</code> is not mandatory to use, but I show here how you can easily inject methods before and after the execution of a command. This could be things like logging or some security check.</p>\n<p>There are two public methods; <code>RegisterHandler</code> that is called to register one handler that a command should be routed to, and then there is the <code>ExecuteCommand</code> that does the actual routing and execution of a command.</p>\n<h2 id=\"thebaserepository\">The base repository</h2>\n<p>There are some logic that is common for the repository for eventstore as well as for the in-memory repository. See below for the code and comments below the code.</p>\n<pre><code>public abstract class DomainRepositoryBase : IDomainRepository\n{\n    public abstract IEnumerable&lt;IEvent&gt; Save&lt;TAggregate&gt;(TAggregate aggregate) where TAggregate : IAggregate;\n    public abstract TResult GetById&lt;TResult&gt;(Guid id) where TResult : IAggregate, new();\n\n    protected int CalculateExpectedVersion(IAggregate aggregate, List&lt;IEvent&gt; events)\n    {\n        var expectedVersion = aggregate.Version - events.Count;\n        return expectedVersion;\n    }\n\n    protected TResult BuildAggregate&lt;TResult&gt;(IEnumerable&lt;IEvent&gt; events) where TResult : IAggregate, new()\n    {\n        var result = new TResult();\n        foreach (var @event in events)\n        {\n            result.ApplyEvent(@event);\n        }\n        return result;\n    }\n}\n</code></pre>\n<p>Two methods here, one that is used to calculate the version of an aggregate, this is important so you don't try to save events when you have the wrong version. The second method, <code>BuildAggregate</code> is used to build up an aggregate for a series of events.</p>\n<h2 id=\"therepositories\">The repositories</h2>\n<p>My eventstore repository is based on the implementation here: <a href=\"http://geteventstore.com/blog/20130220/getting-started-part-2-implementing-the-commondomain-repository-interface/\">http://geteventstore.com/blog/20130220/getting-started-part-2-implementing-the-commondomain-repository-interface/</a>, so I won't go into details of how it works. If you understand that blog post you understand my code as well since it is a simplified version.</p>\n<p>The in-memory won't be covered since it is not mandatory to have, but if you're interested feel free to check it out on github.</p>\n<h2 id=\"thebaseaggregate\">The base aggregate</h2>\n<p>The last thing that will be cover is probably one of the most important classes. One thing that is important to realize before looking to the code is that when working with aggregates in this model the transition of state is separated from the logic that defines if the transition is valid. With that said the code looks like this:</p>\n<pre><code>public class AggregateBase : IAggregate\n{\n    public int Version\n    {\n        get\n        {\n            return _version;\n        }\n        protected set\n        {\n            _version = value;\n        }\n    }\n\n    public Guid Id { get; protected set; }\n\n    private List&lt;IEvent&gt; _uncommitedEvents = new List&lt;IEvent&gt;();\n    private Dictionary&lt;Type, Action&lt;IEvent&gt;&gt; _routes = new Dictionary&lt;Type, Action&lt;IEvent&gt;&gt;();\n    private int _version = -1;\n\n    public void RaiseEvent(IEvent @event)\n    {\n        ApplyEvent(@event);\n        _uncommitedEvents.Add(@event);\n    }\n\n    protected void RegisterTransition&lt;T&gt;(Action&lt;T&gt; transition) where T : class\n    {\n        _routes.Add(typeof(T), o =&gt; transition(o as T));\n    }\n\n    public void ApplyEvent(IEvent @event)\n    {\n        var eventType = @event.GetType();\n        if (_routes.ContainsKey(eventType))\n        {\n            _routes[eventType](@event);\n        }\n        Version++;\n    }\n\n    public IEnumerable&lt;IEvent&gt; UncommitedEvents()\n    {\n        return _uncommitedEvents;\n    }\n\n    public void ClearUncommitedEvents()\n    {\n        _uncommitedEvents.Clear();\n    }\n}\n</code></pre>\n<p>There are two base properties for all aggregates; <code>Id</code> and <code>Version</code>. I think those properties are sort of self explaining, the id tells you what id the aggregate has since an aggregate is something that has an id compared to a value object. The version is an important property on an aggregate when doing things with event sourcing. The <code>RaiseEvent</code> method is called in the aggregates when logic has passed to do a transition. <code>RegisterTransition</code> is used a simple helper method to register transition that should be applied in the case of an event. <code>ApplyEvent</code> is called when an event should be applied to the aggregate, this will make a call to the registered transition method for the aggregate and change the state of the aggregate. The last two methods are the <code>UncommitedEvents</code> and <code>ClearUncommitedEvents</code> which are used to get all the changes caused by a command before saving them, and after save the events are cleared.</p>\n<p>That finish the second post, please feel free to comment. The next part in the series is <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"30","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nLet's start in the deep end\nWriting the infrastructure code is most likely the most technical complex code\nin this project, other parts might be complex due to the actual domain. The\nrepositories and code I'll show here is more complex than the average SQL\nrepository, but the good thing is that we only need one and just one repository\nfor all the aggregates in our domain. The code to the infrastructure project is\nlocated on this url: \nhttps://github.com/mastoj/CQRSShop/tree/master/src/CQRSShop.Infrastructure\n\nThe interfaces\nThere are a couple of interfaces that I thought I will cover first, this\ninterfaces are quite straightforwad so I won't go into great details (the post\nwill most likely be quite long either way).\n\n * ICommand - marker interface for our commands. This can make it possible to\n   find command handlers through reflection (but I like to do that explicitly)\n * IEvent - interface for all our events, which should have an id on them so we\n   know which aggregate that was the cause for an event\n * IAggregate - defines the basic functionality for an aggregate, the base\n   implementation will be covered in more detail later in this post\n * IDomainRepository - interface for what we expect from a domain repository,\n   there will be one repository for eventstore as well as a simple in-memory one\n   used for testing\n\nThe command dispatcher\nThe purpose for the command dispatcher is to route the command to the registered\nhandler, execute the handler and then save the result. Registration of ther to\nroute the message will be done from the actual domain, so this is just the\ninfrastructure part. The implementation is listed below, with comments after the\ncode.\n\npublic class CommandDispatcher\n{\n    private Dictionary<Type, Func<object, IAggregate>> _routes;\n    private IDomainRepository _domainRepository;\n    private readonly IEnumerable<Action<object>> _postExecutionPipe;\n    private readonly IEnumerable<Action<ICommand>> _preExecutionPipe;\n\n    public CommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\n    {\n        _domainRepository = domainRepository;\n        _postExecutionPipe = postExecutionPipe;\n        _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\n        _routes = new Dictionary<Type, Func<object, IAggregate>>();\n    }\n\n    public void RegisterHandler<TCommand>(Func<TCommand, IAggregate> handle) where TCommand : class, ICommand\n    {\n        _routes.Add(typeof(TCommand), o => handle(o as TCommand));\n    }\n\n    public void ExecuteCommand(ICommand command)\n    {\n        var commandType = command.GetType();\n\n        RunPreExecutionPipe(command);\n        if (!_routes.ContainsKey(commandType))\n        {\n            throw new ApplicationException(\"Missing handler for \" + commandType.Name);\n        }\n        var aggregate = _routes[commandType](command);\n        var savedEvents = _domainRepository.Save(aggregate);\n        RunPostExecutionPipe(savedEvents);\n    }\n\n    private void RunPostExecutionPipe(IEnumerable<object> savedEvents)\n    {\n        foreach (var savedEvent in savedEvents)\n        {\n            foreach (var action in _postExecutionPipe)\n            {\n                action(savedEvent);\n            }\n        }\n    }\n\n    private void RunPreExecutionPipe(ICommand command)\n    {\n        foreach (var action in _preExecutionPipe)\n        {\n            action(command);\n        }\n    }\n}\n\n\nThere are three parameters for the constructor, one is the domain repository\nsince that is needed to save the events after a command finished. The \npreExecutionPipe and postExecutionPipe is not mandatory to use, but I show here\nhow you can easily inject methods before and after the execution of a command.\nThis could be things like logging or some security check.\n\nThere are two public methods; RegisterHandler that is called to register one\nhandler that a command should be routed to, and then there is the ExecuteCommand \nthat does the actual routing and execution of a command.\n\nThe base repository\nThere are some logic that is common for the repository for eventstore as well as\nfor the in-memory repository. See below for the code and comments below the\ncode.\n\npublic abstract class DomainRepositoryBase : IDomainRepository\n{\n    public abstract IEnumerable<IEvent> Save<TAggregate>(TAggregate aggregate) where TAggregate : IAggregate;\n    public abstract TResult GetById<TResult>(Guid id) where TResult : IAggregate, new();\n\n    protected int CalculateExpectedVersion(IAggregate aggregate, List<IEvent> events)\n    {\n        var expectedVersion = aggregate.Version - events.Count;\n        return expectedVersion;\n    }\n\n    protected TResult BuildAggregate<TResult>(IEnumerable<IEvent> events) where TResult : IAggregate, new()\n    {\n        var result = new TResult();\n        foreach (var @event in events)\n        {\n            result.ApplyEvent(@event);\n        }\n        return result;\n    }\n}\n\n\nTwo methods here, one that is used to calculate the version of an aggregate,\nthis is important so you don't try to save events when you have the wrong\nversion. The second method, BuildAggregate is used to build up an aggregate for\na series of events.\n\nThe repositories\nMy eventstore repository is based on the implementation here: \nhttp://geteventstore.com/blog/20130220/getting-started-part-2-implementing-the-commondomain-repository-interface/\n, so I won't go into details of how it works. If you understand that blog post\nyou understand my code as well since it is a simplified version.\n\nThe in-memory won't be covered since it is not mandatory to have, but if you're\ninterested feel free to check it out on github.\n\nThe base aggregate\nThe last thing that will be cover is probably one of the most important classes.\nOne thing that is important to realize before looking to the code is that when\nworking with aggregates in this model the transition of state is separated from\nthe logic that defines if the transition is valid. With that said the code looks\nlike this:\n\npublic class AggregateBase : IAggregate\n{\n    public int Version\n    {\n        get\n        {\n            return _version;\n        }\n        protected set\n        {\n            _version = value;\n        }\n    }\n\n    public Guid Id { get; protected set; }\n\n    private List<IEvent> _uncommitedEvents = new List<IEvent>();\n    private Dictionary<Type, Action<IEvent>> _routes = new Dictionary<Type, Action<IEvent>>();\n    private int _version = -1;\n\n    public void RaiseEvent(IEvent @event)\n    {\n        ApplyEvent(@event);\n        _uncommitedEvents.Add(@event);\n    }\n\n    protected void RegisterTransition<T>(Action<T> transition) where T : class\n    {\n        _routes.Add(typeof(T), o => transition(o as T));\n    }\n\n    public void ApplyEvent(IEvent @event)\n    {\n        var eventType = @event.GetType();\n        if (_routes.ContainsKey(eventType))\n        {\n            _routes[eventType](@event);\n        }\n        Version++;\n    }\n\n    public IEnumerable<IEvent> UncommitedEvents()\n    {\n        return _uncommitedEvents;\n    }\n\n    public void ClearUncommitedEvents()\n    {\n        _uncommitedEvents.Clear();\n    }\n}\n\n\nThere are two base properties for all aggregates; Id and Version. I think those\nproperties are sort of self explaining, the id tells you what id the aggregate\nhas since an aggregate is something that has an id compared to a value object.\nThe version is an important property on an aggregate when doing things with\nevent sourcing. The RaiseEvent method is called in the aggregates when logic has\npassed to do a transition. RegisterTransition is used a simple helper method to\nregister transition that should be applied in the case of an event. ApplyEvent \nis called when an event should be applied to the aggregate, this will make a\ncall to the registered transition method for the aggregate and change the state\nof the aggregate. The last two methods are the UncommitedEvents and \nClearUncommitedEvents which are used to get all the changes caused by a command\nbefore saving them, and after save the events are cleared.\n\nThat finish the second post, please feel free to comment. The next part in the\nseries is The goal\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-24T19:55:04.000Z","updated_at":"2014-07-16T06:12:06.000Z","published_at":"2014-06-24T20:16:39.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe052","uuid":"d0010870-f6ae-44ea-be23-0cfbf0e340b9","title":"CQRS the simple way with eventstore and elasticsearch: The Goal","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * The goal\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#What is the goal? \\nThis post should probably have been the first one, but better late than never :). So the purpose of this post is to show the target architecture, more than just the screenshot of the projects in the first [post](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/).\\n\\n##Application description\\nThe application that is being developed is a simple web shop. It isn't a complete web shop with all the possible feature, just enough to show how to put the application together and how to store the views in elasticsearch (and maybe neo4j if I have time).\\n\\nThe features I plan to include are:\\n\\n * Create customer\\n * Make customer preferred --> get's 25% discount\\n * Create product\\n * Create shopping basket\\n * Add item to basket\\n * Go to checkout\\n * Cancel order\\n * Goto payment\\n * Make payment\\n * Approve order, orders over 100000 needs approval\\n * Start shipping of order, after this point the customer can't cancel\\n * Ship order, marks the order as shipped\\n \\nThis features will be implemented and tested in an effective way where I also test the absence of side effects.\\n\\n##Target architecture\\nThe target architecture is illustrated in the my nice looking picture below:\\n![](/content/images/2014/Jun/Architecture.jpg)\\n\\nLet me walk you through it. First we have the user, which have two options. First the user can ask for the data. If you follow the arrows down on the right side you see that there is no possible way for the user to go all the way down to eventstore, it can only query specialized views. The second thing the user can do is place an action through the UI (a web api in this application). The action will be translated to a command (if it is not already one when comming through the api). \\n\\nFrom the way from the UI to the dispatcher the commands go through a pipeline where you can inject functionality as logging and security checks. Since we have a single point of entry to the domain it is really easy to add functionality that concerns every command as a step in this pipeline.  \\n\\nThe dispatcher figure out what should handle the command and execute it. After execution the resulting events are stored in the eventstore.\\n\\nWhen the eventstore is updated we have a service (this will be an actual windows service) that listen to changes to the eventstore and if it sees an event which it is interested in it will update the views which are stored in elasticsearch and maybe neo4j.\\n\\nNow I hope you have a better understanding of what the goal is so we can continue with the implementation in the next post, [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li>The goal</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"whatisthegoal\">What is the goal?</h1>\n<p>This post should probably have been the first one, but better late than never :). So the purpose of this post is to show the target architecture, more than just the screenshot of the projects in the first <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">post</a>.</p>\n<h2 id=\"applicationdescription\">Application description</h2>\n<p>The application that is being developed is a simple web shop. It isn't a complete web shop with all the possible feature, just enough to show how to put the application together and how to store the views in elasticsearch (and maybe neo4j if I have time).</p>\n<p>The features I plan to include are:</p>\n<ul>\n<li>Create customer</li>\n<li>Make customer preferred --&gt; get's 25% discount</li>\n<li>Create product</li>\n<li>Create shopping basket</li>\n<li>Add item to basket</li>\n<li>Go to checkout</li>\n<li>Cancel order</li>\n<li>Goto payment</li>\n<li>Make payment</li>\n<li>Approve order, orders over 100000 needs approval</li>\n<li>Start shipping of order, after this point the customer can't cancel</li>\n<li>Ship order, marks the order as shipped</li>\n</ul>\n<p>This features will be implemented and tested in an effective way where I also test the absence of side effects.</p>\n<h2 id=\"targetarchitecture\">Target architecture</h2>\n<p>The target architecture is illustrated in the my nice looking picture below:<br>\n<img src=\"/content/images/2014/Jun/Architecture.jpg\" alt=\"\"></p>\n<p>Let me walk you through it. First we have the user, which have two options. First the user can ask for the data. If you follow the arrows down on the right side you see that there is no possible way for the user to go all the way down to eventstore, it can only query specialized views. The second thing the user can do is place an action through the UI (a web api in this application). The action will be translated to a command (if it is not already one when comming through the api).</p>\n<p>From the way from the UI to the dispatcher the commands go through a pipeline where you can inject functionality as logging and security checks. Since we have a single point of entry to the domain it is really easy to add functionality that concerns every command as a step in this pipeline.</p>\n<p>The dispatcher figure out what should handle the command and execute it. After execution the resulting events are stored in the eventstore.</p>\n<p>When the eventstore is updated we have a service (this will be an actual windows service) that listen to changes to the eventstore and if it sees an event which it is interested in it will update the views which are stored in elasticsearch and maybe neo4j.</p>\n<p>Now I hope you have a better understanding of what the goal is so we can continue with the implementation in the next post, <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"31","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nWhat is the goal?\nThis post should probably have been the first one, but better late than never\n:). So the purpose of this post is to show the target architecture, more than\njust the screenshot of the projects in the first post\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n.\n\nApplication description\nThe application that is being developed is a simple web shop. It isn't a\ncomplete web shop with all the possible feature, just enough to show how to put\nthe application together and how to store the views in elasticsearch (and maybe\nneo4j if I have time).\n\nThe features I plan to include are:\n\n * Create customer\n * Make customer preferred --> get's 25% discount\n * Create product\n * Create shopping basket\n * Add item to basket\n * Go to checkout\n * Cancel order\n * Goto payment\n * Make payment\n * Approve order, orders over 100000 needs approval\n * Start shipping of order, after this point the customer can't cancel\n * Ship order, marks the order as shipped\n\nThis features will be implemented and tested in an effective way where I also\ntest the absence of side effects.\n\nTarget architecture\nThe target architecture is illustrated in the my nice looking picture below:\n\n\nLet me walk you through it. First we have the user, which have two options.\nFirst the user can ask for the data. If you follow the arrows down on the right\nside you see that there is no possible way for the user to go all the way down\nto eventstore, it can only query specialized views. The second thing the user\ncan do is place an action through the UI (a web api in this application). The\naction will be translated to a command (if it is not already one when comming\nthrough the api).\n\nFrom the way from the UI to the dispatcher the commands go through a pipeline\nwhere you can inject functionality as logging and security checks. Since we have\na single point of entry to the domain it is really easy to add functionality\nthat concerns every command as a step in this pipeline.\n\nThe dispatcher figure out what should handle the command and execute it. After\nexecution the resulting events are stored in the eventstore.\n\nWhen the eventstore is updated we have a service (this will be an actual windows\nservice) that listen to changes to the eventstore and if it sees an event which\nit is interested in it will update the views which are stored in elasticsearch\nand maybe neo4j.\n\nNow I hope you have a better understanding of what the goal is so we can\ncontinue with the implementation in the next post, The first feature\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-25T18:37:14.000Z","updated_at":"2014-07-16T06:12:30.000Z","published_at":"2014-06-25T18:37:21.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe053","uuid":"ef6bb5f9-ca2e-492e-8442-8920799dfb0c","title":"CQRS the simple way with eventstore and elasticsearch: Implementing the first features","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * The first feature\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Look mom... I'm doing TDD, almost.\\nI'm going to implement the features using TDD on a high level. The first feature I'll implement is to create a customer. I'm not going to do strict TDD since I know sort of where I'm going with this. So let us get started.\\n\\n##The base test class\\nAll my test against the domain follow a simple pattern:\\n\\n* Given a set of events (events as precondition)\\n* When I do something (a command)\\n* Then I should get these events as the result or I should get this exception\\n\\nAs long as I don't have to test some complex algorithm all my test can follow that pattern, so I'll write a simple base test class to handle this pattern. I'll not use fluent assertions or anything fancy like that since I don't think I need it to make my code readable. The code for the base test class looks like this: \\n\\n    public class TestBase\\n    {\\n        private InMemoryDomainRespository _domainRepository;\\n        private DomainEntry _domainEntry;\\n        private Dictionary<Guid, IEnumerable<IEvent>> _preConditions = new Dictionary<Guid, IEnumerable<IEvent>>();\\n\\n        private DomainEntry BuildApplication()\\n        {\\n            _domainRepository = new InMemoryDomainRespository();\\n            _domainRepository.AddEvents(_preConditions);\\n            return new DomainEntry(_domainRepository);\\n        }\\n\\n        [TestFixtureTearDown]\\n        public void TearDown()\\n        {\\n            IdGenerator.GuidGenerator = null;\\n            _preConditions = new Dictionary<Guid, IEnumerable<IEvent>>();\\n        }\\n\\n        protected void When(ICommand command)\\n        {\\n            var application = BuildApplication();\\n            application.ExecuteCommand(command);\\n        }\\n\\n        protected void Then(params IEvent[] expectedEvents)\\n        {\\n            var latestEvents = _domainRepository.GetLatestEvents().ToList();\\n            var expectedEventsList = expectedEvents.ToList();\\n            Assert.AreEqual(expectedEventsList.Count, latestEvents.Count);\\n\\n            for (int i = 0; i < latestEvents.Count; i++)\\n            {\\n                Assert.AreEqual(expectedEvents[i], latestEvents[i]);\\n            }\\n        }\\n\\n        protected void WhenThrows<TException>(ICommand command) where TException : Exception\\n        {\\n            Assert.Throws<TException>(() => When(command));\\n        }\\n\\n        protected void Given(params IEvent[] existingEvents)\\n        {\\n            _preConditions = existingEvents\\n                .GroupBy(y => y.Id)\\n                .ToDictionary(y => y.Key, y => y.AsEnumerable());\\n        }\\n    }\\n\\nOne thing that the base class uses which I haven't implemented yet is the `DomainEntry`. The `DomainEntry` is exactly what it says it is, it is the entry point for the domain and its responsibility is to put together all the dependencies used by the domain. The code is not that to follow and the initial code for the `DomainEntry` looks like this: \\n\\n    public class DomainEntry\\n    {\\n        private readonly CommandDispatcher _commandDispatcher;\\n\\n        public DomainEntry(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe = null, IEnumerable<Action<object>> postExecutionPipe = null)\\n        {\\n            preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\\n            postExecutionPipe = CreatePostExecutionPipe(postExecutionPipe);\\n            _commandDispatcher = CreateCommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\\n        }\\n\\n        public void ExecuteCommand(ICommand command)\\n        {\\n            _commandDispatcher.ExecuteCommand(command);\\n        }\\n\\n        private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\\n        {\\n            var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\\n            return commandDispatcher;\\n        }\\n\\n        private IEnumerable<Action<object>> CreatePostExecutionPipe(IEnumerable<Action<object>> postExecutionPipe)\\n        {\\n            if (postExecutionPipe != null)\\n            {\\n                foreach (var action in postExecutionPipe)\\n                {\\n                    yield return action;\\n                }\\n            }\\n        }\\n    }\\n\\nAs I'll show later we are going to modify this class when we add the mapping for command to handler.\\n\\nNow we have all the pieces so we can start writing our first test.\\n\\n##Writing our first test\\nI'll go straight to the code: \\n\\n    [TestFixture]\\n    public class CreateCustomerTest : TestBase\\n    {\\n        [Test]\\n        public void WhenCreatingTheCustomer_TheCustomerShouldBeCreatedWithTheRightName()\\n        {\\n            Guid id = Guid.NewGuid();\\n            When(new CreateCustomer(id, \\\"Tomas\\\"));\\n            Then(new CustomerCreated(id, \\\"Tomas\\\"));\\n        }\\n    }\\n\\nThis feature is quite simple, if you had a more complex system you might want to put the customer handling process in a separate application and only handle the ordering in this application. I you've added just this piece of code it won't compile, we need to add the command and the events. \\n\\n##Time for some f# magic!\\nAll the commands and events will be defined as record set in f#. The reason for this is that they are immutable value type structures, which means that I can compare two difference instances and they will check if they are equal by comparing the values rather than if the reference the same object. Also, it is a really compact and readable way to define the commands. So in the contracts project we add two files; \\\"Commands.fs\\\" and \\\"Events.fs\\\". The first version of \\\"Commands.fs\\\" looks like this: \\n\\n    namespace CQRSShop.Contracts.Commands\\n    open CQRSShop.Infrastructure\\n    open System\\n\\n    type CreateCustomer = {Id: Guid; Name: string } with interface ICommand\\n\\nThe code above defines a f# set, which will be a class when used from C# that has equal and hashcode methods already implemented. A really powerful construct.\\n\\nThe first version of our \\\"Events.fs\\\" almost looks the same:\\n\\n    namespace CQRSShop.Contracts.Commands\\n    open CQRSShop.Infrastructure\\n    open System\\n\\n    type CustomerCreated = {Id: Guid; Name: string } \\n        with interface IEvent with member this.Id with get() = this.Id\\n\\nAs you can see I have to implement the `IEvent` interface which specifies that all the events must have an id. So implementing the events are a little more verbose than the commands, but still much less verbose than it would be if we implemented them in C#.\\n\\nNow we actually can run the test, but it fails and we get the exception: `System.ApplicationException : Missing handler for CreateCustomer.` So let's fix that.\\n\\nThe first step we need to do is to update the `DomainEntry` so it know how the command should be routed. So the `CreateCommandDispatcher` method should be updated to something like this: \\n\\n        private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\\n        {\\n            var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\\n\\n            var customerCommandHandler = new CustomerCommandHandler();\\n            commandDispatcher.RegisterHandler<CreateCustomer>(customerCommandHandler);\\n\\n            return commandDispatcher;\\n        }\\n\\nThis won't fix the test, now we must implement the command handler. I look to put all the command handlers in a folder in the domain, an alternative way of grouping them is that each command handler is located in a folder together with the aggregate it is handling. The code for the `CustomerCommandHandler` that handle the `CreateCustomer` command looks like this: \\n\\n    internal class CustomerCommandHandler : IHandle<CreateCustomer>\\n    {\\n        public CustomerCommandHandler()\\n        {\\n        }\\n\\n        public IAggregate Handle(CreateCustomer command)\\n        {\\n            return Customer.Create(command.Id, command.Name);\\n        }\\n    }\\n\\nIt is still quite straightforward, but there is one last step and that is to create the `Customer` aggregate. \\n\\n    internal class Customer : AggregateBase\\n    {\\n        public Customer()\\n        {\\n            RegisterTransition<CustomerCreated>(Apply);\\n        }\\n\\n        private Customer(Guid id, string name)\\n        {\\n            RaiseEvent(new CustomerCreated(id, name));\\n        }\\n\\n        private void Apply(CustomerCreated obj)\\n        {\\n            Id = obj.Id;\\n        }\\n\\n        public static IAggregate Create(Guid id, string name)\\n        {\\n            return new Customer(id, name);\\n        }\\n    }\\n\\nWhat is going on here you might thing? It is not as complicated as it might first look. The public `Create` method is where we are actually doing things with the customer, it is in this method logic related to customer creation should be placed. The we have a public constructor that registers all transitions for the object that should be applied when an event is raised. The reason for this being public is because we need to create an \\\"empty\\\" aggregate and then build it up by applying events later on. The private constructor just raises the event that the customer has been the created and the `Apply` method is doing the state transition. This way we have a nice separation of concern between checking if we have a valid state and doing the transition.\\n\\n##Changes made to existing code\\nDuring the implementation I added a simple `IHandle<TCommand>` interface to simplify things. \\n\\n    public interface IHandle<in TCommand> where TCommand : ICommand\\n    {\\n        IAggregate Handle(TCommand command);\\n    }\\n    \\nThis is an interface that all the command handlers should implement and simplifies the registration of the command handlers in the `DomainEntry`. This change also resulted in a change in the `CommandDispatcher` in how routes are registered:\\n\\n    public class CommandDispatcher\\n    {\\n        private Dictionary<Type, Func<object, IAggregate>> _routes;\\n        private IDomainRepository _domainRepository;\\n        private readonly IEnumerable<Action<object>> _postExecutionPipe;\\n        private readonly IEnumerable<Action<ICommand>> _preExecutionPipe;\\n\\n        public CommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\\n        {\\n            _domainRepository = domainRepository;\\n            _postExecutionPipe = postExecutionPipe;\\n            _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\\n            _routes =  new Dictionary<Type, Func<object, IAggregate>>();\\n        }\\n\\n        public void RegisterHandler<TCommand>(IHandle<TCommand> handler) where TCommand : class, ICommand\\n        {\\n            _routes.Add(typeof (TCommand), command => handler.Handle(command as TCommand));\\n        }\\n\\n        public void ExecuteCommand<TCommand>(TCommand command) where TCommand : ICommand\\n        {\\n            var commandType = command.GetType();\\n\\n            RunPreExecutionPipe(command);\\n            if (!_routes.ContainsKey(commandType))\\n            {\\n                throw new ApplicationException(\\\"Missing handler for \\\" + commandType.Name);\\n            }\\n            var aggregate = _routes[commandType](command);\\n            var savedEvents = _domainRepository.Save(aggregate);\\n            RunPostExecutionPipe(savedEvents);\\n        }\\n\\n        private void RunPostExecutionPipe(IEnumerable<object> savedEvents)\\n        {\\n            foreach (var savedEvent in savedEvents)\\n            {\\n                foreach (var action in _postExecutionPipe)\\n                {\\n                    action(savedEvent);\\n                }\\n            }\\n        }\\n\\n        private void RunPreExecutionPipe(ICommand command)\\n        {\\n            foreach (var action in _preExecutionPipe)\\n            {\\n                action(command);\\n            }\\n        }\\n    }\\n\\nAnd this finishes the first test. In the next post my goal is to implement the rest of the domain. One thing that some people can argue against is that it seems to be a little bit verbose, and for this case only I agree but the domain will grow. Also, you haven't seen me writing any ugly Entity framework code or any sql mapping code, all just work and is strongly typed.\\n\\nThe next part is [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li>The first feature</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"lookmomimdoingtddalmost\">Look mom... I'm doing TDD, almost.</h1>\n<p>I'm going to implement the features using TDD on a high level. The first feature I'll implement is to create a customer. I'm not going to do strict TDD since I know sort of where I'm going with this. So let us get started.</p>\n<h2 id=\"thebasetestclass\">The base test class</h2>\n<p>All my test against the domain follow a simple pattern:</p>\n<ul>\n<li>Given a set of events (events as precondition)</li>\n<li>When I do something (a command)</li>\n<li>Then I should get these events as the result or I should get this exception</li>\n</ul>\n<p>As long as I don't have to test some complex algorithm all my test can follow that pattern, so I'll write a simple base test class to handle this pattern. I'll not use fluent assertions or anything fancy like that since I don't think I need it to make my code readable. The code for the base test class looks like this:</p>\n<pre><code>public class TestBase\n{\n    private InMemoryDomainRespository _domainRepository;\n    private DomainEntry _domainEntry;\n    private Dictionary&lt;Guid, IEnumerable&lt;IEvent&gt;&gt; _preConditions = new Dictionary&lt;Guid, IEnumerable&lt;IEvent&gt;&gt;();\n\n    private DomainEntry BuildApplication()\n    {\n        _domainRepository = new InMemoryDomainRespository();\n        _domainRepository.AddEvents(_preConditions);\n        return new DomainEntry(_domainRepository);\n    }\n\n    [TestFixtureTearDown]\n    public void TearDown()\n    {\n        IdGenerator.GuidGenerator = null;\n        _preConditions = new Dictionary&lt;Guid, IEnumerable&lt;IEvent&gt;&gt;();\n    }\n\n    protected void When(ICommand command)\n    {\n        var application = BuildApplication();\n        application.ExecuteCommand(command);\n    }\n\n    protected void Then(params IEvent[] expectedEvents)\n    {\n        var latestEvents = _domainRepository.GetLatestEvents().ToList();\n        var expectedEventsList = expectedEvents.ToList();\n        Assert.AreEqual(expectedEventsList.Count, latestEvents.Count);\n\n        for (int i = 0; i &lt; latestEvents.Count; i++)\n        {\n            Assert.AreEqual(expectedEvents[i], latestEvents[i]);\n        }\n    }\n\n    protected void WhenThrows&lt;TException&gt;(ICommand command) where TException : Exception\n    {\n        Assert.Throws&lt;TException&gt;(() =&gt; When(command));\n    }\n\n    protected void Given(params IEvent[] existingEvents)\n    {\n        _preConditions = existingEvents\n            .GroupBy(y =&gt; y.Id)\n            .ToDictionary(y =&gt; y.Key, y =&gt; y.AsEnumerable());\n    }\n}\n</code></pre>\n<p>One thing that the base class uses which I haven't implemented yet is the <code>DomainEntry</code>. The <code>DomainEntry</code> is exactly what it says it is, it is the entry point for the domain and its responsibility is to put together all the dependencies used by the domain. The code is not that to follow and the initial code for the <code>DomainEntry</code> looks like this:</p>\n<pre><code>public class DomainEntry\n{\n    private readonly CommandDispatcher _commandDispatcher;\n\n    public DomainEntry(IDomainRepository domainRepository, IEnumerable&lt;Action&lt;ICommand&gt;&gt; preExecutionPipe = null, IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe = null)\n    {\n        preExecutionPipe = preExecutionPipe ?? Enumerable.Empty&lt;Action&lt;ICommand&gt;&gt;();\n        postExecutionPipe = CreatePostExecutionPipe(postExecutionPipe);\n        _commandDispatcher = CreateCommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n    }\n\n    public void ExecuteCommand(ICommand command)\n    {\n        _commandDispatcher.ExecuteCommand(command);\n    }\n\n    private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable&lt;Action&lt;ICommand&gt;&gt; preExecutionPipe, IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe)\n    {\n        var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n        return commandDispatcher;\n    }\n\n    private IEnumerable&lt;Action&lt;object&gt;&gt; CreatePostExecutionPipe(IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe)\n    {\n        if (postExecutionPipe != null)\n        {\n            foreach (var action in postExecutionPipe)\n            {\n                yield return action;\n            }\n        }\n    }\n}\n</code></pre>\n<p>As I'll show later we are going to modify this class when we add the mapping for command to handler.</p>\n<p>Now we have all the pieces so we can start writing our first test.</p>\n<h2 id=\"writingourfirsttest\">Writing our first test</h2>\n<p>I'll go straight to the code:</p>\n<pre><code>[TestFixture]\npublic class CreateCustomerTest : TestBase\n{\n    [Test]\n    public void WhenCreatingTheCustomer_TheCustomerShouldBeCreatedWithTheRightName()\n    {\n        Guid id = Guid.NewGuid();\n        When(new CreateCustomer(id, &quot;Tomas&quot;));\n        Then(new CustomerCreated(id, &quot;Tomas&quot;));\n    }\n}\n</code></pre>\n<p>This feature is quite simple, if you had a more complex system you might want to put the customer handling process in a separate application and only handle the ordering in this application. I you've added just this piece of code it won't compile, we need to add the command and the events.</p>\n<h2 id=\"timeforsomefmagic\">Time for some f# magic!</h2>\n<p>All the commands and events will be defined as record set in f#. The reason for this is that they are immutable value type structures, which means that I can compare two difference instances and they will check if they are equal by comparing the values rather than if the reference the same object. Also, it is a really compact and readable way to define the commands. So in the contracts project we add two files; &quot;Commands.fs&quot; and &quot;Events.fs&quot;. The first version of &quot;Commands.fs&quot; looks like this:</p>\n<pre><code>namespace CQRSShop.Contracts.Commands\nopen CQRSShop.Infrastructure\nopen System\n\ntype CreateCustomer = {Id: Guid; Name: string } with interface ICommand\n</code></pre>\n<p>The code above defines a f# set, which will be a class when used from C# that has equal and hashcode methods already implemented. A really powerful construct.</p>\n<p>The first version of our &quot;Events.fs&quot; almost looks the same:</p>\n<pre><code>namespace CQRSShop.Contracts.Commands\nopen CQRSShop.Infrastructure\nopen System\n\ntype CustomerCreated = {Id: Guid; Name: string } \n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<p>As you can see I have to implement the <code>IEvent</code> interface which specifies that all the events must have an id. So implementing the events are a little more verbose than the commands, but still much less verbose than it would be if we implemented them in C#.</p>\n<p>Now we actually can run the test, but it fails and we get the exception: <code>System.ApplicationException : Missing handler for CreateCustomer.</code> So let's fix that.</p>\n<p>The first step we need to do is to update the <code>DomainEntry</code> so it know how the command should be routed. So the <code>CreateCommandDispatcher</code> method should be updated to something like this:</p>\n<pre><code>    private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable&lt;Action&lt;ICommand&gt;&gt; preExecutionPipe, IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe)\n    {\n        var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n\n        var customerCommandHandler = new CustomerCommandHandler();\n        commandDispatcher.RegisterHandler&lt;CreateCustomer&gt;(customerCommandHandler);\n\n        return commandDispatcher;\n    }\n</code></pre>\n<p>This won't fix the test, now we must implement the command handler. I look to put all the command handlers in a folder in the domain, an alternative way of grouping them is that each command handler is located in a folder together with the aggregate it is handling. The code for the <code>CustomerCommandHandler</code> that handle the <code>CreateCustomer</code> command looks like this:</p>\n<pre><code>internal class CustomerCommandHandler : IHandle&lt;CreateCustomer&gt;\n{\n    public CustomerCommandHandler()\n    {\n    }\n\n    public IAggregate Handle(CreateCustomer command)\n    {\n        return Customer.Create(command.Id, command.Name);\n    }\n}\n</code></pre>\n<p>It is still quite straightforward, but there is one last step and that is to create the <code>Customer</code> aggregate.</p>\n<pre><code>internal class Customer : AggregateBase\n{\n    public Customer()\n    {\n        RegisterTransition&lt;CustomerCreated&gt;(Apply);\n    }\n\n    private Customer(Guid id, string name)\n    {\n        RaiseEvent(new CustomerCreated(id, name));\n    }\n\n    private void Apply(CustomerCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    public static IAggregate Create(Guid id, string name)\n    {\n        return new Customer(id, name);\n    }\n}\n</code></pre>\n<p>What is going on here you might thing? It is not as complicated as it might first look. The public <code>Create</code> method is where we are actually doing things with the customer, it is in this method logic related to customer creation should be placed. The we have a public constructor that registers all transitions for the object that should be applied when an event is raised. The reason for this being public is because we need to create an &quot;empty&quot; aggregate and then build it up by applying events later on. The private constructor just raises the event that the customer has been the created and the <code>Apply</code> method is doing the state transition. This way we have a nice separation of concern between checking if we have a valid state and doing the transition.</p>\n<h2 id=\"changesmadetoexistingcode\">Changes made to existing code</h2>\n<p>During the implementation I added a simple <code>IHandle&lt;TCommand&gt;</code> interface to simplify things.</p>\n<pre><code>public interface IHandle&lt;in TCommand&gt; where TCommand : ICommand\n{\n    IAggregate Handle(TCommand command);\n}\n</code></pre>\n<p>This is an interface that all the command handlers should implement and simplifies the registration of the command handlers in the <code>DomainEntry</code>. This change also resulted in a change in the <code>CommandDispatcher</code> in how routes are registered:</p>\n<pre><code>public class CommandDispatcher\n{\n    private Dictionary&lt;Type, Func&lt;object, IAggregate&gt;&gt; _routes;\n    private IDomainRepository _domainRepository;\n    private readonly IEnumerable&lt;Action&lt;object&gt;&gt; _postExecutionPipe;\n    private readonly IEnumerable&lt;Action&lt;ICommand&gt;&gt; _preExecutionPipe;\n\n    public CommandDispatcher(IDomainRepository domainRepository, IEnumerable&lt;Action&lt;ICommand&gt;&gt; preExecutionPipe, IEnumerable&lt;Action&lt;object&gt;&gt; postExecutionPipe)\n    {\n        _domainRepository = domainRepository;\n        _postExecutionPipe = postExecutionPipe;\n        _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty&lt;Action&lt;ICommand&gt;&gt;();\n        _routes =  new Dictionary&lt;Type, Func&lt;object, IAggregate&gt;&gt;();\n    }\n\n    public void RegisterHandler&lt;TCommand&gt;(IHandle&lt;TCommand&gt; handler) where TCommand : class, ICommand\n    {\n        _routes.Add(typeof (TCommand), command =&gt; handler.Handle(command as TCommand));\n    }\n\n    public void ExecuteCommand&lt;TCommand&gt;(TCommand command) where TCommand : ICommand\n    {\n        var commandType = command.GetType();\n\n        RunPreExecutionPipe(command);\n        if (!_routes.ContainsKey(commandType))\n        {\n            throw new ApplicationException(&quot;Missing handler for &quot; + commandType.Name);\n        }\n        var aggregate = _routes[commandType](command);\n        var savedEvents = _domainRepository.Save(aggregate);\n        RunPostExecutionPipe(savedEvents);\n    }\n\n    private void RunPostExecutionPipe(IEnumerable&lt;object&gt; savedEvents)\n    {\n        foreach (var savedEvent in savedEvents)\n        {\n            foreach (var action in _postExecutionPipe)\n            {\n                action(savedEvent);\n            }\n        }\n    }\n\n    private void RunPreExecutionPipe(ICommand command)\n    {\n        foreach (var action in _preExecutionPipe)\n        {\n            action(command);\n        }\n    }\n}\n</code></pre>\n<p>And this finishes the first test. In the next post my goal is to implement the rest of the domain. One thing that some people can argue against is that it seems to be a little bit verbose, and for this case only I agree but the domain will grow. Also, you haven't seen me writing any ugly Entity framework code or any sql mapping code, all just work and is strongly typed.</p>\n<p>The next part is <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"32","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nLook mom... I'm doing TDD, almost.\nI'm going to implement the features using TDD on a high level. The first feature\nI'll implement is to create a customer. I'm not going to do strict TDD since I\nknow sort of where I'm going with this. So let us get started.\n\nThe base test class\nAll my test against the domain follow a simple pattern:\n\n * Given a set of events (events as precondition)\n * When I do something (a command)\n * Then I should get these events as the result or I should get this exception\n\nAs long as I don't have to test some complex algorithm all my test can follow\nthat pattern, so I'll write a simple base test class to handle this pattern.\nI'll not use fluent assertions or anything fancy like that since I don't think I\nneed it to make my code readable. The code for the base test class looks like\nthis:\n\npublic class TestBase\n{\n    private InMemoryDomainRespository _domainRepository;\n    private DomainEntry _domainEntry;\n    private Dictionary<Guid, IEnumerable<IEvent>> _preConditions = new Dictionary<Guid, IEnumerable<IEvent>>();\n\n    private DomainEntry BuildApplication()\n    {\n        _domainRepository = new InMemoryDomainRespository();\n        _domainRepository.AddEvents(_preConditions);\n        return new DomainEntry(_domainRepository);\n    }\n\n    [TestFixtureTearDown]\n    public void TearDown()\n    {\n        IdGenerator.GuidGenerator = null;\n        _preConditions = new Dictionary<Guid, IEnumerable<IEvent>>();\n    }\n\n    protected void When(ICommand command)\n    {\n        var application = BuildApplication();\n        application.ExecuteCommand(command);\n    }\n\n    protected void Then(params IEvent[] expectedEvents)\n    {\n        var latestEvents = _domainRepository.GetLatestEvents().ToList();\n        var expectedEventsList = expectedEvents.ToList();\n        Assert.AreEqual(expectedEventsList.Count, latestEvents.Count);\n\n        for (int i = 0; i < latestEvents.Count; i++)\n        {\n            Assert.AreEqual(expectedEvents[i], latestEvents[i]);\n        }\n    }\n\n    protected void WhenThrows<TException>(ICommand command) where TException : Exception\n    {\n        Assert.Throws<TException>(() => When(command));\n    }\n\n    protected void Given(params IEvent[] existingEvents)\n    {\n        _preConditions = existingEvents\n            .GroupBy(y => y.Id)\n            .ToDictionary(y => y.Key, y => y.AsEnumerable());\n    }\n}\n\n\nOne thing that the base class uses which I haven't implemented yet is the \nDomainEntry. The DomainEntry is exactly what it says it is, it is the entry\npoint for the domain and its responsibility is to put together all the\ndependencies used by the domain. The code is not that to follow and the initial\ncode for the DomainEntry looks like this:\n\npublic class DomainEntry\n{\n    private readonly CommandDispatcher _commandDispatcher;\n\n    public DomainEntry(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe = null, IEnumerable<Action<object>> postExecutionPipe = null)\n    {\n        preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\n        postExecutionPipe = CreatePostExecutionPipe(postExecutionPipe);\n        _commandDispatcher = CreateCommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n    }\n\n    public void ExecuteCommand(ICommand command)\n    {\n        _commandDispatcher.ExecuteCommand(command);\n    }\n\n    private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\n    {\n        var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n        return commandDispatcher;\n    }\n\n    private IEnumerable<Action<object>> CreatePostExecutionPipe(IEnumerable<Action<object>> postExecutionPipe)\n    {\n        if (postExecutionPipe != null)\n        {\n            foreach (var action in postExecutionPipe)\n            {\n                yield return action;\n            }\n        }\n    }\n}\n\n\nAs I'll show later we are going to modify this class when we add the mapping for\ncommand to handler.\n\nNow we have all the pieces so we can start writing our first test.\n\nWriting our first test\nI'll go straight to the code:\n\n[TestFixture]\npublic class CreateCustomerTest : TestBase\n{\n    [Test]\n    public void WhenCreatingTheCustomer_TheCustomerShouldBeCreatedWithTheRightName()\n    {\n        Guid id = Guid.NewGuid();\n        When(new CreateCustomer(id, \"Tomas\"));\n        Then(new CustomerCreated(id, \"Tomas\"));\n    }\n}\n\n\nThis feature is quite simple, if you had a more complex system you might want to\nput the customer handling process in a separate application and only handle the\nordering in this application. I you've added just this piece of code it won't\ncompile, we need to add the command and the events.\n\nTime for some f# magic!\nAll the commands and events will be defined as record set in f#. The reason for\nthis is that they are immutable value type structures, which means that I can\ncompare two difference instances and they will check if they are equal by\ncomparing the values rather than if the reference the same object. Also, it is a\nreally compact and readable way to define the commands. So in the contracts\nproject we add two files; \"Commands.fs\" and \"Events.fs\". The first version of\n\"Commands.fs\" looks like this:\n\nnamespace CQRSShop.Contracts.Commands\nopen CQRSShop.Infrastructure\nopen System\n\ntype CreateCustomer = {Id: Guid; Name: string } with interface ICommand\n\n\nThe code above defines a f# set, which will be a class when used from C# that\nhas equal and hashcode methods already implemented. A really powerful construct.\n\nThe first version of our \"Events.fs\" almost looks the same:\n\nnamespace CQRSShop.Contracts.Commands\nopen CQRSShop.Infrastructure\nopen System\n\ntype CustomerCreated = {Id: Guid; Name: string } \n    with interface IEvent with member this.Id with get() = this.Id\n\n\nAs you can see I have to implement the IEvent interface which specifies that all\nthe events must have an id. So implementing the events are a little more verbose\nthan the commands, but still much less verbose than it would be if we\nimplemented them in C#.\n\nNow we actually can run the test, but it fails and we get the exception: \nSystem.ApplicationException : Missing handler for CreateCustomer. So let's fix\nthat.\n\nThe first step we need to do is to update the DomainEntry so it know how the\ncommand should be routed. So the CreateCommandDispatcher method should be\nupdated to something like this:\n\n    private CommandDispatcher CreateCommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\n    {\n        var commandDispatcher = new CommandDispatcher(domainRepository, preExecutionPipe, postExecutionPipe);\n\n        var customerCommandHandler = new CustomerCommandHandler();\n        commandDispatcher.RegisterHandler<CreateCustomer>(customerCommandHandler);\n\n        return commandDispatcher;\n    }\n\n\nThis won't fix the test, now we must implement the command handler. I look to\nput all the command handlers in a folder in the domain, an alternative way of\ngrouping them is that each command handler is located in a folder together with\nthe aggregate it is handling. The code for the CustomerCommandHandler that\nhandle the CreateCustomer command looks like this:\n\ninternal class CustomerCommandHandler : IHandle<CreateCustomer>\n{\n    public CustomerCommandHandler()\n    {\n    }\n\n    public IAggregate Handle(CreateCustomer command)\n    {\n        return Customer.Create(command.Id, command.Name);\n    }\n}\n\n\nIt is still quite straightforward, but there is one last step and that is to\ncreate the Customer aggregate.\n\ninternal class Customer : AggregateBase\n{\n    public Customer()\n    {\n        RegisterTransition<CustomerCreated>(Apply);\n    }\n\n    private Customer(Guid id, string name)\n    {\n        RaiseEvent(new CustomerCreated(id, name));\n    }\n\n    private void Apply(CustomerCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    public static IAggregate Create(Guid id, string name)\n    {\n        return new Customer(id, name);\n    }\n}\n\n\nWhat is going on here you might thing? It is not as complicated as it might\nfirst look. The public Create method is where we are actually doing things with\nthe customer, it is in this method logic related to customer creation should be\nplaced. The we have a public constructor that registers all transitions for the\nobject that should be applied when an event is raised. The reason for this being\npublic is because we need to create an \"empty\" aggregate and then build it up by\napplying events later on. The private constructor just raises the event that the\ncustomer has been the created and the Apply method is doing the state\ntransition. This way we have a nice separation of concern between checking if we\nhave a valid state and doing the transition.\n\nChanges made to existing code\nDuring the implementation I added a simple IHandle<TCommand> interface to\nsimplify things.\n\npublic interface IHandle<in TCommand> where TCommand : ICommand\n{\n    IAggregate Handle(TCommand command);\n}\n\n\nThis is an interface that all the command handlers should implement and\nsimplifies the registration of the command handlers in the DomainEntry. This\nchange also resulted in a change in the CommandDispatcher in how routes are\nregistered:\n\npublic class CommandDispatcher\n{\n    private Dictionary<Type, Func<object, IAggregate>> _routes;\n    private IDomainRepository _domainRepository;\n    private readonly IEnumerable<Action<object>> _postExecutionPipe;\n    private readonly IEnumerable<Action<ICommand>> _preExecutionPipe;\n\n    public CommandDispatcher(IDomainRepository domainRepository, IEnumerable<Action<ICommand>> preExecutionPipe, IEnumerable<Action<object>> postExecutionPipe)\n    {\n        _domainRepository = domainRepository;\n        _postExecutionPipe = postExecutionPipe;\n        _preExecutionPipe = preExecutionPipe ?? Enumerable.Empty<Action<ICommand>>();\n        _routes =  new Dictionary<Type, Func<object, IAggregate>>();\n    }\n\n    public void RegisterHandler<TCommand>(IHandle<TCommand> handler) where TCommand : class, ICommand\n    {\n        _routes.Add(typeof (TCommand), command => handler.Handle(command as TCommand));\n    }\n\n    public void ExecuteCommand<TCommand>(TCommand command) where TCommand : ICommand\n    {\n        var commandType = command.GetType();\n\n        RunPreExecutionPipe(command);\n        if (!_routes.ContainsKey(commandType))\n        {\n            throw new ApplicationException(\"Missing handler for \" + commandType.Name);\n        }\n        var aggregate = _routes[commandType](command);\n        var savedEvents = _domainRepository.Save(aggregate);\n        RunPostExecutionPipe(savedEvents);\n    }\n\n    private void RunPostExecutionPipe(IEnumerable<object> savedEvents)\n    {\n        foreach (var savedEvent in savedEvents)\n        {\n            foreach (var action in _postExecutionPipe)\n            {\n                action(savedEvent);\n            }\n        }\n    }\n\n    private void RunPreExecutionPipe(ICommand command)\n    {\n        foreach (var action in _preExecutionPipe)\n        {\n            action(command);\n        }\n    }\n}\n\n\nAnd this finishes the first test. In the next post my goal is to implement the\nrest of the domain. One thing that some people can argue against is that it\nseems to be a little bit verbose, and for this case only I agree but the domain\nwill grow. Also, you haven't seen me writing any ugly Entity framework code or\nany sql mapping code, all just work and is strongly typed.\n\nThe next part is The rest of the features\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-25T19:26:46.000Z","updated_at":"2014-07-16T06:12:58.000Z","published_at":"2014-06-25T20:06:01.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe054","uuid":"d1fc6027-2a82-465c-b382-4e40cc8e003f","title":"CQRS the simple way with eventstore and elasticsearch: Implementing the rest of the features","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * The rest of the features\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Some starting notes\\nFirst off, all the code is available to github. All I'm providing here is feature by feature implementation and some comments.\\n\\nBefore we start implement the features we need to make a slight change to the `TestBase` class to get better exception handling when expecting exceptions. The `WhenThrows` method should be updated to this: \\n\\n    protected void WhenThrows<TException>(ICommand command) where TException : Exception\\n    {\\n        try\\n        {\\n            When(command);\\n            Assert.Fail(\\\"Expected exception \\\" + typeof(TException));\\n        }\\n        catch (TException)\\n        {\\n        }\\n    }\\nThis will try to execute the command and only catch the exception we are expecting, do we get another exception we get the correct stacktrace for that exception instead of a modified stacktrace.\\n\\nI'll try to group all the features to respective aggregate, and the be as concise as possible focusing on the code. The format will be \\\"Feature group\\\" --> \\\"Feature\\\" --> \\\"Test\\\" and implementation \\\"Code\\\". When showing the code I won't show the changes to the `DomainEntry` since they are just one or two, if any, lines for each feature.\\n\\n#Customer features\\nWe're not 100 % done with the customer yet. As of the moment it is possible to create duplicate user, and that is something we need to prevent. And we also need to add the functionality to make the customer preferred so they can get a discount.\\n\\n##Preventing duplicate users\\nWhen we are trying to create a user with an id that is not unique we should get an exception telling us what the problem is.\\n###Test\\n    [Test]\\n    public void GivenAUserWithIdXExists_WhenCreatingACustomerWithIdX_IShouldGetNotifiedThatTheUserAlreadyExists()\\n    {\\n        Guid id = Guid.NewGuid();\\n        Given(new CustomerCreated(id, \\\"Something I don't care about\\\"));\\n        WhenThrows<CustomerAlreadyExistsException>(new CreateCustomer(id, \\\"Tomas\\\"));\\n    }\\n\\n###Code\\nFirst we are missing an exception so the code won't even compile. So we add that class to the \\\"Exceptions\\\" folder in the domain project.\\n\\n    public class CustomerAlreadyExistsException : Exception\\n    {\\n        public CustomerAlreadyExistsException(Guid id, string name) : base(CreateMessage(id, name))\\n        {\\n            \\n        }\\n\\n        private static string CreateMessage(Guid id, string name)\\n        {\\n            return string.Format(\\\"A customer with id {0} already exists, can't create customer for {1}\\\", id, name);\\n        }\\n    }\\n\\nThe next thing is to actually implement the feature. To get the test green we need to update our `CustomerCommandHandler` to make the duplicate check.\\n\\n    internal class CustomerCommandHandler : IHandle<CreateCustomer>\\n    {\\n        private readonly IDomainRepository _domainRepository;\\n\\n        public CustomerCommandHandler(IDomainRepository domainRepository)\\n        {\\n            _domainRepository = domainRepository;\\n        }\\n\\n        public IAggregate Handle(CreateCustomer command)\\n        {\\n            try\\n            {\\n                var customer = _domainRepository.GetById<Customer>(command.Id);\\n                throw new CustomerAlreadyExistsException(command.Id, command.Name);\\n            }\\n            catch (AggregateNotFoundException)\\n            {\\n                // We expect not to find anything\\n            }\\n            return Customer.Create(command.Id, command.Name);\\n        }\\n    }\\n\\nAdding those changes will make the build fail since we modified the constructor. To make it build and all the tests green again also we need to update the code where we instantiate the `CustomerCommandHandler` in the `DomainRepository`. That's a one line fix, so I won't even show it.\\n\\n##Make customer preferred\\nIt should be possible to mark a customer as preferred and also specify what discount the customer should have.\\n###Test\\n    [TestFixture]\\n    public class MarkCustomerAsPreferredTest : TestBase\\n    {\\n        [TestCase(25)]\\n        [TestCase(50)]\\n        [TestCase(70)]\\n        public void GivenTheUserExists_WhenMarkingCustomerAsPreferred_ThenTheCustomerShouldBePreferred(int discount)\\n        {\\n            Guid id = Guid.NewGuid();\\n            Given(new CustomerCreated(id, \\\"Superman\\\"));\\n            When(new MarkCustomerAsPreferred(id, discount));\\n            Then(new CustomerMarkedAsPreferred(id, discount));\\n        }\\n    }\\n\\n###Command\\n\\n    type MarkCustomerAsPreferred = {Id: Guid; Discount: int } with interface ICommand\\n\\n###Event\\n    type CustomerMarkedAsPreferred = {Id: Guid; Discount: int }\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n\\n###Handler\\nThe following was added to the `CustomerCommandHandler`:\\n\\n    public IAggregate Handle(MarkCustomerAsPreferred command)\\n    {\\n        var customer = _domainRepository.GetById<Customer>(command.Id);\\n        customer.MakePreferred(command.Discount);\\n        return customer;\\n    }\\n\\n###The aggregate\\nAs of know we really don't need to take care of the state to get the test to pass, all we need to do is to raise the event indicating that we have stored the change. So the following should be added to the `Customer` class: \\n\\n\\n    public void MakePreferred(int discount)\\n    {\\n        RaiseEvent(new CustomerMarkedAsPreferred(Id, discount));\\n    }\\n\\n#Product features\\nIt might be that product should be in a whole other system administrating just the product and what you send in to this domain is the product id, name and price when adding to the basket. Since this is just for show I included the product her. I've a really simplified web shop, you can only add products and when they've been added they will be there forever and will never run out. Also, the only thing I need to prevent is duplicate insert as I did with customer.\\n\\n##Creating a product\\nYou should be able to add a product the same way as you can add a customer. To add a product you need to provide the id, name and price. You can't add duplicate products.\\n\\n###Tests\\nI'm using test cases to make sure the values aren't hard coded, except from that everything is straightforward.\\n\\n    [TestFixture]\\n    public class CreateProductTests : TestBase\\n    {\\n        [TestCase(\\\"ball\\\", 1000)]\\n        [TestCase(\\\"train\\\", 10000)]\\n        [TestCase(\\\"universe\\\", 999999)]\\n        public void WhenCreatingAProduct_TheProductShouldBeCreatedWithTheCorrectPrice(string productName, int price)\\n        {\\n            Guid id = Guid.NewGuid();\\n            When(new CreateProduct(id, productName, price));\\n            Then(new ProductCreated(id, productName, price));\\n        }\\n\\n        [Test]\\n        public void GivenProductXExists_WhenCreatingAProductWithIdX_IShouldGetNotifiedThatTheProductAlreadyExists()\\n        {\\n            Guid id = Guid.NewGuid();\\n            Given(new ProductCreated(id, \\\"Something I don't care about\\\", 9999));\\n            WhenThrows<ProductAlreadyExistsException>(new CreateProduct(id, \\\"Sugar\\\", 999));\\n        }\\n    }\\n\\n###Command\\n    type CreateProduct = {Id: Guid; Name: string; Price: int } with interface ICommand\\n\\n\\n###Event\\n    type ProductCreated = {Id: Guid; Name: string; Price: int }\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n###Handler\\n    internal class ProductCommandHandler : \\n        IHandle<CreateProduct>\\n    {\\n        private readonly IDomainRepository _domainRepository;\\n\\n        public ProductCommandHandler(IDomainRepository domainRepository)\\n        {\\n            _domainRepository = domainRepository;\\n        }\\n\\n        public IAggregate Handle(CreateProduct command)\\n        {\\n            try\\n            {\\n                var product = _domainRepository.GetById<Product>(command.Id);\\n                throw new ProductAlreadyExistsException(command.Id, command.Name);\\n            }\\n            catch (AggregateNotFoundException)\\n            {\\n                // We expect not to find anything\\n            }\\n            return Product.Create(command.Id, command.Name, command.Price);\\n        }\\n    }\\n\\n###Aggregate\\nTo get the test pass as of the moment I don't need to store the price and name.\\n\\n    internal class Product : AggregateBase\\n    {\\n        public Product()\\n        {\\n            RegisterTransition<ProductCreated>(Apply);\\n        }\\n\\n        private void Apply(ProductCreated obj)\\n        {\\n            Id = obj.Id;\\n        }\\n\\n        private Product(Guid id, string name, int price) : this()\\n        {\\n            RaiseEvent(new ProductCreated(id, name, price));\\n        }\\n\\n        public static IAggregate Create(Guid id, string name, int price)\\n        {\\n            return new Product(id, name, price);\\n        }\\n    }\\n\\n#Shopping basket features\\nI've chosen to put a lot of the functionality in the basket since the data coming out from it should be used both by the ordering aggregate and to show the value of the basket. Keeping the pricing calculation and where the customer are in the shopping process here until the payment is made simplify a lot of things.\\n\\n##Create shopping basket\\nWhen the shopping basket is created I send in the customer id. That is probably not something you'll require in a real system since a customer should be able to create a basket without being logged in.\\n\\n###Tests\\nThree tests for the basket creation. The happy path and the negative paths when the customer doesn't exists or the basket already exists.\\n\\n    [TestFixture]\\n    public class CreateBasketTests : TestBase\\n    {\\n        [Test]\\n        public void GivenCustomerWithIdXExists_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\\n        {\\n            var id = Guid.NewGuid();\\n            var customerId = Guid.NewGuid();\\n            int discount = 0;\\n            string name = \\\"John doe\\\";\\n            Given(new CustomerCreated(customerId, name));\\n            When(new CreateBasket(id, customerId));\\n            Then(new BasketCreated(id, customerId, discount));\\n        }\\n\\n        [Test]\\n        public void GivenNoCustomerWithIdXExists_WhenCreatingABasketForCustomerX_IShouldGetNotified()\\n        {\\n            var id = Guid.NewGuid();\\n            var customerId = Guid.NewGuid();\\n            WhenThrows<AggregateNotFoundException>(new CreateBasket(id, customerId));\\n        }\\n\\n        [Test]\\n        public void GivenCustomerWithIdXExistsAndBasketAlreadyExistsForIdY_WhenCreatingABasketForCustomerXAndIdY_IShouldGetNotified()\\n        {\\n            var id = Guid.NewGuid();\\n            var customerId = Guid.NewGuid();\\n            string name = \\\"John doe\\\";\\n            int discount = 0;\\n            Given(new BasketCreated(id, Guid.NewGuid(), discount),\\n                new CustomerCreated(customerId, name));\\n            WhenThrows<BasketAlreadExistsException>(new CreateBasket(id, customerId));\\n        }\\n\\n        [Test]\\n        public void GivenACustomerWithADiscount_CreatingABasketForTheCustomer_TheDiscountShouldBeIncluded()\\n        {\\n            var id = Guid.NewGuid();\\n            var customerId = Guid.NewGuid();\\n            int discount = 89;\\n            string name = \\\"John doe\\\";\\n            Given(new CustomerCreated(customerId, name),\\n                new CustomerMarkedAsPreferred(customerId, discount));\\n            When(new CreateBasket(id, customerId));\\n            Then(new BasketCreated(id, customerId, discount));\\n        }\\n    }\\n\\n###Command\\n    type CreateBasket = { Id: Guid; CustomerId: Guid} with interface ICommand\\n\\n###Event\\n    type BasketCreated = { Id: Guid; CustomerId: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n###Handler\\n    internal class BasketCommandHandler :\\n        IHandle<CreateBasket>\\n    {\\n        private readonly IDomainRepository _domainRepository;\\n\\n        public BasketCommandHandler(IDomainRepository domainRepository)\\n        {\\n            _domainRepository = domainRepository;\\n        }\\n\\n        public IAggregate Handle(CreateBasket command)\\n        {\\n            try\\n            {\\n                var basket = _domainRepository.GetById<Basket>(command.Id);\\n                throw new BasketAlreadExistsException(command.Id);\\n            }\\n            catch (AggregateNotFoundException)\\n            {\\n                //Expect this\\n            }\\n            var customer = _domainRepository.GetById<Customer>(command.CustomerId);\\n            return Basket.Create(command.Id, customer);\\n        }\\n    }\\n\\n###Aggregate\\n    internal class Basket : AggregateBase\\n    {\\n        private Basket(Guid id, Guid customerId, int discount) : this()\\n        {\\n            RaiseEvent(new BasketCreated(id, customerId, discount));\\n        }\\n\\n        public Basket()\\n        {\\n            RegisterTransition<BasketCreated>(Apply);\\n        }\\n\\n        private void Apply(BasketCreated obj)\\n        {\\n            Id = obj.Id;\\n        }\\n\\n        public static IAggregate Create(Guid id, Customer customer)\\n        {\\n            return new Basket(id, customer.Id, customer.Discount);\\n        }\\n    }\\n\\n##Add item to basket\\nThere are two happy path to consider; adding an item for a customer with no discount and for a customer with discount. I don't consider all the negative cases to keep me a little bit shorter.\\n\\n###Tests\\n    [TestFixture]\\n    public class AddItemToBasketTest : TestBase\\n    {\\n        [TestCase(\\\"NameA\\\", 100, 10)]\\n        [TestCase(\\\"NameB\\\", 200, 20)]\\n        public void GivenWeHaveABasketForARegularCustomer_WhenAddingItems_ThePriceOfTheBasketShouldNotBeDiscounted(string productName, int itemPrice, int quantity)\\n        {\\n            var customerId = Guid.NewGuid();\\n            var productId = Guid.NewGuid();\\n            var id = Guid.NewGuid();\\n            Given(new ProductCreated(productId, productName, itemPrice),\\n                new BasketCreated(id, customerId, 0));\\n            When(new AddItemToBasket(id, productId, quantity));\\n            Then(new ItemAdded(id, productId, productName, itemPrice, itemPrice, quantity));\\n        }\\n\\n        [TestCase(\\\"NameA\\\", 100, 10, 10, 90)]\\n        [TestCase(\\\"NameB\\\", 200, 20, 80, 40)]\\n        public void GivenWeHaveABasketForAPreferredCustomer_WhenAddingItems_ThePriceOfTheBasketShouldBeDiscounted(string productName, int itemPrice, int quantity, int discountPercentage, int discountedPrice)\\n        {\\n            var customerId = Guid.NewGuid();\\n            var productId = Guid.NewGuid();\\n            var id = Guid.NewGuid();\\n            Given(new CustomerCreated(customerId, \\\"John Doe\\\"),\\n                new CustomerMarkedAsPreferred(customerId, discountPercentage),\\n                new ProductCreated(productId, productName, itemPrice),\\n                new BasketCreated(id, customerId, discountPercentage));\\n            When(new AddItemToBasket(id, productId, quantity));\\n            Then(new ItemAdded(id, productId, productName, itemPrice, discountedPrice, quantity));\\n        }\\n    }\\n\\n###Command\\n    type AddItemToBasket = { Id: Guid; ProductId: Guid; Quantity: int } with interface ICommand\\n\\n###Event\\nI had a minor issue when implementing the feature so I needed to add an implementation of `ToString` so this event is a little more verbose but I thought I keep it for you to see.\\n\\n    type ItemAdded = { Id: Guid; ProductId: Guid; ProductName: string; OriginalPrice: int; DiscountedPrice: int; Quantity: int}\\n        with \\n        override this.ToString() = sprintf \\\"Item added. Id: %O, Price: %d, Discounted: %d, Quantity: %d\\\" this.Id this.OriginalPrice this.DiscountedPrice this.Quantity\\n        interface IEvent with member this.Id with get() = this.Id\\n\\n\\n###Handler\\nThe handler is updated to handle the add item command: \\n\\n    public IAggregate Handle(AddItemToBasket command)\\n    {\\n        var basket = _domainRepository.GetById<Basket>(command.Id);\\n        var product = _domainRepository.GetById<Product>(command.ProductId);\\n        basket.AddItem(product, command.Quantity);\\n        return basket;\\n    }\\n\\n###Aggregate\\nWe only need to add one method to the aggregate that adds the item and calculate the price. \\n\\npublic void AddItem(Product product, int quantity)\\n    {\\n        var discount = (int)(product.Price * ((double)_discount/100));\\n        var discountedPrice = product.Price - discount;\\n        RaiseEvent(new ItemAdded(Id, product.Id, product.Name, product.Price, discountedPrice, quantity));\\n    }\\n\\n##Proceed to checkout\\nThe proceed to checkout feature is just a way to keep track of the customer for future analytics purposes. When at the checkout the user can add shipping address and proceed to payment.\\n\\n###Test\\nOne simple test for the happy path.\\n\\n    [TestFixture]\\n    public class ProceedCheckoutBasketTests : TestBase\\n    {\\n        [Test]\\n        public void GivenABasket_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\\n        {\\n            var id = Guid.NewGuid();\\n            var customerId = Guid.NewGuid();\\n            int discount = 0;\\n            Given(new BasketCreated(id, customerId, discount));\\n            When(new ProceedToCheckout(id));\\n            Then(new CustomerIsCheckingOutBasket(id));\\n        }\\n    }\\n\\n###Command\\n\\n    type ProceedToCheckout = { Id: Guid } with interface ICommand\\n\\n###Event\\n\\n    type CustomerIsCheckingOutBasket = { Id: Guid }\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n###Handler\\nOne more method to the basket command handler:\\n\\n    public IAggregate Handle(ProceedToCheckout command)\\n    {\\n        var basket = _domainRepository.GetById<Basket>(command.Id);\\n        basket.ProceedToCheckout();\\n        return basket;\\n    }\\n\\n###Aggregate\\nThe new method added to the basket aggregate is also trivial.\\n\\n    public void ProceedToCheckout()\\n    {\\n        RaiseEvent(new CustomerIsCheckingOutBasket(Id));\\n    }\\n\\n\\n##Checkout\\nWhen the user is doing the actual checkout we need to collect the shipping address before we proceed to payment, and the address must be specified.\\n\\n###Test\\n\\n    [TestFixture]\\n    public class CheckoutBasketTests : TestBase\\n    {\\n        [TestCase(null)]\\n        [TestCase(\\\"\\\")]\\n        [TestCase(\\\"    \\\")]\\n        public void WhenTheUserCheckoutWithInvalidAddress_IShouldGetNotified(string street)\\n        {\\n            var address = street == null ? null : new Address(street);\\n            var id = Guid.NewGuid();\\n            Given(new BasketCreated(id, Guid.NewGuid(), 0));\\n            WhenThrows<MissingAddressException>(new CheckoutBasket(id, address));\\n        }\\n\\n        [Test]\\n        public void WhenTheUserCheckoutWithAValidAddress_IShouldProceedToTheNextStep()\\n        {\\n            var address = new Address(\\\"Valid street\\\");\\n            var id = Guid.NewGuid();\\n            Given(new BasketCreated(id, Guid.NewGuid(), 0));\\n            When(new CheckoutBasket(id, address));\\n            Then(new BasketCheckedOut(id, address));\\n        }\\n    }\\n\\n###Types\\nTo support address I defined a separate F# record for address and put in a \\\"types.fs\\\" file. This must be located above the other files and should include the following: \\n\\n    namespace CQRSShop.Contracts.Types\\n\\n    type Address = { Street: string }\\n\\n\\n###Command\\nNow when I have the address type specified the command looks like this:\\n\\n    type CheckoutBasket = { Id: Guid; ShippingAddress: Address } with interface ICommand\\n\\n###Event\\n    type BasketCheckedOut = { Id: Guid; ShippingAddress: Address } \\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n###Handler\\nOnce againt a simple method was added to the command handler class.\\n\\n    public IAggregate Handle(CheckoutBasket command)\\n    {\\n        var basket = _domainRepository.GetById<Basket>(command.Id);\\n        basket.Checkout(command.ShippingAddress);\\n        return basket;\\n    }\\n\\n###Aggregate\\nThe aggregate is where the logic to validate the input related to what makes a valid checkout should be: \\n\\n    public void Checkout(Address shippingAddress)\\n    {\\n        if(shippingAddress == null || string.IsNullOrWhiteSpace(shippingAddress.Street))\\n            throw new MissingAddressException();\\n        RaiseEvent(new BasketCheckedOut(Id, shippingAddress));\\n    }\\n\\n##Make payment\\nWhen a payment is made the result will be that an order is created, this order can use the same id as the basket since they will be connected to each other and there can only be one order per basket but many baskets can have no order.\\n\\n###Tests\\nThe happy path for making a payment is a little bit different than the other tests. Here we're going to create another aggregate from the basket aggregate. The tests will fail if we try to pay an unexpected amount. I haven't covered all cases here either. To get the test running as expected I needed control over the id generation, and to do so I wrote the `IdGenerator` class and put it in the infrastructure project.\\n\\n    public class IdGenerator\\n    {\\n        private static Func<Guid> _generator;\\n\\n        public static Func<Guid> GenerateGuid\\n        {\\n            get\\n            {\\n                _generator = _generator ?? Guid.NewGuid;\\n                return _generator;\\n            }\\n            set { _generator = value; }\\n        }\\n    }\\n\\nThe tests when you have that class is pretty straightforward:\\n\\n    [TestFixture]\\n    public class MakePaymentTests : TestBase\\n    {\\n        [TestCase(100, 101)]\\n        [TestCase(100, 99)]\\n        [TestCase(100, 91)]\\n        [TestCase(100, 89)]\\n        public void WhenNotPayingTheExpectedAmount_IShouldGetNotified(int productPrice, int payment)\\n        {\\n            var id = Guid.NewGuid();\\n            Given(new BasketCreated(id, Guid.NewGuid(), 0),\\n                new ItemAdded(id, Guid.NewGuid(), \\\"\\\", productPrice, productPrice, 1));\\n            WhenThrows<UnexpectedPaymentException>(new MakePayment(id, payment));\\n        }\\n\\n        [TestCase(100, 101, 101)]\\n        [TestCase(100, 80, 80)]\\n        public void WhenPayingTheExpectedAmount_ThenANewOrderShouldBeCreatedFromTheResult(int productPrice, int discountPrice, int payment)\\n        {\\n            var id = Guid.NewGuid();\\n            int dontCare = 0;\\n            var orderId = Guid.NewGuid();\\n            IdGenerator.GenerateGuid = () => orderId;\\n            Given(new BasketCreated(id, Guid.NewGuid(), dontCare),\\n                new ItemAdded(id, Guid.NewGuid(), \\\"Ball\\\", productPrice, discountPrice, 1));\\n            When(new MakePayment(id, payment));\\n            Then(new OrderCreated(orderId, id));\\n        }\\n    }\\n\\n###Command\\n    type MakePayment = {Id: Guid; Payment: int } with interface ICommand\\n\\n###Event\\nThe event generated here is order created since that is what we expect when the customer has payed for the basket.\\n\\n    type OrderCreated ={ Id: Guid; BasketId: Guid }\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n###Handler\\nThis is a slitghtly different handler then the previous one. As you can see from the code it returns an `Order` aggregate and that is what gets return from the handler instead of a `Basket` aggregate.\\n\\n    public IAggregate Handle(MakePayment command)\\n    {\\n        var basket = _domainRepository.GetById<Basket>(command.Id);\\n        var order = basket.MakePayment(command.Payment);\\n        return order;\\n    }\\n\\n###Aggregate\\nTo the `Basket` aggregate I added this method: \\n\\n    public IAggregate MakePayment(int payment)\\n    {\\n        var expectedPayment = _items.Sum(y => y.DiscountedPrice);\\n        if(expectedPayment != payment)\\n            throw new UnexpectedPaymentException();\\n        return new Order(Id);\\n    }\\n\\nAlso, to get this to work I had to create the `Order` aggregate:\\n\\n    internal class Order : AggregateBase\\n    {\\n        public Order()\\n        {\\n            RegisterTransition<OrderCreated>(Apply);\\n        }\\n\\n        private void Apply(OrderCreated obj)\\n        {\\n            Id = obj.Id;\\n        }\\n\\n        internal Order(Guid basketId) : this()\\n        {\\n            RaiseEvent(new OrderCreated(IdGenerator.GenerateGuid(), basketId));\\n        }\\n    }\\n\\n#Order features\\nThe `Order` aggregates responsibility in my application is to handle everything that has with the order to do, like if it is possible to cancel the order and keep track on if it is shipped. So let's start with last set of features.\\n\\nTo spare myself some time I put all the tests in one file, and implemented all the `Order` features and not step by step as above. The test is also somewhat intertwined between cancelling and shipping so it might feel like the tests are out of order.\\n\\nThere are also some minor changes to the `OrderCreated` event to include a fsharp list for the order items, this makes it much easier to compare two lists. The in-memory eventstore was also updated to use serializtion to verify that I can handle the serialization of the fsharp list.\\n\\n##Tests\\nAgain I'm not testing every possible edge case, I just want the basic functionality up and running.\\n\\n    [TestFixture]\\n    public class AllTheOrderTests : TestBase\\n    {\\n        [Test]\\n        public void WhenStartingShippingProcess_TheShippingShouldBeStarted()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId:  Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated);\\n            When(new StartShippingProcess(id));\\n            Then(new ShippingProcessStarted(id));\\n        }\\n\\n        [Test]\\n        public void WhenCancellingAnOrderThatHasntBeenStartedShipping_TheOrderShouldBeCancelled()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated);\\n            When(new CancelOrder(id));\\n            Then(new OrderCancelled(id));\\n        }\\n\\n        [Test]\\n        public void WhenTryingToStartShippingACancelledOrder_IShouldBeNotified()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated,\\n                new OrderCancelled(id));\\n            WhenThrows<OrderCancelledException>(new StartShippingProcess(id));\\n        }\\n\\n        [Test]\\n        public void WhenTryingToCancelAnOrderThatIsAboutToShip_IShouldBeNotified()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated,\\n                new ShippingProcessStarted(id));\\n            WhenThrows<ShippingStartedException>(new CancelOrder(id));\\n        }\\n\\n        [Test]\\n        public void WhenShippingAnOrderThatTheShippingProcessIsStarted_ItShouldBeMarkedAsShipped()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated,\\n                new ShippingProcessStarted(id));\\n            When(new ShipOrder(id));\\n            Then(new OrderShipped(id));\\n        }\\n\\n        [Test]\\n        public void WhenShippingAnOrderWhereShippingIsNotStarted_IShouldGetNotified()\\n        {\\n            var id = Guid.NewGuid();\\n            var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\\n            Given(orderCreated);\\n            WhenThrows<InvalidOrderState>(new ShipOrder(id));\\n        }\\n\\n        [Test]\\n        public void WhenTheUserCheckoutWithAnAmountLargerThan100000_TheOrderNeedsApproval()\\n        {\\n            var address = new Address(\\\"Valid street\\\");\\n            var basketId = Guid.NewGuid();\\n            var orderId = Guid.NewGuid();\\n            IdGenerator.GenerateGuid = () => orderId;\\n            var orderLine = new OrderLine(Guid.NewGuid(), \\\"Ball\\\", 100000, 100001, 1);\\n            Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\\n                new ItemAdded(basketId, orderLine),\\n                new BasketCheckedOut(basketId, address));\\n            When(new MakePayment(basketId, 100001));\\n            Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new [] {orderLine})),\\n                new NeedsApproval(orderId));\\n        }\\n\\n        [Test]\\n        public void WhenTheUserCheckoutWithAnAmountLessThan100000_TheOrderIsAutomaticallyApproved()\\n        {\\n            var address = new Address(\\\"Valid street\\\");\\n            var basketId = Guid.NewGuid();\\n            var orderId = Guid.NewGuid();\\n            IdGenerator.GenerateGuid = () => orderId;\\n            var orderLine = new OrderLine(Guid.NewGuid(), \\\"Ball\\\", 100000, 100000, 1);\\n            Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\\n                new ItemAdded(basketId, orderLine),\\n                new BasketCheckedOut(basketId, address));\\n            When(new MakePayment(basketId, 100000));\\n            Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new[] { orderLine })),\\n                new OrderApproved(orderId));\\n        }\\n\\n        [Test]\\n        public void WhenApprovingAnOrder_ItShouldBeApproved()\\n        {\\n            var orderId = Guid.NewGuid();\\n            Given(new OrderCreated(orderId, Guid.NewGuid(), FSharpList<OrderLine>.Empty));\\n            When(new ApproveOrder(orderId));\\n            Then(new OrderApproved(orderId));\\n        }\\n\\n        private OrderCreated BuildOrderCreated(Guid orderId, Guid basketId, int numberOfOrderLines, int pricePerProduct = 100)\\n        {\\n            var orderLines = FSharpList<OrderLine>.Empty;\\n            for (var i = 0; i < numberOfOrderLines; i++)\\n            {\\n                orderLines = FSharpList<OrderLine>.Cons(new OrderLine(Guid.NewGuid(), \\\"Line \\\" + i, pricePerProduct, pricePerProduct, 1), orderLines);\\n            }\\n            return new OrderCreated(orderId, basketId, orderLines);\\n        }\\n    }\\n\\n##Command\\n\\n    type StartShippingProcess = { Id: Guid } with interface ICommand\\n    type CancelOrder = { Id: Guid } with interface ICommand\\n    type ShipOrder = { Id: Guid } with interface ICommand\\n    type ApproveOrder = { Id: Guid } with interface ICommand\\n\\n##Event\\n\\n    type OrderCreated ={ Id: Guid; BasketId: Guid; OrderLines: OrderLine list }\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n    type ShippingProcessStarted = {Id: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n    type OrderCancelled = {Id: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n    type OrderShipped = {Id: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n    type NeedsApproval = {Id: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n    type OrderApproved = {Id: Guid}\\n        with interface IEvent with member this.Id with get() = this.Id\\n\\n\\n##Handler\\n    internal class OrderHandler : \\n        IHandle<ApproveOrder>, \\n        IHandle<StartShippingProcess>, \\n        IHandle<CancelOrder>, \\n        IHandle<ShipOrder>\\n    {\\n        private readonly IDomainRepository _domainRepository;\\n\\n        public OrderHandler(IDomainRepository domainRepository)\\n        {\\n            _domainRepository = domainRepository;\\n        }\\n\\n        public IAggregate Handle(ApproveOrder command)\\n        {\\n            var order = _domainRepository.GetById<Order>(command.Id);\\n            order.Approve();\\n            return order;\\n        }\\n\\n        public IAggregate Handle(StartShippingProcess command)\\n        {\\n            var order = _domainRepository.GetById<Order>(command.Id);\\n            order.StartShippingProcess();\\n            return order;\\n        }\\n\\n        public IAggregate Handle(CancelOrder command)\\n        {\\n            var order = _domainRepository.GetById<Order>(command.Id);\\n            order.Cancel();\\n            return order;\\n        }\\n\\n        public IAggregate Handle(ShipOrder command)\\n        {\\n            var order = _domainRepository.GetById<Order>(command.Id);\\n            order.ShipOrder();\\n            return order;\\n        }\\n    }\\n\\n\\n##Aggregate\\n    internal class Order : AggregateBase\\n    {\\n        private OrderState _orderState;\\n\\n        private enum OrderState\\n        {\\n            ShippingProcessStarted,\\n            Created,\\n            Cancelled\\n        }\\n\\n        public Order()\\n        {\\n            RegisterTransition<OrderCreated>(Apply);\\n            RegisterTransition<ShippingProcessStarted>(Apply);\\n            RegisterTransition<OrderCancelled>(Apply);\\n        }\\n\\n        private void Apply(OrderCancelled obj)\\n        {\\n            _orderState = OrderState.Cancelled;\\n        }\\n\\n        private void Apply(ShippingProcessStarted obj)\\n        {\\n            _orderState = OrderState.ShippingProcessStarted;\\n        }\\n\\n        private void Apply(OrderCreated obj)\\n        {\\n            _orderState = OrderState.Created;\\n            Id = obj.Id;\\n        }\\n\\n        internal Order(Guid basketId, FSharpList<OrderLine> orderLines) : this()\\n        {\\n            var id = IdGenerator.GenerateGuid();\\n            RaiseEvent(new OrderCreated(id, basketId, orderLines));\\n            var totalPrice = orderLines.Sum(y => y.DiscountedPrice);\\n            if (totalPrice > 100000)\\n            {\\n                RaiseEvent(new NeedsApproval(id));\\n            }\\n            else\\n            {\\n                RaiseEvent(new OrderApproved(id));\\n            }\\n        }\\n\\n        public void Approve()\\n        {\\n            RaiseEvent(new OrderApproved(Id));\\n        }\\n\\n        public void StartShippingProcess()\\n        {\\n            if (_orderState == OrderState.Cancelled)\\n                throw new OrderCancelledException();\\n\\n            RaiseEvent(new ShippingProcessStarted(Id));\\n        }\\n\\n        public void Cancel()\\n        {\\n            if (_orderState == OrderState.Created)\\n            {\\n                RaiseEvent(new OrderCancelled(Id));\\n            }\\n            else\\n            {\\n                throw new ShippingStartedException();\\n            }\\n        }\\n\\n        public void ShipOrder()\\n        {\\n            if (_orderState != OrderState.ShippingProcessStarted)\\n                throw new InvalidOrderState();\\n            RaiseEvent(new OrderShipped(Id));\\n        }\\n    }\\n\\nAnd that finished the implementation of all the features.\\n\\nThe part of the series is [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li>The rest of the features</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"somestartingnotes\">Some starting notes</h1>\n<p>First off, all the code is available to github. All I'm providing here is feature by feature implementation and some comments.</p>\n<p>Before we start implement the features we need to make a slight change to the <code>TestBase</code> class to get better exception handling when expecting exceptions. The <code>WhenThrows</code> method should be updated to this:</p>\n<pre><code>protected void WhenThrows&lt;TException&gt;(ICommand command) where TException : Exception\n{\n    try\n    {\n        When(command);\n        Assert.Fail(&quot;Expected exception &quot; + typeof(TException));\n    }\n    catch (TException)\n    {\n    }\n}\n</code></pre>\n<p>This will try to execute the command and only catch the exception we are expecting, do we get another exception we get the correct stacktrace for that exception instead of a modified stacktrace.</p>\n<p>I'll try to group all the features to respective aggregate, and the be as concise as possible focusing on the code. The format will be &quot;Feature group&quot; --&gt; &quot;Feature&quot; --&gt; &quot;Test&quot; and implementation &quot;Code&quot;. When showing the code I won't show the changes to the <code>DomainEntry</code> since they are just one or two, if any, lines for each feature.</p>\n<h1 id=\"customerfeatures\">Customer features</h1>\n<p>We're not 100 % done with the customer yet. As of the moment it is possible to create duplicate user, and that is something we need to prevent. And we also need to add the functionality to make the customer preferred so they can get a discount.</p>\n<h2 id=\"preventingduplicateusers\">Preventing duplicate users</h2>\n<p>When we are trying to create a user with an id that is not unique we should get an exception telling us what the problem is.</p>\n<h3 id=\"test\">Test</h3>\n<pre><code>[Test]\npublic void GivenAUserWithIdXExists_WhenCreatingACustomerWithIdX_IShouldGetNotifiedThatTheUserAlreadyExists()\n{\n    Guid id = Guid.NewGuid();\n    Given(new CustomerCreated(id, &quot;Something I don't care about&quot;));\n    WhenThrows&lt;CustomerAlreadyExistsException&gt;(new CreateCustomer(id, &quot;Tomas&quot;));\n}\n</code></pre>\n<h3 id=\"code\">Code</h3>\n<p>First we are missing an exception so the code won't even compile. So we add that class to the &quot;Exceptions&quot; folder in the domain project.</p>\n<pre><code>public class CustomerAlreadyExistsException : Exception\n{\n    public CustomerAlreadyExistsException(Guid id, string name) : base(CreateMessage(id, name))\n    {\n        \n    }\n\n    private static string CreateMessage(Guid id, string name)\n    {\n        return string.Format(&quot;A customer with id {0} already exists, can't create customer for {1}&quot;, id, name);\n    }\n}\n</code></pre>\n<p>The next thing is to actually implement the feature. To get the test green we need to update our <code>CustomerCommandHandler</code> to make the duplicate check.</p>\n<pre><code>internal class CustomerCommandHandler : IHandle&lt;CreateCustomer&gt;\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public CustomerCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateCustomer command)\n    {\n        try\n        {\n            var customer = _domainRepository.GetById&lt;Customer&gt;(command.Id);\n            throw new CustomerAlreadyExistsException(command.Id, command.Name);\n        }\n        catch (AggregateNotFoundException)\n        {\n            // We expect not to find anything\n        }\n        return Customer.Create(command.Id, command.Name);\n    }\n}\n</code></pre>\n<p>Adding those changes will make the build fail since we modified the constructor. To make it build and all the tests green again also we need to update the code where we instantiate the <code>CustomerCommandHandler</code> in the <code>DomainRepository</code>. That's a one line fix, so I won't even show it.</p>\n<h2 id=\"makecustomerpreferred\">Make customer preferred</h2>\n<p>It should be possible to mark a customer as preferred and also specify what discount the customer should have.</p>\n<h3 id=\"test\">Test</h3>\n<pre><code>[TestFixture]\npublic class MarkCustomerAsPreferredTest : TestBase\n{\n    [TestCase(25)]\n    [TestCase(50)]\n    [TestCase(70)]\n    public void GivenTheUserExists_WhenMarkingCustomerAsPreferred_ThenTheCustomerShouldBePreferred(int discount)\n    {\n        Guid id = Guid.NewGuid();\n        Given(new CustomerCreated(id, &quot;Superman&quot;));\n        When(new MarkCustomerAsPreferred(id, discount));\n        Then(new CustomerMarkedAsPreferred(id, discount));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type MarkCustomerAsPreferred = {Id: Guid; Discount: int } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<pre><code>type CustomerMarkedAsPreferred = {Id: Guid; Discount: int }\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<p>The following was added to the <code>CustomerCommandHandler</code>:</p>\n<pre><code>public IAggregate Handle(MarkCustomerAsPreferred command)\n{\n    var customer = _domainRepository.GetById&lt;Customer&gt;(command.Id);\n    customer.MakePreferred(command.Discount);\n    return customer;\n}\n</code></pre>\n<h3 id=\"theaggregate\">The aggregate</h3>\n<p>As of know we really don't need to take care of the state to get the test to pass, all we need to do is to raise the event indicating that we have stored the change. So the following should be added to the <code>Customer</code> class:</p>\n<pre><code>public void MakePreferred(int discount)\n{\n    RaiseEvent(new CustomerMarkedAsPreferred(Id, discount));\n}\n</code></pre>\n<h1 id=\"productfeatures\">Product features</h1>\n<p>It might be that product should be in a whole other system administrating just the product and what you send in to this domain is the product id, name and price when adding to the basket. Since this is just for show I included the product her. I've a really simplified web shop, you can only add products and when they've been added they will be there forever and will never run out. Also, the only thing I need to prevent is duplicate insert as I did with customer.</p>\n<h2 id=\"creatingaproduct\">Creating a product</h2>\n<p>You should be able to add a product the same way as you can add a customer. To add a product you need to provide the id, name and price. You can't add duplicate products.</p>\n<h3 id=\"tests\">Tests</h3>\n<p>I'm using test cases to make sure the values aren't hard coded, except from that everything is straightforward.</p>\n<pre><code>[TestFixture]\npublic class CreateProductTests : TestBase\n{\n    [TestCase(&quot;ball&quot;, 1000)]\n    [TestCase(&quot;train&quot;, 10000)]\n    [TestCase(&quot;universe&quot;, 999999)]\n    public void WhenCreatingAProduct_TheProductShouldBeCreatedWithTheCorrectPrice(string productName, int price)\n    {\n        Guid id = Guid.NewGuid();\n        When(new CreateProduct(id, productName, price));\n        Then(new ProductCreated(id, productName, price));\n    }\n\n    [Test]\n    public void GivenProductXExists_WhenCreatingAProductWithIdX_IShouldGetNotifiedThatTheProductAlreadyExists()\n    {\n        Guid id = Guid.NewGuid();\n        Given(new ProductCreated(id, &quot;Something I don't care about&quot;, 9999));\n        WhenThrows&lt;ProductAlreadyExistsException&gt;(new CreateProduct(id, &quot;Sugar&quot;, 999));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type CreateProduct = {Id: Guid; Name: string; Price: int } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<pre><code>type ProductCreated = {Id: Guid; Name: string; Price: int }\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<pre><code>internal class ProductCommandHandler : \n    IHandle&lt;CreateProduct&gt;\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public ProductCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateProduct command)\n    {\n        try\n        {\n            var product = _domainRepository.GetById&lt;Product&gt;(command.Id);\n            throw new ProductAlreadyExistsException(command.Id, command.Name);\n        }\n        catch (AggregateNotFoundException)\n        {\n            // We expect not to find anything\n        }\n        return Product.Create(command.Id, command.Name, command.Price);\n    }\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<p>To get the test pass as of the moment I don't need to store the price and name.</p>\n<pre><code>internal class Product : AggregateBase\n{\n    public Product()\n    {\n        RegisterTransition&lt;ProductCreated&gt;(Apply);\n    }\n\n    private void Apply(ProductCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    private Product(Guid id, string name, int price) : this()\n    {\n        RaiseEvent(new ProductCreated(id, name, price));\n    }\n\n    public static IAggregate Create(Guid id, string name, int price)\n    {\n        return new Product(id, name, price);\n    }\n}\n</code></pre>\n<h1 id=\"shoppingbasketfeatures\">Shopping basket features</h1>\n<p>I've chosen to put a lot of the functionality in the basket since the data coming out from it should be used both by the ordering aggregate and to show the value of the basket. Keeping the pricing calculation and where the customer are in the shopping process here until the payment is made simplify a lot of things.</p>\n<h2 id=\"createshoppingbasket\">Create shopping basket</h2>\n<p>When the shopping basket is created I send in the customer id. That is probably not something you'll require in a real system since a customer should be able to create a basket without being logged in.</p>\n<h3 id=\"tests\">Tests</h3>\n<p>Three tests for the basket creation. The happy path and the negative paths when the customer doesn't exists or the basket already exists.</p>\n<pre><code>[TestFixture]\npublic class CreateBasketTests : TestBase\n{\n    [Test]\n    public void GivenCustomerWithIdXExists_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 0;\n        string name = &quot;John doe&quot;;\n        Given(new CustomerCreated(customerId, name));\n        When(new CreateBasket(id, customerId));\n        Then(new BasketCreated(id, customerId, discount));\n    }\n\n    [Test]\n    public void GivenNoCustomerWithIdXExists_WhenCreatingABasketForCustomerX_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        WhenThrows&lt;AggregateNotFoundException&gt;(new CreateBasket(id, customerId));\n    }\n\n    [Test]\n    public void GivenCustomerWithIdXExistsAndBasketAlreadyExistsForIdY_WhenCreatingABasketForCustomerXAndIdY_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        string name = &quot;John doe&quot;;\n        int discount = 0;\n        Given(new BasketCreated(id, Guid.NewGuid(), discount),\n            new CustomerCreated(customerId, name));\n        WhenThrows&lt;BasketAlreadExistsException&gt;(new CreateBasket(id, customerId));\n    }\n\n    [Test]\n    public void GivenACustomerWithADiscount_CreatingABasketForTheCustomer_TheDiscountShouldBeIncluded()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 89;\n        string name = &quot;John doe&quot;;\n        Given(new CustomerCreated(customerId, name),\n            new CustomerMarkedAsPreferred(customerId, discount));\n        When(new CreateBasket(id, customerId));\n        Then(new BasketCreated(id, customerId, discount));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type CreateBasket = { Id: Guid; CustomerId: Guid} with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<pre><code>type BasketCreated = { Id: Guid; CustomerId: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<pre><code>internal class BasketCommandHandler :\n    IHandle&lt;CreateBasket&gt;\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public BasketCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateBasket command)\n    {\n        try\n        {\n            var basket = _domainRepository.GetById&lt;Basket&gt;(command.Id);\n            throw new BasketAlreadExistsException(command.Id);\n        }\n        catch (AggregateNotFoundException)\n        {\n            //Expect this\n        }\n        var customer = _domainRepository.GetById&lt;Customer&gt;(command.CustomerId);\n        return Basket.Create(command.Id, customer);\n    }\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<pre><code>internal class Basket : AggregateBase\n{\n    private Basket(Guid id, Guid customerId, int discount) : this()\n    {\n        RaiseEvent(new BasketCreated(id, customerId, discount));\n    }\n\n    public Basket()\n    {\n        RegisterTransition&lt;BasketCreated&gt;(Apply);\n    }\n\n    private void Apply(BasketCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    public static IAggregate Create(Guid id, Customer customer)\n    {\n        return new Basket(id, customer.Id, customer.Discount);\n    }\n}\n</code></pre>\n<h2 id=\"additemtobasket\">Add item to basket</h2>\n<p>There are two happy path to consider; adding an item for a customer with no discount and for a customer with discount. I don't consider all the negative cases to keep me a little bit shorter.</p>\n<h3 id=\"tests\">Tests</h3>\n<pre><code>[TestFixture]\npublic class AddItemToBasketTest : TestBase\n{\n    [TestCase(&quot;NameA&quot;, 100, 10)]\n    [TestCase(&quot;NameB&quot;, 200, 20)]\n    public void GivenWeHaveABasketForARegularCustomer_WhenAddingItems_ThePriceOfTheBasketShouldNotBeDiscounted(string productName, int itemPrice, int quantity)\n    {\n        var customerId = Guid.NewGuid();\n        var productId = Guid.NewGuid();\n        var id = Guid.NewGuid();\n        Given(new ProductCreated(productId, productName, itemPrice),\n            new BasketCreated(id, customerId, 0));\n        When(new AddItemToBasket(id, productId, quantity));\n        Then(new ItemAdded(id, productId, productName, itemPrice, itemPrice, quantity));\n    }\n\n    [TestCase(&quot;NameA&quot;, 100, 10, 10, 90)]\n    [TestCase(&quot;NameB&quot;, 200, 20, 80, 40)]\n    public void GivenWeHaveABasketForAPreferredCustomer_WhenAddingItems_ThePriceOfTheBasketShouldBeDiscounted(string productName, int itemPrice, int quantity, int discountPercentage, int discountedPrice)\n    {\n        var customerId = Guid.NewGuid();\n        var productId = Guid.NewGuid();\n        var id = Guid.NewGuid();\n        Given(new CustomerCreated(customerId, &quot;John Doe&quot;),\n            new CustomerMarkedAsPreferred(customerId, discountPercentage),\n            new ProductCreated(productId, productName, itemPrice),\n            new BasketCreated(id, customerId, discountPercentage));\n        When(new AddItemToBasket(id, productId, quantity));\n        Then(new ItemAdded(id, productId, productName, itemPrice, discountedPrice, quantity));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type AddItemToBasket = { Id: Guid; ProductId: Guid; Quantity: int } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<p>I had a minor issue when implementing the feature so I needed to add an implementation of <code>ToString</code> so this event is a little more verbose but I thought I keep it for you to see.</p>\n<pre><code>type ItemAdded = { Id: Guid; ProductId: Guid; ProductName: string; OriginalPrice: int; DiscountedPrice: int; Quantity: int}\n    with \n    override this.ToString() = sprintf &quot;Item added. Id: %O, Price: %d, Discounted: %d, Quantity: %d&quot; this.Id this.OriginalPrice this.DiscountedPrice this.Quantity\n    interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<p>The handler is updated to handle the add item command:</p>\n<pre><code>public IAggregate Handle(AddItemToBasket command)\n{\n    var basket = _domainRepository.GetById&lt;Basket&gt;(command.Id);\n    var product = _domainRepository.GetById&lt;Product&gt;(command.ProductId);\n    basket.AddItem(product, command.Quantity);\n    return basket;\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<p>We only need to add one method to the aggregate that adds the item and calculate the price.</p>\n<p>public void AddItem(Product product, int quantity)<br>\n{<br>\nvar discount = (int)(product.Price * ((double)_discount/100));<br>\nvar discountedPrice = product.Price - discount;<br>\nRaiseEvent(new ItemAdded(Id, product.Id, product.Name, product.Price, discountedPrice, quantity));<br>\n}</p>\n<h2 id=\"proceedtocheckout\">Proceed to checkout</h2>\n<p>The proceed to checkout feature is just a way to keep track of the customer for future analytics purposes. When at the checkout the user can add shipping address and proceed to payment.</p>\n<h3 id=\"test\">Test</h3>\n<p>One simple test for the happy path.</p>\n<pre><code>[TestFixture]\npublic class ProceedCheckoutBasketTests : TestBase\n{\n    [Test]\n    public void GivenABasket_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 0;\n        Given(new BasketCreated(id, customerId, discount));\n        When(new ProceedToCheckout(id));\n        Then(new CustomerIsCheckingOutBasket(id));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type ProceedToCheckout = { Id: Guid } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<pre><code>type CustomerIsCheckingOutBasket = { Id: Guid }\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<p>One more method to the basket command handler:</p>\n<pre><code>public IAggregate Handle(ProceedToCheckout command)\n{\n    var basket = _domainRepository.GetById&lt;Basket&gt;(command.Id);\n    basket.ProceedToCheckout();\n    return basket;\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<p>The new method added to the basket aggregate is also trivial.</p>\n<pre><code>public void ProceedToCheckout()\n{\n    RaiseEvent(new CustomerIsCheckingOutBasket(Id));\n}\n</code></pre>\n<h2 id=\"checkout\">Checkout</h2>\n<p>When the user is doing the actual checkout we need to collect the shipping address before we proceed to payment, and the address must be specified.</p>\n<h3 id=\"test\">Test</h3>\n<pre><code>[TestFixture]\npublic class CheckoutBasketTests : TestBase\n{\n    [TestCase(null)]\n    [TestCase(&quot;&quot;)]\n    [TestCase(&quot;    &quot;)]\n    public void WhenTheUserCheckoutWithInvalidAddress_IShouldGetNotified(string street)\n    {\n        var address = street == null ? null : new Address(street);\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0));\n        WhenThrows&lt;MissingAddressException&gt;(new CheckoutBasket(id, address));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAValidAddress_IShouldProceedToTheNextStep()\n    {\n        var address = new Address(&quot;Valid street&quot;);\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0));\n        When(new CheckoutBasket(id, address));\n        Then(new BasketCheckedOut(id, address));\n    }\n}\n</code></pre>\n<h3 id=\"types\">Types</h3>\n<p>To support address I defined a separate F# record for address and put in a &quot;types.fs&quot; file. This must be located above the other files and should include the following:</p>\n<pre><code>namespace CQRSShop.Contracts.Types\n\ntype Address = { Street: string }\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<p>Now when I have the address type specified the command looks like this:</p>\n<pre><code>type CheckoutBasket = { Id: Guid; ShippingAddress: Address } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<pre><code>type BasketCheckedOut = { Id: Guid; ShippingAddress: Address } \n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<p>Once againt a simple method was added to the command handler class.</p>\n<pre><code>public IAggregate Handle(CheckoutBasket command)\n{\n    var basket = _domainRepository.GetById&lt;Basket&gt;(command.Id);\n    basket.Checkout(command.ShippingAddress);\n    return basket;\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<p>The aggregate is where the logic to validate the input related to what makes a valid checkout should be:</p>\n<pre><code>public void Checkout(Address shippingAddress)\n{\n    if(shippingAddress == null || string.IsNullOrWhiteSpace(shippingAddress.Street))\n        throw new MissingAddressException();\n    RaiseEvent(new BasketCheckedOut(Id, shippingAddress));\n}\n</code></pre>\n<h2 id=\"makepayment\">Make payment</h2>\n<p>When a payment is made the result will be that an order is created, this order can use the same id as the basket since they will be connected to each other and there can only be one order per basket but many baskets can have no order.</p>\n<h3 id=\"tests\">Tests</h3>\n<p>The happy path for making a payment is a little bit different than the other tests. Here we're going to create another aggregate from the basket aggregate. The tests will fail if we try to pay an unexpected amount. I haven't covered all cases here either. To get the test running as expected I needed control over the id generation, and to do so I wrote the <code>IdGenerator</code> class and put it in the infrastructure project.</p>\n<pre><code>public class IdGenerator\n{\n    private static Func&lt;Guid&gt; _generator;\n\n    public static Func&lt;Guid&gt; GenerateGuid\n    {\n        get\n        {\n            _generator = _generator ?? Guid.NewGuid;\n            return _generator;\n        }\n        set { _generator = value; }\n    }\n}\n</code></pre>\n<p>The tests when you have that class is pretty straightforward:</p>\n<pre><code>[TestFixture]\npublic class MakePaymentTests : TestBase\n{\n    [TestCase(100, 101)]\n    [TestCase(100, 99)]\n    [TestCase(100, 91)]\n    [TestCase(100, 89)]\n    public void WhenNotPayingTheExpectedAmount_IShouldGetNotified(int productPrice, int payment)\n    {\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0),\n            new ItemAdded(id, Guid.NewGuid(), &quot;&quot;, productPrice, productPrice, 1));\n        WhenThrows&lt;UnexpectedPaymentException&gt;(new MakePayment(id, payment));\n    }\n\n    [TestCase(100, 101, 101)]\n    [TestCase(100, 80, 80)]\n    public void WhenPayingTheExpectedAmount_ThenANewOrderShouldBeCreatedFromTheResult(int productPrice, int discountPrice, int payment)\n    {\n        var id = Guid.NewGuid();\n        int dontCare = 0;\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () =&gt; orderId;\n        Given(new BasketCreated(id, Guid.NewGuid(), dontCare),\n            new ItemAdded(id, Guid.NewGuid(), &quot;Ball&quot;, productPrice, discountPrice, 1));\n        When(new MakePayment(id, payment));\n        Then(new OrderCreated(orderId, id));\n    }\n}\n</code></pre>\n<h3 id=\"command\">Command</h3>\n<pre><code>type MakePayment = {Id: Guid; Payment: int } with interface ICommand\n</code></pre>\n<h3 id=\"event\">Event</h3>\n<p>The event generated here is order created since that is what we expect when the customer has payed for the basket.</p>\n<pre><code>type OrderCreated ={ Id: Guid; BasketId: Guid }\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h3 id=\"handler\">Handler</h3>\n<p>This is a slitghtly different handler then the previous one. As you can see from the code it returns an <code>Order</code> aggregate and that is what gets return from the handler instead of a <code>Basket</code> aggregate.</p>\n<pre><code>public IAggregate Handle(MakePayment command)\n{\n    var basket = _domainRepository.GetById&lt;Basket&gt;(command.Id);\n    var order = basket.MakePayment(command.Payment);\n    return order;\n}\n</code></pre>\n<h3 id=\"aggregate\">Aggregate</h3>\n<p>To the <code>Basket</code> aggregate I added this method:</p>\n<pre><code>public IAggregate MakePayment(int payment)\n{\n    var expectedPayment = _items.Sum(y =&gt; y.DiscountedPrice);\n    if(expectedPayment != payment)\n        throw new UnexpectedPaymentException();\n    return new Order(Id);\n}\n</code></pre>\n<p>Also, to get this to work I had to create the <code>Order</code> aggregate:</p>\n<pre><code>internal class Order : AggregateBase\n{\n    public Order()\n    {\n        RegisterTransition&lt;OrderCreated&gt;(Apply);\n    }\n\n    private void Apply(OrderCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    internal Order(Guid basketId) : this()\n    {\n        RaiseEvent(new OrderCreated(IdGenerator.GenerateGuid(), basketId));\n    }\n}\n</code></pre>\n<h1 id=\"orderfeatures\">Order features</h1>\n<p>The <code>Order</code> aggregates responsibility in my application is to handle everything that has with the order to do, like if it is possible to cancel the order and keep track on if it is shipped. So let's start with last set of features.</p>\n<p>To spare myself some time I put all the tests in one file, and implemented all the <code>Order</code> features and not step by step as above. The test is also somewhat intertwined between cancelling and shipping so it might feel like the tests are out of order.</p>\n<p>There are also some minor changes to the <code>OrderCreated</code> event to include a fsharp list for the order items, this makes it much easier to compare two lists. The in-memory eventstore was also updated to use serializtion to verify that I can handle the serialization of the fsharp list.</p>\n<h2 id=\"tests\">Tests</h2>\n<p>Again I'm not testing every possible edge case, I just want the basic functionality up and running.</p>\n<pre><code>[TestFixture]\npublic class AllTheOrderTests : TestBase\n{\n    [Test]\n    public void WhenStartingShippingProcess_TheShippingShouldBeStarted()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId:  Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        When(new StartShippingProcess(id));\n        Then(new ShippingProcessStarted(id));\n    }\n\n    [Test]\n    public void WhenCancellingAnOrderThatHasntBeenStartedShipping_TheOrderShouldBeCancelled()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        When(new CancelOrder(id));\n        Then(new OrderCancelled(id));\n    }\n\n    [Test]\n    public void WhenTryingToStartShippingACancelledOrder_IShouldBeNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new OrderCancelled(id));\n        WhenThrows&lt;OrderCancelledException&gt;(new StartShippingProcess(id));\n    }\n\n    [Test]\n    public void WhenTryingToCancelAnOrderThatIsAboutToShip_IShouldBeNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new ShippingProcessStarted(id));\n        WhenThrows&lt;ShippingStartedException&gt;(new CancelOrder(id));\n    }\n\n    [Test]\n    public void WhenShippingAnOrderThatTheShippingProcessIsStarted_ItShouldBeMarkedAsShipped()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new ShippingProcessStarted(id));\n        When(new ShipOrder(id));\n        Then(new OrderShipped(id));\n    }\n\n    [Test]\n    public void WhenShippingAnOrderWhereShippingIsNotStarted_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        WhenThrows&lt;InvalidOrderState&gt;(new ShipOrder(id));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAnAmountLargerThan100000_TheOrderNeedsApproval()\n    {\n        var address = new Address(&quot;Valid street&quot;);\n        var basketId = Guid.NewGuid();\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () =&gt; orderId;\n        var orderLine = new OrderLine(Guid.NewGuid(), &quot;Ball&quot;, 100000, 100001, 1);\n        Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\n            new ItemAdded(basketId, orderLine),\n            new BasketCheckedOut(basketId, address));\n        When(new MakePayment(basketId, 100001));\n        Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new [] {orderLine})),\n            new NeedsApproval(orderId));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAnAmountLessThan100000_TheOrderIsAutomaticallyApproved()\n    {\n        var address = new Address(&quot;Valid street&quot;);\n        var basketId = Guid.NewGuid();\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () =&gt; orderId;\n        var orderLine = new OrderLine(Guid.NewGuid(), &quot;Ball&quot;, 100000, 100000, 1);\n        Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\n            new ItemAdded(basketId, orderLine),\n            new BasketCheckedOut(basketId, address));\n        When(new MakePayment(basketId, 100000));\n        Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new[] { orderLine })),\n            new OrderApproved(orderId));\n    }\n\n    [Test]\n    public void WhenApprovingAnOrder_ItShouldBeApproved()\n    {\n        var orderId = Guid.NewGuid();\n        Given(new OrderCreated(orderId, Guid.NewGuid(), FSharpList&lt;OrderLine&gt;.Empty));\n        When(new ApproveOrder(orderId));\n        Then(new OrderApproved(orderId));\n    }\n\n    private OrderCreated BuildOrderCreated(Guid orderId, Guid basketId, int numberOfOrderLines, int pricePerProduct = 100)\n    {\n        var orderLines = FSharpList&lt;OrderLine&gt;.Empty;\n        for (var i = 0; i &lt; numberOfOrderLines; i++)\n        {\n            orderLines = FSharpList&lt;OrderLine&gt;.Cons(new OrderLine(Guid.NewGuid(), &quot;Line &quot; + i, pricePerProduct, pricePerProduct, 1), orderLines);\n        }\n        return new OrderCreated(orderId, basketId, orderLines);\n    }\n}\n</code></pre>\n<h2 id=\"command\">Command</h2>\n<pre><code>type StartShippingProcess = { Id: Guid } with interface ICommand\ntype CancelOrder = { Id: Guid } with interface ICommand\ntype ShipOrder = { Id: Guid } with interface ICommand\ntype ApproveOrder = { Id: Guid } with interface ICommand\n</code></pre>\n<h2 id=\"event\">Event</h2>\n<pre><code>type OrderCreated ={ Id: Guid; BasketId: Guid; OrderLines: OrderLine list }\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype ShippingProcessStarted = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderCancelled = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderShipped = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype NeedsApproval = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderApproved = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n</code></pre>\n<h2 id=\"handler\">Handler</h2>\n<pre><code>internal class OrderHandler : \n    IHandle&lt;ApproveOrder&gt;, \n    IHandle&lt;StartShippingProcess&gt;, \n    IHandle&lt;CancelOrder&gt;, \n    IHandle&lt;ShipOrder&gt;\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public OrderHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(ApproveOrder command)\n    {\n        var order = _domainRepository.GetById&lt;Order&gt;(command.Id);\n        order.Approve();\n        return order;\n    }\n\n    public IAggregate Handle(StartShippingProcess command)\n    {\n        var order = _domainRepository.GetById&lt;Order&gt;(command.Id);\n        order.StartShippingProcess();\n        return order;\n    }\n\n    public IAggregate Handle(CancelOrder command)\n    {\n        var order = _domainRepository.GetById&lt;Order&gt;(command.Id);\n        order.Cancel();\n        return order;\n    }\n\n    public IAggregate Handle(ShipOrder command)\n    {\n        var order = _domainRepository.GetById&lt;Order&gt;(command.Id);\n        order.ShipOrder();\n        return order;\n    }\n}\n</code></pre>\n<h2 id=\"aggregate\">Aggregate</h2>\n<pre><code>internal class Order : AggregateBase\n{\n    private OrderState _orderState;\n\n    private enum OrderState\n    {\n        ShippingProcessStarted,\n        Created,\n        Cancelled\n    }\n\n    public Order()\n    {\n        RegisterTransition&lt;OrderCreated&gt;(Apply);\n        RegisterTransition&lt;ShippingProcessStarted&gt;(Apply);\n        RegisterTransition&lt;OrderCancelled&gt;(Apply);\n    }\n\n    private void Apply(OrderCancelled obj)\n    {\n        _orderState = OrderState.Cancelled;\n    }\n\n    private void Apply(ShippingProcessStarted obj)\n    {\n        _orderState = OrderState.ShippingProcessStarted;\n    }\n\n    private void Apply(OrderCreated obj)\n    {\n        _orderState = OrderState.Created;\n        Id = obj.Id;\n    }\n\n    internal Order(Guid basketId, FSharpList&lt;OrderLine&gt; orderLines) : this()\n    {\n        var id = IdGenerator.GenerateGuid();\n        RaiseEvent(new OrderCreated(id, basketId, orderLines));\n        var totalPrice = orderLines.Sum(y =&gt; y.DiscountedPrice);\n        if (totalPrice &gt; 100000)\n        {\n            RaiseEvent(new NeedsApproval(id));\n        }\n        else\n        {\n            RaiseEvent(new OrderApproved(id));\n        }\n    }\n\n    public void Approve()\n    {\n        RaiseEvent(new OrderApproved(Id));\n    }\n\n    public void StartShippingProcess()\n    {\n        if (_orderState == OrderState.Cancelled)\n            throw new OrderCancelledException();\n\n        RaiseEvent(new ShippingProcessStarted(Id));\n    }\n\n    public void Cancel()\n    {\n        if (_orderState == OrderState.Created)\n        {\n            RaiseEvent(new OrderCancelled(Id));\n        }\n        else\n        {\n            throw new ShippingStartedException();\n        }\n    }\n\n    public void ShipOrder()\n    {\n        if (_orderState != OrderState.ShippingProcessStarted)\n            throw new InvalidOrderState();\n        RaiseEvent(new OrderShipped(Id));\n    }\n}\n</code></pre>\n<p>And that finished the implementation of all the features.</p>\n<p>The part of the series is <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"33","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nSome starting notes\nFirst off, all the code is available to github. All I'm providing here is\nfeature by feature implementation and some comments.\n\nBefore we start implement the features we need to make a slight change to the \nTestBase class to get better exception handling when expecting exceptions. The \nWhenThrows method should be updated to this:\n\nprotected void WhenThrows<TException>(ICommand command) where TException : Exception\n{\n    try\n    {\n        When(command);\n        Assert.Fail(\"Expected exception \" + typeof(TException));\n    }\n    catch (TException)\n    {\n    }\n}\n\n\nThis will try to execute the command and only catch the exception we are\nexpecting, do we get another exception we get the correct stacktrace for that\nexception instead of a modified stacktrace.\n\nI'll try to group all the features to respective aggregate, and the be as\nconcise as possible focusing on the code. The format will be \"Feature group\" -->\n\"Feature\" --> \"Test\" and implementation \"Code\". When showing the code I won't\nshow the changes to the DomainEntry since they are just one or two, if any,\nlines for each feature.\n\nCustomer features\nWe're not 100 % done with the customer yet. As of the moment it is possible to\ncreate duplicate user, and that is something we need to prevent. And we also\nneed to add the functionality to make the customer preferred so they can get a\ndiscount.\n\nPreventing duplicate users\nWhen we are trying to create a user with an id that is not unique we should get\nan exception telling us what the problem is.\n\nTest\n[Test]\npublic void GivenAUserWithIdXExists_WhenCreatingACustomerWithIdX_IShouldGetNotifiedThatTheUserAlreadyExists()\n{\n    Guid id = Guid.NewGuid();\n    Given(new CustomerCreated(id, \"Something I don't care about\"));\n    WhenThrows<CustomerAlreadyExistsException>(new CreateCustomer(id, \"Tomas\"));\n}\n\n\nCode\nFirst we are missing an exception so the code won't even compile. So we add that\nclass to the \"Exceptions\" folder in the domain project.\n\npublic class CustomerAlreadyExistsException : Exception\n{\n    public CustomerAlreadyExistsException(Guid id, string name) : base(CreateMessage(id, name))\n    {\n        \n    }\n\n    private static string CreateMessage(Guid id, string name)\n    {\n        return string.Format(\"A customer with id {0} already exists, can't create customer for {1}\", id, name);\n    }\n}\n\n\nThe next thing is to actually implement the feature. To get the test green we\nneed to update our CustomerCommandHandler to make the duplicate check.\n\ninternal class CustomerCommandHandler : IHandle<CreateCustomer>\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public CustomerCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateCustomer command)\n    {\n        try\n        {\n            var customer = _domainRepository.GetById<Customer>(command.Id);\n            throw new CustomerAlreadyExistsException(command.Id, command.Name);\n        }\n        catch (AggregateNotFoundException)\n        {\n            // We expect not to find anything\n        }\n        return Customer.Create(command.Id, command.Name);\n    }\n}\n\n\nAdding those changes will make the build fail since we modified the constructor.\nTo make it build and all the tests green again also we need to update the code\nwhere we instantiate the CustomerCommandHandler in the DomainRepository. That's\na one line fix, so I won't even show it.\n\nMake customer preferred\nIt should be possible to mark a customer as preferred and also specify what\ndiscount the customer should have.\n\nTest\n[TestFixture]\npublic class MarkCustomerAsPreferredTest : TestBase\n{\n    [TestCase(25)]\n    [TestCase(50)]\n    [TestCase(70)]\n    public void GivenTheUserExists_WhenMarkingCustomerAsPreferred_ThenTheCustomerShouldBePreferred(int discount)\n    {\n        Guid id = Guid.NewGuid();\n        Given(new CustomerCreated(id, \"Superman\"));\n        When(new MarkCustomerAsPreferred(id, discount));\n        Then(new CustomerMarkedAsPreferred(id, discount));\n    }\n}\n\n\nCommand\ntype MarkCustomerAsPreferred = {Id: Guid; Discount: int } with interface ICommand\n\n\nEvent\ntype CustomerMarkedAsPreferred = {Id: Guid; Discount: int }\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\nThe following was added to the CustomerCommandHandler:\n\npublic IAggregate Handle(MarkCustomerAsPreferred command)\n{\n    var customer = _domainRepository.GetById<Customer>(command.Id);\n    customer.MakePreferred(command.Discount);\n    return customer;\n}\n\n\nThe aggregate\nAs of know we really don't need to take care of the state to get the test to\npass, all we need to do is to raise the event indicating that we have stored the\nchange. So the following should be added to the Customer class:\n\npublic void MakePreferred(int discount)\n{\n    RaiseEvent(new CustomerMarkedAsPreferred(Id, discount));\n}\n\n\nProduct features\nIt might be that product should be in a whole other system administrating just\nthe product and what you send in to this domain is the product id, name and\nprice when adding to the basket. Since this is just for show I included the\nproduct her. I've a really simplified web shop, you can only add products and\nwhen they've been added they will be there forever and will never run out. Also,\nthe only thing I need to prevent is duplicate insert as I did with customer.\n\nCreating a product\nYou should be able to add a product the same way as you can add a customer. To\nadd a product you need to provide the id, name and price. You can't add\nduplicate products.\n\nTests\nI'm using test cases to make sure the values aren't hard coded, except from that\neverything is straightforward.\n\n[TestFixture]\npublic class CreateProductTests : TestBase\n{\n    [TestCase(\"ball\", 1000)]\n    [TestCase(\"train\", 10000)]\n    [TestCase(\"universe\", 999999)]\n    public void WhenCreatingAProduct_TheProductShouldBeCreatedWithTheCorrectPrice(string productName, int price)\n    {\n        Guid id = Guid.NewGuid();\n        When(new CreateProduct(id, productName, price));\n        Then(new ProductCreated(id, productName, price));\n    }\n\n    [Test]\n    public void GivenProductXExists_WhenCreatingAProductWithIdX_IShouldGetNotifiedThatTheProductAlreadyExists()\n    {\n        Guid id = Guid.NewGuid();\n        Given(new ProductCreated(id, \"Something I don't care about\", 9999));\n        WhenThrows<ProductAlreadyExistsException>(new CreateProduct(id, \"Sugar\", 999));\n    }\n}\n\n\nCommand\ntype CreateProduct = {Id: Guid; Name: string; Price: int } with interface ICommand\n\n\nEvent\ntype ProductCreated = {Id: Guid; Name: string; Price: int }\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\ninternal class ProductCommandHandler : \n    IHandle<CreateProduct>\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public ProductCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateProduct command)\n    {\n        try\n        {\n            var product = _domainRepository.GetById<Product>(command.Id);\n            throw new ProductAlreadyExistsException(command.Id, command.Name);\n        }\n        catch (AggregateNotFoundException)\n        {\n            // We expect not to find anything\n        }\n        return Product.Create(command.Id, command.Name, command.Price);\n    }\n}\n\n\nAggregate\nTo get the test pass as of the moment I don't need to store the price and name.\n\ninternal class Product : AggregateBase\n{\n    public Product()\n    {\n        RegisterTransition<ProductCreated>(Apply);\n    }\n\n    private void Apply(ProductCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    private Product(Guid id, string name, int price) : this()\n    {\n        RaiseEvent(new ProductCreated(id, name, price));\n    }\n\n    public static IAggregate Create(Guid id, string name, int price)\n    {\n        return new Product(id, name, price);\n    }\n}\n\n\nShopping basket features\nI've chosen to put a lot of the functionality in the basket since the data\ncoming out from it should be used both by the ordering aggregate and to show the\nvalue of the basket. Keeping the pricing calculation and where the customer are\nin the shopping process here until the payment is made simplify a lot of things.\n\nCreate shopping basket\nWhen the shopping basket is created I send in the customer id. That is probably\nnot something you'll require in a real system since a customer should be able to\ncreate a basket without being logged in.\n\nTests\nThree tests for the basket creation. The happy path and the negative paths when\nthe customer doesn't exists or the basket already exists.\n\n[TestFixture]\npublic class CreateBasketTests : TestBase\n{\n    [Test]\n    public void GivenCustomerWithIdXExists_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 0;\n        string name = \"John doe\";\n        Given(new CustomerCreated(customerId, name));\n        When(new CreateBasket(id, customerId));\n        Then(new BasketCreated(id, customerId, discount));\n    }\n\n    [Test]\n    public void GivenNoCustomerWithIdXExists_WhenCreatingABasketForCustomerX_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        WhenThrows<AggregateNotFoundException>(new CreateBasket(id, customerId));\n    }\n\n    [Test]\n    public void GivenCustomerWithIdXExistsAndBasketAlreadyExistsForIdY_WhenCreatingABasketForCustomerXAndIdY_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        string name = \"John doe\";\n        int discount = 0;\n        Given(new BasketCreated(id, Guid.NewGuid(), discount),\n            new CustomerCreated(customerId, name));\n        WhenThrows<BasketAlreadExistsException>(new CreateBasket(id, customerId));\n    }\n\n    [Test]\n    public void GivenACustomerWithADiscount_CreatingABasketForTheCustomer_TheDiscountShouldBeIncluded()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 89;\n        string name = \"John doe\";\n        Given(new CustomerCreated(customerId, name),\n            new CustomerMarkedAsPreferred(customerId, discount));\n        When(new CreateBasket(id, customerId));\n        Then(new BasketCreated(id, customerId, discount));\n    }\n}\n\n\nCommand\ntype CreateBasket = { Id: Guid; CustomerId: Guid} with interface ICommand\n\n\nEvent\ntype BasketCreated = { Id: Guid; CustomerId: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\ninternal class BasketCommandHandler :\n    IHandle<CreateBasket>\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public BasketCommandHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(CreateBasket command)\n    {\n        try\n        {\n            var basket = _domainRepository.GetById<Basket>(command.Id);\n            throw new BasketAlreadExistsException(command.Id);\n        }\n        catch (AggregateNotFoundException)\n        {\n            //Expect this\n        }\n        var customer = _domainRepository.GetById<Customer>(command.CustomerId);\n        return Basket.Create(command.Id, customer);\n    }\n}\n\n\nAggregate\ninternal class Basket : AggregateBase\n{\n    private Basket(Guid id, Guid customerId, int discount) : this()\n    {\n        RaiseEvent(new BasketCreated(id, customerId, discount));\n    }\n\n    public Basket()\n    {\n        RegisterTransition<BasketCreated>(Apply);\n    }\n\n    private void Apply(BasketCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    public static IAggregate Create(Guid id, Customer customer)\n    {\n        return new Basket(id, customer.Id, customer.Discount);\n    }\n}\n\n\nAdd item to basket\nThere are two happy path to consider; adding an item for a customer with no\ndiscount and for a customer with discount. I don't consider all the negative\ncases to keep me a little bit shorter.\n\nTests\n[TestFixture]\npublic class AddItemToBasketTest : TestBase\n{\n    [TestCase(\"NameA\", 100, 10)]\n    [TestCase(\"NameB\", 200, 20)]\n    public void GivenWeHaveABasketForARegularCustomer_WhenAddingItems_ThePriceOfTheBasketShouldNotBeDiscounted(string productName, int itemPrice, int quantity)\n    {\n        var customerId = Guid.NewGuid();\n        var productId = Guid.NewGuid();\n        var id = Guid.NewGuid();\n        Given(new ProductCreated(productId, productName, itemPrice),\n            new BasketCreated(id, customerId, 0));\n        When(new AddItemToBasket(id, productId, quantity));\n        Then(new ItemAdded(id, productId, productName, itemPrice, itemPrice, quantity));\n    }\n\n    [TestCase(\"NameA\", 100, 10, 10, 90)]\n    [TestCase(\"NameB\", 200, 20, 80, 40)]\n    public void GivenWeHaveABasketForAPreferredCustomer_WhenAddingItems_ThePriceOfTheBasketShouldBeDiscounted(string productName, int itemPrice, int quantity, int discountPercentage, int discountedPrice)\n    {\n        var customerId = Guid.NewGuid();\n        var productId = Guid.NewGuid();\n        var id = Guid.NewGuid();\n        Given(new CustomerCreated(customerId, \"John Doe\"),\n            new CustomerMarkedAsPreferred(customerId, discountPercentage),\n            new ProductCreated(productId, productName, itemPrice),\n            new BasketCreated(id, customerId, discountPercentage));\n        When(new AddItemToBasket(id, productId, quantity));\n        Then(new ItemAdded(id, productId, productName, itemPrice, discountedPrice, quantity));\n    }\n}\n\n\nCommand\ntype AddItemToBasket = { Id: Guid; ProductId: Guid; Quantity: int } with interface ICommand\n\n\nEvent\nI had a minor issue when implementing the feature so I needed to add an\nimplementation of ToString so this event is a little more verbose but I thought\nI keep it for you to see.\n\ntype ItemAdded = { Id: Guid; ProductId: Guid; ProductName: string; OriginalPrice: int; DiscountedPrice: int; Quantity: int}\n    with \n    override this.ToString() = sprintf \"Item added. Id: %O, Price: %d, Discounted: %d, Quantity: %d\" this.Id this.OriginalPrice this.DiscountedPrice this.Quantity\n    interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\nThe handler is updated to handle the add item command:\n\npublic IAggregate Handle(AddItemToBasket command)\n{\n    var basket = _domainRepository.GetById<Basket>(command.Id);\n    var product = _domainRepository.GetById<Product>(command.ProductId);\n    basket.AddItem(product, command.Quantity);\n    return basket;\n}\n\n\nAggregate\nWe only need to add one method to the aggregate that adds the item and calculate\nthe price.\n\npublic void AddItem(Product product, int quantity)\n{\nvar discount = (int)(product.Price * ((double)_discount/100));\nvar discountedPrice = product.Price - discount;\nRaiseEvent(new ItemAdded(Id, product.Id, product.Name, product.Price,\ndiscountedPrice, quantity));\n}\n\nProceed to checkout\nThe proceed to checkout feature is just a way to keep track of the customer for\nfuture analytics purposes. When at the checkout the user can add shipping\naddress and proceed to payment.\n\nTest\nOne simple test for the happy path.\n\n[TestFixture]\npublic class ProceedCheckoutBasketTests : TestBase\n{\n    [Test]\n    public void GivenABasket_WhenCreatingABasketForCustomerX_ThenTheBasketShouldBeCreated()\n    {\n        var id = Guid.NewGuid();\n        var customerId = Guid.NewGuid();\n        int discount = 0;\n        Given(new BasketCreated(id, customerId, discount));\n        When(new ProceedToCheckout(id));\n        Then(new CustomerIsCheckingOutBasket(id));\n    }\n}\n\n\nCommand\ntype ProceedToCheckout = { Id: Guid } with interface ICommand\n\n\nEvent\ntype CustomerIsCheckingOutBasket = { Id: Guid }\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\nOne more method to the basket command handler:\n\npublic IAggregate Handle(ProceedToCheckout command)\n{\n    var basket = _domainRepository.GetById<Basket>(command.Id);\n    basket.ProceedToCheckout();\n    return basket;\n}\n\n\nAggregate\nThe new method added to the basket aggregate is also trivial.\n\npublic void ProceedToCheckout()\n{\n    RaiseEvent(new CustomerIsCheckingOutBasket(Id));\n}\n\n\nCheckout\nWhen the user is doing the actual checkout we need to collect the shipping\naddress before we proceed to payment, and the address must be specified.\n\nTest\n[TestFixture]\npublic class CheckoutBasketTests : TestBase\n{\n    [TestCase(null)]\n    [TestCase(\"\")]\n    [TestCase(\"    \")]\n    public void WhenTheUserCheckoutWithInvalidAddress_IShouldGetNotified(string street)\n    {\n        var address = street == null ? null : new Address(street);\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0));\n        WhenThrows<MissingAddressException>(new CheckoutBasket(id, address));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAValidAddress_IShouldProceedToTheNextStep()\n    {\n        var address = new Address(\"Valid street\");\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0));\n        When(new CheckoutBasket(id, address));\n        Then(new BasketCheckedOut(id, address));\n    }\n}\n\n\nTypes\nTo support address I defined a separate F# record for address and put in a\n\"types.fs\" file. This must be located above the other files and should include\nthe following:\n\nnamespace CQRSShop.Contracts.Types\n\ntype Address = { Street: string }\n\n\nCommand\nNow when I have the address type specified the command looks like this:\n\ntype CheckoutBasket = { Id: Guid; ShippingAddress: Address } with interface ICommand\n\n\nEvent\ntype BasketCheckedOut = { Id: Guid; ShippingAddress: Address } \n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\nOnce againt a simple method was added to the command handler class.\n\npublic IAggregate Handle(CheckoutBasket command)\n{\n    var basket = _domainRepository.GetById<Basket>(command.Id);\n    basket.Checkout(command.ShippingAddress);\n    return basket;\n}\n\n\nAggregate\nThe aggregate is where the logic to validate the input related to what makes a\nvalid checkout should be:\n\npublic void Checkout(Address shippingAddress)\n{\n    if(shippingAddress == null || string.IsNullOrWhiteSpace(shippingAddress.Street))\n        throw new MissingAddressException();\n    RaiseEvent(new BasketCheckedOut(Id, shippingAddress));\n}\n\n\nMake payment\nWhen a payment is made the result will be that an order is created, this order\ncan use the same id as the basket since they will be connected to each other and\nthere can only be one order per basket but many baskets can have no order.\n\nTests\nThe happy path for making a payment is a little bit different than the other\ntests. Here we're going to create another aggregate from the basket aggregate.\nThe tests will fail if we try to pay an unexpected amount. I haven't covered all\ncases here either. To get the test running as expected I needed control over the\nid generation, and to do so I wrote the IdGenerator class and put it in the\ninfrastructure project.\n\npublic class IdGenerator\n{\n    private static Func<Guid> _generator;\n\n    public static Func<Guid> GenerateGuid\n    {\n        get\n        {\n            _generator = _generator ?? Guid.NewGuid;\n            return _generator;\n        }\n        set { _generator = value; }\n    }\n}\n\n\nThe tests when you have that class is pretty straightforward:\n\n[TestFixture]\npublic class MakePaymentTests : TestBase\n{\n    [TestCase(100, 101)]\n    [TestCase(100, 99)]\n    [TestCase(100, 91)]\n    [TestCase(100, 89)]\n    public void WhenNotPayingTheExpectedAmount_IShouldGetNotified(int productPrice, int payment)\n    {\n        var id = Guid.NewGuid();\n        Given(new BasketCreated(id, Guid.NewGuid(), 0),\n            new ItemAdded(id, Guid.NewGuid(), \"\", productPrice, productPrice, 1));\n        WhenThrows<UnexpectedPaymentException>(new MakePayment(id, payment));\n    }\n\n    [TestCase(100, 101, 101)]\n    [TestCase(100, 80, 80)]\n    public void WhenPayingTheExpectedAmount_ThenANewOrderShouldBeCreatedFromTheResult(int productPrice, int discountPrice, int payment)\n    {\n        var id = Guid.NewGuid();\n        int dontCare = 0;\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () => orderId;\n        Given(new BasketCreated(id, Guid.NewGuid(), dontCare),\n            new ItemAdded(id, Guid.NewGuid(), \"Ball\", productPrice, discountPrice, 1));\n        When(new MakePayment(id, payment));\n        Then(new OrderCreated(orderId, id));\n    }\n}\n\n\nCommand\ntype MakePayment = {Id: Guid; Payment: int } with interface ICommand\n\n\nEvent\nThe event generated here is order created since that is what we expect when the\ncustomer has payed for the basket.\n\ntype OrderCreated ={ Id: Guid; BasketId: Guid }\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\nThis is a slitghtly different handler then the previous one. As you can see from\nthe code it returns an Order aggregate and that is what gets return from the\nhandler instead of a Basket aggregate.\n\npublic IAggregate Handle(MakePayment command)\n{\n    var basket = _domainRepository.GetById<Basket>(command.Id);\n    var order = basket.MakePayment(command.Payment);\n    return order;\n}\n\n\nAggregate\nTo the Basket aggregate I added this method:\n\npublic IAggregate MakePayment(int payment)\n{\n    var expectedPayment = _items.Sum(y => y.DiscountedPrice);\n    if(expectedPayment != payment)\n        throw new UnexpectedPaymentException();\n    return new Order(Id);\n}\n\n\nAlso, to get this to work I had to create the Order aggregate:\n\ninternal class Order : AggregateBase\n{\n    public Order()\n    {\n        RegisterTransition<OrderCreated>(Apply);\n    }\n\n    private void Apply(OrderCreated obj)\n    {\n        Id = obj.Id;\n    }\n\n    internal Order(Guid basketId) : this()\n    {\n        RaiseEvent(new OrderCreated(IdGenerator.GenerateGuid(), basketId));\n    }\n}\n\n\nOrder features\nThe Order aggregates responsibility in my application is to handle everything\nthat has with the order to do, like if it is possible to cancel the order and\nkeep track on if it is shipped. So let's start with last set of features.\n\nTo spare myself some time I put all the tests in one file, and implemented all\nthe Order features and not step by step as above. The test is also somewhat\nintertwined between cancelling and shipping so it might feel like the tests are\nout of order.\n\nThere are also some minor changes to the OrderCreated event to include a fsharp\nlist for the order items, this makes it much easier to compare two lists. The\nin-memory eventstore was also updated to use serializtion to verify that I can\nhandle the serialization of the fsharp list.\n\nTests\nAgain I'm not testing every possible edge case, I just want the basic\nfunctionality up and running.\n\n[TestFixture]\npublic class AllTheOrderTests : TestBase\n{\n    [Test]\n    public void WhenStartingShippingProcess_TheShippingShouldBeStarted()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId:  Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        When(new StartShippingProcess(id));\n        Then(new ShippingProcessStarted(id));\n    }\n\n    [Test]\n    public void WhenCancellingAnOrderThatHasntBeenStartedShipping_TheOrderShouldBeCancelled()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        When(new CancelOrder(id));\n        Then(new OrderCancelled(id));\n    }\n\n    [Test]\n    public void WhenTryingToStartShippingACancelledOrder_IShouldBeNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new OrderCancelled(id));\n        WhenThrows<OrderCancelledException>(new StartShippingProcess(id));\n    }\n\n    [Test]\n    public void WhenTryingToCancelAnOrderThatIsAboutToShip_IShouldBeNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new ShippingProcessStarted(id));\n        WhenThrows<ShippingStartedException>(new CancelOrder(id));\n    }\n\n    [Test]\n    public void WhenShippingAnOrderThatTheShippingProcessIsStarted_ItShouldBeMarkedAsShipped()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated,\n            new ShippingProcessStarted(id));\n        When(new ShipOrder(id));\n        Then(new OrderShipped(id));\n    }\n\n    [Test]\n    public void WhenShippingAnOrderWhereShippingIsNotStarted_IShouldGetNotified()\n    {\n        var id = Guid.NewGuid();\n        var orderCreated = BuildOrderCreated(id, basketId: Guid.NewGuid(), numberOfOrderLines: 1);\n        Given(orderCreated);\n        WhenThrows<InvalidOrderState>(new ShipOrder(id));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAnAmountLargerThan100000_TheOrderNeedsApproval()\n    {\n        var address = new Address(\"Valid street\");\n        var basketId = Guid.NewGuid();\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () => orderId;\n        var orderLine = new OrderLine(Guid.NewGuid(), \"Ball\", 100000, 100001, 1);\n        Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\n            new ItemAdded(basketId, orderLine),\n            new BasketCheckedOut(basketId, address));\n        When(new MakePayment(basketId, 100001));\n        Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new [] {orderLine})),\n            new NeedsApproval(orderId));\n    }\n\n    [Test]\n    public void WhenTheUserCheckoutWithAnAmountLessThan100000_TheOrderIsAutomaticallyApproved()\n    {\n        var address = new Address(\"Valid street\");\n        var basketId = Guid.NewGuid();\n        var orderId = Guid.NewGuid();\n        IdGenerator.GenerateGuid = () => orderId;\n        var orderLine = new OrderLine(Guid.NewGuid(), \"Ball\", 100000, 100000, 1);\n        Given(new BasketCreated(basketId, Guid.NewGuid(), 0),\n            new ItemAdded(basketId, orderLine),\n            new BasketCheckedOut(basketId, address));\n        When(new MakePayment(basketId, 100000));\n        Then(new OrderCreated(orderId, basketId, Helpers.ToFSharpList(new[] { orderLine })),\n            new OrderApproved(orderId));\n    }\n\n    [Test]\n    public void WhenApprovingAnOrder_ItShouldBeApproved()\n    {\n        var orderId = Guid.NewGuid();\n        Given(new OrderCreated(orderId, Guid.NewGuid(), FSharpList<OrderLine>.Empty));\n        When(new ApproveOrder(orderId));\n        Then(new OrderApproved(orderId));\n    }\n\n    private OrderCreated BuildOrderCreated(Guid orderId, Guid basketId, int numberOfOrderLines, int pricePerProduct = 100)\n    {\n        var orderLines = FSharpList<OrderLine>.Empty;\n        for (var i = 0; i < numberOfOrderLines; i++)\n        {\n            orderLines = FSharpList<OrderLine>.Cons(new OrderLine(Guid.NewGuid(), \"Line \" + i, pricePerProduct, pricePerProduct, 1), orderLines);\n        }\n        return new OrderCreated(orderId, basketId, orderLines);\n    }\n}\n\n\nCommand\ntype StartShippingProcess = { Id: Guid } with interface ICommand\ntype CancelOrder = { Id: Guid } with interface ICommand\ntype ShipOrder = { Id: Guid } with interface ICommand\ntype ApproveOrder = { Id: Guid } with interface ICommand\n\n\nEvent\ntype OrderCreated ={ Id: Guid; BasketId: Guid; OrderLines: OrderLine list }\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype ShippingProcessStarted = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderCancelled = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderShipped = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype NeedsApproval = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\ntype OrderApproved = {Id: Guid}\n    with interface IEvent with member this.Id with get() = this.Id\n\n\nHandler\ninternal class OrderHandler : \n    IHandle<ApproveOrder>, \n    IHandle<StartShippingProcess>, \n    IHandle<CancelOrder>, \n    IHandle<ShipOrder>\n{\n    private readonly IDomainRepository _domainRepository;\n\n    public OrderHandler(IDomainRepository domainRepository)\n    {\n        _domainRepository = domainRepository;\n    }\n\n    public IAggregate Handle(ApproveOrder command)\n    {\n        var order = _domainRepository.GetById<Order>(command.Id);\n        order.Approve();\n        return order;\n    }\n\n    public IAggregate Handle(StartShippingProcess command)\n    {\n        var order = _domainRepository.GetById<Order>(command.Id);\n        order.StartShippingProcess();\n        return order;\n    }\n\n    public IAggregate Handle(CancelOrder command)\n    {\n        var order = _domainRepository.GetById<Order>(command.Id);\n        order.Cancel();\n        return order;\n    }\n\n    public IAggregate Handle(ShipOrder command)\n    {\n        var order = _domainRepository.GetById<Order>(command.Id);\n        order.ShipOrder();\n        return order;\n    }\n}\n\n\nAggregate\ninternal class Order : AggregateBase\n{\n    private OrderState _orderState;\n\n    private enum OrderState\n    {\n        ShippingProcessStarted,\n        Created,\n        Cancelled\n    }\n\n    public Order()\n    {\n        RegisterTransition<OrderCreated>(Apply);\n        RegisterTransition<ShippingProcessStarted>(Apply);\n        RegisterTransition<OrderCancelled>(Apply);\n    }\n\n    private void Apply(OrderCancelled obj)\n    {\n        _orderState = OrderState.Cancelled;\n    }\n\n    private void Apply(ShippingProcessStarted obj)\n    {\n        _orderState = OrderState.ShippingProcessStarted;\n    }\n\n    private void Apply(OrderCreated obj)\n    {\n        _orderState = OrderState.Created;\n        Id = obj.Id;\n    }\n\n    internal Order(Guid basketId, FSharpList<OrderLine> orderLines) : this()\n    {\n        var id = IdGenerator.GenerateGuid();\n        RaiseEvent(new OrderCreated(id, basketId, orderLines));\n        var totalPrice = orderLines.Sum(y => y.DiscountedPrice);\n        if (totalPrice > 100000)\n        {\n            RaiseEvent(new NeedsApproval(id));\n        }\n        else\n        {\n            RaiseEvent(new OrderApproved(id));\n        }\n    }\n\n    public void Approve()\n    {\n        RaiseEvent(new OrderApproved(Id));\n    }\n\n    public void StartShippingProcess()\n    {\n        if (_orderState == OrderState.Cancelled)\n            throw new OrderCancelledException();\n\n        RaiseEvent(new ShippingProcessStarted(Id));\n    }\n\n    public void Cancel()\n    {\n        if (_orderState == OrderState.Created)\n        {\n            RaiseEvent(new OrderCancelled(Id));\n        }\n        else\n        {\n            throw new ShippingStartedException();\n        }\n    }\n\n    public void ShipOrder()\n    {\n        if (_orderState != OrderState.ShippingProcessStarted)\n            throw new InvalidOrderState();\n        RaiseEvent(new OrderShipped(Id));\n    }\n}\n\n\nAnd that finished the implementation of all the features.\n\nThe part of the series is Time for reflection\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-26T05:13:18.000Z","updated_at":"2014-07-16T06:13:26.000Z","published_at":"2014-06-27T17:24:26.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe055","uuid":"0401138e-a0e8-47db-93bf-82cf92f12397","title":"CQRS the simple way with eventstore and elasticsearch: Time for reflection","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post since it is too much for one cover, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * Time for reflection\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#So what did just happen in the previous post?\\nIn the last [post](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/) I implemented all the features for the application, but what was special with the implementation? Here I'll list the positive effects we've seen so far.\\n\\n##CQRS is divide and conquer on an architectural level\\nA lot of the positive effects we see is because of CQRS. You can see CQRS as a divide and conquer algorithm on an architectural level. Since we are applying CQRS to the system it allows us to focus on just the behavior when implementing the functionality in the domain. When a feature is implemented we can switch focus to how the result should be presented for the user, which I'll do in a post or two. \\n\\nThis divide and conquering technique is really powerful since we get less distraction when implementing a feature, and the same thing applies when we implement the views. Of course you have to know how to put the system together afterwards, which can seem like extra complexity, but you have to compare it to the alternative. If you're doing read and write at the same time you're handling two problems at the same time that might have two complete different models, or even worse, one model squeezed into the another model where it is just making a mess.\\n\\n##No side effects\\nWhen working with events rather than state we are actually testing for absence of side effects as the same time as we are testing a feature. If we would get one event we are not expecting that should result in at least one failing test.\\n\\nTesting the absence of side effects is something that is almost impossible in a system where you store the state instead of the events, since that basically would require you to check the state of the whole system. It is doable, but is really hard.\\n\\n##Focus on behavior rather than the data\\nYou can't ignore the data 100 % but the focus will shift from the data to the behavior and the intention of the application. When implementing the featueres the focus was all the time on what the result of an action should be instead of what the new state should be. This will make it easier to talk to the business side since they can describe the actions and what should happen. Often the business side really don't care about the state, since the state often is a technical representation rather than connected to the business.\\n\\nThe state should only be used as data for the user to make the next action.\\n\\n##Events > state\\nIf you have the events you can derive the state, but it is not possible to derive the events from the state. Why is this good? This is invaluable to fix really critical bugs in a system. If you find a invalid state somewhere in a system it is possible to find the pattern within the event stream that lead up to that state, but if you only have the state it might be impossible to derive what actually caused that invalid state and you won't be able to fix the problem except for that specific case.\\n\\n##No mapping\\nI'm not sure if you noticed, but I basically haven't written a single line of mapping code between a database and my application. You could consider the state transitions in the aggregates are some sort of mapping, but I don't see it that way since the events are first class citizens of the domain. I only have one repository and that can get and save all types of aggregates in my domain, compared to one repository for every single type or complex repositories for weird constructs.\\n\\n##Single point of entry\\nSince we only have one point of entry, the testing base class is made really simple and all my against the domain have the same structure. This also make it possible, as mentioned before, to add things generic functions that should be applied before every command in an easy manner. \\n\\n##F# is a powerful language\\nIf you followed so far you've seen that F# make a perfect sort of \\\"dsl\\\" for us. Since we can express the commands and events in such a compact and powerful way it is really easy to grasp the functionality of the system since we can see them all on one page.\\n\\n##What is the future?\\nI'll finish this blog series, but after that I'll consider rewriting the code in 100% F# instead of a mixture. I think it will be a great fit, but I thought I would finish this first.\\n\\nNow when all the features is implemented it is time to implement the api, and that will be done in [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post since it is too much for one cover, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li>Time for reflection</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"sowhatdidjusthappeninthepreviouspost\">So what did just happen in the previous post?</h1>\n<p>In the last <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">post</a> I implemented all the features for the application, but what was special with the implementation? Here I'll list the positive effects we've seen so far.</p>\n<h2 id=\"cqrsisdivideandconqueronanarchitecturallevel\">CQRS is divide and conquer on an architectural level</h2>\n<p>A lot of the positive effects we see is because of CQRS. You can see CQRS as a divide and conquer algorithm on an architectural level. Since we are applying CQRS to the system it allows us to focus on just the behavior when implementing the functionality in the domain. When a feature is implemented we can switch focus to how the result should be presented for the user, which I'll do in a post or two.</p>\n<p>This divide and conquering technique is really powerful since we get less distraction when implementing a feature, and the same thing applies when we implement the views. Of course you have to know how to put the system together afterwards, which can seem like extra complexity, but you have to compare it to the alternative. If you're doing read and write at the same time you're handling two problems at the same time that might have two complete different models, or even worse, one model squeezed into the another model where it is just making a mess.</p>\n<h2 id=\"nosideeffects\">No side effects</h2>\n<p>When working with events rather than state we are actually testing for absence of side effects as the same time as we are testing a feature. If we would get one event we are not expecting that should result in at least one failing test.</p>\n<p>Testing the absence of side effects is something that is almost impossible in a system where you store the state instead of the events, since that basically would require you to check the state of the whole system. It is doable, but is really hard.</p>\n<h2 id=\"focusonbehaviorratherthanthedata\">Focus on behavior rather than the data</h2>\n<p>You can't ignore the data 100 % but the focus will shift from the data to the behavior and the intention of the application. When implementing the featueres the focus was all the time on what the result of an action should be instead of what the new state should be. This will make it easier to talk to the business side since they can describe the actions and what should happen. Often the business side really don't care about the state, since the state often is a technical representation rather than connected to the business.</p>\n<p>The state should only be used as data for the user to make the next action.</p>\n<h2 id=\"eventsstate\">Events &gt; state</h2>\n<p>If you have the events you can derive the state, but it is not possible to derive the events from the state. Why is this good? This is invaluable to fix really critical bugs in a system. If you find a invalid state somewhere in a system it is possible to find the pattern within the event stream that lead up to that state, but if you only have the state it might be impossible to derive what actually caused that invalid state and you won't be able to fix the problem except for that specific case.</p>\n<h2 id=\"nomapping\">No mapping</h2>\n<p>I'm not sure if you noticed, but I basically haven't written a single line of mapping code between a database and my application. You could consider the state transitions in the aggregates are some sort of mapping, but I don't see it that way since the events are first class citizens of the domain. I only have one repository and that can get and save all types of aggregates in my domain, compared to one repository for every single type or complex repositories for weird constructs.</p>\n<h2 id=\"singlepointofentry\">Single point of entry</h2>\n<p>Since we only have one point of entry, the testing base class is made really simple and all my against the domain have the same structure. This also make it possible, as mentioned before, to add things generic functions that should be applied before every command in an easy manner.</p>\n<h2 id=\"fisapowerfullanguage\">F# is a powerful language</h2>\n<p>If you followed so far you've seen that F# make a perfect sort of &quot;dsl&quot; for us. Since we can express the commands and events in such a compact and powerful way it is really easy to grasp the functionality of the system since we can see them all on one page.</p>\n<h2 id=\"whatisthefuture\">What is the future?</h2>\n<p>I'll finish this blog series, but after that I'll consider rewriting the code in 100% F# instead of a mixture. I think it will be a great fit, but I thought I would finish this first.</p>\n<p>Now when all the features is implemented it is time to implement the api, and that will be done in <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"34","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post since it is too much for one cover, where\nI will walk you through different stages of the implementation of a simple web\nshop. Everything will be implemented in C# and F# to start with but I might\nchange to 100 % F#, but that is something that will come later. The code will be\navailable in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nSo what did just happen in the previous post?\nIn the last post\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/] \nI implemented all the features for the application, but what was special with\nthe implementation? Here I'll list the positive effects we've seen so far.\n\nCQRS is divide and conquer on an architectural level\nA lot of the positive effects we see is because of CQRS. You can see CQRS as a\ndivide and conquer algorithm on an architectural level. Since we are applying\nCQRS to the system it allows us to focus on just the behavior when implementing\nthe functionality in the domain. When a feature is implemented we can switch\nfocus to how the result should be presented for the user, which I'll do in a\npost or two.\n\nThis divide and conquering technique is really powerful since we get less\ndistraction when implementing a feature, and the same thing applies when we\nimplement the views. Of course you have to know how to put the system together\nafterwards, which can seem like extra complexity, but you have to compare it to\nthe alternative. If you're doing read and write at the same time you're handling\ntwo problems at the same time that might have two complete different models, or\neven worse, one model squeezed into the another model where it is just making a\nmess.\n\nNo side effects\nWhen working with events rather than state we are actually testing for absence\nof side effects as the same time as we are testing a feature. If we would get\none event we are not expecting that should result in at least one failing test.\n\nTesting the absence of side effects is something that is almost impossible in a\nsystem where you store the state instead of the events, since that basically\nwould require you to check the state of the whole system. It is doable, but is\nreally hard.\n\nFocus on behavior rather than the data\nYou can't ignore the data 100 % but the focus will shift from the data to the\nbehavior and the intention of the application. When implementing the featueres\nthe focus was all the time on what the result of an action should be instead of\nwhat the new state should be. This will make it easier to talk to the business\nside since they can describe the actions and what should happen. Often the\nbusiness side really don't care about the state, since the state often is a\ntechnical representation rather than connected to the business.\n\nThe state should only be used as data for the user to make the next action.\n\nEvents > state\nIf you have the events you can derive the state, but it is not possible to\nderive the events from the state. Why is this good? This is invaluable to fix\nreally critical bugs in a system. If you find a invalid state somewhere in a\nsystem it is possible to find the pattern within the event stream that lead up\nto that state, but if you only have the state it might be impossible to derive\nwhat actually caused that invalid state and you won't be able to fix the problem\nexcept for that specific case.\n\nNo mapping\nI'm not sure if you noticed, but I basically haven't written a single line of\nmapping code between a database and my application. You could consider the state\ntransitions in the aggregates are some sort of mapping, but I don't see it that\nway since the events are first class citizens of the domain. I only have one\nrepository and that can get and save all types of aggregates in my domain,\ncompared to one repository for every single type or complex repositories for\nweird constructs.\n\nSingle point of entry\nSince we only have one point of entry, the testing base class is made really\nsimple and all my against the domain have the same structure. This also make it\npossible, as mentioned before, to add things generic functions that should be\napplied before every command in an easy manner.\n\nF# is a powerful language\nIf you followed so far you've seen that F# make a perfect sort of \"dsl\" for us.\nSince we can express the commands and events in such a compact and powerful way\nit is really easy to grasp the functionality of the system since we can see them\nall on one page.\n\nWhat is the future?\nI'll finish this blog series, but after that I'll consider rewriting the code in\n100% F# instead of a mixture. I think it will be a great fit, but I thought I\nwould finish this first.\n\nNow when all the features is implemented it is time to implement the api, and\nthat will be done in Building the API with Simple.Web\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-27T18:47:04.000Z","updated_at":"2014-07-16T06:14:13.000Z","published_at":"2014-06-27T18:47:11.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe056","uuid":"4f81fdfa-aa83-4a4a-b65a-03e4d53922d6","title":"CQRS the simple way with eventstore and elasticsearch: Build the API with simple.web","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * Building the API with Simple.Web\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#What are we doing?\\nI'm not planning to build a nice looking UI, but we'll see. The user interface will be the API as of the moment, and the views will later on be what is stored in elasticsearch. So here I'm planning to show you how to write the API with [Simple.Web](https://github.com/markrendle/Simple.Web). \\n\\n##The application setup\\nAs I wrote above I'm going to use Simple.Web on top of [OWIN](http://owin.org/) as web framework. To get started you need to install a couple of nuget packages:\\n \\n * Microsoft.Owin.Host.SystemWeb\\n * Simple.Web.JsonNet\\n \\nThose two packages should bring all the dependencies needed.\\n\\nEvery OWIN application need a setup class and that is what is following: \\n\\n    [assembly: OwinStartup(typeof(OwinAppSetup))]\\n    namespace CQRSShop.Web\\n    {\\n        public class OwinAppSetup\\n        {\\n            public static Type[] EnforceReferencesFor =\\n                    {\\n                        typeof (Simple.Web.JsonNet.JsonMediaTypeHandler)\\n                    };\\n\\n            public void Configuration(IAppBuilder app)\\n            {\\n                JsonConvert.DefaultSettings = () => new JsonSerializerSettings()\\n                {\\n                    TypeNameHandling = TypeNameHandling.Objects\\n                };\\n\\n                app.Run(context => Application.App(_ =>\\n                {\\n                    var task = context.Response.WriteAsync(\\\"Hello world!\\\");\\n                    return task;\\n                })(context.Environment));\\n            }\\n        }\\n    }\\n\\nFirst of all I'm forcing some references since Simple.Web need it for serialization. After that we have the acutal application setup with serialization settings and then starting the application with `app.Run`. `Application.App` is from the Simple.Web framework and if the request is not handled by the framework it will execute the inner function writing \\\"Hello world!\\\" to the output.\\n\\n##Writing the base handler\\nThe design we have chose allow it for us to have one handler that can handler all the commands, yes I'm exposing the commands straight from the API and leaving the id generation to the client. How bad can it be you might ask? Not so bad if you think about it, as long as you have a check that the id is unique for the aggregate you are creating. And since I'm using guids they are most likely unique and hopefully the clients use a proper guid generation framework. Also, are command doesn't return anything so that also simplifies everything.\\n\\nEnough said, here is the base handler: \\n\\n    public abstract class BasePostEndpoint<TCommand> : IPost, IInput<TCommand> where TCommand : ICommand\\n    {\\n        public Status Post()\\n        {\\n            try\\n            {\\n                var connection = Configuration.CreateConnection();\\n                var domainRepository = new EventStoreDomainRepository(connection);\\n                var application = new DomainEntry(domainRepository);\\n                application.ExecuteCommand(Input);\\n            }\\n            catch (Exception)\\n            {\\n                return Status.InternalServerError;\\n            }\\n\\n            return Status.OK;\\n        }\\n\\n        public TCommand Input { set; private get; }\\n    }\\n    \\nIn a Simple.Web language that code says an endpoint implementing this class will accept `POST` with the payload of type `TCommand`. If everything went as expected 200 is returned from the API, otherwise it will be a 400 error. This code could be improved if I would have a common `DomainException` and `ValidationException` to create better error responses, but this is not the focus of here.\\n\\n##Writing a command handler\\nThis is going to be remarkably simple since all my endpoints will all have the same structure:\\n\\n    [UriTemplate(\\\"/api/customer\\\")]\\n    public class PostEndpoint : BasePostEndpoint<CreateCustomer>\\n    {\\n         \\n    }\\n\\nFirst we have the `UriTemplate` that specify where the endpoint is located. Then we have the implementation... oh wait, everything is implemented in the base class! Let's look at a more complex scenario:\\n\\n    [UriTemplate(\\\"/api/basket/{BasketId}/items\\\")]\\n    public class PostEndpoint : BasePostEndpoint<AddItemToBasket>\\n    {\\n         \\n    }\\n\\nThe reason this is more complex is because I have a little bit more complex `UriTemplate`. Again, there might be some improvements that can be done when I have code like this. As of the moment the `BasketId` parameter isn't validated and there is no check that the parameter has the same value as in the command.\\n\\n##Something extra\\nI did some extra work just to show you how you can get this API discoverable, and with Simple.Web that is really simple. So let us start with root url \\\"/api\\\". \\n\\n    [UriTemplate(\\\"/api\\\")]\\n    public class GetEndpoint : IGet, IOutput<IEnumerable<Link>>\\n    {\\n        public Status Get()\\n        {\\n            Output = LinkHelper.GetRootLinks();\\n            return 200;\\n        }\\n\\n        public IEnumerable<Link> Output { get; set; }\\n    }\\n\\nNow if we visit \\\"/api\\\" we'll see all the root links of the application. So let's implement a sample root link.\\n\\n    [UriTemplate(\\\"/api/product\\\")]\\n    [Root(Rel = \\\"product\\\", Title = \\\"Product\\\", Type = \\\"application/vnd.cqrsshop.createproduct\\\")]\\n    public class PostEndpoint : BasePostEndpoint<CreateProduct>\\n    {\\n         \\n    }\\n\\nAdding that extra `Root` attribute to the `PostEndpoint` will make it appear as a link when you visit the \\\"/api\\\" url. I didn't do this for the whole application since it requires some strategy for links under roots etc, but hopefully you get the idea and get inspired and try it out.\\n\\nWith the api finished we need to create the integration with elasticsearch, you can read about that in [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li>Building the API with Simple.Web</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"whatarewedoing\">What are we doing?</h1>\n<p>I'm not planning to build a nice looking UI, but we'll see. The user interface will be the API as of the moment, and the views will later on be what is stored in elasticsearch. So here I'm planning to show you how to write the API with <a href=\"https://github.com/markrendle/Simple.Web\">Simple.Web</a>.</p>\n<h2 id=\"theapplicationsetup\">The application setup</h2>\n<p>As I wrote above I'm going to use Simple.Web on top of <a href=\"http://owin.org/\">OWIN</a> as web framework. To get started you need to install a couple of nuget packages:</p>\n<ul>\n<li>Microsoft.Owin.Host.SystemWeb</li>\n<li>Simple.Web.JsonNet</li>\n</ul>\n<p>Those two packages should bring all the dependencies needed.</p>\n<p>Every OWIN application need a setup class and that is what is following:</p>\n<pre><code>[assembly: OwinStartup(typeof(OwinAppSetup))]\nnamespace CQRSShop.Web\n{\n    public class OwinAppSetup\n    {\n        public static Type[] EnforceReferencesFor =\n                {\n                    typeof (Simple.Web.JsonNet.JsonMediaTypeHandler)\n                };\n\n        public void Configuration(IAppBuilder app)\n        {\n            JsonConvert.DefaultSettings = () =&gt; new JsonSerializerSettings()\n            {\n                TypeNameHandling = TypeNameHandling.Objects\n            };\n\n            app.Run(context =&gt; Application.App(_ =&gt;\n            {\n                var task = context.Response.WriteAsync(&quot;Hello world!&quot;);\n                return task;\n            })(context.Environment));\n        }\n    }\n}\n</code></pre>\n<p>First of all I'm forcing some references since Simple.Web need it for serialization. After that we have the acutal application setup with serialization settings and then starting the application with <code>app.Run</code>. <code>Application.App</code> is from the Simple.Web framework and if the request is not handled by the framework it will execute the inner function writing &quot;Hello world!&quot; to the output.</p>\n<h2 id=\"writingthebasehandler\">Writing the base handler</h2>\n<p>The design we have chose allow it for us to have one handler that can handler all the commands, yes I'm exposing the commands straight from the API and leaving the id generation to the client. How bad can it be you might ask? Not so bad if you think about it, as long as you have a check that the id is unique for the aggregate you are creating. And since I'm using guids they are most likely unique and hopefully the clients use a proper guid generation framework. Also, are command doesn't return anything so that also simplifies everything.</p>\n<p>Enough said, here is the base handler:</p>\n<pre><code>public abstract class BasePostEndpoint&lt;TCommand&gt; : IPost, IInput&lt;TCommand&gt; where TCommand : ICommand\n{\n    public Status Post()\n    {\n        try\n        {\n            var connection = Configuration.CreateConnection();\n            var domainRepository = new EventStoreDomainRepository(connection);\n            var application = new DomainEntry(domainRepository);\n            application.ExecuteCommand(Input);\n        }\n        catch (Exception)\n        {\n            return Status.InternalServerError;\n        }\n\n        return Status.OK;\n    }\n\n    public TCommand Input { set; private get; }\n}\n</code></pre>\n<p>In a Simple.Web language that code says an endpoint implementing this class will accept <code>POST</code> with the payload of type <code>TCommand</code>. If everything went as expected 200 is returned from the API, otherwise it will be a 400 error. This code could be improved if I would have a common <code>DomainException</code> and <code>ValidationException</code> to create better error responses, but this is not the focus of here.</p>\n<h2 id=\"writingacommandhandler\">Writing a command handler</h2>\n<p>This is going to be remarkably simple since all my endpoints will all have the same structure:</p>\n<pre><code>[UriTemplate(&quot;/api/customer&quot;)]\npublic class PostEndpoint : BasePostEndpoint&lt;CreateCustomer&gt;\n{\n     \n}\n</code></pre>\n<p>First we have the <code>UriTemplate</code> that specify where the endpoint is located. Then we have the implementation... oh wait, everything is implemented in the base class! Let's look at a more complex scenario:</p>\n<pre><code>[UriTemplate(&quot;/api/basket/{BasketId}/items&quot;)]\npublic class PostEndpoint : BasePostEndpoint&lt;AddItemToBasket&gt;\n{\n     \n}\n</code></pre>\n<p>The reason this is more complex is because I have a little bit more complex <code>UriTemplate</code>. Again, there might be some improvements that can be done when I have code like this. As of the moment the <code>BasketId</code> parameter isn't validated and there is no check that the parameter has the same value as in the command.</p>\n<h2 id=\"somethingextra\">Something extra</h2>\n<p>I did some extra work just to show you how you can get this API discoverable, and with Simple.Web that is really simple. So let us start with root url &quot;/api&quot;.</p>\n<pre><code>[UriTemplate(&quot;/api&quot;)]\npublic class GetEndpoint : IGet, IOutput&lt;IEnumerable&lt;Link&gt;&gt;\n{\n    public Status Get()\n    {\n        Output = LinkHelper.GetRootLinks();\n        return 200;\n    }\n\n    public IEnumerable&lt;Link&gt; Output { get; set; }\n}\n</code></pre>\n<p>Now if we visit &quot;/api&quot; we'll see all the root links of the application. So let's implement a sample root link.</p>\n<pre><code>[UriTemplate(&quot;/api/product&quot;)]\n[Root(Rel = &quot;product&quot;, Title = &quot;Product&quot;, Type = &quot;application/vnd.cqrsshop.createproduct&quot;)]\npublic class PostEndpoint : BasePostEndpoint&lt;CreateProduct&gt;\n{\n     \n}\n</code></pre>\n<p>Adding that extra <code>Root</code> attribute to the <code>PostEndpoint</code> will make it appear as a link when you visit the &quot;/api&quot; url. I didn't do this for the whole application since it requires some strategy for links under roots etc, but hopefully you get the idea and get inspired and try it out.</p>\n<p>With the api finished we need to create the integration with elasticsearch, you can read about that in <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"35","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nWhat are we doing?\nI'm not planning to build a nice looking UI, but we'll see. The user interface\nwill be the API as of the moment, and the views will later on be what is stored\nin elasticsearch. So here I'm planning to show you how to write the API with \nSimple.Web [https://github.com/markrendle/Simple.Web].\n\nThe application setup\nAs I wrote above I'm going to use Simple.Web on top of OWIN [http://owin.org/] \nas web framework. To get started you need to install a couple of nuget packages:\n\n * Microsoft.Owin.Host.SystemWeb\n * Simple.Web.JsonNet\n\nThose two packages should bring all the dependencies needed.\n\nEvery OWIN application need a setup class and that is what is following:\n\n[assembly: OwinStartup(typeof(OwinAppSetup))]\nnamespace CQRSShop.Web\n{\n    public class OwinAppSetup\n    {\n        public static Type[] EnforceReferencesFor =\n                {\n                    typeof (Simple.Web.JsonNet.JsonMediaTypeHandler)\n                };\n\n        public void Configuration(IAppBuilder app)\n        {\n            JsonConvert.DefaultSettings = () => new JsonSerializerSettings()\n            {\n                TypeNameHandling = TypeNameHandling.Objects\n            };\n\n            app.Run(context => Application.App(_ =>\n            {\n                var task = context.Response.WriteAsync(\"Hello world!\");\n                return task;\n            })(context.Environment));\n        }\n    }\n}\n\n\nFirst of all I'm forcing some references since Simple.Web need it for\nserialization. After that we have the acutal application setup with\nserialization settings and then starting the application with app.Run. \nApplication.App is from the Simple.Web framework and if the request is not\nhandled by the framework it will execute the inner function writing \"Hello\nworld!\" to the output.\n\nWriting the base handler\nThe design we have chose allow it for us to have one handler that can handler\nall the commands, yes I'm exposing the commands straight from the API and\nleaving the id generation to the client. How bad can it be you might ask? Not so\nbad if you think about it, as long as you have a check that the id is unique for\nthe aggregate you are creating. And since I'm using guids they are most likely\nunique and hopefully the clients use a proper guid generation framework. Also,\nare command doesn't return anything so that also simplifies everything.\n\nEnough said, here is the base handler:\n\npublic abstract class BasePostEndpoint<TCommand> : IPost, IInput<TCommand> where TCommand : ICommand\n{\n    public Status Post()\n    {\n        try\n        {\n            var connection = Configuration.CreateConnection();\n            var domainRepository = new EventStoreDomainRepository(connection);\n            var application = new DomainEntry(domainRepository);\n            application.ExecuteCommand(Input);\n        }\n        catch (Exception)\n        {\n            return Status.InternalServerError;\n        }\n\n        return Status.OK;\n    }\n\n    public TCommand Input { set; private get; }\n}\n\n\nIn a Simple.Web language that code says an endpoint implementing this class will\naccept POST with the payload of type TCommand. If everything went as expected\n200 is returned from the API, otherwise it will be a 400 error. This code could\nbe improved if I would have a common DomainException and ValidationException to\ncreate better error responses, but this is not the focus of here.\n\nWriting a command handler\nThis is going to be remarkably simple since all my endpoints will all have the\nsame structure:\n\n[UriTemplate(\"/api/customer\")]\npublic class PostEndpoint : BasePostEndpoint<CreateCustomer>\n{\n     \n}\n\n\nFirst we have the UriTemplate that specify where the endpoint is located. Then\nwe have the implementation... oh wait, everything is implemented in the base\nclass! Let's look at a more complex scenario:\n\n[UriTemplate(\"/api/basket/{BasketId}/items\")]\npublic class PostEndpoint : BasePostEndpoint<AddItemToBasket>\n{\n     \n}\n\n\nThe reason this is more complex is because I have a little bit more complex \nUriTemplate. Again, there might be some improvements that can be done when I\nhave code like this. As of the moment the BasketId parameter isn't validated and\nthere is no check that the parameter has the same value as in the command.\n\nSomething extra\nI did some extra work just to show you how you can get this API discoverable,\nand with Simple.Web that is really simple. So let us start with root url \"/api\".\n\n[UriTemplate(\"/api\")]\npublic class GetEndpoint : IGet, IOutput<IEnumerable<Link>>\n{\n    public Status Get()\n    {\n        Output = LinkHelper.GetRootLinks();\n        return 200;\n    }\n\n    public IEnumerable<Link> Output { get; set; }\n}\n\n\nNow if we visit \"/api\" we'll see all the root links of the application. So let's\nimplement a sample root link.\n\n[UriTemplate(\"/api/product\")]\n[Root(Rel = \"product\", Title = \"Product\", Type = \"application/vnd.cqrsshop.createproduct\")]\npublic class PostEndpoint : BasePostEndpoint<CreateProduct>\n{\n     \n}\n\n\nAdding that extra Root attribute to the PostEndpoint will make it appear as a\nlink when you visit the \"/api\" url. I didn't do this for the whole application\nsince it requires some strategy for links under roots etc, but hopefully you get\nthe idea and get inspired and try it out.\n\nWith the api finished we need to create the integration with elasticsearch, you\ncan read about that in Integrating elasticsearch\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-27T20:44:53.000Z","updated_at":"2014-07-16T06:14:55.000Z","published_at":"2014-06-27T20:44:56.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe057","uuid":"0da8027d-49eb-40ff-81cf-16d2379bff3e","title":"CQRS the simple way with eventstore and elasticsearch: Integrating Elasticsearch","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * Integrating elasticsearch\\n * [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/)\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Time to create the views\\nIn most system you need to be able to show the data to the user in one way, so in this example I've decided to use elasticsearch. The update of the views will be taken care of by a separate service, which also make that the views will be eventual consistent. In this specific sample it will be consisten within less than half a second if everything is up and running and the service is connected to the eventstore to listen for changes. \\n\\n##What do we want a view of?\\nSince we have access to a complete audit log of the system we can basically create any view we can think of so we just have to think of a few. The views I choose to create to illustrate the integration is a list of the products so we can search for products, the customers and baskets. For the basket, customer and the orders a graph database like neo4j might make more sense so I might just do that in a future post but for now we put everything except the orders in elasticsearch. And since elasticsearch is a document/search database using lucene it will be really fast to lookup a single document as well as doing full text search.\\n\\n##Disclamer\\nSince this is a demo I'll show you a quick way of doing things which you might not want to do in a production system. I won't store the last read position in the stream, but instead rebuilding the views every single time the service starts. That is doable for small amount of data, but probably not something you want.\\n\\n##Creataing the hosting service with topshelf\\nTo make it easier to host and debug the service I'll use [topshelf](http://topshelf-project.com/), which is an great framework to create windows services. To get started with topshelf you just create a console application and then install the `Topshelf` nuget package. When topshelf is added to your console project update your \\\"Program.cs\\\" to look something like this: \\n\\n    class Program\\n    {\\n        public static void Main(string[] args)\\n        {\\n            HostFactory.Run(x =>\\n            {\\n                x.Service<IndexingServie>(s =>\\n                {\\n                    s.ConstructUsing(name => new IndexingServie());\\n                    s.WhenStarted(tc => tc.Start());\\n                    s.WhenStopped(tc => tc.Stop());\\n                });\\n                x.RunAsLocalSystem();\\n\\n                x.SetDescription(\\\"CQRSShop.Service\\\");\\n                x.SetDisplayName(\\\"CQRSShop.Service\\\");\\n                x.SetServiceName(\\\"CQRSShop.Service\\\");\\n            });\\n        }\\n    }\\n\\nThat's all to it. If you build this project now you'll get an exe file which you can install as a service, and at the same time if you just try to debug it in Visual Studio that will work as well. The code above is really straightforward. It will run a service of type `IndexingService` and when it starts it should call the `Start` method and `Stop` on stop. It can't be much simpler than that so let's keep on going to the `IndexingService`.\\n\\n##Creating the indexing service\\nTo understand what is going on we take it piece by piece.\\n\\n###Constructing it\\n    internal class IndexingServie\\n    {\\n        private Indexer _indexer;\\n        private Dictionary<Type, Action<object>> _eventHandlerMapping;\\n        private Position? _latestPosition;\\n        private IEventStoreConnection _connection;\\n\\n        public void Start()\\n        {\\n            _indexer = CreateIndexer();\\n            _eventHandlerMapping = CreateEventHandlerMapping();\\n            ConnectToEventstore();\\n        }\\n\\n        private Indexer CreateIndexer()\\n        {\\n            var indexer = new Indexer();\\n            indexer.Init();\\n            return indexer;\\n        }\\n\\n        private void ConnectToEventstore()\\n        {\\n\\n            _latestPosition = Position.Start;\\n            _connection = EventStoreConnectionWrapper.Connect();\\n            _connection.Connected +=\\n                (sender, args) => _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\\n            Console.WriteLine(\\\"Indexing service started\\\");\\n        }\\n        \\n        ...\\n    }\\n    \\nThe `Indexer` class which is instantiated is a wrapper on top of [Nest](http://nest.azurewebsites.net/) which is the .NET client for elasticsearch. `Init` is called when creating it to create the index we are going to put our documents in.\\n\\n###Connecting to eventstore\\nTo connect to the eventstore I have a simple wrapper that can be reused for possible other subscribers of the eventstore, and I also get less code in the `IndexingService`. The code for the `EventStoreConnectionWrapper` is really simple: \\n\\n    public class EventStoreConnectionWrapper\\n    {\\n        private static IEventStoreConnection _connection;\\n\\n        public static IEventStoreConnection Connect()\\n        {\\n            ConnectionSettings settings =\\n                ConnectionSettings.Create()\\n                    .UseConsoleLogger()\\n                    .KeepReconnecting()\\n                    .SetDefaultUserCredentials(new UserCredentials(\\\"admin\\\", \\\"changeit\\\"));\\n            var endPoint = new IPEndPoint(IPAddress.Loopback, 1113);\\n            _connection = EventStoreConnection.Create(settings, endPoint, null);\\n            _connection.Connect();\\n            return _connection;\\n        }\\n    }\\n\\nNote that this is for the v 3.0 RC of eventstore, there might be minor changes for previous versions. Of course you shouldn't store the user name and password in code like this but for demo purposes it is just fine.\\n\\n###Creating the mapping for the events\\nSince we are not interested in exactly all the events as of the moment, and we need a way to know what to do for each event we create a simple mapping dictionary for that: \\n\\n    private Dictionary<Type, Action<object>> CreateEventHandlerMapping()\\n    {\\n        return new Dictionary<Type, Action<object>>()\\n        {\\n            {typeof (CustomerCreated), o => Handle(o as CustomerCreated)},\\n            {typeof (CustomerMarkedAsPreferred), o => Handle(o as CustomerMarkedAsPreferred)},\\n            {typeof (BasketCreated), o => Handle(o as BasketCreated)},\\n            {typeof (ItemAdded), o => Handle(o as ItemAdded)},\\n            {typeof (CustomerIsCheckingOutBasket), o => Handle(o as CustomerIsCheckingOutBasket)},\\n            {typeof (BasketCheckedOut), o => Handle(o as BasketCheckedOut)},\\n            {typeof (OrderCreated), o => Handle(o as OrderCreated)},\\n            {typeof (ProductCreated), o => Handle(o as ProductCreated)}\\n        }; \\n    }\\n\\nHere we have all the mappings in one place instead of doing some really boring if/else or switch/case programming later on.\\n\\n###Handle the events\\nWe first have a method that will be called for each event, that was what specified in the subscription earlier.\\n\\n\\n    private void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\\n    {\\n        var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\\n        if (@event != null)\\n        {\\n            var eventType = @event.GetType();\\n            if (_eventHandlerMapping.ContainsKey(eventType))\\n            {\\n                _eventHandlerMapping[eventType](@event);\\n            }\\n        }\\n        _latestPosition = arg2.OriginalPosition;\\n    }\\n\\nWhat this does is that it deserialize the event (see code below), finds the type of the event and check if it exists in our mapping. If it exists in our mapping we execute the function that mapping is pointing too.\\n\\nThe actual handlers will all follow a pattern like this:\\n\\n    private void Handle(ItemAdded evt)\\n    {\\n        var basket = _indexer.Get<Basket>(evt.Id);\\n        var orderLines = basket.OrderLines;\\n        if (orderLines == null || orderLines.Length == 0)\\n        {\\n            basket.OrderLines = new[] {evt.OrderLine};\\n        }\\n        else\\n        {\\n            var orderLineList = orderLines.ToList();\\n            orderLineList.Add(evt.OrderLine);\\n            basket.OrderLines = orderLineList.ToArray();\\n        }\\n        _indexer.Index(basket);\\n    }\\n\\nFirst we get the existing document from elasticsearch, then we update it and save it again. I won't show all the handlers since they are really simple and on github.\\n\\n###Event deserialization\\nI extracted the event deserialization to a separate class since it really doesn't have much with the indexing to do. The deserializer code looks like this:\\n\\n    public class EventSerialization\\n    {\\n        public static object DeserializeEvent(RecordedEvent originalEvent)\\n        {\\n            if (originalEvent.Metadata != null)\\n            {\\n                var metadata = DeserializeObject<Dictionary<string, string>>(originalEvent.Metadata);\\n                if (metadata != null && metadata.ContainsKey(EventClrTypeHeader))\\n                {\\n                    var eventData = DeserializeObject(originalEvent.Data, metadata[EventClrTypeHeader]);\\n                    return eventData;\\n                }\\n            }\\n            return null;\\n        }\\n\\n        private static T DeserializeObject<T>(byte[] data)\\n        {\\n            return (T)(DeserializeObject(data, typeof(T).AssemblyQualifiedName));\\n        }\\n\\n        private static object DeserializeObject(byte[] data, string typeName)\\n        {\\n            var jsonString = Encoding.UTF8.GetString(data);\\n            try\\n            {\\n                return JsonConvert.DeserializeObject(jsonString, Type.GetType(typeName));\\n            }\\n            catch (JsonReaderException)\\n            {\\n                return null;\\n            }\\n        }\\n        public static string EventClrTypeHeader = \\\"EventClrTypeName\\\";\\n    }\\n\\nThere is one entry point, `DeserializeEvent`, that expects a `RecordedEvent` as parameter. The rest of the code is bascially just reverting the serialization of the code in the repository: https://github.com/mastoj/CQRSShop/blob/master/src/CQRSShop.Infrastructure/EventStoreDomainRepository.cs. First we check for the metadata since we have stored the type of the event in the metadata for each event. When we have the type we can deserialize the actual event data and we got our event.\\n\\n##Indexing to elasticsearch\\nTo be able to index anything you need something to index, so first we define our documents that we would like to store in the index.\\n\\n###The documents\\n    public class Customer\\n    {\\n        [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\\n        public Guid Id { get; set; }\\n        public string Name { get; set; }\\n        public bool IsPreferred { get; set; }\\n        public int Discount { get; set; }\\n    }\\n    \\n    public class Basket\\n    {\\n        [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\\n        public Guid Id { get; set; }\\n        [ElasticProperty(Type = FieldType.nested)]\\n        public OrderLine[] OrderLines { get; set; }\\n        public BasketState BasketState { get; set; }\\n        [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\\n        public Guid OrderId { get; set; }\\n    }\\n\\n    public enum BasketState\\n    {\\n        Shopping,\\n        CheckingOut,\\n        CheckedOut,\\n        Paid\\n    }\\n\\n    public class Product\\n    {\\n        [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\\n        public Guid Id { get; set; }\\n        public string Name { get; set; }\\n        public int Price { get; set; }\\n    }\\n\\nWe have three documents that we want to index; `Customer`, `Basket` and `Product`. For all of them we add one attribute that tells `Nest` to not analyze the `Id` property before indexing the documents. This means that it will be searchable but only as the full value. Also, if you have an `Id` `Nest` will use that as the document id in elasticsearch.\\n\\n###Indexing the documents\\n*The code I'll show to you here is probably **NOT** what you want in your production environment. You'll probably take advantage of features like multiple indexes, aliases and so on. This is just for demo purpose.*\\n\\n    internal class Indexer\\n    {\\n        private readonly ElasticClient _esClient;\\n        private string _index = \\\"cqrsshop\\\";\\n\\n        public Indexer()\\n        {\\n            var settings = new ConnectionSettings(new Uri(\\\"http://localhost:9200\\\"));\\n            settings.SetDefaultIndex(_index);\\n            _esClient = new ElasticClient(settings);\\n        }\\n\\n        public TDocument Get<TDocument>(Guid id) where TDocument : class\\n        {\\n            return _esClient.Get<TDocument>(id.ToString()).Source;\\n        }\\n\\n        public void Index<TDocument>(TDocument document) where TDocument : class\\n        {\\n            _esClient.Index(document, y => y.Index(_index));\\n        }\\n\\n        public void Init()\\n        {\\n            _esClient.CreateIndex(_index, y => y\\n                .AddMapping<Basket>(m => m.MapFromAttributes())\\n                .AddMapping<Customer>(m => m.MapFromAttributes())\\n                .AddMapping<Product>(m => m.MapFromAttributes()));\\n        }\\n    }\\n\\nLet's start from the bottom, the `Init` method. Here we create the actual index in elasticsearch and and the mapping for our documents to that index. If we change our document structure we need to recreate the index and mapping. Then we have to simple wrapper methods; `Get<T>` and `Index<T>`.\\n\\n##And we are done!\\nNow I finished what I started, but I think I'll do at least two more posts. One where I show how to integrate neo4j, and one with a summary of what, why and answers to some of the questions some people have against this type of architecture.\\n\\n##Screen shots from result in elasticsearch\\nIf we have created a customer, a product and a basket we can query elasticsearch using [Sense](https://github.com/bleskes/sense), which is/was a plugin to chrome. I'm not sure if you need a license for it now since it moved into another product called Marvel. If we ask to search for everything you would got a result like this.\\n\\n![Sample result from sense](/content/images/2014/Jun/Sense_Result.PNG)\\n\\nElasticsearch is now up and running, but I'll add one more integration in [Integrating neo4j](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li>Integrating elasticsearch</li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a></li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"timetocreatetheviews\">Time to create the views</h1>\n<p>In most system you need to be able to show the data to the user in one way, so in this example I've decided to use elasticsearch. The update of the views will be taken care of by a separate service, which also make that the views will be eventual consistent. In this specific sample it will be consisten within less than half a second if everything is up and running and the service is connected to the eventstore to listen for changes.</p>\n<h2 id=\"whatdowewantaviewof\">What do we want a view of?</h2>\n<p>Since we have access to a complete audit log of the system we can basically create any view we can think of so we just have to think of a few. The views I choose to create to illustrate the integration is a list of the products so we can search for products, the customers and baskets. For the basket, customer and the orders a graph database like neo4j might make more sense so I might just do that in a future post but for now we put everything except the orders in elasticsearch. And since elasticsearch is a document/search database using lucene it will be really fast to lookup a single document as well as doing full text search.</p>\n<h2 id=\"disclamer\">Disclamer</h2>\n<p>Since this is a demo I'll show you a quick way of doing things which you might not want to do in a production system. I won't store the last read position in the stream, but instead rebuilding the views every single time the service starts. That is doable for small amount of data, but probably not something you want.</p>\n<h2 id=\"creataingthehostingservicewithtopshelf\">Creataing the hosting service with topshelf</h2>\n<p>To make it easier to host and debug the service I'll use <a href=\"http://topshelf-project.com/\">topshelf</a>, which is an great framework to create windows services. To get started with topshelf you just create a console application and then install the <code>Topshelf</code> nuget package. When topshelf is added to your console project update your &quot;Program.cs&quot; to look something like this:</p>\n<pre><code>class Program\n{\n    public static void Main(string[] args)\n    {\n        HostFactory.Run(x =&gt;\n        {\n            x.Service&lt;IndexingServie&gt;(s =&gt;\n            {\n                s.ConstructUsing(name =&gt; new IndexingServie());\n                s.WhenStarted(tc =&gt; tc.Start());\n                s.WhenStopped(tc =&gt; tc.Stop());\n            });\n            x.RunAsLocalSystem();\n\n            x.SetDescription(&quot;CQRSShop.Service&quot;);\n            x.SetDisplayName(&quot;CQRSShop.Service&quot;);\n            x.SetServiceName(&quot;CQRSShop.Service&quot;);\n        });\n    }\n}\n</code></pre>\n<p>That's all to it. If you build this project now you'll get an exe file which you can install as a service, and at the same time if you just try to debug it in Visual Studio that will work as well. The code above is really straightforward. It will run a service of type <code>IndexingService</code> and when it starts it should call the <code>Start</code> method and <code>Stop</code> on stop. It can't be much simpler than that so let's keep on going to the <code>IndexingService</code>.</p>\n<h2 id=\"creatingtheindexingservice\">Creating the indexing service</h2>\n<p>To understand what is going on we take it piece by piece.</p>\n<h3 id=\"constructingit\">Constructing it</h3>\n<pre><code>internal class IndexingServie\n{\n    private Indexer _indexer;\n    private Dictionary&lt;Type, Action&lt;object&gt;&gt; _eventHandlerMapping;\n    private Position? _latestPosition;\n    private IEventStoreConnection _connection;\n\n    public void Start()\n    {\n        _indexer = CreateIndexer();\n        _eventHandlerMapping = CreateEventHandlerMapping();\n        ConnectToEventstore();\n    }\n\n    private Indexer CreateIndexer()\n    {\n        var indexer = new Indexer();\n        indexer.Init();\n        return indexer;\n    }\n\n    private void ConnectToEventstore()\n    {\n\n        _latestPosition = Position.Start;\n        _connection = EventStoreConnectionWrapper.Connect();\n        _connection.Connected +=\n            (sender, args) =&gt; _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\n        Console.WriteLine(&quot;Indexing service started&quot;);\n    }\n    \n    ...\n}\n</code></pre>\n<p>The <code>Indexer</code> class which is instantiated is a wrapper on top of <a href=\"http://nest.azurewebsites.net/\">Nest</a> which is the .NET client for elasticsearch. <code>Init</code> is called when creating it to create the index we are going to put our documents in.</p>\n<h3 id=\"connectingtoeventstore\">Connecting to eventstore</h3>\n<p>To connect to the eventstore I have a simple wrapper that can be reused for possible other subscribers of the eventstore, and I also get less code in the <code>IndexingService</code>. The code for the <code>EventStoreConnectionWrapper</code> is really simple:</p>\n<pre><code>public class EventStoreConnectionWrapper\n{\n    private static IEventStoreConnection _connection;\n\n    public static IEventStoreConnection Connect()\n    {\n        ConnectionSettings settings =\n            ConnectionSettings.Create()\n                .UseConsoleLogger()\n                .KeepReconnecting()\n                .SetDefaultUserCredentials(new UserCredentials(&quot;admin&quot;, &quot;changeit&quot;));\n        var endPoint = new IPEndPoint(IPAddress.Loopback, 1113);\n        _connection = EventStoreConnection.Create(settings, endPoint, null);\n        _connection.Connect();\n        return _connection;\n    }\n}\n</code></pre>\n<p>Note that this is for the v 3.0 RC of eventstore, there might be minor changes for previous versions. Of course you shouldn't store the user name and password in code like this but for demo purposes it is just fine.</p>\n<h3 id=\"creatingthemappingfortheevents\">Creating the mapping for the events</h3>\n<p>Since we are not interested in exactly all the events as of the moment, and we need a way to know what to do for each event we create a simple mapping dictionary for that:</p>\n<pre><code>private Dictionary&lt;Type, Action&lt;object&gt;&gt; CreateEventHandlerMapping()\n{\n    return new Dictionary&lt;Type, Action&lt;object&gt;&gt;()\n    {\n        {typeof (CustomerCreated), o =&gt; Handle(o as CustomerCreated)},\n        {typeof (CustomerMarkedAsPreferred), o =&gt; Handle(o as CustomerMarkedAsPreferred)},\n        {typeof (BasketCreated), o =&gt; Handle(o as BasketCreated)},\n        {typeof (ItemAdded), o =&gt; Handle(o as ItemAdded)},\n        {typeof (CustomerIsCheckingOutBasket), o =&gt; Handle(o as CustomerIsCheckingOutBasket)},\n        {typeof (BasketCheckedOut), o =&gt; Handle(o as BasketCheckedOut)},\n        {typeof (OrderCreated), o =&gt; Handle(o as OrderCreated)},\n        {typeof (ProductCreated), o =&gt; Handle(o as ProductCreated)}\n    }; \n}\n</code></pre>\n<p>Here we have all the mappings in one place instead of doing some really boring if/else or switch/case programming later on.</p>\n<h3 id=\"handletheevents\">Handle the events</h3>\n<p>We first have a method that will be called for each event, that was what specified in the subscription earlier.</p>\n<pre><code>private void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\n{\n    var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\n    if (@event != null)\n    {\n        var eventType = @event.GetType();\n        if (_eventHandlerMapping.ContainsKey(eventType))\n        {\n            _eventHandlerMapping[eventType](@event);\n        }\n    }\n    _latestPosition = arg2.OriginalPosition;\n}\n</code></pre>\n<p>What this does is that it deserialize the event (see code below), finds the type of the event and check if it exists in our mapping. If it exists in our mapping we execute the function that mapping is pointing too.</p>\n<p>The actual handlers will all follow a pattern like this:</p>\n<pre><code>private void Handle(ItemAdded evt)\n{\n    var basket = _indexer.Get&lt;Basket&gt;(evt.Id);\n    var orderLines = basket.OrderLines;\n    if (orderLines == null || orderLines.Length == 0)\n    {\n        basket.OrderLines = new[] {evt.OrderLine};\n    }\n    else\n    {\n        var orderLineList = orderLines.ToList();\n        orderLineList.Add(evt.OrderLine);\n        basket.OrderLines = orderLineList.ToArray();\n    }\n    _indexer.Index(basket);\n}\n</code></pre>\n<p>First we get the existing document from elasticsearch, then we update it and save it again. I won't show all the handlers since they are really simple and on github.</p>\n<h3 id=\"eventdeserialization\">Event deserialization</h3>\n<p>I extracted the event deserialization to a separate class since it really doesn't have much with the indexing to do. The deserializer code looks like this:</p>\n<pre><code>public class EventSerialization\n{\n    public static object DeserializeEvent(RecordedEvent originalEvent)\n    {\n        if (originalEvent.Metadata != null)\n        {\n            var metadata = DeserializeObject&lt;Dictionary&lt;string, string&gt;&gt;(originalEvent.Metadata);\n            if (metadata != null &amp;&amp; metadata.ContainsKey(EventClrTypeHeader))\n            {\n                var eventData = DeserializeObject(originalEvent.Data, metadata[EventClrTypeHeader]);\n                return eventData;\n            }\n        }\n        return null;\n    }\n\n    private static T DeserializeObject&lt;T&gt;(byte[] data)\n    {\n        return (T)(DeserializeObject(data, typeof(T).AssemblyQualifiedName));\n    }\n\n    private static object DeserializeObject(byte[] data, string typeName)\n    {\n        var jsonString = Encoding.UTF8.GetString(data);\n        try\n        {\n            return JsonConvert.DeserializeObject(jsonString, Type.GetType(typeName));\n        }\n        catch (JsonReaderException)\n        {\n            return null;\n        }\n    }\n    public static string EventClrTypeHeader = &quot;EventClrTypeName&quot;;\n}\n</code></pre>\n<p>There is one entry point, <code>DeserializeEvent</code>, that expects a <code>RecordedEvent</code> as parameter. The rest of the code is bascially just reverting the serialization of the code in the repository: <a href=\"https://github.com/mastoj/CQRSShop/blob/master/src/CQRSShop.Infrastructure/EventStoreDomainRepository.cs\">https://github.com/mastoj/CQRSShop/blob/master/src/CQRSShop.Infrastructure/EventStoreDomainRepository.cs</a>. First we check for the metadata since we have stored the type of the event in the metadata for each event. When we have the type we can deserialize the actual event data and we got our event.</p>\n<h2 id=\"indexingtoelasticsearch\">Indexing to elasticsearch</h2>\n<p>To be able to index anything you need something to index, so first we define our documents that we would like to store in the index.</p>\n<h3 id=\"thedocuments\">The documents</h3>\n<pre><code>public class Customer\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public bool IsPreferred { get; set; }\n    public int Discount { get; set; }\n}\n\npublic class Basket\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    [ElasticProperty(Type = FieldType.nested)]\n    public OrderLine[] OrderLines { get; set; }\n    public BasketState BasketState { get; set; }\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid OrderId { get; set; }\n}\n\npublic enum BasketState\n{\n    Shopping,\n    CheckingOut,\n    CheckedOut,\n    Paid\n}\n\npublic class Product\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public int Price { get; set; }\n}\n</code></pre>\n<p>We have three documents that we want to index; <code>Customer</code>, <code>Basket</code> and <code>Product</code>. For all of them we add one attribute that tells <code>Nest</code> to not analyze the <code>Id</code> property before indexing the documents. This means that it will be searchable but only as the full value. Also, if you have an <code>Id</code> <code>Nest</code> will use that as the document id in elasticsearch.</p>\n<h3 id=\"indexingthedocuments\">Indexing the documents</h3>\n<p><em>The code I'll show to you here is probably <strong>NOT</strong> what you want in your production environment. You'll probably take advantage of features like multiple indexes, aliases and so on. This is just for demo purpose.</em></p>\n<pre><code>internal class Indexer\n{\n    private readonly ElasticClient _esClient;\n    private string _index = &quot;cqrsshop&quot;;\n\n    public Indexer()\n    {\n        var settings = new ConnectionSettings(new Uri(&quot;http://localhost:9200&quot;));\n        settings.SetDefaultIndex(_index);\n        _esClient = new ElasticClient(settings);\n    }\n\n    public TDocument Get&lt;TDocument&gt;(Guid id) where TDocument : class\n    {\n        return _esClient.Get&lt;TDocument&gt;(id.ToString()).Source;\n    }\n\n    public void Index&lt;TDocument&gt;(TDocument document) where TDocument : class\n    {\n        _esClient.Index(document, y =&gt; y.Index(_index));\n    }\n\n    public void Init()\n    {\n        _esClient.CreateIndex(_index, y =&gt; y\n            .AddMapping&lt;Basket&gt;(m =&gt; m.MapFromAttributes())\n            .AddMapping&lt;Customer&gt;(m =&gt; m.MapFromAttributes())\n            .AddMapping&lt;Product&gt;(m =&gt; m.MapFromAttributes()));\n    }\n}\n</code></pre>\n<p>Let's start from the bottom, the <code>Init</code> method. Here we create the actual index in elasticsearch and and the mapping for our documents to that index. If we change our document structure we need to recreate the index and mapping. Then we have to simple wrapper methods; <code>Get&lt;T&gt;</code> and <code>Index&lt;T&gt;</code>.</p>\n<h2 id=\"andwearedone\">And we are done!</h2>\n<p>Now I finished what I started, but I think I'll do at least two more posts. One where I show how to integrate neo4j, and one with a summary of what, why and answers to some of the questions some people have against this type of architecture.</p>\n<h2 id=\"screenshotsfromresultinelasticsearch\">Screen shots from result in elasticsearch</h2>\n<p>If we have created a customer, a product and a basket we can query elasticsearch using <a href=\"https://github.com/bleskes/sense\">Sense</a>, which is/was a plugin to chrome. I'm not sure if you need a license for it now since it moved into another product called Marvel. If we ask to search for everything you would got a result like this.</p>\n<p><img src=\"/content/images/2014/Jun/Sense_Result.PNG\" alt=\"Sample result from sense\"></p>\n<p>Elasticsearch is now up and running, but I'll add one more integration in <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/\">Integrating neo4j</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"36","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n * Integrating neo4j\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nTime to create the views\nIn most system you need to be able to show the data to the user in one way, so\nin this example I've decided to use elasticsearch. The update of the views will\nbe taken care of by a separate service, which also make that the views will be\neventual consistent. In this specific sample it will be consisten within less\nthan half a second if everything is up and running and the service is connected\nto the eventstore to listen for changes.\n\nWhat do we want a view of?\nSince we have access to a complete audit log of the system we can basically\ncreate any view we can think of so we just have to think of a few. The views I\nchoose to create to illustrate the integration is a list of the products so we\ncan search for products, the customers and baskets. For the basket, customer and\nthe orders a graph database like neo4j might make more sense so I might just do\nthat in a future post but for now we put everything except the orders in\nelasticsearch. And since elasticsearch is a document/search database using\nlucene it will be really fast to lookup a single document as well as doing full\ntext search.\n\nDisclamer\nSince this is a demo I'll show you a quick way of doing things which you might\nnot want to do in a production system. I won't store the last read position in\nthe stream, but instead rebuilding the views every single time the service\nstarts. That is doable for small amount of data, but probably not something you\nwant.\n\nCreataing the hosting service with topshelf\nTo make it easier to host and debug the service I'll use topshelf\n[http://topshelf-project.com/], which is an great framework to create windows\nservices. To get started with topshelf you just create a console application and\nthen install the Topshelf nuget package. When topshelf is added to your console\nproject update your \"Program.cs\" to look something like this:\n\nclass Program\n{\n    public static void Main(string[] args)\n    {\n        HostFactory.Run(x =>\n        {\n            x.Service<IndexingServie>(s =>\n            {\n                s.ConstructUsing(name => new IndexingServie());\n                s.WhenStarted(tc => tc.Start());\n                s.WhenStopped(tc => tc.Stop());\n            });\n            x.RunAsLocalSystem();\n\n            x.SetDescription(\"CQRSShop.Service\");\n            x.SetDisplayName(\"CQRSShop.Service\");\n            x.SetServiceName(\"CQRSShop.Service\");\n        });\n    }\n}\n\n\nThat's all to it. If you build this project now you'll get an exe file which you\ncan install as a service, and at the same time if you just try to debug it in\nVisual Studio that will work as well. The code above is really straightforward.\nIt will run a service of type IndexingService and when it starts it should call\nthe Start method and Stop on stop. It can't be much simpler than that so let's\nkeep on going to the IndexingService.\n\nCreating the indexing service\nTo understand what is going on we take it piece by piece.\n\nConstructing it\ninternal class IndexingServie\n{\n    private Indexer _indexer;\n    private Dictionary<Type, Action<object>> _eventHandlerMapping;\n    private Position? _latestPosition;\n    private IEventStoreConnection _connection;\n\n    public void Start()\n    {\n        _indexer = CreateIndexer();\n        _eventHandlerMapping = CreateEventHandlerMapping();\n        ConnectToEventstore();\n    }\n\n    private Indexer CreateIndexer()\n    {\n        var indexer = new Indexer();\n        indexer.Init();\n        return indexer;\n    }\n\n    private void ConnectToEventstore()\n    {\n\n        _latestPosition = Position.Start;\n        _connection = EventStoreConnectionWrapper.Connect();\n        _connection.Connected +=\n            (sender, args) => _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\n        Console.WriteLine(\"Indexing service started\");\n    }\n    \n    ...\n}\n\n\nThe Indexer class which is instantiated is a wrapper on top of Nest\n[http://nest.azurewebsites.net/] which is the .NET client for elasticsearch. \nInit is called when creating it to create the index we are going to put our\ndocuments in.\n\nConnecting to eventstore\nTo connect to the eventstore I have a simple wrapper that can be reused for\npossible other subscribers of the eventstore, and I also get less code in the \nIndexingService. The code for the EventStoreConnectionWrapper is really simple:\n\npublic class EventStoreConnectionWrapper\n{\n    private static IEventStoreConnection _connection;\n\n    public static IEventStoreConnection Connect()\n    {\n        ConnectionSettings settings =\n            ConnectionSettings.Create()\n                .UseConsoleLogger()\n                .KeepReconnecting()\n                .SetDefaultUserCredentials(new UserCredentials(\"admin\", \"changeit\"));\n        var endPoint = new IPEndPoint(IPAddress.Loopback, 1113);\n        _connection = EventStoreConnection.Create(settings, endPoint, null);\n        _connection.Connect();\n        return _connection;\n    }\n}\n\n\nNote that this is for the v 3.0 RC of eventstore, there might be minor changes\nfor previous versions. Of course you shouldn't store the user name and password\nin code like this but for demo purposes it is just fine.\n\nCreating the mapping for the events\nSince we are not interested in exactly all the events as of the moment, and we\nneed a way to know what to do for each event we create a simple mapping\ndictionary for that:\n\nprivate Dictionary<Type, Action<object>> CreateEventHandlerMapping()\n{\n    return new Dictionary<Type, Action<object>>()\n    {\n        {typeof (CustomerCreated), o => Handle(o as CustomerCreated)},\n        {typeof (CustomerMarkedAsPreferred), o => Handle(o as CustomerMarkedAsPreferred)},\n        {typeof (BasketCreated), o => Handle(o as BasketCreated)},\n        {typeof (ItemAdded), o => Handle(o as ItemAdded)},\n        {typeof (CustomerIsCheckingOutBasket), o => Handle(o as CustomerIsCheckingOutBasket)},\n        {typeof (BasketCheckedOut), o => Handle(o as BasketCheckedOut)},\n        {typeof (OrderCreated), o => Handle(o as OrderCreated)},\n        {typeof (ProductCreated), o => Handle(o as ProductCreated)}\n    }; \n}\n\n\nHere we have all the mappings in one place instead of doing some really boring\nif/else or switch/case programming later on.\n\nHandle the events\nWe first have a method that will be called for each event, that was what\nspecified in the subscription earlier.\n\nprivate void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\n{\n    var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\n    if (@event != null)\n    {\n        var eventType = @event.GetType();\n        if (_eventHandlerMapping.ContainsKey(eventType))\n        {\n            _eventHandlerMapping[eventType](@event);\n        }\n    }\n    _latestPosition = arg2.OriginalPosition;\n}\n\n\nWhat this does is that it deserialize the event (see code below), finds the type\nof the event and check if it exists in our mapping. If it exists in our mapping\nwe execute the function that mapping is pointing too.\n\nThe actual handlers will all follow a pattern like this:\n\nprivate void Handle(ItemAdded evt)\n{\n    var basket = _indexer.Get<Basket>(evt.Id);\n    var orderLines = basket.OrderLines;\n    if (orderLines == null || orderLines.Length == 0)\n    {\n        basket.OrderLines = new[] {evt.OrderLine};\n    }\n    else\n    {\n        var orderLineList = orderLines.ToList();\n        orderLineList.Add(evt.OrderLine);\n        basket.OrderLines = orderLineList.ToArray();\n    }\n    _indexer.Index(basket);\n}\n\n\nFirst we get the existing document from elasticsearch, then we update it and\nsave it again. I won't show all the handlers since they are really simple and on\ngithub.\n\nEvent deserialization\nI extracted the event deserialization to a separate class since it really\ndoesn't have much with the indexing to do. The deserializer code looks like\nthis:\n\npublic class EventSerialization\n{\n    public static object DeserializeEvent(RecordedEvent originalEvent)\n    {\n        if (originalEvent.Metadata != null)\n        {\n            var metadata = DeserializeObject<Dictionary<string, string>>(originalEvent.Metadata);\n            if (metadata != null && metadata.ContainsKey(EventClrTypeHeader))\n            {\n                var eventData = DeserializeObject(originalEvent.Data, metadata[EventClrTypeHeader]);\n                return eventData;\n            }\n        }\n        return null;\n    }\n\n    private static T DeserializeObject<T>(byte[] data)\n    {\n        return (T)(DeserializeObject(data, typeof(T).AssemblyQualifiedName));\n    }\n\n    private static object DeserializeObject(byte[] data, string typeName)\n    {\n        var jsonString = Encoding.UTF8.GetString(data);\n        try\n        {\n            return JsonConvert.DeserializeObject(jsonString, Type.GetType(typeName));\n        }\n        catch (JsonReaderException)\n        {\n            return null;\n        }\n    }\n    public static string EventClrTypeHeader = \"EventClrTypeName\";\n}\n\n\nThere is one entry point, DeserializeEvent, that expects a RecordedEvent as\nparameter. The rest of the code is bascially just reverting the serialization of\nthe code in the repository: \nhttps://github.com/mastoj/CQRSShop/blob/master/src/CQRSShop.Infrastructure/EventStoreDomainRepository.cs\n. First we check for the metadata since we have stored the type of the event in\nthe metadata for each event. When we have the type we can deserialize the actual\nevent data and we got our event.\n\nIndexing to elasticsearch\nTo be able to index anything you need something to index, so first we define our\ndocuments that we would like to store in the index.\n\nThe documents\npublic class Customer\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public bool IsPreferred { get; set; }\n    public int Discount { get; set; }\n}\n\npublic class Basket\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    [ElasticProperty(Type = FieldType.nested)]\n    public OrderLine[] OrderLines { get; set; }\n    public BasketState BasketState { get; set; }\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid OrderId { get; set; }\n}\n\npublic enum BasketState\n{\n    Shopping,\n    CheckingOut,\n    CheckedOut,\n    Paid\n}\n\npublic class Product\n{\n    [ElasticProperty(Index = FieldIndexOption.not_analyzed)]\n    public Guid Id { get; set; }\n    public string Name { get; set; }\n    public int Price { get; set; }\n}\n\n\nWe have three documents that we want to index; Customer, Basket and Product. For\nall of them we add one attribute that tells Nest to not analyze the Id property\nbefore indexing the documents. This means that it will be searchable but only as\nthe full value. Also, if you have an Id Nest will use that as the document id in\nelasticsearch.\n\nIndexing the documents\nThe code I'll show to you here is probably NOT what you want in your production\nenvironment. You'll probably take advantage of features like multiple indexes,\naliases and so on. This is just for demo purpose.\n\ninternal class Indexer\n{\n    private readonly ElasticClient _esClient;\n    private string _index = \"cqrsshop\";\n\n    public Indexer()\n    {\n        var settings = new ConnectionSettings(new Uri(\"http://localhost:9200\"));\n        settings.SetDefaultIndex(_index);\n        _esClient = new ElasticClient(settings);\n    }\n\n    public TDocument Get<TDocument>(Guid id) where TDocument : class\n    {\n        return _esClient.Get<TDocument>(id.ToString()).Source;\n    }\n\n    public void Index<TDocument>(TDocument document) where TDocument : class\n    {\n        _esClient.Index(document, y => y.Index(_index));\n    }\n\n    public void Init()\n    {\n        _esClient.CreateIndex(_index, y => y\n            .AddMapping<Basket>(m => m.MapFromAttributes())\n            .AddMapping<Customer>(m => m.MapFromAttributes())\n            .AddMapping<Product>(m => m.MapFromAttributes()));\n    }\n}\n\n\nLet's start from the bottom, the Init method. Here we create the actual index in\nelasticsearch and and the mapping for our documents to that index. If we change\nour document structure we need to recreate the index and mapping. Then we have\nto simple wrapper methods; Get<T> and Index<T>.\n\nAnd we are done!\nNow I finished what I started, but I think I'll do at least two more posts. One\nwhere I show how to integrate neo4j, and one with a summary of what, why and\nanswers to some of the questions some people have against this type of\narchitecture.\n\nScreen shots from result in elasticsearch\nIf we have created a customer, a product and a basket we can query elasticsearch\nusing Sense [https://github.com/bleskes/sense], which is/was a plugin to chrome.\nI'm not sure if you need a license for it now since it moved into another\nproduct called Marvel. If we ask to search for everything you would got a result\nlike this.\n\n\n\nElasticsearch is now up and running, but I'll add one more integration in \nIntegrating neo4j\n[http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-27T21:21:04.000Z","updated_at":"2014-07-16T06:15:37.000Z","published_at":"2014-06-28T09:13:01.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe058","uuid":"3c724c65-0e8c-407c-9b8f-1ae6d84fa0c1","title":"CQRS the simple way with eventstore and elasticsearch: Let us throw neo4j into the mix","slug":"cqrs-the-simple-way-with-eventstore-and-elasticsearch-let-us-throw-neo4j-into-the-mix","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch. \\n\\nI'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my [CQRSShop repository on github](https://github.com/mastoj/CQRSShop). Please feel free to comment or come with pull request. Even though I'll use [eventstore](http://geteventstore.com) and [elasticsearch](http://www.elasticsearch.org/) I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.\\n\\nAll the posts will be tagged so you can find them on this url: http://blog.tomasjansson.com/tag/cqrsshop/\\n\\nHope you'll enjoy the read.\\n\\n##Content in the serie\\n * [Project structure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/)\\n * [Infrastructure](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/)\\n * [The goal](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/)\\n * [The first feature](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/)\\n * [The rest of the features](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/)\\n * [Time for reflection](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/)\\n * [Building the API with Simple.Web](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/)\\n * [Integrating elasticsearch](http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/)\\n * Integrating neo4j\\n * [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/)\\n\\n#Things change\\nMy original plan was that this was going to be a summary post, but things change. So in this post I'll add integration with [neo4j](http://www.neo4j.org/) so I can ask really weird question like \\\"for product x give me all other products that some customer has bought where the total order value was above y.\\\" Doing that type of query is really tricky in sql and document databases, but much easier in a graphdatabase as neo4j.\\n\\n##We will be cheating\\nThe quickes way to goal here is just to extend the `IndexingService`, and that is what I want to do. In the handlers methods I'll also update neo4j and not just elasticsearch. In a real environment you might want this as a separate service.\\n\\n##Getting started\\nAfter you've installed neo4j, which you have to figure out yourself how to do (click download, yes, next yes or something like that), you need to add the `Neo4jClient` nuget package to the `Service` project. I want go into the details in how to connect and make the actual queries since there is tons of documentation on that. It took me roughly 30 minutes to figure everything out, so I guess you'll manage it as well.\\n\\n##The result\\nI thought I show you the result before the actual code. The UI that comes with neo4j is a really amazing, easy to understand tool to explore your data. So after creating some customer, products and orders etc. you can get a full graphical overview of what is going on like in the picture below:\\n\\n![](/content/images/2014/Jun/Graph.PNG)\\n\\nI don't have nodes for the order lines, instead I just link a basket to a product but put all the value that customer is going to pay for that product on the relation, and that is what is highlighted in the picture. When a customer finished an order and paid for the product I link the customer directly to the product with a `BOUGHT` relation, this will make it really to find recommendation for customers based on what other customers has bought. An example cypher, the language to query neo4j, query that will get all the customers and products that has bought the product another customer is looking at might look like this:\\n\\n    MATCH (p:Product {Id: '3b4173e6-b3cd-4565-868d-f810c5a04c43'})<-\\n    [:BOUGHT]-(c:Customer)-[:BOUGHT]->(p2:Product)\\n    WHERE p2.Id <> '3b4173e6-b3cd-4565-868d-f810c5a04c43'\\n    RETURN c,p2\\n\\nHere I first locate the product we are looking at right now and which customer that has bought that product. The second `BOUGHT` relation finds all the products those customers has bought and the last where clause filter out so we don't get the product we are looking at right now in the result set. \\n\\nAs you can see, using a graphdatabase you will be able to query really complex structures in a much simpler way than you would in a sql database.\\n\\n##The updated IndexingService\\nNow when you know what we are going to produce, let's look at the code changes.\\n\\nTo make it simple I just pushed all the code into the `IndexingService`, I know it should probably be somewhere else, but this is just for show. So the updated version looks like this:\\n\\n    internal class IndexingServie\\n    {\\n        private Indexer _indexer;\\n        private Dictionary<Type, Action<object>> _eventHandlerMapping;\\n        private Position? _latestPosition;\\n        private IEventStoreConnection _connection;\\n        private GraphClient _graphClient;\\n\\n        public void Start()\\n        {\\n            _graphClient = CreateGraphClient();\\n            _indexer = CreateIndexer();\\n            _eventHandlerMapping = CreateEventHandlerMapping();\\n            ConnectToEventstore();\\n        }\\n\\n        private GraphClient CreateGraphClient()\\n        {\\n            var graphClient = new GraphClient(new Uri(\\\"http://localhost:7474/db/data\\\"));\\n            graphClient.Connect();\\n            DeleteAll(graphClient);\\n            return graphClient;\\n        }\\n\\n        private void DeleteAll(GraphClient graphClient)\\n        {\\n            graphClient.Cypher.Match(\\\"(n)\\\")\\n                .OptionalMatch(\\\"(n)-[r]-()\\\")\\n                .Delete(\\\"n,r\\\")\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private Indexer CreateIndexer()\\n        {\\n            var indexer = new Indexer();\\n            indexer.Init();\\n            return indexer;\\n        }\\n\\n        private void ConnectToEventstore()\\n        {\\n\\n            _latestPosition = Position.Start;\\n            _connection = EventStoreConnectionWrapper.Connect();\\n            _connection.Connected +=\\n                (sender, args) => _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\\n            Console.WriteLine(\\\"Indexing service started\\\");\\n        }\\n\\n        private void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\\n        {\\n            var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\\n            if (@event != null)\\n            {\\n                var eventType = @event.GetType();\\n                if (_eventHandlerMapping.ContainsKey(eventType))\\n                {\\n                    _eventHandlerMapping[eventType](@event);\\n                }\\n            }\\n            _latestPosition = arg2.OriginalPosition;\\n        }\\n\\n        private Dictionary<Type, Action<object>> CreateEventHandlerMapping()\\n        {\\n            return new Dictionary<Type, Action<object>>()\\n            {\\n                {typeof (CustomerCreated), o => Handle(o as CustomerCreated)},\\n                {typeof (CustomerMarkedAsPreferred), o => Handle(o as CustomerMarkedAsPreferred)},\\n                {typeof (BasketCreated), o => Handle(o as BasketCreated)},\\n                {typeof (ItemAdded), o => Handle(o as ItemAdded)},\\n                {typeof (CustomerIsCheckingOutBasket), o => Handle(o as CustomerIsCheckingOutBasket)},\\n                {typeof (BasketCheckedOut), o => Handle(o as BasketCheckedOut)},\\n                {typeof (OrderCreated), o => Handle(o as OrderCreated)},\\n                {typeof (ProductCreated), o => Handle(o as ProductCreated)}\\n            }; \\n        }\\n\\n        private void Handle(OrderCreated evt)\\n        {\\n            var existinBasket = _indexer.Get<Basket>(evt.BasketId);\\n            existinBasket.BasketState = BasketState.Paid;\\n            _indexer.Index(existinBasket);\\n\\n            _graphClient.Cypher\\n                .Match(\\\"(customer:Customer)-[:HAS_BASKET]->(basket:Basket)-[]->(product:Product)\\\")\\n                .Where((Basket basket) => basket.Id == evt.BasketId)\\n                .Create(\\\"customer-[:BOUGHT]->product\\\")\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private void Handle(BasketCheckedOut evt)\\n        {\\n            var basket = _indexer.Get<Basket>(evt.Id);\\n            basket.BasketState = BasketState.CheckedOut;\\n            _indexer.Index(basket);\\n        }\\n\\n        private void Handle(CustomerIsCheckingOutBasket evt)\\n        {\\n            var basket = _indexer.Get<Basket>(evt.Id);\\n            basket.BasketState = BasketState.CheckingOut;\\n            _indexer.Index(basket);\\n        }\\n\\n        private void Handle(ItemAdded evt)\\n        {\\n            var existingBasket = _indexer.Get<Basket>(evt.Id);\\n            var orderLines = existingBasket.OrderLines;\\n            if (orderLines == null || orderLines.Length == 0)\\n            {\\n                existingBasket.OrderLines = new[] {evt.OrderLine};\\n            }\\n            else\\n            {\\n                var orderLineList = orderLines.ToList();\\n                orderLineList.Add(evt.OrderLine);\\n                existingBasket.OrderLines = orderLineList.ToArray();\\n            }\\n\\n            _indexer.Index(existingBasket);\\n\\n            _graphClient.Cypher\\n                .Match(\\\"(basket:Basket)\\\", \\\"(product:Product)\\\")\\n                .Where((Basket basket) => basket.Id == evt.Id)\\n                .AndWhere((Product product) => product.Id == evt.OrderLine.ProductId)\\n                .Create(\\\"basket-[:HAS_ORDERLINE {orderLine}]->product\\\")\\n                .WithParam(\\\"orderLine\\\", evt.OrderLine)\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private void Handle(BasketCreated evt)\\n        {\\n            var newBasket = new Basket()\\n            {\\n                Id = evt.Id,\\n                OrderLines = null,\\n                BasketState = BasketState.Shopping\\n            };\\n            _indexer.Index(newBasket);\\n            _graphClient.Cypher\\n                .Create(\\\"(basket:Basket {newBasket})\\\")\\n                .WithParam(\\\"newBasket\\\", newBasket)\\n                .ExecuteWithoutResults();\\n\\n            _graphClient.Cypher\\n                .Match(\\\"(customer:Customer)\\\", \\\"(basket:Basket)\\\")\\n                .Where((Customer customer) => customer.Id == evt.CustomerId)\\n                .AndWhere((Basket basket) => basket.Id == evt.Id)\\n                .Create(\\\"customer-[:HAS_BASKET]->basket\\\")\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private void Handle(ProductCreated evt)\\n        {\\n            var product = new Product()\\n            {\\n                Id = evt.Id,\\n                Name = evt.Name,\\n                Price = evt.Price\\n            };\\n            _indexer.Index(product);\\n            _graphClient.Cypher\\n                .Create(\\\"(product:Product {newProduct})\\\")\\n                .WithParam(\\\"newProduct\\\", product)\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private void Handle(CustomerMarkedAsPreferred evt)\\n        {\\n            var customer = _indexer.Get<Customer>(evt.Id);\\n            customer.IsPreferred = true;\\n            customer.Discount = evt.Discount;\\n            _indexer.Index(customer);\\n\\n            _graphClient.Cypher\\n                .Match(\\\"(c:Customer)\\\")\\n                .Where((Customer c) => c.Id == customer.Id)\\n                .Set(\\\"c = {c}\\\")\\n                .WithParam(\\\"c\\\", customer)\\n                .ExecuteWithoutResults();\\n        }\\n\\n        private void Handle(CustomerCreated evt)\\n        {\\n            var customer = new Customer()\\n            {\\n                Id = evt.Id,\\n                Name = evt.Name\\n            };\\n            _indexer.Index(customer);\\n\\n            _graphClient.Cypher\\n                .Create(\\\"(customer:Customer {newCustomer})\\\")\\n                .WithParam(\\\"newCustomer\\\", customer)\\n                .ExecuteWithoutResults(); \\n        }\\n\\n        public void Stop()\\n        {\\n        }\\n    }\\n\\nThe `CreateGraphClient` creates the graph client for us, as well as make sure the database is empty. The way I delete all the data is not how you should do it for large sets of data, again, this is just for show. When we have the graph client we can now update all our handlers to update the graph database as well as elasticsearch and we are all done.\\n\\n##Everything has to come to an end\\nIt has been a really productive week, but now I almost consider this blog series as done. I'll write one more post with some discussion and answers to questions some people might have regarding this type of design. I hope I can finish the discussion later today. The last part is available here: [Ending discussion](http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>We need a change in the industry. All to often we defend old solutions just because we don't know the alternative. I know that sometime it might be good to slow things down and keep using the old stuff, but if that old stuff is storing state using SQL to do so everytime I don't think you're making that choice with the right reasons in mind. My contribution to this problem is a series of blog post where I'll walk you through how an alternative solution might look using event sourcing and CQRS. To store the event I'll use eventstore, and to store the view models I'll use elasticsearch.</p>\n<p>I'm planning to write a series of post, since it is too much to cover for one post, where I will walk you through different stages of the implementation of a simple web shop. Everything will be implemented in C# and F# to start with but I might change to 100 % F#, but that is something that will come later. The code will be available in my <a href=\"https://github.com/mastoj/CQRSShop\">CQRSShop repository on github</a>. Please feel free to comment or come with pull request. Even though I'll use <a href=\"http://geteventstore.com\">eventstore</a> and <a href=\"http://www.elasticsearch.org/\">elasticsearch</a> I won't cover how to install those products. I will not cover CQRS in depth either since there is a lot of material about CQRS and what it is, this will be a more practical view of how to do things.</p>\n<p>All the posts will be tagged so you can find them on this url: <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">http://blog.tomasjansson.com/tag/cqrsshop/</a></p>\n<p>Hope you'll enjoy the read.</p>\n<h2 id=\"contentintheserie\">Content in the serie</h2>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/\">Project structure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/\">Infrastructure</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/\">The goal</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/\">The first feature</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/\">The rest of the features</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">Time for reflection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/\">Building the API with Simple.Web</a></li>\n<li><a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/\">Integrating elasticsearch</a></li>\n<li>Integrating neo4j</li>\n<li><a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a></li>\n</ul>\n<h1 id=\"thingschange\">Things change</h1>\n<p>My original plan was that this was going to be a summary post, but things change. So in this post I'll add integration with <a href=\"http://www.neo4j.org/\">neo4j</a> so I can ask really weird question like &quot;for product x give me all other products that some customer has bought where the total order value was above y.&quot; Doing that type of query is really tricky in sql and document databases, but much easier in a graphdatabase as neo4j.</p>\n<h2 id=\"wewillbecheating\">We will be cheating</h2>\n<p>The quickes way to goal here is just to extend the <code>IndexingService</code>, and that is what I want to do. In the handlers methods I'll also update neo4j and not just elasticsearch. In a real environment you might want this as a separate service.</p>\n<h2 id=\"gettingstarted\">Getting started</h2>\n<p>After you've installed neo4j, which you have to figure out yourself how to do (click download, yes, next yes or something like that), you need to add the <code>Neo4jClient</code> nuget package to the <code>Service</code> project. I want go into the details in how to connect and make the actual queries since there is tons of documentation on that. It took me roughly 30 minutes to figure everything out, so I guess you'll manage it as well.</p>\n<h2 id=\"theresult\">The result</h2>\n<p>I thought I show you the result before the actual code. The UI that comes with neo4j is a really amazing, easy to understand tool to explore your data. So after creating some customer, products and orders etc. you can get a full graphical overview of what is going on like in the picture below:</p>\n<p><img src=\"/content/images/2014/Jun/Graph.PNG\" alt=\"\"></p>\n<p>I don't have nodes for the order lines, instead I just link a basket to a product but put all the value that customer is going to pay for that product on the relation, and that is what is highlighted in the picture. When a customer finished an order and paid for the product I link the customer directly to the product with a <code>BOUGHT</code> relation, this will make it really to find recommendation for customers based on what other customers has bought. An example cypher, the language to query neo4j, query that will get all the customers and products that has bought the product another customer is looking at might look like this:</p>\n<pre><code>MATCH (p:Product {Id: '3b4173e6-b3cd-4565-868d-f810c5a04c43'})&lt;-\n[:BOUGHT]-(c:Customer)-[:BOUGHT]-&gt;(p2:Product)\nWHERE p2.Id &lt;&gt; '3b4173e6-b3cd-4565-868d-f810c5a04c43'\nRETURN c,p2\n</code></pre>\n<p>Here I first locate the product we are looking at right now and which customer that has bought that product. The second <code>BOUGHT</code> relation finds all the products those customers has bought and the last where clause filter out so we don't get the product we are looking at right now in the result set.</p>\n<p>As you can see, using a graphdatabase you will be able to query really complex structures in a much simpler way than you would in a sql database.</p>\n<h2 id=\"theupdatedindexingservice\">The updated IndexingService</h2>\n<p>Now when you know what we are going to produce, let's look at the code changes.</p>\n<p>To make it simple I just pushed all the code into the <code>IndexingService</code>, I know it should probably be somewhere else, but this is just for show. So the updated version looks like this:</p>\n<pre><code>internal class IndexingServie\n{\n    private Indexer _indexer;\n    private Dictionary&lt;Type, Action&lt;object&gt;&gt; _eventHandlerMapping;\n    private Position? _latestPosition;\n    private IEventStoreConnection _connection;\n    private GraphClient _graphClient;\n\n    public void Start()\n    {\n        _graphClient = CreateGraphClient();\n        _indexer = CreateIndexer();\n        _eventHandlerMapping = CreateEventHandlerMapping();\n        ConnectToEventstore();\n    }\n\n    private GraphClient CreateGraphClient()\n    {\n        var graphClient = new GraphClient(new Uri(&quot;http://localhost:7474/db/data&quot;));\n        graphClient.Connect();\n        DeleteAll(graphClient);\n        return graphClient;\n    }\n\n    private void DeleteAll(GraphClient graphClient)\n    {\n        graphClient.Cypher.Match(&quot;(n)&quot;)\n            .OptionalMatch(&quot;(n)-[r]-()&quot;)\n            .Delete(&quot;n,r&quot;)\n            .ExecuteWithoutResults();\n    }\n\n    private Indexer CreateIndexer()\n    {\n        var indexer = new Indexer();\n        indexer.Init();\n        return indexer;\n    }\n\n    private void ConnectToEventstore()\n    {\n\n        _latestPosition = Position.Start;\n        _connection = EventStoreConnectionWrapper.Connect();\n        _connection.Connected +=\n            (sender, args) =&gt; _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\n        Console.WriteLine(&quot;Indexing service started&quot;);\n    }\n\n    private void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\n    {\n        var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\n        if (@event != null)\n        {\n            var eventType = @event.GetType();\n            if (_eventHandlerMapping.ContainsKey(eventType))\n            {\n                _eventHandlerMapping[eventType](@event);\n            }\n        }\n        _latestPosition = arg2.OriginalPosition;\n    }\n\n    private Dictionary&lt;Type, Action&lt;object&gt;&gt; CreateEventHandlerMapping()\n    {\n        return new Dictionary&lt;Type, Action&lt;object&gt;&gt;()\n        {\n            {typeof (CustomerCreated), o =&gt; Handle(o as CustomerCreated)},\n            {typeof (CustomerMarkedAsPreferred), o =&gt; Handle(o as CustomerMarkedAsPreferred)},\n            {typeof (BasketCreated), o =&gt; Handle(o as BasketCreated)},\n            {typeof (ItemAdded), o =&gt; Handle(o as ItemAdded)},\n            {typeof (CustomerIsCheckingOutBasket), o =&gt; Handle(o as CustomerIsCheckingOutBasket)},\n            {typeof (BasketCheckedOut), o =&gt; Handle(o as BasketCheckedOut)},\n            {typeof (OrderCreated), o =&gt; Handle(o as OrderCreated)},\n            {typeof (ProductCreated), o =&gt; Handle(o as ProductCreated)}\n        }; \n    }\n\n    private void Handle(OrderCreated evt)\n    {\n        var existinBasket = _indexer.Get&lt;Basket&gt;(evt.BasketId);\n        existinBasket.BasketState = BasketState.Paid;\n        _indexer.Index(existinBasket);\n\n        _graphClient.Cypher\n            .Match(&quot;(customer:Customer)-[:HAS_BASKET]-&gt;(basket:Basket)-[]-&gt;(product:Product)&quot;)\n            .Where((Basket basket) =&gt; basket.Id == evt.BasketId)\n            .Create(&quot;customer-[:BOUGHT]-&gt;product&quot;)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(BasketCheckedOut evt)\n    {\n        var basket = _indexer.Get&lt;Basket&gt;(evt.Id);\n        basket.BasketState = BasketState.CheckedOut;\n        _indexer.Index(basket);\n    }\n\n    private void Handle(CustomerIsCheckingOutBasket evt)\n    {\n        var basket = _indexer.Get&lt;Basket&gt;(evt.Id);\n        basket.BasketState = BasketState.CheckingOut;\n        _indexer.Index(basket);\n    }\n\n    private void Handle(ItemAdded evt)\n    {\n        var existingBasket = _indexer.Get&lt;Basket&gt;(evt.Id);\n        var orderLines = existingBasket.OrderLines;\n        if (orderLines == null || orderLines.Length == 0)\n        {\n            existingBasket.OrderLines = new[] {evt.OrderLine};\n        }\n        else\n        {\n            var orderLineList = orderLines.ToList();\n            orderLineList.Add(evt.OrderLine);\n            existingBasket.OrderLines = orderLineList.ToArray();\n        }\n\n        _indexer.Index(existingBasket);\n\n        _graphClient.Cypher\n            .Match(&quot;(basket:Basket)&quot;, &quot;(product:Product)&quot;)\n            .Where((Basket basket) =&gt; basket.Id == evt.Id)\n            .AndWhere((Product product) =&gt; product.Id == evt.OrderLine.ProductId)\n            .Create(&quot;basket-[:HAS_ORDERLINE {orderLine}]-&gt;product&quot;)\n            .WithParam(&quot;orderLine&quot;, evt.OrderLine)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(BasketCreated evt)\n    {\n        var newBasket = new Basket()\n        {\n            Id = evt.Id,\n            OrderLines = null,\n            BasketState = BasketState.Shopping\n        };\n        _indexer.Index(newBasket);\n        _graphClient.Cypher\n            .Create(&quot;(basket:Basket {newBasket})&quot;)\n            .WithParam(&quot;newBasket&quot;, newBasket)\n            .ExecuteWithoutResults();\n\n        _graphClient.Cypher\n            .Match(&quot;(customer:Customer)&quot;, &quot;(basket:Basket)&quot;)\n            .Where((Customer customer) =&gt; customer.Id == evt.CustomerId)\n            .AndWhere((Basket basket) =&gt; basket.Id == evt.Id)\n            .Create(&quot;customer-[:HAS_BASKET]-&gt;basket&quot;)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(ProductCreated evt)\n    {\n        var product = new Product()\n        {\n            Id = evt.Id,\n            Name = evt.Name,\n            Price = evt.Price\n        };\n        _indexer.Index(product);\n        _graphClient.Cypher\n            .Create(&quot;(product:Product {newProduct})&quot;)\n            .WithParam(&quot;newProduct&quot;, product)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(CustomerMarkedAsPreferred evt)\n    {\n        var customer = _indexer.Get&lt;Customer&gt;(evt.Id);\n        customer.IsPreferred = true;\n        customer.Discount = evt.Discount;\n        _indexer.Index(customer);\n\n        _graphClient.Cypher\n            .Match(&quot;(c:Customer)&quot;)\n            .Where((Customer c) =&gt; c.Id == customer.Id)\n            .Set(&quot;c = {c}&quot;)\n            .WithParam(&quot;c&quot;, customer)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(CustomerCreated evt)\n    {\n        var customer = new Customer()\n        {\n            Id = evt.Id,\n            Name = evt.Name\n        };\n        _indexer.Index(customer);\n\n        _graphClient.Cypher\n            .Create(&quot;(customer:Customer {newCustomer})&quot;)\n            .WithParam(&quot;newCustomer&quot;, customer)\n            .ExecuteWithoutResults(); \n    }\n\n    public void Stop()\n    {\n    }\n}\n</code></pre>\n<p>The <code>CreateGraphClient</code> creates the graph client for us, as well as make sure the database is empty. The way I delete all the data is not how you should do it for large sets of data, again, this is just for show. When we have the graph client we can now update all our handlers to update the graph database as well as elasticsearch and we are all done.</p>\n<h2 id=\"everythinghastocometoanend\">Everything has to come to an end</h2>\n<p>It has been a really productive week, but now I almost consider this blog series as done. I'll write one more post with some discussion and answers to questions some people might have regarding this type of design. I hope I can finish the discussion later today. The last part is available here: <a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">Ending discussion</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"37","plaintext":"We need a change in the industry. All to often we defend old solutions just\nbecause we don't know the alternative. I know that sometime it might be good to\nslow things down and keep using the old stuff, but if that old stuff is storing\nstate using SQL to do so everytime I don't think you're making that choice with\nthe right reasons in mind. My contribution to this problem is a series of blog\npost where I'll walk you through how an alternative solution might look using\nevent sourcing and CQRS. To store the event I'll use eventstore, and to store\nthe view models I'll use elasticsearch.\n\nI'm planning to write a series of post, since it is too much to cover for one\npost, where I will walk you through different stages of the implementation of a\nsimple web shop. Everything will be implemented in C# and F# to start with but I\nmight change to 100 % F#, but that is something that will come later. The code\nwill be available in my CQRSShop repository on github\n[https://github.com/mastoj/CQRSShop]. Please feel free to comment or come with\npull request. Even though I'll use eventstore [http://geteventstore.com] and \nelasticsearch [http://www.elasticsearch.org/] I won't cover how to install those\nproducts. I will not cover CQRS in depth either since there is a lot of material\nabout CQRS and what it is, this will be a more practical view of how to do\nthings.\n\nAll the posts will be tagged so you can find them on this url: \nhttp://blog.tomasjansson.com/tag/cqrsshop/\n\nHope you'll enjoy the read.\n\nContent in the serie\n * Project structure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-project-structure/]\n * Infrastructure\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-infrastructure/]\n * The goal\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-the-goal/]\n * The first feature\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-first-features/]\n * The rest of the features\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-implementing-the-rest-of-the-features/]\n * Time for reflection\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/]\n * Building the API with Simple.Web\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-build-the-api-with-simple-web/]\n * Integrating elasticsearch\n   [http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-integrating-elasticsearch/]\n * Integrating neo4j\n * Ending discussion\n   [http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n\nThings change\nMy original plan was that this was going to be a summary post, but things\nchange. So in this post I'll add integration with neo4j [http://www.neo4j.org/] \nso I can ask really weird question like \"for product x give me all other\nproducts that some customer has bought where the total order value was above y.\"\nDoing that type of query is really tricky in sql and document databases, but\nmuch easier in a graphdatabase as neo4j.\n\nWe will be cheating\nThe quickes way to goal here is just to extend the IndexingService, and that is\nwhat I want to do. In the handlers methods I'll also update neo4j and not just\nelasticsearch. In a real environment you might want this as a separate service.\n\nGetting started\nAfter you've installed neo4j, which you have to figure out yourself how to do\n(click download, yes, next yes or something like that), you need to add the \nNeo4jClient nuget package to the Service project. I want go into the details in\nhow to connect and make the actual queries since there is tons of documentation\non that. It took me roughly 30 minutes to figure everything out, so I guess\nyou'll manage it as well.\n\nThe result\nI thought I show you the result before the actual code. The UI that comes with\nneo4j is a really amazing, easy to understand tool to explore your data. So\nafter creating some customer, products and orders etc. you can get a full\ngraphical overview of what is going on like in the picture below:\n\n\n\nI don't have nodes for the order lines, instead I just link a basket to a\nproduct but put all the value that customer is going to pay for that product on\nthe relation, and that is what is highlighted in the picture. When a customer\nfinished an order and paid for the product I link the customer directly to the\nproduct with a BOUGHT relation, this will make it really to find recommendation\nfor customers based on what other customers has bought. An example cypher, the\nlanguage to query neo4j, query that will get all the customers and products that\nhas bought the product another customer is looking at might look like this:\n\nMATCH (p:Product {Id: '3b4173e6-b3cd-4565-868d-f810c5a04c43'})<-\n[:BOUGHT]-(c:Customer)-[:BOUGHT]->(p2:Product)\nWHERE p2.Id <> '3b4173e6-b3cd-4565-868d-f810c5a04c43'\nRETURN c,p2\n\n\nHere I first locate the product we are looking at right now and which customer\nthat has bought that product. The second BOUGHT relation finds all the products\nthose customers has bought and the last where clause filter out so we don't get\nthe product we are looking at right now in the result set.\n\nAs you can see, using a graphdatabase you will be able to query really complex\nstructures in a much simpler way than you would in a sql database.\n\nThe updated IndexingService\nNow when you know what we are going to produce, let's look at the code changes.\n\nTo make it simple I just pushed all the code into the IndexingService, I know it\nshould probably be somewhere else, but this is just for show. So the updated\nversion looks like this:\n\ninternal class IndexingServie\n{\n    private Indexer _indexer;\n    private Dictionary<Type, Action<object>> _eventHandlerMapping;\n    private Position? _latestPosition;\n    private IEventStoreConnection _connection;\n    private GraphClient _graphClient;\n\n    public void Start()\n    {\n        _graphClient = CreateGraphClient();\n        _indexer = CreateIndexer();\n        _eventHandlerMapping = CreateEventHandlerMapping();\n        ConnectToEventstore();\n    }\n\n    private GraphClient CreateGraphClient()\n    {\n        var graphClient = new GraphClient(new Uri(\"http://localhost:7474/db/data\"));\n        graphClient.Connect();\n        DeleteAll(graphClient);\n        return graphClient;\n    }\n\n    private void DeleteAll(GraphClient graphClient)\n    {\n        graphClient.Cypher.Match(\"(n)\")\n            .OptionalMatch(\"(n)-[r]-()\")\n            .Delete(\"n,r\")\n            .ExecuteWithoutResults();\n    }\n\n    private Indexer CreateIndexer()\n    {\n        var indexer = new Indexer();\n        indexer.Init();\n        return indexer;\n    }\n\n    private void ConnectToEventstore()\n    {\n\n        _latestPosition = Position.Start;\n        _connection = EventStoreConnectionWrapper.Connect();\n        _connection.Connected +=\n            (sender, args) => _connection.SubscribeToAllFrom(_latestPosition, false, HandleEvent);\n        Console.WriteLine(\"Indexing service started\");\n    }\n\n    private void HandleEvent(EventStoreCatchUpSubscription arg1, ResolvedEvent arg2)\n    {\n        var @event = EventSerialization.DeserializeEvent(arg2.OriginalEvent);\n        if (@event != null)\n        {\n            var eventType = @event.GetType();\n            if (_eventHandlerMapping.ContainsKey(eventType))\n            {\n                _eventHandlerMapping[eventType](@event);\n            }\n        }\n        _latestPosition = arg2.OriginalPosition;\n    }\n\n    private Dictionary<Type, Action<object>> CreateEventHandlerMapping()\n    {\n        return new Dictionary<Type, Action<object>>()\n        {\n            {typeof (CustomerCreated), o => Handle(o as CustomerCreated)},\n            {typeof (CustomerMarkedAsPreferred), o => Handle(o as CustomerMarkedAsPreferred)},\n            {typeof (BasketCreated), o => Handle(o as BasketCreated)},\n            {typeof (ItemAdded), o => Handle(o as ItemAdded)},\n            {typeof (CustomerIsCheckingOutBasket), o => Handle(o as CustomerIsCheckingOutBasket)},\n            {typeof (BasketCheckedOut), o => Handle(o as BasketCheckedOut)},\n            {typeof (OrderCreated), o => Handle(o as OrderCreated)},\n            {typeof (ProductCreated), o => Handle(o as ProductCreated)}\n        }; \n    }\n\n    private void Handle(OrderCreated evt)\n    {\n        var existinBasket = _indexer.Get<Basket>(evt.BasketId);\n        existinBasket.BasketState = BasketState.Paid;\n        _indexer.Index(existinBasket);\n\n        _graphClient.Cypher\n            .Match(\"(customer:Customer)-[:HAS_BASKET]->(basket:Basket)-[]->(product:Product)\")\n            .Where((Basket basket) => basket.Id == evt.BasketId)\n            .Create(\"customer-[:BOUGHT]->product\")\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(BasketCheckedOut evt)\n    {\n        var basket = _indexer.Get<Basket>(evt.Id);\n        basket.BasketState = BasketState.CheckedOut;\n        _indexer.Index(basket);\n    }\n\n    private void Handle(CustomerIsCheckingOutBasket evt)\n    {\n        var basket = _indexer.Get<Basket>(evt.Id);\n        basket.BasketState = BasketState.CheckingOut;\n        _indexer.Index(basket);\n    }\n\n    private void Handle(ItemAdded evt)\n    {\n        var existingBasket = _indexer.Get<Basket>(evt.Id);\n        var orderLines = existingBasket.OrderLines;\n        if (orderLines == null || orderLines.Length == 0)\n        {\n            existingBasket.OrderLines = new[] {evt.OrderLine};\n        }\n        else\n        {\n            var orderLineList = orderLines.ToList();\n            orderLineList.Add(evt.OrderLine);\n            existingBasket.OrderLines = orderLineList.ToArray();\n        }\n\n        _indexer.Index(existingBasket);\n\n        _graphClient.Cypher\n            .Match(\"(basket:Basket)\", \"(product:Product)\")\n            .Where((Basket basket) => basket.Id == evt.Id)\n            .AndWhere((Product product) => product.Id == evt.OrderLine.ProductId)\n            .Create(\"basket-[:HAS_ORDERLINE {orderLine}]->product\")\n            .WithParam(\"orderLine\", evt.OrderLine)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(BasketCreated evt)\n    {\n        var newBasket = new Basket()\n        {\n            Id = evt.Id,\n            OrderLines = null,\n            BasketState = BasketState.Shopping\n        };\n        _indexer.Index(newBasket);\n        _graphClient.Cypher\n            .Create(\"(basket:Basket {newBasket})\")\n            .WithParam(\"newBasket\", newBasket)\n            .ExecuteWithoutResults();\n\n        _graphClient.Cypher\n            .Match(\"(customer:Customer)\", \"(basket:Basket)\")\n            .Where((Customer customer) => customer.Id == evt.CustomerId)\n            .AndWhere((Basket basket) => basket.Id == evt.Id)\n            .Create(\"customer-[:HAS_BASKET]->basket\")\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(ProductCreated evt)\n    {\n        var product = new Product()\n        {\n            Id = evt.Id,\n            Name = evt.Name,\n            Price = evt.Price\n        };\n        _indexer.Index(product);\n        _graphClient.Cypher\n            .Create(\"(product:Product {newProduct})\")\n            .WithParam(\"newProduct\", product)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(CustomerMarkedAsPreferred evt)\n    {\n        var customer = _indexer.Get<Customer>(evt.Id);\n        customer.IsPreferred = true;\n        customer.Discount = evt.Discount;\n        _indexer.Index(customer);\n\n        _graphClient.Cypher\n            .Match(\"(c:Customer)\")\n            .Where((Customer c) => c.Id == customer.Id)\n            .Set(\"c = {c}\")\n            .WithParam(\"c\", customer)\n            .ExecuteWithoutResults();\n    }\n\n    private void Handle(CustomerCreated evt)\n    {\n        var customer = new Customer()\n        {\n            Id = evt.Id,\n            Name = evt.Name\n        };\n        _indexer.Index(customer);\n\n        _graphClient.Cypher\n            .Create(\"(customer:Customer {newCustomer})\")\n            .WithParam(\"newCustomer\", customer)\n            .ExecuteWithoutResults(); \n    }\n\n    public void Stop()\n    {\n    }\n}\n\n\nThe CreateGraphClient creates the graph client for us, as well as make sure the\ndatabase is empty. The way I delete all the data is not how you should do it for\nlarge sets of data, again, this is just for show. When we have the graph client\nwe can now update all our handlers to update the graph database as well as\nelasticsearch and we are all done.\n\nEverything has to come to an end\nIt has been a really productive week, but now I almost consider this blog series\nas done. I'll write one more post with some discussion and answers to questions\nsome people might have regarding this type of design. I hope I can finish the\ndiscussion later today. The last part is available here: Ending discussion\n[http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/]\n.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-06-28T11:47:10.000Z","updated_at":"2014-07-16T06:16:50.000Z","published_at":"2014-06-28T11:47:15.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe059","uuid":"d5a4baea-2414-46ee-8666-841de4085d83","title":"So you want to program the web with FSharp?","slug":"so-you-want-to-program-the-web-with-fsharp","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I couldn't find any really great how tos about doing web programming with FSharp, so I thought I would figure it out myself my own way.\\n\\n# The problem\\nThe problem with web programming and FSharp is two as I see it right now. One is regarding the tooling, the default tooling from Visual Studio which I think most people use doesn't have a project template for web together with FSharp. The second part is that a lot of web frameworks are mutable in one way or another so it is hard doing pure development. With that said I'll show I way/work around in how to do web development on OWIN in general and with Simple.Web to be more specific.\\n\\n# FSharp and OWIN\\nTo get started you start of as you should with any web project, create an empty C# web project.\\n\\n![New project](/content/images/2014/Jul/NewProject.PNG)\\n\\nI'm going for a windows hosting so also install the nuget package `Microsoft.Owin.Host.SystemWeb`. The next thing we are going to do is add a second project, this time a FSharp library. Update the `Library.fs` file so it looks something like this:\\n\\n\\n    namespace Web2Empty.Web\\n    open Owin\\n\\n    type OwinAppSetup() =\\n        member this.Configuration (app:IAppBuilder) = \\n            app.Run(fun c -> c.Response.WriteAsync(\\\"Hello FSharp World!\\\"))\\n\\nThat is going to be your OWIN entry point. To get this to compile you need to add `Owin` nuget package to the FSharp project. When it compiles you add a line like this to your `Assembly.cs` file in the C# project and add a project reference to the FSharp project: \\n\\n    [assembly: OwinStartup(typeof(OwinAppSetup))]\\n\\nThis will tell the OWIN host to use the `OwinAppSetup` class which is defined in the FSharp class library to run the web application. If you run the application now you should get some output on the screen.\\n\\n# FSharp and Simple.Web\\nThe last couple of months I've come to like Simple.Web more and more, so I wanted to try to get that to work together with FSharp. To really put it to a test I wanted to use discriminated unions as well.\\n\\n## Adding Simple.Web\\nAdding `Simple.Web` with json support is quite easy, just install the `Simple.Web.JsonNet` package in the FSharp project and you'll get the dependencies you packages. When it is installed you need to update `Newtonsoft.Json` to the latest version to get the FSharp support. \\n\\n## The sample data\\nThe data structure I'll use in my requests look like this\\n\\n    type Life =\\n        | Great of reason: string\\n        | Sucks of reason: string\\n        | Complex of Life * Life\\n\\n    type LifeContainer = {Life: Life}\\n\\nFor some reason I need the container to be able to serialize the request when using Simple.Web, it doesn't bother me that much so I can live with it.\\n\\n## Creating the handlers\\n`Simple.Web` is using a handler/class per route per request model, so to handle this I need to create one FSharp type per route per request. To support a `GET` request you need something like this:\\n\\n    [<UriTemplate(\\\"/\\\")>]\\n    type RootGetEndPoint() = \\n        interface IGet with\\n            member this.Get() = Status.OK\\n        interface IOutput<LifeContainer> with\\n            member this.Output = {Life=Complex(Great(\\\"FSharp\\\"), Sucks(\\\"Inheritance\\\"))}\\n\\nHere we first have the `UriTemplateAttribute` that defines which url this handler should handle. After that it is straight forward implementation of the interfaces we need to answer a request.\\n\\nFor the `POST` endpoint we don't need to use the container for the input but we still need it for the output: \\n\\n    [<UriTemplate(\\\"/\\\")>]\\n    type RootPostEndPoint() = \\n        let mutable life:Option<Life> = None\\n        interface IPost with\\n            member this.Post() = Status.OK    \\n        interface IInput<Life> with\\n            member this.Input with set(value) = life <- Some(value)\\n        interface IOutput<LifeContainer> with\\n            member this.Output = {Life = life.Value}\\n\\nThis handler takes the input and returns it with a 200 response.\\n\\nThe last part is the update of the `OwinAppSetup`: \\n\\n    type OwinAppSetup() =\\n        static member enforcedRefs = [typeof<Simple.Web.JsonNet.JsonMediaTypeHandler>] \\n        member this.Configuration (app:IAppBuilder) = \\n            JsonConvert.DefaultSettings <- fun () -> \\n                let settings = new JsonSerializerSettings()\\n                settings.TypeNameHandling <- TypeNameHandling.Objects\\n                settings\\n            app.Run(fun c -> \\n                Application.App(\\n                    fun _ -> \\n                        c.Response.WriteAsync(\\\"Hello FSharp world!\\\")).Invoke(c.Environment))\\n\\nThe first static part is to load the media type handler, I don't know a better way of doing it at the moment for Simple.Web and I don't think it exist a better way at the momement. The next part is the configuration part where I first configure `Json.NET` and then setting up the application. The application is using `Application.App` from `Simple.Web` to configure `Simple.Web` but we still run the next step if we didn't found a route. So if you go anywhere except for the root, \\\"/\\\", you'll see \\\"Hello FSharp world!\\\".\\n\\nWitht that finished it is now possible to post discriminated unions like: \\n\\n    {\\n        \\\"case\\\":\\\"Complex\\\",\\n        \\\"fields\\\":[\\n            {\\\"case\\\":\\\"Great\\\",\\\"fields\\\":[\\\"FSharp\\\"]},\\n            {\\\"case\\\":\\\"Sucks\\\",\\\"fields\\\":[\\\"What\\\"]}\\n        ]\\n    }\\n\\nThis will return in a 200 and you'll get a container back with that `Life`. \\n\\nThe full code for the fsharp file looks like this: \\n\\n    namespace Web2Empty.Web\\n    open System.Text\\n    open Owin\\n    open Simple.Web\\n    open Simple.Web.Behaviors\\n    open Simple.Web.MediaTypeHandling\\n    open Newtonsoft.Json\\n\\n    type Life =\\n        | Great of reason: string\\n        | Sucks of reason: string\\n        | Complex of Life * Life\\n\\n    type LifeContainer = {Life: Life}\\n\\n    [<UriTemplate(\\\"/\\\")>]\\n    type RootGetEndPoint() = \\n        interface IGet with\\n            member this.Get() = Status.OK\\n        interface IOutput<LifeContainer> with\\n            member this.Output = {Life=Complex(Great(\\\"FSharp\\\"), Sucks(\\\"Inheritance\\\"))}\\n\\n    [<UriTemplate(\\\"/\\\")>]\\n    type RootPostEndPoint() = \\n        let mutable life:Option<Life> = None\\n        interface IPost with\\n            member this.Post() = Status.OK    \\n        interface IInput<Life> with\\n            member this.Input with set(value) = life <- Some(value)\\n        interface IOutput<LifeContainer> with\\n            member this.Output = {Life = life.Value}\\n\\n    type OwinAppSetup() =\\n        static member enforcedRefs = [typeof<Simple.Web.JsonNet.JsonMediaTypeHandler>] \\n        member this.Configuration (app:IAppBuilder) = \\n            JsonConvert.DefaultSettings <- fun () -> \\n                let settings = new JsonSerializerSettings()\\n                settings.TypeNameHandling <- TypeNameHandling.Objects\\n                settings\\n            app.Run(fun c -> \\n                Application.App(\\n                    fun _ -> \\n                        c.Response.WriteAsync(\\\"Hello FSharp world!\\\")).Invoke(c.Environment))\\n\\n# Ending note\\nI did not work much on structuring the code into separate files etc. since it was not my goal, I just wanted it to work. Also, I'm not sure if this is the recommended way of doing things, but it works. Of course it would be better if you just needed one project instead of two, but this is a nice work around until that time. I'm not sure, but you might be able to manipulatae the `.fsproj` file to get it working without the extra project, but that is just a wild guess.\\n\\nAs you can see, there is nothing stopping us from doing all the code on the server in FSharp, we do miss some code generation support from Visual Studio with scaffolding, but that is something I want miss since I haven't found it useful for \\\"real\\\" business applications.\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I couldn't find any really great how tos about doing web programming with FSharp, so I thought I would figure it out myself my own way.</p>\n<h1 id=\"theproblem\">The problem</h1>\n<p>The problem with web programming and FSharp is two as I see it right now. One is regarding the tooling, the default tooling from Visual Studio which I think most people use doesn't have a project template for web together with FSharp. The second part is that a lot of web frameworks are mutable in one way or another so it is hard doing pure development. With that said I'll show I way/work around in how to do web development on OWIN in general and with Simple.Web to be more specific.</p>\n<h1 id=\"fsharpandowin\">FSharp and OWIN</h1>\n<p>To get started you start of as you should with any web project, create an empty C# web project.</p>\n<p><img src=\"/content/images/2014/Jul/NewProject.PNG\" alt=\"New project\"></p>\n<p>I'm going for a windows hosting so also install the nuget package <code>Microsoft.Owin.Host.SystemWeb</code>. The next thing we are going to do is add a second project, this time a FSharp library. Update the <code>Library.fs</code> file so it looks something like this:</p>\n<pre><code>namespace Web2Empty.Web\nopen Owin\n\ntype OwinAppSetup() =\n    member this.Configuration (app:IAppBuilder) = \n        app.Run(fun c -&gt; c.Response.WriteAsync(&quot;Hello FSharp World!&quot;))\n</code></pre>\n<p>That is going to be your OWIN entry point. To get this to compile you need to add <code>Owin</code> nuget package to the FSharp project. When it compiles you add a line like this to your <code>Assembly.cs</code> file in the C# project and add a project reference to the FSharp project:</p>\n<pre><code>[assembly: OwinStartup(typeof(OwinAppSetup))]\n</code></pre>\n<p>This will tell the OWIN host to use the <code>OwinAppSetup</code> class which is defined in the FSharp class library to run the web application. If you run the application now you should get some output on the screen.</p>\n<h1 id=\"fsharpandsimpleweb\">FSharp and Simple.Web</h1>\n<p>The last couple of months I've come to like Simple.Web more and more, so I wanted to try to get that to work together with FSharp. To really put it to a test I wanted to use discriminated unions as well.</p>\n<h2 id=\"addingsimpleweb\">Adding Simple.Web</h2>\n<p>Adding <code>Simple.Web</code> with json support is quite easy, just install the <code>Simple.Web.JsonNet</code> package in the FSharp project and you'll get the dependencies you packages. When it is installed you need to update <code>Newtonsoft.Json</code> to the latest version to get the FSharp support.</p>\n<h2 id=\"thesampledata\">The sample data</h2>\n<p>The data structure I'll use in my requests look like this</p>\n<pre><code>type Life =\n    | Great of reason: string\n    | Sucks of reason: string\n    | Complex of Life * Life\n\ntype LifeContainer = {Life: Life}\n</code></pre>\n<p>For some reason I need the container to be able to serialize the request when using Simple.Web, it doesn't bother me that much so I can live with it.</p>\n<h2 id=\"creatingthehandlers\">Creating the handlers</h2>\n<p><code>Simple.Web</code> is using a handler/class per route per request model, so to handle this I need to create one FSharp type per route per request. To support a <code>GET</code> request you need something like this:</p>\n<pre><code>[&lt;UriTemplate(&quot;/&quot;)&gt;]\ntype RootGetEndPoint() = \n    interface IGet with\n        member this.Get() = Status.OK\n    interface IOutput&lt;LifeContainer&gt; with\n        member this.Output = {Life=Complex(Great(&quot;FSharp&quot;), Sucks(&quot;Inheritance&quot;))}\n</code></pre>\n<p>Here we first have the <code>UriTemplateAttribute</code> that defines which url this handler should handle. After that it is straight forward implementation of the interfaces we need to answer a request.</p>\n<p>For the <code>POST</code> endpoint we don't need to use the container for the input but we still need it for the output:</p>\n<pre><code>[&lt;UriTemplate(&quot;/&quot;)&gt;]\ntype RootPostEndPoint() = \n    let mutable life:Option&lt;Life&gt; = None\n    interface IPost with\n        member this.Post() = Status.OK    \n    interface IInput&lt;Life&gt; with\n        member this.Input with set(value) = life &lt;- Some(value)\n    interface IOutput&lt;LifeContainer&gt; with\n        member this.Output = {Life = life.Value}\n</code></pre>\n<p>This handler takes the input and returns it with a 200 response.</p>\n<p>The last part is the update of the <code>OwinAppSetup</code>:</p>\n<pre><code>type OwinAppSetup() =\n    static member enforcedRefs = [typeof&lt;Simple.Web.JsonNet.JsonMediaTypeHandler&gt;] \n    member this.Configuration (app:IAppBuilder) = \n        JsonConvert.DefaultSettings &lt;- fun () -&gt; \n            let settings = new JsonSerializerSettings()\n            settings.TypeNameHandling &lt;- TypeNameHandling.Objects\n            settings\n        app.Run(fun c -&gt; \n            Application.App(\n                fun _ -&gt; \n                    c.Response.WriteAsync(&quot;Hello FSharp world!&quot;)).Invoke(c.Environment))\n</code></pre>\n<p>The first static part is to load the media type handler, I don't know a better way of doing it at the moment for Simple.Web and I don't think it exist a better way at the momement. The next part is the configuration part where I first configure <code>Json.NET</code> and then setting up the application. The application is using <code>Application.App</code> from <code>Simple.Web</code> to configure <code>Simple.Web</code> but we still run the next step if we didn't found a route. So if you go anywhere except for the root, &quot;/&quot;, you'll see &quot;Hello FSharp world!&quot;.</p>\n<p>Witht that finished it is now possible to post discriminated unions like:</p>\n<pre><code>{\n    &quot;case&quot;:&quot;Complex&quot;,\n    &quot;fields&quot;:[\n        {&quot;case&quot;:&quot;Great&quot;,&quot;fields&quot;:[&quot;FSharp&quot;]},\n        {&quot;case&quot;:&quot;Sucks&quot;,&quot;fields&quot;:[&quot;What&quot;]}\n    ]\n}\n</code></pre>\n<p>This will return in a 200 and you'll get a container back with that <code>Life</code>.</p>\n<p>The full code for the fsharp file looks like this:</p>\n<pre><code>namespace Web2Empty.Web\nopen System.Text\nopen Owin\nopen Simple.Web\nopen Simple.Web.Behaviors\nopen Simple.Web.MediaTypeHandling\nopen Newtonsoft.Json\n\ntype Life =\n    | Great of reason: string\n    | Sucks of reason: string\n    | Complex of Life * Life\n\ntype LifeContainer = {Life: Life}\n\n[&lt;UriTemplate(&quot;/&quot;)&gt;]\ntype RootGetEndPoint() = \n    interface IGet with\n        member this.Get() = Status.OK\n    interface IOutput&lt;LifeContainer&gt; with\n        member this.Output = {Life=Complex(Great(&quot;FSharp&quot;), Sucks(&quot;Inheritance&quot;))}\n\n[&lt;UriTemplate(&quot;/&quot;)&gt;]\ntype RootPostEndPoint() = \n    let mutable life:Option&lt;Life&gt; = None\n    interface IPost with\n        member this.Post() = Status.OK    \n    interface IInput&lt;Life&gt; with\n        member this.Input with set(value) = life &lt;- Some(value)\n    interface IOutput&lt;LifeContainer&gt; with\n        member this.Output = {Life = life.Value}\n\ntype OwinAppSetup() =\n    static member enforcedRefs = [typeof&lt;Simple.Web.JsonNet.JsonMediaTypeHandler&gt;] \n    member this.Configuration (app:IAppBuilder) = \n        JsonConvert.DefaultSettings &lt;- fun () -&gt; \n            let settings = new JsonSerializerSettings()\n            settings.TypeNameHandling &lt;- TypeNameHandling.Objects\n            settings\n        app.Run(fun c -&gt; \n            Application.App(\n                fun _ -&gt; \n                    c.Response.WriteAsync(&quot;Hello FSharp world!&quot;)).Invoke(c.Environment))\n</code></pre>\n<h1 id=\"endingnote\">Ending note</h1>\n<p>I did not work much on structuring the code into separate files etc. since it was not my goal, I just wanted it to work. Also, I'm not sure if this is the recommended way of doing things, but it works. Of course it would be better if you just needed one project instead of two, but this is a nice work around until that time. I'm not sure, but you might be able to manipulatae the <code>.fsproj</code> file to get it working without the extra project, but that is just a wild guess.</p>\n<p>As you can see, there is nothing stopping us from doing all the code on the server in FSharp, we do miss some code generation support from Visual Studio with scaffolding, but that is something I want miss since I haven't found it useful for &quot;real&quot; business applications.</p>\n<!--kg-card-end: markdown-->","comment_id":"38","plaintext":"I couldn't find any really great how tos about doing web programming with\nFSharp, so I thought I would figure it out myself my own way.\n\nThe problem\nThe problem with web programming and FSharp is two as I see it right now. One is\nregarding the tooling, the default tooling from Visual Studio which I think most\npeople use doesn't have a project template for web together with FSharp. The\nsecond part is that a lot of web frameworks are mutable in one way or another so\nit is hard doing pure development. With that said I'll show I way/work around in\nhow to do web development on OWIN in general and with Simple.Web to be more\nspecific.\n\nFSharp and OWIN\nTo get started you start of as you should with any web project, create an empty\nC# web project.\n\n\n\nI'm going for a windows hosting so also install the nuget package \nMicrosoft.Owin.Host.SystemWeb. The next thing we are going to do is add a second\nproject, this time a FSharp library. Update the Library.fs file so it looks\nsomething like this:\n\nnamespace Web2Empty.Web\nopen Owin\n\ntype OwinAppSetup() =\n    member this.Configuration (app:IAppBuilder) = \n        app.Run(fun c -> c.Response.WriteAsync(\"Hello FSharp World!\"))\n\n\nThat is going to be your OWIN entry point. To get this to compile you need to\nadd Owin nuget package to the FSharp project. When it compiles you add a line\nlike this to your Assembly.cs file in the C# project and add a project reference\nto the FSharp project:\n\n[assembly: OwinStartup(typeof(OwinAppSetup))]\n\n\nThis will tell the OWIN host to use the OwinAppSetup class which is defined in\nthe FSharp class library to run the web application. If you run the application\nnow you should get some output on the screen.\n\nFSharp and Simple.Web\nThe last couple of months I've come to like Simple.Web more and more, so I\nwanted to try to get that to work together with FSharp. To really put it to a\ntest I wanted to use discriminated unions as well.\n\nAdding Simple.Web\nAdding Simple.Web with json support is quite easy, just install the \nSimple.Web.JsonNet package in the FSharp project and you'll get the dependencies\nyou packages. When it is installed you need to update Newtonsoft.Json to the\nlatest version to get the FSharp support.\n\nThe sample data\nThe data structure I'll use in my requests look like this\n\ntype Life =\n    | Great of reason: string\n    | Sucks of reason: string\n    | Complex of Life * Life\n\ntype LifeContainer = {Life: Life}\n\n\nFor some reason I need the container to be able to serialize the request when\nusing Simple.Web, it doesn't bother me that much so I can live with it.\n\nCreating the handlers\nSimple.Web is using a handler/class per route per request model, so to handle\nthis I need to create one FSharp type per route per request. To support a GET \nrequest you need something like this:\n\n[<UriTemplate(\"/\")>]\ntype RootGetEndPoint() = \n    interface IGet with\n        member this.Get() = Status.OK\n    interface IOutput<LifeContainer> with\n        member this.Output = {Life=Complex(Great(\"FSharp\"), Sucks(\"Inheritance\"))}\n\n\nHere we first have the UriTemplateAttribute that defines which url this handler\nshould handle. After that it is straight forward implementation of the\ninterfaces we need to answer a request.\n\nFor the POST endpoint we don't need to use the container for the input but we\nstill need it for the output:\n\n[<UriTemplate(\"/\")>]\ntype RootPostEndPoint() = \n    let mutable life:Option<Life> = None\n    interface IPost with\n        member this.Post() = Status.OK    \n    interface IInput<Life> with\n        member this.Input with set(value) = life <- Some(value)\n    interface IOutput<LifeContainer> with\n        member this.Output = {Life = life.Value}\n\n\nThis handler takes the input and returns it with a 200 response.\n\nThe last part is the update of the OwinAppSetup:\n\ntype OwinAppSetup() =\n    static member enforcedRefs = [typeof<Simple.Web.JsonNet.JsonMediaTypeHandler>] \n    member this.Configuration (app:IAppBuilder) = \n        JsonConvert.DefaultSettings <- fun () -> \n            let settings = new JsonSerializerSettings()\n            settings.TypeNameHandling <- TypeNameHandling.Objects\n            settings\n        app.Run(fun c -> \n            Application.App(\n                fun _ -> \n                    c.Response.WriteAsync(\"Hello FSharp world!\")).Invoke(c.Environment))\n\n\nThe first static part is to load the media type handler, I don't know a better\nway of doing it at the moment for Simple.Web and I don't think it exist a better\nway at the momement. The next part is the configuration part where I first\nconfigure Json.NET and then setting up the application. The application is using \nApplication.App from Simple.Web to configure Simple.Web but we still run the\nnext step if we didn't found a route. So if you go anywhere except for the root,\n\"/\", you'll see \"Hello FSharp world!\".\n\nWitht that finished it is now possible to post discriminated unions like:\n\n{\n    \"case\":\"Complex\",\n    \"fields\":[\n        {\"case\":\"Great\",\"fields\":[\"FSharp\"]},\n        {\"case\":\"Sucks\",\"fields\":[\"What\"]}\n    ]\n}\n\n\nThis will return in a 200 and you'll get a container back with that Life.\n\nThe full code for the fsharp file looks like this:\n\nnamespace Web2Empty.Web\nopen System.Text\nopen Owin\nopen Simple.Web\nopen Simple.Web.Behaviors\nopen Simple.Web.MediaTypeHandling\nopen Newtonsoft.Json\n\ntype Life =\n    | Great of reason: string\n    | Sucks of reason: string\n    | Complex of Life * Life\n\ntype LifeContainer = {Life: Life}\n\n[<UriTemplate(\"/\")>]\ntype RootGetEndPoint() = \n    interface IGet with\n        member this.Get() = Status.OK\n    interface IOutput<LifeContainer> with\n        member this.Output = {Life=Complex(Great(\"FSharp\"), Sucks(\"Inheritance\"))}\n\n[<UriTemplate(\"/\")>]\ntype RootPostEndPoint() = \n    let mutable life:Option<Life> = None\n    interface IPost with\n        member this.Post() = Status.OK    \n    interface IInput<Life> with\n        member this.Input with set(value) = life <- Some(value)\n    interface IOutput<LifeContainer> with\n        member this.Output = {Life = life.Value}\n\ntype OwinAppSetup() =\n    static member enforcedRefs = [typeof<Simple.Web.JsonNet.JsonMediaTypeHandler>] \n    member this.Configuration (app:IAppBuilder) = \n        JsonConvert.DefaultSettings <- fun () -> \n            let settings = new JsonSerializerSettings()\n            settings.TypeNameHandling <- TypeNameHandling.Objects\n            settings\n        app.Run(fun c -> \n            Application.App(\n                fun _ -> \n                    c.Response.WriteAsync(\"Hello FSharp world!\")).Invoke(c.Environment))\n\n\nEnding note\nI did not work much on structuring the code into separate files etc. since it\nwas not my goal, I just wanted it to work. Also, I'm not sure if this is the\nrecommended way of doing things, but it works. Of course it would be better if\nyou just needed one project instead of two, but this is a nice work around until\nthat time. I'm not sure, but you might be able to manipulatae the .fsproj file\nto get it working without the extra project, but that is just a wild guess.\n\nAs you can see, there is nothing stopping us from doing all the code on the\nserver in FSharp, we do miss some code generation support from Visual Studio\nwith scaffolding, but that is something I want miss since I haven't found it\nuseful for \"real\" business applications.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-07-07T17:29:20.000Z","updated_at":"2014-07-07T19:02:28.000Z","published_at":"2014-07-07T18:11:38.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05a","uuid":"accc0e27-47bc-46a5-8780-46ec32f11547","title":"Useful OWIN middleware: HealthCheck","slug":"useful-owin-middleware-healthcheck","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In many of todays web application we often forget to implement some kind of health check. I think this is because we tend to overthink the problem. Health check might be hard to solve in a generic way that checks the complete system health, but it doesn't have to be hard to implement a generic middleware that every application can hook into and provide the status of the application but don't have to implement the actual endpoint. One advantage you get with this approach is that each application developer still controls what needs to be checked, but they all should follow a protocol to provide the result which makes it easier to monitor. The sample middleware I'm going to show is clean and simple, but that doesn't mean it fits everyone. \\n\\n## The goal\\nThe goal of the middleware is to make it really simple to add health check to an existing application and specify how the health check should be executed inside the boundaries of the application. The code in the application should look like this:\\n\\n    public class Startup\\n    {\\n        public void Configuration(IAppBuilder app)\\n        {\\n            var config =new HealthCheckConfiguration(\\\"/hc\\\", new Checker());\\n            app.UseHealthCheck(config);\\n            app.Run(async context =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello\\\");\\n            });\\n        } \\n    }\\n\\n    public class Checker : ICheckHealth\\n    {\\n        public HealthStatus CheckHealth()\\n        {\\n            return new HealthStatus()\\n            {\\n                Components = new[] {new ComponentHealth(\\\"Universe\\\", ComponentStatus.Weird)}\\n            };\\n        }\\n    }\\n\\nHere I have the `Checker` which is specific for this application, and I've also a simple extension making it possible to use `app.UseHealthCheck(config)`.\\n\\n## The health of the application\\nThey way I thought about this is that just responding with a `pong` to a request doesn't actually say something about the health of the application, how do you know that the external dependencies are up and running with a simple `pong`? So to tackle this I implented a couple of classes, first the `HealthStatus`:\\n\\n    public class HealthStatus\\n    {\\n        public IEnumerable<ComponentHealth> Components { get; set; }\\n    }\\n\\nthe goal with this class is to give the application developer a chance to specify the health of different components that the application is dependent upon. The `ComponentHealth` is also a straightforward class:\\n\\n    public class ComponentHealth\\n    {\\n        public string ComponentName { get; set; }\\n\\n        [JsonConverter(typeof(StringEnumConverter))]\\n        public ComponentStatus StatusName\\n        {\\n            get { return Status; }\\n        }\\n\\n        public ComponentStatus Status { get; set; }\\n\\n        public ComponentHealth(string componentName, ComponentStatus status)\\n        {\\n            ComponentName = componentName;\\n            Status = status;\\n        }\\n    }\\n\\n    public enum ComponentStatus\\n    {\\n        Healthy,    // Ok\\n        Fatal,      // Error\\n        Weird       // Might be problems\\n    }\\n\\nEach component has a name and status. I'm using [Json.NET](http://james.newtonking.com/json) to provide the result and to serialize the status with both the enum value and the enum name I'm using the `JsonConverter` attribute.\\n\\n## Health checking\\nI've abstracted away the actual health checking so that the applications can define how it should be checked. I have also implemented a default health checker if you only want a simple `pong` response. The abstractions and implementation are also straightforward: \\n\\n    public interface ICheckHealth\\n    {\\n        HealthStatus CheckHealth();\\n    }\\n\\n    internal class DefaultHealthChecker : ICheckHealth\\n    {\\n        public HealthStatus CheckHealth()\\n        {\\n            return new HealthStatus()\\n            {\\n                Components = new [] { new ComponentHealth(\\\"Web\\\", ComponentStatus.Healthy)}\\n            };\\n        }\\n    }\\n\\n## Configurating the middleware\\nSince I want to make it easy for the application developer to specify health checker and endpoint for the health check I've provided a simple configuration class:\\n\\n    public class HealthCheckConfiguration\\n    {\\n        public ICheckHealth HealthChecker { get; private set; }\\n        public string Endpoint { get; private set; }\\n\\n        public HealthCheckConfiguration(string endpoint = null, ICheckHealth healthChecker = null)\\n        {\\n            HealthChecker = healthChecker ?? new DefaultHealthChecker();\\n            Endpoint = endpoint ?? \\\"/api/healthcheck\\\";\\n        }\\n    }\\n\\n## The actual middleware\\nNow when we have all the pieces it is rather simple to implement the actual middleware:\\n\\n    public class HealthCheckMiddleware : OwinMiddleware\\n    {\\n        private ICheckHealth _healthChecker;\\n        private string _endpointUrl;\\n\\n        public HealthCheckMiddleware(OwinMiddleware next, HealthCheckConfiguration config) : base(next)\\n        {\\n            _healthChecker = config.HealthChecker;\\n            _endpointUrl = config.Endpoint;\\n        }\\n\\n        public override async Task Invoke(IOwinContext context)\\n        {\\n            if (context.Request.Uri.AbsolutePath == _endpointUrl)\\n            {\\n                var healthStatus = _healthChecker.CheckHealth();\\n                var response = JsonConvert.SerializeObject(healthStatus);\\n                context.Response.ContentType = \\\"application/json\\\";\\n                await context.Response.WriteAsync(response);\\n            }\\n            else\\n            {\\n                await Next.Invoke(context);\\n            }\\n        }\\n    }\\n\\nIf the `AbsolutePath` matches the endpoint url exactly the health check is going to run, if it isn't the next middleware in the pipeline will be invoked. To make it easier to use I've also added an extension to the `IAppBuilder` interface: \\n\\n    public static class AppBuilderExtensions\\n    {\\n        public static IAppBuilder UseHealthCheck(this IAppBuilder app, HealthCheckConfiguration config = null)\\n        {\\n            config = config ?? new HealthCheckConfiguration();\\n            return app.Use<HealthCheckMiddleware>(config);\\n        }\\n    }\\n\\nThe extension above is what makes it possible to use `app.UseHealthCheck(config)`.\\n\\n## Ending comments\\nI know this might now be optimal for many scenarios, but it's often good to start simple. Also, one might think that it's better if the application has a heartbeat instead of being passive and listens to request. There is pros and cons with both I would say. Being passive and listen to an request also tests that the application actually is reachable, which might not be tested in a heartbeat scenario.\\n\\nThe code above could also be a foundation for many small simple Owin middlewares that you might think of. I you want to look at the code in a more structured way I've put the code up on [github](https://github.com/mastoj/TJOwin). I'm planning to write more small middlewares for things that I want to use and put it here.\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>In many of todays web application we often forget to implement some kind of health check. I think this is because we tend to overthink the problem. Health check might be hard to solve in a generic way that checks the complete system health, but it doesn't have to be hard to implement a generic middleware that every application can hook into and provide the status of the application but don't have to implement the actual endpoint. One advantage you get with this approach is that each application developer still controls what needs to be checked, but they all should follow a protocol to provide the result which makes it easier to monitor. The sample middleware I'm going to show is clean and simple, but that doesn't mean it fits everyone.</p>\n<h2 id=\"thegoal\">The goal</h2>\n<p>The goal of the middleware is to make it really simple to add health check to an existing application and specify how the health check should be executed inside the boundaries of the application. The code in the application should look like this:</p>\n<pre><code>public class Startup\n{\n    public void Configuration(IAppBuilder app)\n    {\n        var config =new HealthCheckConfiguration(&quot;/hc&quot;, new Checker());\n        app.UseHealthCheck(config);\n        app.Run(async context =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello&quot;);\n        });\n    } \n}\n\npublic class Checker : ICheckHealth\n{\n    public HealthStatus CheckHealth()\n    {\n        return new HealthStatus()\n        {\n            Components = new[] {new ComponentHealth(&quot;Universe&quot;, ComponentStatus.Weird)}\n        };\n    }\n}\n</code></pre>\n<p>Here I have the <code>Checker</code> which is specific for this application, and I've also a simple extension making it possible to use <code>app.UseHealthCheck(config)</code>.</p>\n<h2 id=\"thehealthoftheapplication\">The health of the application</h2>\n<p>They way I thought about this is that just responding with a <code>pong</code> to a request doesn't actually say something about the health of the application, how do you know that the external dependencies are up and running with a simple <code>pong</code>? So to tackle this I implented a couple of classes, first the <code>HealthStatus</code>:</p>\n<pre><code>public class HealthStatus\n{\n    public IEnumerable&lt;ComponentHealth&gt; Components { get; set; }\n}\n</code></pre>\n<p>the goal with this class is to give the application developer a chance to specify the health of different components that the application is dependent upon. The <code>ComponentHealth</code> is also a straightforward class:</p>\n<pre><code>public class ComponentHealth\n{\n    public string ComponentName { get; set; }\n\n    [JsonConverter(typeof(StringEnumConverter))]\n    public ComponentStatus StatusName\n    {\n        get { return Status; }\n    }\n\n    public ComponentStatus Status { get; set; }\n\n    public ComponentHealth(string componentName, ComponentStatus status)\n    {\n        ComponentName = componentName;\n        Status = status;\n    }\n}\n\npublic enum ComponentStatus\n{\n    Healthy,    // Ok\n    Fatal,      // Error\n    Weird       // Might be problems\n}\n</code></pre>\n<p>Each component has a name and status. I'm using <a href=\"http://james.newtonking.com/json\">Json.NET</a> to provide the result and to serialize the status with both the enum value and the enum name I'm using the <code>JsonConverter</code> attribute.</p>\n<h2 id=\"healthchecking\">Health checking</h2>\n<p>I've abstracted away the actual health checking so that the applications can define how it should be checked. I have also implemented a default health checker if you only want a simple <code>pong</code> response. The abstractions and implementation are also straightforward:</p>\n<pre><code>public interface ICheckHealth\n{\n    HealthStatus CheckHealth();\n}\n\ninternal class DefaultHealthChecker : ICheckHealth\n{\n    public HealthStatus CheckHealth()\n    {\n        return new HealthStatus()\n        {\n            Components = new [] { new ComponentHealth(&quot;Web&quot;, ComponentStatus.Healthy)}\n        };\n    }\n}\n</code></pre>\n<h2 id=\"configuratingthemiddleware\">Configurating the middleware</h2>\n<p>Since I want to make it easy for the application developer to specify health checker and endpoint for the health check I've provided a simple configuration class:</p>\n<pre><code>public class HealthCheckConfiguration\n{\n    public ICheckHealth HealthChecker { get; private set; }\n    public string Endpoint { get; private set; }\n\n    public HealthCheckConfiguration(string endpoint = null, ICheckHealth healthChecker = null)\n    {\n        HealthChecker = healthChecker ?? new DefaultHealthChecker();\n        Endpoint = endpoint ?? &quot;/api/healthcheck&quot;;\n    }\n}\n</code></pre>\n<h2 id=\"theactualmiddleware\">The actual middleware</h2>\n<p>Now when we have all the pieces it is rather simple to implement the actual middleware:</p>\n<pre><code>public class HealthCheckMiddleware : OwinMiddleware\n{\n    private ICheckHealth _healthChecker;\n    private string _endpointUrl;\n\n    public HealthCheckMiddleware(OwinMiddleware next, HealthCheckConfiguration config) : base(next)\n    {\n        _healthChecker = config.HealthChecker;\n        _endpointUrl = config.Endpoint;\n    }\n\n    public override async Task Invoke(IOwinContext context)\n    {\n        if (context.Request.Uri.AbsolutePath == _endpointUrl)\n        {\n            var healthStatus = _healthChecker.CheckHealth();\n            var response = JsonConvert.SerializeObject(healthStatus);\n            context.Response.ContentType = &quot;application/json&quot;;\n            await context.Response.WriteAsync(response);\n        }\n        else\n        {\n            await Next.Invoke(context);\n        }\n    }\n}\n</code></pre>\n<p>If the <code>AbsolutePath</code> matches the endpoint url exactly the health check is going to run, if it isn't the next middleware in the pipeline will be invoked. To make it easier to use I've also added an extension to the <code>IAppBuilder</code> interface:</p>\n<pre><code>public static class AppBuilderExtensions\n{\n    public static IAppBuilder UseHealthCheck(this IAppBuilder app, HealthCheckConfiguration config = null)\n    {\n        config = config ?? new HealthCheckConfiguration();\n        return app.Use&lt;HealthCheckMiddleware&gt;(config);\n    }\n}\n</code></pre>\n<p>The extension above is what makes it possible to use <code>app.UseHealthCheck(config)</code>.</p>\n<h2 id=\"endingcomments\">Ending comments</h2>\n<p>I know this might now be optimal for many scenarios, but it's often good to start simple. Also, one might think that it's better if the application has a heartbeat instead of being passive and listens to request. There is pros and cons with both I would say. Being passive and listen to an request also tests that the application actually is reachable, which might not be tested in a heartbeat scenario.</p>\n<p>The code above could also be a foundation for many small simple Owin middlewares that you might think of. I you want to look at the code in a more structured way I've put the code up on <a href=\"https://github.com/mastoj/TJOwin\">github</a>. I'm planning to write more small middlewares for things that I want to use and put it here.</p>\n<!--kg-card-end: markdown-->","comment_id":"39","plaintext":"In many of todays web application we often forget to implement some kind of\nhealth check. I think this is because we tend to overthink the problem. Health\ncheck might be hard to solve in a generic way that checks the complete system\nhealth, but it doesn't have to be hard to implement a generic middleware that\nevery application can hook into and provide the status of the application but\ndon't have to implement the actual endpoint. One advantage you get with this\napproach is that each application developer still controls what needs to be\nchecked, but they all should follow a protocol to provide the result which makes\nit easier to monitor. The sample middleware I'm going to show is clean and\nsimple, but that doesn't mean it fits everyone.\n\nThe goal\nThe goal of the middleware is to make it really simple to add health check to an\nexisting application and specify how the health check should be executed inside\nthe boundaries of the application. The code in the application should look like\nthis:\n\npublic class Startup\n{\n    public void Configuration(IAppBuilder app)\n    {\n        var config =new HealthCheckConfiguration(\"/hc\", new Checker());\n        app.UseHealthCheck(config);\n        app.Run(async context =>\n        {\n            await context.Response.WriteAsync(\"Hello\");\n        });\n    } \n}\n\npublic class Checker : ICheckHealth\n{\n    public HealthStatus CheckHealth()\n    {\n        return new HealthStatus()\n        {\n            Components = new[] {new ComponentHealth(\"Universe\", ComponentStatus.Weird)}\n        };\n    }\n}\n\n\nHere I have the Checker which is specific for this application, and I've also a\nsimple extension making it possible to use app.UseHealthCheck(config).\n\nThe health of the application\nThey way I thought about this is that just responding with a pong to a request\ndoesn't actually say something about the health of the application, how do you\nknow that the external dependencies are up and running with a simple pong? So to\ntackle this I implented a couple of classes, first the HealthStatus:\n\npublic class HealthStatus\n{\n    public IEnumerable<ComponentHealth> Components { get; set; }\n}\n\n\nthe goal with this class is to give the application developer a chance to\nspecify the health of different components that the application is dependent\nupon. The ComponentHealth is also a straightforward class:\n\npublic class ComponentHealth\n{\n    public string ComponentName { get; set; }\n\n    [JsonConverter(typeof(StringEnumConverter))]\n    public ComponentStatus StatusName\n    {\n        get { return Status; }\n    }\n\n    public ComponentStatus Status { get; set; }\n\n    public ComponentHealth(string componentName, ComponentStatus status)\n    {\n        ComponentName = componentName;\n        Status = status;\n    }\n}\n\npublic enum ComponentStatus\n{\n    Healthy,    // Ok\n    Fatal,      // Error\n    Weird       // Might be problems\n}\n\n\nEach component has a name and status. I'm using Json.NET\n[http://james.newtonking.com/json] to provide the result and to serialize the\nstatus with both the enum value and the enum name I'm using the JsonConverter \nattribute.\n\nHealth checking\nI've abstracted away the actual health checking so that the applications can\ndefine how it should be checked. I have also implemented a default health\nchecker if you only want a simple pong response. The abstractions and\nimplementation are also straightforward:\n\npublic interface ICheckHealth\n{\n    HealthStatus CheckHealth();\n}\n\ninternal class DefaultHealthChecker : ICheckHealth\n{\n    public HealthStatus CheckHealth()\n    {\n        return new HealthStatus()\n        {\n            Components = new [] { new ComponentHealth(\"Web\", ComponentStatus.Healthy)}\n        };\n    }\n}\n\n\nConfigurating the middleware\nSince I want to make it easy for the application developer to specify health\nchecker and endpoint for the health check I've provided a simple configuration\nclass:\n\npublic class HealthCheckConfiguration\n{\n    public ICheckHealth HealthChecker { get; private set; }\n    public string Endpoint { get; private set; }\n\n    public HealthCheckConfiguration(string endpoint = null, ICheckHealth healthChecker = null)\n    {\n        HealthChecker = healthChecker ?? new DefaultHealthChecker();\n        Endpoint = endpoint ?? \"/api/healthcheck\";\n    }\n}\n\n\nThe actual middleware\nNow when we have all the pieces it is rather simple to implement the actual\nmiddleware:\n\npublic class HealthCheckMiddleware : OwinMiddleware\n{\n    private ICheckHealth _healthChecker;\n    private string _endpointUrl;\n\n    public HealthCheckMiddleware(OwinMiddleware next, HealthCheckConfiguration config) : base(next)\n    {\n        _healthChecker = config.HealthChecker;\n        _endpointUrl = config.Endpoint;\n    }\n\n    public override async Task Invoke(IOwinContext context)\n    {\n        if (context.Request.Uri.AbsolutePath == _endpointUrl)\n        {\n            var healthStatus = _healthChecker.CheckHealth();\n            var response = JsonConvert.SerializeObject(healthStatus);\n            context.Response.ContentType = \"application/json\";\n            await context.Response.WriteAsync(response);\n        }\n        else\n        {\n            await Next.Invoke(context);\n        }\n    }\n}\n\n\nIf the AbsolutePath matches the endpoint url exactly the health check is going\nto run, if it isn't the next middleware in the pipeline will be invoked. To make\nit easier to use I've also added an extension to the IAppBuilder interface:\n\npublic static class AppBuilderExtensions\n{\n    public static IAppBuilder UseHealthCheck(this IAppBuilder app, HealthCheckConfiguration config = null)\n    {\n        config = config ?? new HealthCheckConfiguration();\n        return app.Use<HealthCheckMiddleware>(config);\n    }\n}\n\n\nThe extension above is what makes it possible to use app.UseHealthCheck(config).\n\nEnding comments\nI know this might now be optimal for many scenarios, but it's often good to\nstart simple. Also, one might think that it's better if the application has a\nheartbeat instead of being passive and listens to request. There is pros and\ncons with both I would say. Being passive and listen to an request also tests\nthat the application actually is reachable, which might not be tested in a\nheartbeat scenario.\n\nThe code above could also be a foundation for many small simple Owin middlewares\nthat you might think of. I you want to look at the code in a more structured way\nI've put the code up on github [https://github.com/mastoj/TJOwin]. I'm planning\nto write more small middlewares for things that I want to use and put it here.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-09-22T05:15:11.000Z","updated_at":"2014-09-22T05:15:11.000Z","published_at":"2014-09-22T05:15:11.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05b","uuid":"05ff12af-5303-4f37-81de-37d3ad6b4ac9","title":"I'll be doing a polyglot talk at NDC London","slug":"ill-be-doing-a-polyglot-talk-at-ndc-london","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Yey! My polyglot talk for NDC London got accepted so it looks like I'll be in London in December for the second year in a row. The talk will be a compressed version of the [CQRS blog serie](http://blog.tomasjansson.com/tag/cqrsshop/) I wrote earlier this year. The title of the talk is \\\"Polyglot heaven - how to build a async, reactive application in 2014\\\". As you realize from the title the focus is on polyglot rather than CQRS and picking the right tools for the right job. There is a lot of things that I'll cover in the talk like, CQRS, event sourcing, [Event Store](http://geteventstore.com), [Neo4j](http://www.neo4j.org/) (graph database), [Elastichsearch](http://www.elasticsearch.org/) (search database) and I'll do programming in both F# and C#. I'm not sure how I'll fit it into one hour, but that is my goal.\\n\\n![](/content/images/2014/10/NDCLondon.PNG)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Yey! My polyglot talk for NDC London got accepted so it looks like I'll be in London in December for the second year in a row. The talk will be a compressed version of the <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">CQRS blog serie</a> I wrote earlier this year. The title of the talk is &quot;Polyglot heaven - how to build a async, reactive application in 2014&quot;. As you realize from the title the focus is on polyglot rather than CQRS and picking the right tools for the right job. There is a lot of things that I'll cover in the talk like, CQRS, event sourcing, <a href=\"http://geteventstore.com\">Event Store</a>, <a href=\"http://www.neo4j.org/\">Neo4j</a> (graph database), <a href=\"http://www.elasticsearch.org/\">Elastichsearch</a> (search database) and I'll do programming in both F# and C#. I'm not sure how I'll fit it into one hour, but that is my goal.</p>\n<p><img src=\"/content/images/2014/10/NDCLondon.PNG\" alt=\"\"></p>\n<!--kg-card-end: markdown-->","comment_id":"40","plaintext":"Yey! My polyglot talk for NDC London got accepted so it looks like I'll be in\nLondon in December for the second year in a row. The talk will be a compressed\nversion of the CQRS blog serie [http://blog.tomasjansson.com/tag/cqrsshop/] I\nwrote earlier this year. The title of the talk is \"Polyglot heaven - how to\nbuild a async, reactive application in 2014\". As you realize from the title the\nfocus is on polyglot rather than CQRS and picking the right tools for the right\njob. There is a lot of things that I'll cover in the talk like, CQRS, event\nsourcing, Event Store [http://geteventstore.com], Neo4j [http://www.neo4j.org/] \n(graph database), Elastichsearch [http://www.elasticsearch.org/] (search\ndatabase) and I'll do programming in both F# and C#. I'm not sure how I'll fit\nit into one hour, but that is my goal.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-10-04T18:57:09.000Z","updated_at":"2014-10-04T18:57:48.000Z","published_at":"2014-10-04T18:57:48.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05c","uuid":"e222ffb9-8088-4ab5-a66d-5488781c3c95","title":"Puppet & Windows","slug":"puppet-windows","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I've spent some time the last couple of weeks playing around with [Puppet](http://puppetlabs.com/) (and [Vagrant](http://www.vagrantup.com/)) on Windows after reading through Puppet Cookbook and Beginner's guide from [John Arundel](http://bitfieldconsulting.com/). Even though the books doesn't cover Windows that much they are a good introduction to Puppet.\\n\\nI'll post a longer post later on describing what I've done in more detail, but as of know I just thought I would point you to the repos and links where you can see me playing with this stuff.\\n\\n * [WinPuppet](https://github.com/mastoj/winpuppet) - this is where I keep the puppet code for this experiment\\n * [NirvanaService](https://github.com/mastoj/nirvanaservice) - a service wrapper that allow you to basically wrap any executable as a service in a really flexible manner\\n * [My nuget feed](https://www.myget.org/feed/Packages/crazy-choco) - this is a where I keep random [Chocolatey packages](https://chocolatey.org/), try `nuget list -source https://www.myget.org/F/crazy-choco/`\\n \\nTo get started you should just be able to clone the WinPuppet repo and run `vagrant up` (if you have vagrant installed and virtualbox installed). That will download a vagrant box, create a vm from the box, install puppet, install chocolatey and provision the virtual machine.\\n\\nSo far I think everything works really smooth. Everything above is work in progress, but I think the `NirvanaService` is sort of version 1, at least for my needs. The way I've done it is to create chocolatey packages for things I would like to install, use the chocolatey provider in puppet to install them and then run executable with `NirvanaService`. Please come with feedback or suggestion if you have any.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I've spent some time the last couple of weeks playing around with <a href=\"http://puppetlabs.com/\">Puppet</a> (and <a href=\"http://www.vagrantup.com/\">Vagrant</a>) on Windows after reading through Puppet Cookbook and Beginner's guide from <a href=\"http://bitfieldconsulting.com/\">John Arundel</a>. Even though the books doesn't cover Windows that much they are a good introduction to Puppet.</p>\n<p>I'll post a longer post later on describing what I've done in more detail, but as of know I just thought I would point you to the repos and links where you can see me playing with this stuff.</p>\n<ul>\n<li><a href=\"https://github.com/mastoj/winpuppet\">WinPuppet</a> - this is where I keep the puppet code for this experiment</li>\n<li><a href=\"https://github.com/mastoj/nirvanaservice\">NirvanaService</a> - a service wrapper that allow you to basically wrap any executable as a service in a really flexible manner</li>\n<li><a href=\"https://www.myget.org/feed/Packages/crazy-choco\">My nuget feed</a> - this is a where I keep random <a href=\"https://chocolatey.org/\">Chocolatey packages</a>, try <code>nuget list -source https://www.myget.org/F/crazy-choco/</code></li>\n</ul>\n<p>To get started you should just be able to clone the WinPuppet repo and run <code>vagrant up</code> (if you have vagrant installed and virtualbox installed). That will download a vagrant box, create a vm from the box, install puppet, install chocolatey and provision the virtual machine.</p>\n<p>So far I think everything works really smooth. Everything above is work in progress, but I think the <code>NirvanaService</code> is sort of version 1, at least for my needs. The way I've done it is to create chocolatey packages for things I would like to install, use the chocolatey provider in puppet to install them and then run executable with <code>NirvanaService</code>. Please come with feedback or suggestion if you have any.</p>\n<!--kg-card-end: markdown-->","comment_id":"41","plaintext":"I've spent some time the last couple of weeks playing around with Puppet\n[http://puppetlabs.com/] (and Vagrant [http://www.vagrantup.com/]) on Windows\nafter reading through Puppet Cookbook and Beginner's guide from John Arundel\n[http://bitfieldconsulting.com/]. Even though the books doesn't cover Windows\nthat much they are a good introduction to Puppet.\n\nI'll post a longer post later on describing what I've done in more detail, but\nas of know I just thought I would point you to the repos and links where you can\nsee me playing with this stuff.\n\n * WinPuppet [https://github.com/mastoj/winpuppet] - this is where I keep the\n   puppet code for this experiment\n * NirvanaService [https://github.com/mastoj/nirvanaservice] - a service wrapper\n   that allow you to basically wrap any executable as a service in a really\n   flexible manner\n * My nuget feed [https://www.myget.org/feed/Packages/crazy-choco] - this is a\n   where I keep random Chocolatey packages [https://chocolatey.org/], try nuget\n   list -source https://www.myget.org/F/crazy-choco/\n\nTo get started you should just be able to clone the WinPuppet repo and run \nvagrant up (if you have vagrant installed and virtualbox installed). That will\ndownload a vagrant box, create a vm from the box, install puppet, install\nchocolatey and provision the virtual machine.\n\nSo far I think everything works really smooth. Everything above is work in\nprogress, but I think the NirvanaService is sort of version 1, at least for my\nneeds. The way I've done it is to create chocolatey packages for things I would\nlike to install, use the chocolatey provider in puppet to install them and then\nrun executable with NirvanaService. Please come with feedback or suggestion if\nyou have any.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-10-13T14:25:43.000Z","updated_at":"2014-10-14T06:05:37.000Z","published_at":"2014-10-13T14:25:45.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05d","uuid":"55475bd6-36c0-4c67-aa5c-a9e619ab32cf","title":"I was features on two Microsoft blogs this week","slug":"i-was-features-on-two-microsoft-blogs-this-week","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Earlier this week I got features on two Microsoft blogs, both in Norwegian though. Both the articles are based on the same interview, but they have two different angles; one is about inspiration and one is about developers. The one with the inspirational angle has the title: \\\"It is as a developer you get things done\\\" and the other one is just a presentation about the developers behind the code. This is part of serie of articles where Microsoft will show some people that do work in the Microsoft landscape and is visible in the community. I'm honored and glad to be the first one out. If you want to read they are located here: https://inspirasjon.microsoft.no/2014/11/05/det-er-som-utvikler-man-far-ting-gjort/ (inspiration) and http://blogs.msdn.com/b/dpenorway/archive/2014/11/06/ansiktet-bak-koden.aspx (general development)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Earlier this week I got features on two Microsoft blogs, both in Norwegian though. Both the articles are based on the same interview, but they have two different angles; one is about inspiration and one is about developers. The one with the inspirational angle has the title: &quot;It is as a developer you get things done&quot; and the other one is just a presentation about the developers behind the code. This is part of serie of articles where Microsoft will show some people that do work in the Microsoft landscape and is visible in the community. I'm honored and glad to be the first one out. If you want to read they are located here: <a href=\"https://inspirasjon.microsoft.no/2014/11/05/det-er-som-utvikler-man-far-ting-gjort/\">https://inspirasjon.microsoft.no/2014/11/05/det-er-som-utvikler-man-far-ting-gjort/</a> (inspiration) and <a href=\"http://blogs.msdn.com/b/dpenorway/archive/2014/11/06/ansiktet-bak-koden.aspx\">http://blogs.msdn.com/b/dpenorway/archive/2014/11/06/ansiktet-bak-koden.aspx</a> (general development)</p>\n<!--kg-card-end: markdown-->","comment_id":"42","plaintext":"Earlier this week I got features on two Microsoft blogs, both in Norwegian\nthough. Both the articles are based on the same interview, but they have two\ndifferent angles; one is about inspiration and one is about developers. The one\nwith the inspirational angle has the title: \"It is as a developer you get things\ndone\" and the other one is just a presentation about the developers behind the\ncode. This is part of serie of articles where Microsoft will show some people\nthat do work in the Microsoft landscape and is visible in the community. I'm\nhonored and glad to be the first one out. If you want to read they are located\nhere: \nhttps://inspirasjon.microsoft.no/2014/11/05/det-er-som-utvikler-man-far-ting-gjort/ \n(inspiration) and \nhttp://blogs.msdn.com/b/dpenorway/archive/2014/11/06/ansiktet-bak-koden.aspx \n(general development)","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-11-09T14:48:10.000Z","updated_at":"2014-11-09T14:53:25.000Z","published_at":"2014-11-09T14:53:25.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05e","uuid":"0d712b3c-d1d3-4a1c-bf46-40dcea5307c0","title":"Your application should be a pure function","slug":"your-application-should-be-a-pure-function","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"What does application has to do with functions you might ask? Everything is my answer! \\n\\nIt's now a couple of months since I finished my blog serie about CQRS and event sourcing. You can read the ending discussion here: http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/. In those couple of months I had some more time to think about what I'm actually doing and then it hit me. Of course I touched on it through the blog serie, but I never wrote it down. Before I put it on print let us see what a pure function is\\n\\n## Pure function\\nLet's keep it simple and take the defintion from [wikipedia](http://en.wikipedia.org/wiki/Pure_function). \\n\\n> In computer programming, a function may be described as a pure function if both these statements about the function hold:\\n\\n> 1. The function always evaluates the same result value given the same argument value(s). The function result value cannot depend on any hidden information or state that may change as program execution proceeds or between different executions of the program, nor can it depend on any external input from I/O devices (usually—see below).\\n> 2. Evaluation of the result does not cause any semantically observable side effect or output, such as mutation of mutable objects or output to I/O devices (usually—see below).\\n\\nWhat does this mean? If you put it in the context of your application you should be able to provide it a set of input and always expect the same output for that input. If your application depends on some kind of state that should be provided to the application as input. Also, you should not have any unexpected side effects like change in state of some object which you did not expect to change.\\n\\n## Why is pure functions good?\\nIf you have a pure function it's very easy to reason about it since you don't have to think about some kind of magic state. Two of the main reasons you should want to have your application as a pure function are:\\n\\n1. A pure function is highly testable\\n2. You will focus on behaviour rather than state (what do you care about in your application)\\n\\n## What does a \\\"standard\\\" application look likes?\\nBelow I've attached a really simplifed picture of what most of the systems build look like today. \\n\\n![Standard application](/content/images/2014/11/StandardApplication-1.png)\\n\\nThe problem with this is that the application is both dependent on the input and the state, but the state is not given as input to the application. Another issue with this is that two models are mixed as one, both the model you do action against and the model which you query. Of course I've over simplified things since I put basically all the \\\"layers\\\" in a block called \\\"application\\\", but is that a bad thing? Adding all the layers will most likely screw this model up even more.\\n\\n## Alternative solution with CQRS and event sourcing\\nYou could see CQRS and event sourcing as an application where you have a write side and a read side of your application, but I think an alternative presentation might even make it more clear what is going on.\\n\\n![Pure application](/content/images/2014/11/PureApplication.png)\\n\\nWhat has happened here is that I have diveded the application into two parts; application and projection. As I wrote here, http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/, CQRS is just divided and conquer on an architectural level, and that is exactly what is going on here. I've turned the application into two pure functions, one which I still call \\\"application\\\" and one which I call \\\"projections.\\\" The most important one here is the \\\"application\\\" since it is what creates change, the \\\"projection\\\" is just one or more interpretations of those changes. Input to the application could be a command and events if that is needed to execute the command, the important part is that they are also important to the application. I haven't drawn that here, since I consider it to be input as well. Before you call the application you read all or the relevant events from the event store and provide them as input so you keep the application as a pure function.\\n\\nIf it is not clear, the most important data store in that picture is the event store! That is where you store the set of changes of the system, and that is what matter!\\n\\nNow when we have made this separation it is really easy to write test against both the application and the projection side of the system since they both are pure functions.\\n\\n## Ending notes\\nIf you come this long you've seen that I don't mention functional programming language, and that is because this hasn't to do with the language. You can achieve this in the language of your choice. Of course maybe functional languages will have some benefits, but that is not the point here. The point here is that you should start about what does your application really look like and what data do you have.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>What does application has to do with functions you might ask? Everything is my answer!</p>\n<p>It's now a couple of months since I finished my blog serie about CQRS and event sourcing. You can read the ending discussion here: <a href=\"http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\">http://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/</a>. In those couple of months I had some more time to think about what I'm actually doing and then it hit me. Of course I touched on it through the blog serie, but I never wrote it down. Before I put it on print let us see what a pure function is</p>\n<h2 id=\"purefunction\">Pure function</h2>\n<p>Let's keep it simple and take the defintion from <a href=\"http://en.wikipedia.org/wiki/Pure_function\">wikipedia</a>.</p>\n<blockquote>\n<p>In computer programming, a function may be described as a pure function if both these statements about the function hold:</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>The function always evaluates the same result value given the same argument value(s). The function result value cannot depend on any hidden information or state that may change as program execution proceeds or between different executions of the program, nor can it depend on any external input from I/O devices (usually—see below).</li>\n<li>Evaluation of the result does not cause any semantically observable side effect or output, such as mutation of mutable objects or output to I/O devices (usually—see below).</li>\n</ol>\n</blockquote>\n<p>What does this mean? If you put it in the context of your application you should be able to provide it a set of input and always expect the same output for that input. If your application depends on some kind of state that should be provided to the application as input. Also, you should not have any unexpected side effects like change in state of some object which you did not expect to change.</p>\n<h2 id=\"whyispurefunctionsgood\">Why is pure functions good?</h2>\n<p>If you have a pure function it's very easy to reason about it since you don't have to think about some kind of magic state. Two of the main reasons you should want to have your application as a pure function are:</p>\n<ol>\n<li>A pure function is highly testable</li>\n<li>You will focus on behaviour rather than state (what do you care about in your application)</li>\n</ol>\n<h2 id=\"whatdoesastandardapplicationlooklikes\">What does a &quot;standard&quot; application look likes?</h2>\n<p>Below I've attached a really simplifed picture of what most of the systems build look like today.</p>\n<p><img src=\"/content/images/2014/11/StandardApplication-1.png\" alt=\"Standard application\"></p>\n<p>The problem with this is that the application is both dependent on the input and the state, but the state is not given as input to the application. Another issue with this is that two models are mixed as one, both the model you do action against and the model which you query. Of course I've over simplified things since I put basically all the &quot;layers&quot; in a block called &quot;application&quot;, but is that a bad thing? Adding all the layers will most likely screw this model up even more.</p>\n<h2 id=\"alternativesolutionwithcqrsandeventsourcing\">Alternative solution with CQRS and event sourcing</h2>\n<p>You could see CQRS and event sourcing as an application where you have a write side and a read side of your application, but I think an alternative presentation might even make it more clear what is going on.</p>\n<p><img src=\"/content/images/2014/11/PureApplication.png\" alt=\"Pure application\"></p>\n<p>What has happened here is that I have diveded the application into two parts; application and projection. As I wrote here, <a href=\"http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\">http://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/</a>, CQRS is just divided and conquer on an architectural level, and that is exactly what is going on here. I've turned the application into two pure functions, one which I still call &quot;application&quot; and one which I call &quot;projections.&quot; The most important one here is the &quot;application&quot; since it is what creates change, the &quot;projection&quot; is just one or more interpretations of those changes. Input to the application could be a command and events if that is needed to execute the command, the important part is that they are also important to the application. I haven't drawn that here, since I consider it to be input as well. Before you call the application you read all or the relevant events from the event store and provide them as input so you keep the application as a pure function.</p>\n<p>If it is not clear, the most important data store in that picture is the event store! That is where you store the set of changes of the system, and that is what matter!</p>\n<p>Now when we have made this separation it is really easy to write test against both the application and the projection side of the system since they both are pure functions.</p>\n<h2 id=\"endingnotes\">Ending notes</h2>\n<p>If you come this long you've seen that I don't mention functional programming language, and that is because this hasn't to do with the language. You can achieve this in the language of your choice. Of course maybe functional languages will have some benefits, but that is not the point here. The point here is that you should start about what does your application really look like and what data do you have.</p>\n<!--kg-card-end: markdown-->","comment_id":"43","plaintext":"What does application has to do with functions you might ask? Everything is my\nanswer!\n\nIt's now a couple of months since I finished my blog serie about CQRS and event\nsourcing. You can read the ending discussion here: \nhttp://blog.tomasjansson.com/ending-discussion-to-my-blog-series-about-cqrs-and-event-sourcing/\n. In those couple of months I had some more time to think about what I'm\nactually doing and then it hit me. Of course I touched on it through the blog\nserie, but I never wrote it down. Before I put it on print let us see what a\npure function is\n\nPure function\nLet's keep it simple and take the defintion from wikipedia\n[http://en.wikipedia.org/wiki/Pure_function].\n\n> In computer programming, a function may be described as a pure function if both\nthese statements about the function hold:\n\n\n>  1. The function always evaluates the same result value given the same argument\n    value(s). The function result value cannot depend on any hidden information\n    or state that may change as program execution proceeds or between different\n    executions of the program, nor can it depend on any external input from I/O\n    devices (usually—see below).\n 2. Evaluation of the result does not cause any semantically observable side\n    effect or output, such as mutation of mutable objects or output to I/O\n    devices (usually—see below).\n\n\nWhat does this mean? If you put it in the context of your application you should\nbe able to provide it a set of input and always expect the same output for that\ninput. If your application depends on some kind of state that should be provided\nto the application as input. Also, you should not have any unexpected side\neffects like change in state of some object which you did not expect to change.\n\nWhy is pure functions good?\nIf you have a pure function it's very easy to reason about it since you don't\nhave to think about some kind of magic state. Two of the main reasons you should\nwant to have your application as a pure function are:\n\n 1. A pure function is highly testable\n 2. You will focus on behaviour rather than state (what do you care about in\n    your application)\n\nWhat does a \"standard\" application look likes?\nBelow I've attached a really simplifed picture of what most of the systems build\nlook like today.\n\n\n\nThe problem with this is that the application is both dependent on the input and\nthe state, but the state is not given as input to the application. Another issue\nwith this is that two models are mixed as one, both the model you do action\nagainst and the model which you query. Of course I've over simplified things\nsince I put basically all the \"layers\" in a block called \"application\", but is\nthat a bad thing? Adding all the layers will most likely screw this model up\neven more.\n\nAlternative solution with CQRS and event sourcing\nYou could see CQRS and event sourcing as an application where you have a write\nside and a read side of your application, but I think an alternative\npresentation might even make it more clear what is going on.\n\n\n\nWhat has happened here is that I have diveded the application into two parts;\napplication and projection. As I wrote here, \nhttp://blog.tomasjansson.com/cqrs-the-simple-way-with-eventstore-and-elasticsearch-time-for-reflection/\n, CQRS is just divided and conquer on an architectural level, and that is\nexactly what is going on here. I've turned the application into two pure\nfunctions, one which I still call \"application\" and one which I call\n\"projections.\" The most important one here is the \"application\" since it is what\ncreates change, the \"projection\" is just one or more interpretations of those\nchanges. Input to the application could be a command and events if that is\nneeded to execute the command, the important part is that they are also\nimportant to the application. I haven't drawn that here, since I consider it to\nbe input as well. Before you call the application you read all or the relevant\nevents from the event store and provide them as input so you keep the\napplication as a pure function.\n\nIf it is not clear, the most important data store in that picture is the event\nstore! That is where you store the set of changes of the system, and that is\nwhat matter!\n\nNow when we have made this separation it is really easy to write test against\nboth the application and the projection side of the system since they both are\npure functions.\n\nEnding notes\nIf you come this long you've seen that I don't mention functional programming\nlanguage, and that is because this hasn't to do with the language. You can\nachieve this in the language of your choice. Of course maybe functional\nlanguages will have some benefits, but that is not the point here. The point\nhere is that you should start about what does your application really look like\nand what data do you have.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-11-09T14:54:35.000Z","updated_at":"2015-05-25T15:59:38.000Z","published_at":"2014-11-09T15:36:53.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe05f","uuid":"3c0878a9-7231-4700-ae56-86425cb2ca0b","title":"Moving standard OWIN middleware to vNext middleware","slug":"moving-standard-owin-middleware-to-vnext-middleware","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Earlier I wrote a post in how to create a OWIN middleware, http://blog.tomasjansson.com/useful-owin-middleware-healthcheck/. Of course I should have written it targeting [vNext](http://asp.net/vnext) but I didn't. One good thing about that is that I can explain how to create a vNext middleware from a simple OWIN middleware. \\n\\nBefore we get started there are two tings:\\n\\n1. vNext is changing, so the thing I show here might not be applicable when you read it. Most likely a interface might have been renamed or moved.\\n2. The main concepts are still quite similar with OWIN, so the migration shouldn't be to hard if your middleware is simple.\\n\\n## Creating the middleware\\nAll the code is here: https://github.com/mastoj/TJOwin. There you can find both the old standard OWIN middleware as well as the one targeting vnext. This part will only focus on the middleware and not the actual web application. The application I used to try the middleware is located on github and the startup code of what we want to achieve look like this:\\n\\n    public class Startup\\n    {\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            //TJOwin.HealthCheck.AppBuilderExtensions.UseHealthCheck(null);\\n            var config = new HealthCheckConfiguration();\\n            app.UseHealthCheck(config);\\n            app.Use(helloworldMiddleware);\\n        }\\n\\n        private RequestDelegate helloworldMiddleware(RequestDelegate arg)\\n        {\\n            RequestDelegate rd = (context) => \\n            {\\n                context.Response.WriteAsync(\\\"Hello world\\\");\\n                return arg(context);\\n            };\\n            return rd;\\n        }\\n    }\\n    \\nIn the `Configure` method you see the call to `UseHealthCheck(config)`, and that is where we start. But before you can add the actual code you need to add the project, which should be a `ASP.NET vNext Class Library`. I don't know if that will change, but you need that and not a standard class library.\\n\\n### The ApplicationBuilderExtensions\\nTo be able to call `UseHealthCheck` I need to create an extension method for `IApplicationBuilder`. The extension method class look like this:\\n\\n    public static class AppBuilderExtensions\\n    {\\n        public static IApplicationBuilder UseHealthCheck(this IApplicationBuilder app, HealthCheckConfiguration config = null)\\n        {\\n            config = config ?? new HealthCheckConfiguration();\\n            return app.Use(next => new HealthCheckMiddleware(next, config).Invoke);\\n        }\\n    }\\n    \\nIt might be hard to spot changes compared to OWIN here, but there are some. The `IApplicationBuilder` interface is new to start with. The `next` parameter is now of a type called `RequestDelegate` which has the following signature:\\n\\n    public delegate Task RequestDelegate(HttpContext context);\\n    \\nHaving a more explicit signature make it a little bit easier to use than before I think.\\n\\n### The project file\\nThe changes to the extensions class was all the code changes I did actually, except from that I just copied over all the old classes from the old project and everything worked. To get things to compile you need to add the right references to your `project.json` and in this case my `project.json` looked like:\\n\\n    {\\n        \\\"dependencies\\\": {\\n            \\\"Newtonsoft.Json\\\": \\\"6.0.5\\\",\\n            \\\"Microsoft.AspNet.Http\\\": \\\"1.0.0-alpha4\\\"\\n        },\\n\\n        \\\"frameworks\\\" : {\\n            \\\"aspnet50\\\" : { \\n                \\\"dependencies\\\": {\\n                }\\n            },\\n            \\\"aspnetcore50\\\" : { \\n                \\\"dependencies\\\": {\\n                    \\\"System.Runtime\\\": \\\"4.0.20.0\\\"\\n                }\\n            }\\n        }\\n    }\\n    \\nI added json.net for seralization and `Microsoft.AspNet.Http` since it contains the dll where `IApplicationBuilder` is defined.\\n\\nI also opened up the properties of the project and changed to target `ASP.NET Core 5.0` which is the \\\"core optimized\\\" framework. This makes everything a little bit more lightweight and I don't have more in my application that I need.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Earlier I wrote a post in how to create a OWIN middleware, <a href=\"http://blog.tomasjansson.com/useful-owin-middleware-healthcheck/\">http://blog.tomasjansson.com/useful-owin-middleware-healthcheck/</a>. Of course I should have written it targeting <a href=\"http://asp.net/vnext\">vNext</a> but I didn't. One good thing about that is that I can explain how to create a vNext middleware from a simple OWIN middleware.</p>\n<p>Before we get started there are two tings:</p>\n<ol>\n<li>vNext is changing, so the thing I show here might not be applicable when you read it. Most likely a interface might have been renamed or moved.</li>\n<li>The main concepts are still quite similar with OWIN, so the migration shouldn't be to hard if your middleware is simple.</li>\n</ol>\n<h2 id=\"creatingthemiddleware\">Creating the middleware</h2>\n<p>All the code is here: <a href=\"https://github.com/mastoj/TJOwin\">https://github.com/mastoj/TJOwin</a>. There you can find both the old standard OWIN middleware as well as the one targeting vnext. This part will only focus on the middleware and not the actual web application. The application I used to try the middleware is located on github and the startup code of what we want to achieve look like this:</p>\n<pre><code>public class Startup\n{\n    public void Configure(IApplicationBuilder app)\n    {\n        //TJOwin.HealthCheck.AppBuilderExtensions.UseHealthCheck(null);\n        var config = new HealthCheckConfiguration();\n        app.UseHealthCheck(config);\n        app.Use(helloworldMiddleware);\n    }\n\n    private RequestDelegate helloworldMiddleware(RequestDelegate arg)\n    {\n        RequestDelegate rd = (context) =&gt; \n        {\n            context.Response.WriteAsync(&quot;Hello world&quot;);\n            return arg(context);\n        };\n        return rd;\n    }\n}\n</code></pre>\n<p>In the <code>Configure</code> method you see the call to <code>UseHealthCheck(config)</code>, and that is where we start. But before you can add the actual code you need to add the project, which should be a <code>ASP.NET vNext Class Library</code>. I don't know if that will change, but you need that and not a standard class library.</p>\n<h3 id=\"theapplicationbuilderextensions\">The ApplicationBuilderExtensions</h3>\n<p>To be able to call <code>UseHealthCheck</code> I need to create an extension method for <code>IApplicationBuilder</code>. The extension method class look like this:</p>\n<pre><code>public static class AppBuilderExtensions\n{\n    public static IApplicationBuilder UseHealthCheck(this IApplicationBuilder app, HealthCheckConfiguration config = null)\n    {\n        config = config ?? new HealthCheckConfiguration();\n        return app.Use(next =&gt; new HealthCheckMiddleware(next, config).Invoke);\n    }\n}\n</code></pre>\n<p>It might be hard to spot changes compared to OWIN here, but there are some. The <code>IApplicationBuilder</code> interface is new to start with. The <code>next</code> parameter is now of a type called <code>RequestDelegate</code> which has the following signature:</p>\n<pre><code>public delegate Task RequestDelegate(HttpContext context);\n</code></pre>\n<p>Having a more explicit signature make it a little bit easier to use than before I think.</p>\n<h3 id=\"theprojectfile\">The project file</h3>\n<p>The changes to the extensions class was all the code changes I did actually, except from that I just copied over all the old classes from the old project and everything worked. To get things to compile you need to add the right references to your <code>project.json</code> and in this case my <code>project.json</code> looked like:</p>\n<pre><code>{\n    &quot;dependencies&quot;: {\n        &quot;Newtonsoft.Json&quot;: &quot;6.0.5&quot;,\n        &quot;Microsoft.AspNet.Http&quot;: &quot;1.0.0-alpha4&quot;\n    },\n\n    &quot;frameworks&quot; : {\n        &quot;aspnet50&quot; : { \n            &quot;dependencies&quot;: {\n            }\n        },\n        &quot;aspnetcore50&quot; : { \n            &quot;dependencies&quot;: {\n                &quot;System.Runtime&quot;: &quot;4.0.20.0&quot;\n            }\n        }\n    }\n}\n</code></pre>\n<p>I added json.net for seralization and <code>Microsoft.AspNet.Http</code> since it contains the dll where <code>IApplicationBuilder</code> is defined.</p>\n<p>I also opened up the properties of the project and changed to target <code>ASP.NET Core 5.0</code> which is the &quot;core optimized&quot; framework. This makes everything a little bit more lightweight and I don't have more in my application that I need.</p>\n<!--kg-card-end: markdown-->","comment_id":"44","plaintext":"Earlier I wrote a post in how to create a OWIN middleware, \nhttp://blog.tomasjansson.com/useful-owin-middleware-healthcheck/. Of course I\nshould have written it targeting vNext [http://asp.net/vnext] but I didn't. One\ngood thing about that is that I can explain how to create a vNext middleware\nfrom a simple OWIN middleware.\n\nBefore we get started there are two tings:\n\n 1. vNext is changing, so the thing I show here might not be applicable when you\n    read it. Most likely a interface might have been renamed or moved.\n 2. The main concepts are still quite similar with OWIN, so the migration\n    shouldn't be to hard if your middleware is simple.\n\nCreating the middleware\nAll the code is here: https://github.com/mastoj/TJOwin. There you can find both\nthe old standard OWIN middleware as well as the one targeting vnext. This part\nwill only focus on the middleware and not the actual web application. The\napplication I used to try the middleware is located on github and the startup\ncode of what we want to achieve look like this:\n\npublic class Startup\n{\n    public void Configure(IApplicationBuilder app)\n    {\n        //TJOwin.HealthCheck.AppBuilderExtensions.UseHealthCheck(null);\n        var config = new HealthCheckConfiguration();\n        app.UseHealthCheck(config);\n        app.Use(helloworldMiddleware);\n    }\n\n    private RequestDelegate helloworldMiddleware(RequestDelegate arg)\n    {\n        RequestDelegate rd = (context) => \n        {\n            context.Response.WriteAsync(\"Hello world\");\n            return arg(context);\n        };\n        return rd;\n    }\n}\n\n\nIn the Configure method you see the call to UseHealthCheck(config), and that is\nwhere we start. But before you can add the actual code you need to add the\nproject, which should be a ASP.NET vNext Class Library. I don't know if that\nwill change, but you need that and not a standard class library.\n\nThe ApplicationBuilderExtensions\nTo be able to call UseHealthCheck I need to create an extension method for \nIApplicationBuilder. The extension method class look like this:\n\npublic static class AppBuilderExtensions\n{\n    public static IApplicationBuilder UseHealthCheck(this IApplicationBuilder app, HealthCheckConfiguration config = null)\n    {\n        config = config ?? new HealthCheckConfiguration();\n        return app.Use(next => new HealthCheckMiddleware(next, config).Invoke);\n    }\n}\n\n\nIt might be hard to spot changes compared to OWIN here, but there are some. The \nIApplicationBuilder interface is new to start with. The next parameter is now of\na type called RequestDelegate which has the following signature:\n\npublic delegate Task RequestDelegate(HttpContext context);\n\n\nHaving a more explicit signature make it a little bit easier to use than before\nI think.\n\nThe project file\nThe changes to the extensions class was all the code changes I did actually,\nexcept from that I just copied over all the old classes from the old project and\neverything worked. To get things to compile you need to add the right references\nto your project.json and in this case my project.json looked like:\n\n{\n    \"dependencies\": {\n        \"Newtonsoft.Json\": \"6.0.5\",\n        \"Microsoft.AspNet.Http\": \"1.0.0-alpha4\"\n    },\n\n    \"frameworks\" : {\n        \"aspnet50\" : { \n            \"dependencies\": {\n            }\n        },\n        \"aspnetcore50\" : { \n            \"dependencies\": {\n                \"System.Runtime\": \"4.0.20.0\"\n            }\n        }\n    }\n}\n\n\nI added json.net for seralization and Microsoft.AspNet.Http since it contains\nthe dll where IApplicationBuilder is defined.\n\nI also opened up the properties of the project and changed to target ASP.NET\nCore 5.0 which is the \"core optimized\" framework. This makes everything a little\nbit more lightweight and I don't have more in my application that I need.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-11-09T17:32:42.000Z","updated_at":"2016-01-15T22:22:30.000Z","published_at":"2014-11-09T17:50:48.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe060","uuid":"334671e4-4d60-4aeb-9516-c690d7034b10","title":"Pure functional applications (in F#)","slug":"pure-application-in-fsharp","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"How did I end up here? I'm not an F# expert, so it is somewhat weird to be in an F# calendar with all these awesome F# developers. It started with me [tweeting](https://twitter.com/TomasJansson/status/536874619480064000) the obvious to [Sergey Tihon](https://twitter.com/sergey_tihon). My tweet, that this is a great initiative, was answered with a \\\"question\\\" if I wanted to join, and it is really hard to say no in the public so here we are. My goal of this post is to show some of my F# playground and hopefully also get some feedback on the work I've done so far. The topic of this post is based on a previous post [\\\"Your application should be a pure function\\\"](http://blog.tomasjansson.com/your-application-should-be-a-pure-function/) that I wrote about a month ago. I didn't provide any sample in F# so that is what I aim to do in this post as a part of the [F# advent calendar](https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/). If you haven't read the previous posts in the calendar I really recommend it, and also follow up the rest of the posts that are to come.\\n\\n##TL;DR;\\nIf you just want to see the code go to [GitHub](https://github.com/mastoj/FsCQRSShop).\\n\\n##Credits to the people who deserve it\\nMost of what I will present are my take on how I want to implement things, but I have been inspired and copied/stole ideas and code from other people who really deservere some credit.\\n\\n * The guys behind the [FsUno.Prod](https://github.com/thinkbeforecoding/FsUno.Prod); [Jérémie Chassaing](https://twitter.com/thinkb4coding) and [Ruben Bartelink](https://twitter.com/rbartelink)\\n * A lot of the thoughts here has also been inspired by what [Greg Young](https://twitter.com/gregyoung) has done in the general space of event sourcing and CQRS\\n * I've also done an effort to add the [\\\"Railway Oriented Programming\\\"](http://fsharpforfunandprofit.com/rop/) \\\"pattern\\\" to the sample which I was introduced to by [Scott Wlaschin](https://twitter.com/ScottWlaschin)\\n\\n##Let's get started\\n[\\\"Your application should be a pure function\\\"](http://blog.tomasjansson.com/your-application-should-be-a-pure-function/) is something I can't say have lived up to in the past, but it is something I really try to do as much as I can in my current project. This is most important where you have some \\\"real\\\" logic/domain to talk about, since this makes it much easier to test the application as well as reason about it since it is just a function. Another great advantage of doing it this way is that as soon as you only have **one entry point** to your application you can pass the arguments through a pipeline streamlining things like authorization and logging for example. Before digging in to the code I just want to clarify that this is just a sample domain and the actions and events are probably somewhat stupid, but try to see beyond that. Also, I want walk all the code for my sample only the most important bits, but it is all available on [GitHub](https://github.com/mastoj/FsCQRSShop).\\n\\n##General architecture\\nI've been studying, experimenting and now working with CQRS and event sourcing for some years now and one of the feelings that I had is that this is a functional way of doing things. Also, when people like Greg Young also mention it and when you find projects like the [**FsUno.Prod**](http://thinkbeforecoding.github.io/FsUno.Prod/) project where they are describing their \\\"functional event sourcing\\\" journey, you do get more confident in that you are on to something and that CQRS with event sourcing is really functional by nature. So the code samples below is  my take on how I would like it to work when implementing a CQRS based architecture with event sourcing. The idea is really simple, you pass in commands to the application and out comes events (or errors).\\n\\nIf you are interested in more general CQRS stuff I've written a serie of 10 blog posts about it which you can find [here](http://blog.tomasjansson.com/tag/cqrsshop/).\\n\\n##Test first\\nAs all good developers we start with a test :). The type of test I want to write are those that focus on the behavior of the application, since it is at the boundaries I want to have the test disregarding the internals of the application. The application specify external dependencies as functions and those can be passed in when building the application. There are two types of tests I want to be able to write; \\\"positive\\\" tests and \\\"negative\\\" tests:\\n\\n    module ``When making customer preferred`` =\\n\\n        [<Fact>]\\n        let ``the customer should get the discount``() =\\n            let id = Guid.NewGuid()\\n            Given ([(id, [CustomerCreated(CustomerId id, \\\"tomas jansson\\\")])], None)\\n            |> When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\\n            |> Expect [CustomerMarkedAsPreferred(CustomerId id, 80)]\\n\\n        [<Fact>]\\n        let ``it should fail if customer doesn't exist``() =\\n            let id = Guid.NewGuid()\\n            Given ([], None)\\n            |> When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\\n            |> ExpectFail (InvalidState \\\"Customer\\\")\\n            \\n**Notes about the tests**\\n\\n* The tests are somewhat implementation agnostic, that is, I don't specify in the test what part of the application I test, all I specify is the input and output for the application.\\n* The `Given` clause might look weird, but that's just to make it easer for my dummy event store in supporting the test. Also, the `Option` parameter to the `Given` clause is just a way to pass in external dependents if I need to when setting the pre conditions of the test.\\n* I can `Expect` events comming out for the application, or\\n* I can `ExpectFail`, which is a better way to handle errors than throwing exceptions.\\n\\n###The specification\\nI wouldn't be able to write a test like that without some helper functions that made it possible. So my specification helper module look like this:\\n\\n    let createTestApplication dependencies events = \\n            let es = create()\\n            let toStreamId (id:Guid) = sprintf \\\"%O\\\" id\\n            let readStream id = readFromStream es (toStreamId id)\\n            events |> List.map (fun (id, evts) -> appendToStream es (toStreamId id) -1 evts) |> ignore\\n            let deps = match dependencies with\\n                       | None -> { defaultDependencies with readEvents = readStream}\\n                       | Some d -> { d with readEvents = readStream }\\n\\n            let save res = Success res\\n            buildDomainEntry save deps\\n\\n    let Given (events, dependencies) = events, dependencies\\n    let When command (events, dependencies) = events, dependencies, command\\n\\n    let Expect expectedEvents (events, dependencies, command) = \\n        printfn \\\"Given: %A\\\" events\\n        printfn \\\"When: %A\\\" command\\n        printfn \\\"Expects: %A\\\" expectedEvents\\n        command \\n        |> (createTestApplication dependencies events) \\n        |> (fun (Success (id, version, events)) -> events)\\n        |> should equal expectedEvents\\n\\n    let ExpectFail failure (events, dependencies, command) =\\n        printfn \\\"Given: %A\\\" events\\n        printfn \\\"When: %A\\\" command\\n        printfn \\\"Should fail with: %A\\\" failure\\n\\n        command \\n        |> (createTestApplication dependencies events) \\n        |> (fun r -> r = Failure failure)\\n        |> should equal true\\n\\nThere are basically four parts to it.\\n\\n1. The `createTestApplication` function which basically sets up the infrastructure for the application, but the actual \\\"application\\\" is created with the call to `buildDomainEntry` and that is the same call I'm doing outside of the tests as well. The `createTestApplication` function creates a dummy event store and adds my pre-condition events to it which might be used by the application.\\n2. The `Given` and `When` are two simple helper functions to build up the test case.\\n3. The `Expect` is the \\\"positive\\\" test function where I check that I get the expected events when executing a command.\\n4. The `ExpectFail` is the \\\"negative\\\" test function where I check for expected error conditions in the application.\\n\\n##Building the application\\nSo far I don't have that much external dependencies, and I hope it stays that way, and my pipeline which the commands go through doesn't do much things either so the application building function is not that complex as you see below.\\n\\n    let validateCommand c = \\n        match c with\\n        | Command.BasketCommand(CheckoutBasket(id, addr)) -> \\n            match addr.Street.Trim() with\\n            | \\\"\\\" -> Failure (ValidationError \\\"Invalid address\\\")\\n            | trimmed -> Success (BasketCommand(CheckoutBasket(id, {addr with Street = trimmed})))\\n        | _ -> Success c\\n\\n    let buildDomainEntry save deps c = \\n        (validateCommand c) >>= (handle deps) >>= save\\n\\nI've added a simple validation function to the application pipeline to show how one could inject things to the pipeline. Other things that could be added to the `buildDomainEntry` function are logging, correlation handling, authorization and things like that. The result handling is inspired byt the [\\\"Railway Oriented Programming\\\"](http://fsharpforfunandprofit.com/rop/) and the type I've added to have support for that is also really simple, but important.\\n\\n    type Error = \\n        | InvalidState of string\\n        | NotSupportedCommand of string\\n\\n    type Result<'T> =\\n        | Success of 'T\\n        | Fail of Error\\n\\n    let bind switchFunction = \\n        fun input -> match input with\\n                     | Success s -> switchFunction s\\n                     | Fail s -> Fail s\\n\\n    let (>>=) input switchFunction = bind switchFunction input\\n    \\n##Commands and events\\nThis is just the data types for the application, and I've just discriminated union to represent them. I don't show all the commands and events either, but I think you get the point.\\n\\n**Commands**\\n\\n    type Command = \\n        | CustomerCommand of CustomerCommand\\n    and CustomerCommand = \\n        | CreateCustomer of CustomerId:CustomerId * Name:string\\n        | MarkCustomerAsPreferred of CustomerId:CustomerId * Discount:int\\n\\n**Events**\\n\\n    type Event = \\n        | CustomerCreated of Id:CustomerId * Name:string\\n        | CustomerMarkedAsPreferred of Id:CustomerId * Discount:int\\n\\nThese are the commands and events that goes in and out from the application, but what is actually passed in from the UI or sent to the data storage might be something else that is mapped to these types. The reason the might be something else is because serialization with DUs is not that pretty for other consumers than F# as of the moment.\\n\\n##Handling the command and evolving state\\nThe `handle` function is sort of a router, it takes a command and passes it to the correct \\\"sub-handler\\\" if you will.\\n\\n    let handle deps c =\\n        match c with\\n        | Command.CustomerCommand(cc) -> handleCustomer deps cc\\n        | Command.BasketCommand(bc) -> handleBasket deps bc\\n        | Command.OrderCommand(oc) -> handleOrder deps oc\\n        | Command.ProductCommand(pc) -> handleProduct deps pc\\n\\nNothing magic going on there so we'll get going. Before we look into `handleCustomer` I'll go into the process of building up state from events. The general idea is just to do a left fold of all the events and executing a function evolving a state from one state to another based on every event. So the general function look like this.\\n\\n    let evolve evolveOne initState events =\\n        List.fold (fun result e -> match result with\\n                                   | Failure f -> Failure f\\n                                   | Success (v,s) -> match (evolveOne s e) with\\n                                                      | Success s -> Success (v+1, s) \\n                                                      | Failure f -> Failure f) \\n                  (Success (-1, initState)) events  \\n                  \\nFor the specific scenario of a customer we have parts:\\n\\n    type Customer = \\n        | Init\\n        | Created of CustomerInfo\\n        | Preferred of CustomerInfo * Discount:int\\n\\n    let evolveOneCustomer state event =\\n        match state with\\n        | Init -> match event with\\n                  | CustomerCreated(id, name) -> Success ( Created{Id = id; Name = name})\\n                  | _ -> stateTransitionFail event state\\n        | Created info -> match event with\\n                          | CustomerMarkedAsPreferred(id, discount) -> Success (Preferred(info,discount))\\n                          | _ -> stateTransitionFail event state\\n        | Preferred (info, _) -> match event with\\n                                 | CustomerMarkedAsPreferred(id, discount) -> Success (Preferred(info,discount))\\n                                 | _ -> stateTransitionFail event state\\n\\n    let evolveCustomer = evolve evolveOneCustomer\\n\\n    let getCustomerState deps id = evolveCustomer initCustomer ((deps.readEvents id) |> (fun (_, e) -> e))\\n\\n* `evolveOneCustomer` is sort of a state machine that executes the transitions. You shouldn't execute business logic concerning command execution, just logic concerning if you are allowed to make a state transition.\\n* `evolveCustomer` is a simple helper, created with the helper function `evolve`.\\n* `getCustomerState` is a function that actually produces the states and also gets the events to evolve the state from.\\n\\nNow when we know how to evolve the state of a customer from a set of events it is time to handle the commands.\\n\\n    let handleCustomer deps cc =\\n        let createCustomer id name (version, state) =\\n            match state with\\n            | Init -> Success (id, version, [CustomerCreated(CustomerId id, name)])\\n            | _ -> Failure (InvalidState \\\"Customer\\\")\\n        let markAsPreferred id discount (version, state) = \\n            match state with\\n            | Init -> Failure (InvalidState \\\"Customer\\\")\\n            | _ -> Success (id, version, [CustomerMarkedAsPreferred(CustomerId id, discount)])\\n\\n        match cc with\\n        | CreateCustomer(CustomerId id, name) -> \\n            getCustomerState deps id >>= (createCustomer id name)\\n        | MarkCustomerAsPreferred(CustomerId id, discount) -> \\n            getCustomerState deps id >>= (markAsPreferred id discount)\\n            \\nAs you see the result type from the function is a `Result<'T>` so it can ride the train. In the `Success` scenarios I return three things; the id of \\\"aggregate\\\", the expected version that is expected in the event stream when committing and the events to commit. It's pretty straightforward. I know I can probably clean some of these things up, but that is a later project.\\n\\n##Summary\\nWhat I presented here is my first attempt to a functional event sourced application and how I would like it to work. Building an event sourced application and treating your application as a pure function is so useful in many ways, and at the same time it also makes you focus on the most important parts when you have the infrastructure set up. You could argue that it would be the same if the application returned the object/document instead of events, but doing so will actually make your application \\\"loose\\\" data since you are only dealing with state and not what caused the state to change as you do in an event sourced application.\\n\\nThe code is running if you clone the whole thing from GitHub, but here I've gone through the most important parts of the code and if you have any question regarding it just comment below or send me a tweet. I didn't cover [Event Store](http://geteventstore.com/), which I do use in the sample application and recommend you to look at if you haven't since it is a perfect fit for event sourcing.\\n\\nI'm not an F# expert so please suggest improvements if you have any. I know the code is somewhat verbose in some areas, but I'm still restructuring it a little bit now and then. If you think I've abused F# please let me know :).\\n\\nSo this finishes my contribution to the [F# advent calendar](https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/), it was fun to force my self to actually do this in F#. I've been thinking about it for a long time and had something going, but this forced me to actually do something that would work. \\n\\nThanks for reading and Merry Christmas!\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>How did I end up here? I'm not an F# expert, so it is somewhat weird to be in an F# calendar with all these awesome F# developers. It started with me <a href=\"https://twitter.com/TomasJansson/status/536874619480064000\">tweeting</a> the obvious to <a href=\"https://twitter.com/sergey_tihon\">Sergey Tihon</a>. My tweet, that this is a great initiative, was answered with a &quot;question&quot; if I wanted to join, and it is really hard to say no in the public so here we are. My goal of this post is to show some of my F# playground and hopefully also get some feedback on the work I've done so far. The topic of this post is based on a previous post <a href=\"http://blog.tomasjansson.com/your-application-should-be-a-pure-function/\">&quot;Your application should be a pure function&quot;</a> that I wrote about a month ago. I didn't provide any sample in F# so that is what I aim to do in this post as a part of the <a href=\"https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/\">F# advent calendar</a>. If you haven't read the previous posts in the calendar I really recommend it, and also follow up the rest of the posts that are to come.</p>\n<h2 id=\"tldr\">TL;DR;</h2>\n<p>If you just want to see the code go to <a href=\"https://github.com/mastoj/FsCQRSShop\">GitHub</a>.</p>\n<h2 id=\"creditstothepeoplewhodeserveit\">Credits to the people who deserve it</h2>\n<p>Most of what I will present are my take on how I want to implement things, but I have been inspired and copied/stole ideas and code from other people who really deservere some credit.</p>\n<ul>\n<li>The guys behind the <a href=\"https://github.com/thinkbeforecoding/FsUno.Prod\">FsUno.Prod</a>; <a href=\"https://twitter.com/thinkb4coding\">Jérémie Chassaing</a> and <a href=\"https://twitter.com/rbartelink\">Ruben Bartelink</a></li>\n<li>A lot of the thoughts here has also been inspired by what <a href=\"https://twitter.com/gregyoung\">Greg Young</a> has done in the general space of event sourcing and CQRS</li>\n<li>I've also done an effort to add the <a href=\"http://fsharpforfunandprofit.com/rop/\">&quot;Railway Oriented Programming&quot;</a> &quot;pattern&quot; to the sample which I was introduced to by <a href=\"https://twitter.com/ScottWlaschin\">Scott Wlaschin</a></li>\n</ul>\n<h2 id=\"letsgetstarted\">Let's get started</h2>\n<p><a href=\"http://blog.tomasjansson.com/your-application-should-be-a-pure-function/\">&quot;Your application should be a pure function&quot;</a> is something I can't say have lived up to in the past, but it is something I really try to do as much as I can in my current project. This is most important where you have some &quot;real&quot; logic/domain to talk about, since this makes it much easier to test the application as well as reason about it since it is just a function. Another great advantage of doing it this way is that as soon as you only have <strong>one entry point</strong> to your application you can pass the arguments through a pipeline streamlining things like authorization and logging for example. Before digging in to the code I just want to clarify that this is just a sample domain and the actions and events are probably somewhat stupid, but try to see beyond that. Also, I want walk all the code for my sample only the most important bits, but it is all available on <a href=\"https://github.com/mastoj/FsCQRSShop\">GitHub</a>.</p>\n<h2 id=\"generalarchitecture\">General architecture</h2>\n<p>I've been studying, experimenting and now working with CQRS and event sourcing for some years now and one of the feelings that I had is that this is a functional way of doing things. Also, when people like Greg Young also mention it and when you find projects like the <a href=\"http://thinkbeforecoding.github.io/FsUno.Prod/\"><strong>FsUno.Prod</strong></a> project where they are describing their &quot;functional event sourcing&quot; journey, you do get more confident in that you are on to something and that CQRS with event sourcing is really functional by nature. So the code samples below is  my take on how I would like it to work when implementing a CQRS based architecture with event sourcing. The idea is really simple, you pass in commands to the application and out comes events (or errors).</p>\n<p>If you are interested in more general CQRS stuff I've written a serie of 10 blog posts about it which you can find <a href=\"http://blog.tomasjansson.com/tag/cqrsshop/\">here</a>.</p>\n<h2 id=\"testfirst\">Test first</h2>\n<p>As all good developers we start with a test :). The type of test I want to write are those that focus on the behavior of the application, since it is at the boundaries I want to have the test disregarding the internals of the application. The application specify external dependencies as functions and those can be passed in when building the application. There are two types of tests I want to be able to write; &quot;positive&quot; tests and &quot;negative&quot; tests:</p>\n<pre><code>module ``When making customer preferred`` =\n\n    [&lt;Fact&gt;]\n    let ``the customer should get the discount``() =\n        let id = Guid.NewGuid()\n        Given ([(id, [CustomerCreated(CustomerId id, &quot;tomas jansson&quot;)])], None)\n        |&gt; When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\n        |&gt; Expect [CustomerMarkedAsPreferred(CustomerId id, 80)]\n\n    [&lt;Fact&gt;]\n    let ``it should fail if customer doesn't exist``() =\n        let id = Guid.NewGuid()\n        Given ([], None)\n        |&gt; When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\n        |&gt; ExpectFail (InvalidState &quot;Customer&quot;)\n</code></pre>\n<p><strong>Notes about the tests</strong></p>\n<ul>\n<li>The tests are somewhat implementation agnostic, that is, I don't specify in the test what part of the application I test, all I specify is the input and output for the application.</li>\n<li>The <code>Given</code> clause might look weird, but that's just to make it easer for my dummy event store in supporting the test. Also, the <code>Option</code> parameter to the <code>Given</code> clause is just a way to pass in external dependents if I need to when setting the pre conditions of the test.</li>\n<li>I can <code>Expect</code> events comming out for the application, or</li>\n<li>I can <code>ExpectFail</code>, which is a better way to handle errors than throwing exceptions.</li>\n</ul>\n<h3 id=\"thespecification\">The specification</h3>\n<p>I wouldn't be able to write a test like that without some helper functions that made it possible. So my specification helper module look like this:</p>\n<pre><code>let createTestApplication dependencies events = \n        let es = create()\n        let toStreamId (id:Guid) = sprintf &quot;%O&quot; id\n        let readStream id = readFromStream es (toStreamId id)\n        events |&gt; List.map (fun (id, evts) -&gt; appendToStream es (toStreamId id) -1 evts) |&gt; ignore\n        let deps = match dependencies with\n                   | None -&gt; { defaultDependencies with readEvents = readStream}\n                   | Some d -&gt; { d with readEvents = readStream }\n\n        let save res = Success res\n        buildDomainEntry save deps\n\nlet Given (events, dependencies) = events, dependencies\nlet When command (events, dependencies) = events, dependencies, command\n\nlet Expect expectedEvents (events, dependencies, command) = \n    printfn &quot;Given: %A&quot; events\n    printfn &quot;When: %A&quot; command\n    printfn &quot;Expects: %A&quot; expectedEvents\n    command \n    |&gt; (createTestApplication dependencies events) \n    |&gt; (fun (Success (id, version, events)) -&gt; events)\n    |&gt; should equal expectedEvents\n\nlet ExpectFail failure (events, dependencies, command) =\n    printfn &quot;Given: %A&quot; events\n    printfn &quot;When: %A&quot; command\n    printfn &quot;Should fail with: %A&quot; failure\n\n    command \n    |&gt; (createTestApplication dependencies events) \n    |&gt; (fun r -&gt; r = Failure failure)\n    |&gt; should equal true\n</code></pre>\n<p>There are basically four parts to it.</p>\n<ol>\n<li>The <code>createTestApplication</code> function which basically sets up the infrastructure for the application, but the actual &quot;application&quot; is created with the call to <code>buildDomainEntry</code> and that is the same call I'm doing outside of the tests as well. The <code>createTestApplication</code> function creates a dummy event store and adds my pre-condition events to it which might be used by the application.</li>\n<li>The <code>Given</code> and <code>When</code> are two simple helper functions to build up the test case.</li>\n<li>The <code>Expect</code> is the &quot;positive&quot; test function where I check that I get the expected events when executing a command.</li>\n<li>The <code>ExpectFail</code> is the &quot;negative&quot; test function where I check for expected error conditions in the application.</li>\n</ol>\n<h2 id=\"buildingtheapplication\">Building the application</h2>\n<p>So far I don't have that much external dependencies, and I hope it stays that way, and my pipeline which the commands go through doesn't do much things either so the application building function is not that complex as you see below.</p>\n<pre><code>let validateCommand c = \n    match c with\n    | Command.BasketCommand(CheckoutBasket(id, addr)) -&gt; \n        match addr.Street.Trim() with\n        | &quot;&quot; -&gt; Failure (ValidationError &quot;Invalid address&quot;)\n        | trimmed -&gt; Success (BasketCommand(CheckoutBasket(id, {addr with Street = trimmed})))\n    | _ -&gt; Success c\n\nlet buildDomainEntry save deps c = \n    (validateCommand c) &gt;&gt;= (handle deps) &gt;&gt;= save\n</code></pre>\n<p>I've added a simple validation function to the application pipeline to show how one could inject things to the pipeline. Other things that could be added to the <code>buildDomainEntry</code> function are logging, correlation handling, authorization and things like that. The result handling is inspired byt the <a href=\"http://fsharpforfunandprofit.com/rop/\">&quot;Railway Oriented Programming&quot;</a> and the type I've added to have support for that is also really simple, but important.</p>\n<pre><code>type Error = \n    | InvalidState of string\n    | NotSupportedCommand of string\n\ntype Result&lt;'T&gt; =\n    | Success of 'T\n    | Fail of Error\n\nlet bind switchFunction = \n    fun input -&gt; match input with\n                 | Success s -&gt; switchFunction s\n                 | Fail s -&gt; Fail s\n\nlet (&gt;&gt;=) input switchFunction = bind switchFunction input\n</code></pre>\n<h2 id=\"commandsandevents\">Commands and events</h2>\n<p>This is just the data types for the application, and I've just discriminated union to represent them. I don't show all the commands and events either, but I think you get the point.</p>\n<p><strong>Commands</strong></p>\n<pre><code>type Command = \n    | CustomerCommand of CustomerCommand\nand CustomerCommand = \n    | CreateCustomer of CustomerId:CustomerId * Name:string\n    | MarkCustomerAsPreferred of CustomerId:CustomerId * Discount:int\n</code></pre>\n<p><strong>Events</strong></p>\n<pre><code>type Event = \n    | CustomerCreated of Id:CustomerId * Name:string\n    | CustomerMarkedAsPreferred of Id:CustomerId * Discount:int\n</code></pre>\n<p>These are the commands and events that goes in and out from the application, but what is actually passed in from the UI or sent to the data storage might be something else that is mapped to these types. The reason the might be something else is because serialization with DUs is not that pretty for other consumers than F# as of the moment.</p>\n<h2 id=\"handlingthecommandandevolvingstate\">Handling the command and evolving state</h2>\n<p>The <code>handle</code> function is sort of a router, it takes a command and passes it to the correct &quot;sub-handler&quot; if you will.</p>\n<pre><code>let handle deps c =\n    match c with\n    | Command.CustomerCommand(cc) -&gt; handleCustomer deps cc\n    | Command.BasketCommand(bc) -&gt; handleBasket deps bc\n    | Command.OrderCommand(oc) -&gt; handleOrder deps oc\n    | Command.ProductCommand(pc) -&gt; handleProduct deps pc\n</code></pre>\n<p>Nothing magic going on there so we'll get going. Before we look into <code>handleCustomer</code> I'll go into the process of building up state from events. The general idea is just to do a left fold of all the events and executing a function evolving a state from one state to another based on every event. So the general function look like this.</p>\n<pre><code>let evolve evolveOne initState events =\n    List.fold (fun result e -&gt; match result with\n                               | Failure f -&gt; Failure f\n                               | Success (v,s) -&gt; match (evolveOne s e) with\n                                                  | Success s -&gt; Success (v+1, s) \n                                                  | Failure f -&gt; Failure f) \n              (Success (-1, initState)) events  \n</code></pre>\n<p>For the specific scenario of a customer we have parts:</p>\n<pre><code>type Customer = \n    | Init\n    | Created of CustomerInfo\n    | Preferred of CustomerInfo * Discount:int\n\nlet evolveOneCustomer state event =\n    match state with\n    | Init -&gt; match event with\n              | CustomerCreated(id, name) -&gt; Success ( Created{Id = id; Name = name})\n              | _ -&gt; stateTransitionFail event state\n    | Created info -&gt; match event with\n                      | CustomerMarkedAsPreferred(id, discount) -&gt; Success (Preferred(info,discount))\n                      | _ -&gt; stateTransitionFail event state\n    | Preferred (info, _) -&gt; match event with\n                             | CustomerMarkedAsPreferred(id, discount) -&gt; Success (Preferred(info,discount))\n                             | _ -&gt; stateTransitionFail event state\n\nlet evolveCustomer = evolve evolveOneCustomer\n\nlet getCustomerState deps id = evolveCustomer initCustomer ((deps.readEvents id) |&gt; (fun (_, e) -&gt; e))\n</code></pre>\n<ul>\n<li><code>evolveOneCustomer</code> is sort of a state machine that executes the transitions. You shouldn't execute business logic concerning command execution, just logic concerning if you are allowed to make a state transition.</li>\n<li><code>evolveCustomer</code> is a simple helper, created with the helper function <code>evolve</code>.</li>\n<li><code>getCustomerState</code> is a function that actually produces the states and also gets the events to evolve the state from.</li>\n</ul>\n<p>Now when we know how to evolve the state of a customer from a set of events it is time to handle the commands.</p>\n<pre><code>let handleCustomer deps cc =\n    let createCustomer id name (version, state) =\n        match state with\n        | Init -&gt; Success (id, version, [CustomerCreated(CustomerId id, name)])\n        | _ -&gt; Failure (InvalidState &quot;Customer&quot;)\n    let markAsPreferred id discount (version, state) = \n        match state with\n        | Init -&gt; Failure (InvalidState &quot;Customer&quot;)\n        | _ -&gt; Success (id, version, [CustomerMarkedAsPreferred(CustomerId id, discount)])\n\n    match cc with\n    | CreateCustomer(CustomerId id, name) -&gt; \n        getCustomerState deps id &gt;&gt;= (createCustomer id name)\n    | MarkCustomerAsPreferred(CustomerId id, discount) -&gt; \n        getCustomerState deps id &gt;&gt;= (markAsPreferred id discount)\n</code></pre>\n<p>As you see the result type from the function is a <code>Result&lt;'T&gt;</code> so it can ride the train. In the <code>Success</code> scenarios I return three things; the id of &quot;aggregate&quot;, the expected version that is expected in the event stream when committing and the events to commit. It's pretty straightforward. I know I can probably clean some of these things up, but that is a later project.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>What I presented here is my first attempt to a functional event sourced application and how I would like it to work. Building an event sourced application and treating your application as a pure function is so useful in many ways, and at the same time it also makes you focus on the most important parts when you have the infrastructure set up. You could argue that it would be the same if the application returned the object/document instead of events, but doing so will actually make your application &quot;loose&quot; data since you are only dealing with state and not what caused the state to change as you do in an event sourced application.</p>\n<p>The code is running if you clone the whole thing from GitHub, but here I've gone through the most important parts of the code and if you have any question regarding it just comment below or send me a tweet. I didn't cover <a href=\"http://geteventstore.com/\">Event Store</a>, which I do use in the sample application and recommend you to look at if you haven't since it is a perfect fit for event sourcing.</p>\n<p>I'm not an F# expert so please suggest improvements if you have any. I know the code is somewhat verbose in some areas, but I'm still restructuring it a little bit now and then. If you think I've abused F# please let me know :).</p>\n<p>So this finishes my contribution to the <a href=\"https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/\">F# advent calendar</a>, it was fun to force my self to actually do this in F#. I've been thinking about it for a long time and had something going, but this forced me to actually do something that would work.</p>\n<p>Thanks for reading and Merry Christmas!</p>\n<!--kg-card-end: markdown-->","comment_id":"45","plaintext":"How did I end up here? I'm not an F# expert, so it is somewhat weird to be in an\nF# calendar with all these awesome F# developers. It started with me tweeting\n[https://twitter.com/TomasJansson/status/536874619480064000] the obvious to \nSergey Tihon [https://twitter.com/sergey_tihon]. My tweet, that this is a great\ninitiative, was answered with a \"question\" if I wanted to join, and it is really\nhard to say no in the public so here we are. My goal of this post is to show\nsome of my F# playground and hopefully also get some feedback on the work I've\ndone so far. The topic of this post is based on a previous post \"Your\napplication should be a pure function\"\n[http://blog.tomasjansson.com/your-application-should-be-a-pure-function/] that\nI wrote about a month ago. I didn't provide any sample in F# so that is what I\naim to do in this post as a part of the F# advent calendar\n[https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/]\n. If you haven't read the previous posts in the calendar I really recommend it,\nand also follow up the rest of the posts that are to come.\n\nTL;DR;\nIf you just want to see the code go to GitHub\n[https://github.com/mastoj/FsCQRSShop].\n\nCredits to the people who deserve it\nMost of what I will present are my take on how I want to implement things, but I\nhave been inspired and copied/stole ideas and code from other people who really\ndeservere some credit.\n\n * The guys behind the FsUno.Prod\n   [https://github.com/thinkbeforecoding/FsUno.Prod]; Jérémie Chassaing\n   [https://twitter.com/thinkb4coding] and Ruben Bartelink\n   [https://twitter.com/rbartelink]\n * A lot of the thoughts here has also been inspired by what Greg Young\n   [https://twitter.com/gregyoung] has done in the general space of event\n   sourcing and CQRS\n * I've also done an effort to add the \"Railway Oriented Programming\"\n   [http://fsharpforfunandprofit.com/rop/] \"pattern\" to the sample which I was\n   introduced to by Scott Wlaschin [https://twitter.com/ScottWlaschin]\n\nLet's get started\n\"Your application should be a pure function\"\n[http://blog.tomasjansson.com/your-application-should-be-a-pure-function/] is\nsomething I can't say have lived up to in the past, but it is something I really\ntry to do as much as I can in my current project. This is most important where\nyou have some \"real\" logic/domain to talk about, since this makes it much easier\nto test the application as well as reason about it since it is just a function.\nAnother great advantage of doing it this way is that as soon as you only have \none entry point to your application you can pass the arguments through a\npipeline streamlining things like authorization and logging for example. Before\ndigging in to the code I just want to clarify that this is just a sample domain\nand the actions and events are probably somewhat stupid, but try to see beyond\nthat. Also, I want walk all the code for my sample only the most important bits,\nbut it is all available on GitHub [https://github.com/mastoj/FsCQRSShop].\n\nGeneral architecture\nI've been studying, experimenting and now working with CQRS and event sourcing\nfor some years now and one of the feelings that I had is that this is a\nfunctional way of doing things. Also, when people like Greg Young also mention\nit and when you find projects like the FsUno.Prod\n[http://thinkbeforecoding.github.io/FsUno.Prod/] project where they are\ndescribing their \"functional event sourcing\" journey, you do get more confident\nin that you are on to something and that CQRS with event sourcing is really\nfunctional by nature. So the code samples below is my take on how I would like\nit to work when implementing a CQRS based architecture with event sourcing. The\nidea is really simple, you pass in commands to the application and out comes\nevents (or errors).\n\nIf you are interested in more general CQRS stuff I've written a serie of 10 blog\nposts about it which you can find here\n[http://blog.tomasjansson.com/tag/cqrsshop/].\n\nTest first\nAs all good developers we start with a test :). The type of test I want to write\nare those that focus on the behavior of the application, since it is at the\nboundaries I want to have the test disregarding the internals of the\napplication. The application specify external dependencies as functions and\nthose can be passed in when building the application. There are two types of\ntests I want to be able to write; \"positive\" tests and \"negative\" tests:\n\nmodule ``When making customer preferred`` =\n\n    [<Fact>]\n    let ``the customer should get the discount``() =\n        let id = Guid.NewGuid()\n        Given ([(id, [CustomerCreated(CustomerId id, \"tomas jansson\")])], None)\n        |> When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\n        |> Expect [CustomerMarkedAsPreferred(CustomerId id, 80)]\n\n    [<Fact>]\n    let ``it should fail if customer doesn't exist``() =\n        let id = Guid.NewGuid()\n        Given ([], None)\n        |> When (Command.CustomerCommand(MarkCustomerAsPreferred(CustomerId id, 80)))\n        |> ExpectFail (InvalidState \"Customer\")\n\n\nNotes about the tests\n\n * The tests are somewhat implementation agnostic, that is, I don't specify in\n   the test what part of the application I test, all I specify is the input and\n   output for the application.\n * The Given clause might look weird, but that's just to make it easer for my\n   dummy event store in supporting the test. Also, the Option parameter to the \n   Given clause is just a way to pass in external dependents if I need to when\n   setting the pre conditions of the test.\n * I can Expect events comming out for the application, or\n * I can ExpectFail, which is a better way to handle errors than throwing\n   exceptions.\n\nThe specification\nI wouldn't be able to write a test like that without some helper functions that\nmade it possible. So my specification helper module look like this:\n\nlet createTestApplication dependencies events = \n        let es = create()\n        let toStreamId (id:Guid) = sprintf \"%O\" id\n        let readStream id = readFromStream es (toStreamId id)\n        events |> List.map (fun (id, evts) -> appendToStream es (toStreamId id) -1 evts) |> ignore\n        let deps = match dependencies with\n                   | None -> { defaultDependencies with readEvents = readStream}\n                   | Some d -> { d with readEvents = readStream }\n\n        let save res = Success res\n        buildDomainEntry save deps\n\nlet Given (events, dependencies) = events, dependencies\nlet When command (events, dependencies) = events, dependencies, command\n\nlet Expect expectedEvents (events, dependencies, command) = \n    printfn \"Given: %A\" events\n    printfn \"When: %A\" command\n    printfn \"Expects: %A\" expectedEvents\n    command \n    |> (createTestApplication dependencies events) \n    |> (fun (Success (id, version, events)) -> events)\n    |> should equal expectedEvents\n\nlet ExpectFail failure (events, dependencies, command) =\n    printfn \"Given: %A\" events\n    printfn \"When: %A\" command\n    printfn \"Should fail with: %A\" failure\n\n    command \n    |> (createTestApplication dependencies events) \n    |> (fun r -> r = Failure failure)\n    |> should equal true\n\n\nThere are basically four parts to it.\n\n 1. The createTestApplication function which basically sets up the\n    infrastructure for the application, but the actual \"application\" is created\n    with the call to buildDomainEntry and that is the same call I'm doing\n    outside of the tests as well. The createTestApplication function creates a\n    dummy event store and adds my pre-condition events to it which might be used\n    by the application.\n 2. The Given and When are two simple helper functions to build up the test\n    case.\n 3. The Expect is the \"positive\" test function where I check that I get the\n    expected events when executing a command.\n 4. The ExpectFail is the \"negative\" test function where I check for expected\n    error conditions in the application.\n\nBuilding the application\nSo far I don't have that much external dependencies, and I hope it stays that\nway, and my pipeline which the commands go through doesn't do much things either\nso the application building function is not that complex as you see below.\n\nlet validateCommand c = \n    match c with\n    | Command.BasketCommand(CheckoutBasket(id, addr)) -> \n        match addr.Street.Trim() with\n        | \"\" -> Failure (ValidationError \"Invalid address\")\n        | trimmed -> Success (BasketCommand(CheckoutBasket(id, {addr with Street = trimmed})))\n    | _ -> Success c\n\nlet buildDomainEntry save deps c = \n    (validateCommand c) >>= (handle deps) >>= save\n\n\nI've added a simple validation function to the application pipeline to show how\none could inject things to the pipeline. Other things that could be added to the \nbuildDomainEntry function are logging, correlation handling, authorization and\nthings like that. The result handling is inspired byt the \"Railway Oriented\nProgramming\" [http://fsharpforfunandprofit.com/rop/] and the type I've added to\nhave support for that is also really simple, but important.\n\ntype Error = \n    | InvalidState of string\n    | NotSupportedCommand of string\n\ntype Result<'T> =\n    | Success of 'T\n    | Fail of Error\n\nlet bind switchFunction = \n    fun input -> match input with\n                 | Success s -> switchFunction s\n                 | Fail s -> Fail s\n\nlet (>>=) input switchFunction = bind switchFunction input\n\n\nCommands and events\nThis is just the data types for the application, and I've just discriminated\nunion to represent them. I don't show all the commands and events either, but I\nthink you get the point.\n\nCommands\n\ntype Command = \n    | CustomerCommand of CustomerCommand\nand CustomerCommand = \n    | CreateCustomer of CustomerId:CustomerId * Name:string\n    | MarkCustomerAsPreferred of CustomerId:CustomerId * Discount:int\n\n\nEvents\n\ntype Event = \n    | CustomerCreated of Id:CustomerId * Name:string\n    | CustomerMarkedAsPreferred of Id:CustomerId * Discount:int\n\n\nThese are the commands and events that goes in and out from the application, but\nwhat is actually passed in from the UI or sent to the data storage might be\nsomething else that is mapped to these types. The reason the might be something\nelse is because serialization with DUs is not that pretty for other consumers\nthan F# as of the moment.\n\nHandling the command and evolving state\nThe handle function is sort of a router, it takes a command and passes it to the\ncorrect \"sub-handler\" if you will.\n\nlet handle deps c =\n    match c with\n    | Command.CustomerCommand(cc) -> handleCustomer deps cc\n    | Command.BasketCommand(bc) -> handleBasket deps bc\n    | Command.OrderCommand(oc) -> handleOrder deps oc\n    | Command.ProductCommand(pc) -> handleProduct deps pc\n\n\nNothing magic going on there so we'll get going. Before we look into \nhandleCustomer I'll go into the process of building up state from events. The\ngeneral idea is just to do a left fold of all the events and executing a\nfunction evolving a state from one state to another based on every event. So the\ngeneral function look like this.\n\nlet evolve evolveOne initState events =\n    List.fold (fun result e -> match result with\n                               | Failure f -> Failure f\n                               | Success (v,s) -> match (evolveOne s e) with\n                                                  | Success s -> Success (v+1, s) \n                                                  | Failure f -> Failure f) \n              (Success (-1, initState)) events  \n\n\nFor the specific scenario of a customer we have parts:\n\ntype Customer = \n    | Init\n    | Created of CustomerInfo\n    | Preferred of CustomerInfo * Discount:int\n\nlet evolveOneCustomer state event =\n    match state with\n    | Init -> match event with\n              | CustomerCreated(id, name) -> Success ( Created{Id = id; Name = name})\n              | _ -> stateTransitionFail event state\n    | Created info -> match event with\n                      | CustomerMarkedAsPreferred(id, discount) -> Success (Preferred(info,discount))\n                      | _ -> stateTransitionFail event state\n    | Preferred (info, _) -> match event with\n                             | CustomerMarkedAsPreferred(id, discount) -> Success (Preferred(info,discount))\n                             | _ -> stateTransitionFail event state\n\nlet evolveCustomer = evolve evolveOneCustomer\n\nlet getCustomerState deps id = evolveCustomer initCustomer ((deps.readEvents id) |> (fun (_, e) -> e))\n\n\n * evolveOneCustomer is sort of a state machine that executes the transitions.\n   You shouldn't execute business logic concerning command execution, just logic\n   concerning if you are allowed to make a state transition.\n * evolveCustomer is a simple helper, created with the helper function evolve.\n * getCustomerState is a function that actually produces the states and also\n   gets the events to evolve the state from.\n\nNow when we know how to evolve the state of a customer from a set of events it\nis time to handle the commands.\n\nlet handleCustomer deps cc =\n    let createCustomer id name (version, state) =\n        match state with\n        | Init -> Success (id, version, [CustomerCreated(CustomerId id, name)])\n        | _ -> Failure (InvalidState \"Customer\")\n    let markAsPreferred id discount (version, state) = \n        match state with\n        | Init -> Failure (InvalidState \"Customer\")\n        | _ -> Success (id, version, [CustomerMarkedAsPreferred(CustomerId id, discount)])\n\n    match cc with\n    | CreateCustomer(CustomerId id, name) -> \n        getCustomerState deps id >>= (createCustomer id name)\n    | MarkCustomerAsPreferred(CustomerId id, discount) -> \n        getCustomerState deps id >>= (markAsPreferred id discount)\n\n\nAs you see the result type from the function is a Result<'T> so it can ride the\ntrain. In the Success scenarios I return three things; the id of \"aggregate\",\nthe expected version that is expected in the event stream when committing and\nthe events to commit. It's pretty straightforward. I know I can probably clean\nsome of these things up, but that is a later project.\n\nSummary\nWhat I presented here is my first attempt to a functional event sourced\napplication and how I would like it to work. Building an event sourced\napplication and treating your application as a pure function is so useful in\nmany ways, and at the same time it also makes you focus on the most important\nparts when you have the infrastructure set up. You could argue that it would be\nthe same if the application returned the object/document instead of events, but\ndoing so will actually make your application \"loose\" data since you are only\ndealing with state and not what caused the state to change as you do in an event\nsourced application.\n\nThe code is running if you clone the whole thing from GitHub, but here I've gone\nthrough the most important parts of the code and if you have any question\nregarding it just comment below or send me a tweet. I didn't cover Event Store\n[http://geteventstore.com/], which I do use in the sample application and\nrecommend you to look at if you haven't since it is a perfect fit for event\nsourcing.\n\nI'm not an F# expert so please suggest improvements if you have any. I know the\ncode is somewhat verbose in some areas, but I'm still restructuring it a little\nbit now and then. If you think I've abused F# please let me know :).\n\nSo this finishes my contribution to the F# advent calendar\n[https://sergeytihon.wordpress.com/2014/11/24/f-advent-calendar-in-english-2014/]\n, it was fun to force my self to actually do this in F#. I've been thinking\nabout it for a long time and had something going, but this forced me to actually\ndo something that would work.\n\nThanks for reading and Merry Christmas!","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-12-10T19:58:58.000Z","updated_at":"2015-05-25T16:01:56.000Z","published_at":"2014-12-16T07:12:07.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe061","uuid":"5715f77b-413e-4fae-b2a4-bd086e3cb0b5","title":"Programming the Azure Active Directory with F#","slug":"programming-the-azure-active-directory-with-f","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I've been playing around with both Azure and F# lately and thought that I would share my knowledge about programming the Azure Active Directory using F#. It took some time to find some good example code, but after some searching I did find this project: https://github.com/AzureADSamples/ConsoleApp-GraphAPI-DotNet. The project shows most of the basic functionality, but I thought I would show it in F# and at the same time try to make it easier for others to understand. So let's started.\\n\\n##TL;DR;\\nI've put the code on github, https://github.com/mastoj/ADTutorial, so you can download it easier. This will get you started quite easy and manipulating Active Directory in no time at all. When working with this short post/tutorial I had some issues with connecting to the AD's to start with, but the problem just went a way. My guess is that it might have taken some time from the creation of the Active Directory and when it was accessible by code, but that's just a guess.\\n\\n##Creating the Active Directory in Azure\\nWe will create a new directory that we can play with. So to create the directory choose \\\"CUSTOM CREATE\\\" that is available after following the onscreen menu in the picture below.\\n![Create Active Directory](/content/images/2015/01/CreateAD.PNG)\\n\\nEnter some information in the \\\"Add directory\\\" (sample data below).\\n![Add directory](/content/images/2015/01/AddDirectory.PNG)\\n\\nIn the active directory list click the arrow for your new directory to get to the details of this directory.\\n![Active directory list](/content/images/2015/01/ADList.PNG)\\n\\nTo connect to the Active Directory, which are going to do through a console application, we need to add an application to the Active Directory. So go to the \\\"Application\\\" tab and then click \\\"ADD\\\" at the bottom of the screen. In the dialogue choose \\\"Add an application my organization is developing\\\" and then \\\"WEB APPLICATION AND/OR WEB API\\\", as \\\"REDIRECT URI\\\" and just choose a unique for \\\"APP ID URI\\\" use \\\"http://localhost\\\".\\n![Add application](/content/images/2015/01/AddApplication-1.PNG)\\n\\n##Collecting and configuring the data needed to connect\\nFor the things we are going to do later there are some data we need to collect to be able to connect to the active directory. The things we need are:\\n\\n * Client Id\\n * Client secret\\n * Tenant name\\n * Tenant id\\n\\nI had problem finding this information, so I'll try to make it as clear as I can. Before we locate those for things there is also some other configuration settings which is more static. \\n\\n * Resource url (url to the resource we acces to configure AD): https://graph.windows.net\\n * Authentication string: https://login.windows.net/<Tenant name>\\n \\nThe last thing we need to do is configure the permissions of the application so we can access the active directory.\\n\\n### The client id\\nThe client id is found under the \\\"CONFIGURE\\\" tab.\\n![Client id](/content/images/2015/01/ClientId.PNG)\\n\\n### The client secret\\nThis wasn't as straightforward as the id, but with secret they often mean \\\"key\\\". So on the \\\"CONFIGURE\\\" tab we need to generate a 1 or 2 year key that we can use. The key will only be visible after you save and only that time.\\n![Client secret](/content/images/2015/01/ClientSecret.PNG)\\n\\n### The tenant name and id\\nThe name is the easiest part, if you haven't got any custom domain names this is the domain name you used for the \\\"Active Directory\\\". So in this example it is \\\"fsharptest.onmicrosoft.com\\\" (no http before the name).\\n\\nThe id is a little bit harder to find. There are probably many ways to find the id, one is to visit the url: https://login.windows.net/fsharptest.onmicrosoft.com/FederationMetadata/2007-06/FederationMetadata.xml, but change my domain with yours. The tenant id is the guid in the \\\"entityId\\\" url on the first node in the document. \\n\\nAnother way to find the tenant id is to expand the \\\"ENABLE USERS TO SIGN ON\\\" and there it is in the magic url:\\n![Tenant id](/content/images/2015/01/TenantId.PNG)\\n\\n##Time to code\\nSince I do prefer F# when I can the coding part will be done in F#, but you can easily translate the code to C# if you want to. I'll write everything in one file in a console app. To get started the easiest way is to use to nuget packages:\\n\\n * `Microsoft.IdentityModel.Clients.ActiveDirectory` (I used version 2.13.112191810)\\n * `Microsoft.Azure.ActiveDirectory.GraphClient` (I used version 2.0.3)\\n\\nThe first part of the files are the settings, I like to keep those in a separate module:\\n\\n    [<AutoOpen>]\\n    module AdSettings = \\n        let clientId = \\\"the client id guid\\\"\\n        let clientSecret = \\\"the key from azure\\\"\\n        let tenantId = \\\"the tenant id\\\"\\n        let tenantName = \\\"the domain\\\"\\n        let resourceUrl = \\\"https://graph.windows.net\\\"\\n        let authString = \\\"https://login.windows.net/\\\" + tenantName\\n\\nThen we have the actual client code, the code is not intended for production use I'm just showing of the AD integration.\\n\\n    [<AutoOpen>]\\n    module AdClient = \\n        open Microsoft.Azure.ActiveDirectory.GraphClient\\n        open Microsoft.IdentityModel.Clients.ActiveDirectory\\n        open System\\n        open System.Linq.Expressions\\n        open System.Threading.Tasks\\n        open Microsoft.FSharp.Linq.RuntimeHelpers\\n    \\n        let getAuthenticationToken (clientId:string) (clientSecret:string) tenantName = \\n            let authenticationContext = AuthenticationContext(authString, false)\\n            let credentials = ClientCredential(clientId, clientSecret)\\n            let authenticationResult = authenticationContext.AcquireToken(resourceUrl, credentials)\\n            authenticationResult.AccessToken\\n\\n        let activeDirectoryClient (tenantId:string) token = \\n            let serviceRoot = Uri(resourceUrl + \\\"/\\\" + tenantId)\\n            let tokenTask = Func<Task<string>>(fun() ->Task.Factory.StartNew<string>(fun() -> token))\\n            let activeDirectoryClient = ActiveDirectoryClient(serviceRoot, tokenTask)\\n            activeDirectoryClient\\n\\n        let client = getAuthenticationToken clientId clientSecret tenantName |> activeDirectoryClient tenantId\\n\\n        let toExpression<'a> quotationExpression = quotationExpression |> LeafExpressionConverter.QuotationToExpression |> unbox<Expression<'a>>\\n\\n        let getGroup groupName = \\n            let matchExpression = <@Func<IGroup,bool>(fun (group:IGroup) -> group.DisplayName = groupName) @>\\n            let filter = toExpression<Func<IGroup,bool>> matchExpression\\n            let groups = client\\n                            .Groups\\n                            .Where(filter)\\n                            .ExecuteAsync()\\n                            .Result\\n                            .CurrentPage\\n                            |> List.ofSeq\\n            match groups with\\n            | [] -> None\\n            | x::[] -> Some (x :?> Group)\\n            | _ -> raise (Exception(\\\"more than one group exists\\\"))\\n\\n        let addGroup groupName = \\n            match getGroup groupName with\\n            | None ->\\n                let group = Group()\\n                group.DisplayName <- groupName\\n                group.Description <- groupName\\n                group.MailNickname <- groupName\\n                group.MailEnabled <- Nullable(false)\\n                group.SecurityEnabled <- Nullable(true)\\n                client.Groups.AddGroupAsync(group).Wait()\\n                Some group\\n            | Some x -> Some (x :?> Group)\\n\\n        let getUser userName = \\n            let matchExpression = <@Func<IUser,bool>(fun (user:IUser) -> user.DisplayName = userName) @>\\n            let filter = toExpression<Func<IUser,bool>> matchExpression\\n            let users = client\\n                            .Users\\n                            .Where(filter)\\n                            .ExecuteAsync()\\n                            .Result\\n                            .CurrentPage\\n                            |> List.ofSeq\\n            match users with\\n            | [] -> None\\n            | x::[] -> Some x\\n            | _ -> raise (Exception(\\\"more than one user exists with that name\\\"))\\n\\n        let addUser userName = \\n            match getUser userName with\\n            | None ->\\n                let passwordProfile() =\\n                    let passwd = PasswordProfile()\\n                    passwd.ForceChangePasswordNextLogin <- Nullable(true)\\n                    passwd.Password <- \\\"Ch@ng3NoW!\\\"\\n                    passwd\\n                let user = User()\\n                user.PasswordProfile <- passwordProfile()\\n                user.DisplayName <- userName\\n                user.UserPrincipalName <- userName + \\\"@fsharptest.onmicrosoft.com\\\"\\n                user.AccountEnabled <- Nullable(true)\\n                user.MailNickname <- userName\\n                client.Users.AddUserAsync(user).Wait()\\n                Some user\\n            | Some x -> Some (x :?> User)\\n\\n        let getMembers (group:Group) = \\n            let groupFetcher = (group :> IGroupFetcher)\\n            let members = groupFetcher.Members.ExecuteAsync().Result\\n            members.CurrentPage\\n\\n        let groupContainsUser (group:Group) (user:User) = \\n            group |> getMembers |> Seq.map (fun o -> (o :?> User).DisplayName) |> Seq.exists (fun s -> s = user.DisplayName)\\n\\n        let addUserToGroup (group:Group) (user:User) = \\n            match groupContainsUser (group:Group) (user:User) with\\n            | false ->\\n                group.Members.Add(user)\\n                group.UpdateAsync().Wait()\\n                group\\n            | true ->\\n                group\\n\\nThe client code is sort of straightforward. The first part, down to the `client` declaration is just about connecting to the API. The names of the functions after that explains them self. To do a query we need to create an expression which we do by creating a code quotation which we translate to a `Func`. The code here is not by any means what you should have in production, but it show you some part of the API and what you can do.\\n\\nThe last part is a small program that uses the module we just created:\\n\\n    open System\\n    open Microsoft.Azure.ActiveDirectory.GraphClient\\n    [<EntryPoint>]\\n    let main argv = \\n        try\\n            let group = addGroup \\\"newgroup\\\" |> Option.get\\n            let user = addUser \\\"charlie\\\" |> Option.get\\n            user |> addUserToGroup group |> ignore\\n            let group2 = getGroup \\\"newgroup\\\" |> Option.get\\n\\n            printfn \\\"Group name: %s\\\" group2.DisplayName\\n            let membersOfGroup = getMembers group\\n            let members = membersOfGroup |> Seq.map (fun o -> (o :?> User).DisplayName) |> String.concat \\\", \\\"\\n            printfn \\\"Members: %s\\\" members\\n            Console.ReadLine() |> ignore\\n        with\\n        | :? Exception as ex -> printfn \\\"%s\\\" ex.Message\\n        0 \\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I've been playing around with both Azure and F# lately and thought that I would share my knowledge about programming the Azure Active Directory using F#. It took some time to find some good example code, but after some searching I did find this project: <a href=\"https://github.com/AzureADSamples/ConsoleApp-GraphAPI-DotNet\">https://github.com/AzureADSamples/ConsoleApp-GraphAPI-DotNet</a>. The project shows most of the basic functionality, but I thought I would show it in F# and at the same time try to make it easier for others to understand. So let's started.</p>\n<h2 id=\"tldr\">TL;DR;</h2>\n<p>I've put the code on github, <a href=\"https://github.com/mastoj/ADTutorial\">https://github.com/mastoj/ADTutorial</a>, so you can download it easier. This will get you started quite easy and manipulating Active Directory in no time at all. When working with this short post/tutorial I had some issues with connecting to the AD's to start with, but the problem just went a way. My guess is that it might have taken some time from the creation of the Active Directory and when it was accessible by code, but that's just a guess.</p>\n<h2 id=\"creatingtheactivedirectoryinazure\">Creating the Active Directory in Azure</h2>\n<p>We will create a new directory that we can play with. So to create the directory choose &quot;CUSTOM CREATE&quot; that is available after following the onscreen menu in the picture below.<br>\n<img src=\"/content/images/2015/01/CreateAD.PNG\" alt=\"Create Active Directory\"></p>\n<p>Enter some information in the &quot;Add directory&quot; (sample data below).<br>\n<img src=\"/content/images/2015/01/AddDirectory.PNG\" alt=\"Add directory\"></p>\n<p>In the active directory list click the arrow for your new directory to get to the details of this directory.<br>\n<img src=\"/content/images/2015/01/ADList.PNG\" alt=\"Active directory list\"></p>\n<p>To connect to the Active Directory, which are going to do through a console application, we need to add an application to the Active Directory. So go to the &quot;Application&quot; tab and then click &quot;ADD&quot; at the bottom of the screen. In the dialogue choose &quot;Add an application my organization is developing&quot; and then &quot;WEB APPLICATION AND/OR WEB API&quot;, as &quot;REDIRECT URI&quot; and just choose a unique for &quot;APP ID URI&quot; use &quot;<a href=\"http://localhost\">http://localhost</a>&quot;.<br>\n<img src=\"/content/images/2015/01/AddApplication-1.PNG\" alt=\"Add application\"></p>\n<h2 id=\"collectingandconfiguringthedataneededtoconnect\">Collecting and configuring the data needed to connect</h2>\n<p>For the things we are going to do later there are some data we need to collect to be able to connect to the active directory. The things we need are:</p>\n<ul>\n<li>Client Id</li>\n<li>Client secret</li>\n<li>Tenant name</li>\n<li>Tenant id</li>\n</ul>\n<p>I had problem finding this information, so I'll try to make it as clear as I can. Before we locate those for things there is also some other configuration settings which is more static.</p>\n<ul>\n<li>Resource url (url to the resource we acces to configure AD): <a href=\"https://graph.windows.net\">https://graph.windows.net</a></li>\n<li>Authentication string: <a href=\"https://login.windows.net/\">https://login.windows.net/</a><Tenant name></li>\n</ul>\n<p>The last thing we need to do is configure the permissions of the application so we can access the active directory.</p>\n<h3 id=\"theclientid\">The client id</h3>\n<p>The client id is found under the &quot;CONFIGURE&quot; tab.<br>\n<img src=\"/content/images/2015/01/ClientId.PNG\" alt=\"Client id\"></p>\n<h3 id=\"theclientsecret\">The client secret</h3>\n<p>This wasn't as straightforward as the id, but with secret they often mean &quot;key&quot;. So on the &quot;CONFIGURE&quot; tab we need to generate a 1 or 2 year key that we can use. The key will only be visible after you save and only that time.<br>\n<img src=\"/content/images/2015/01/ClientSecret.PNG\" alt=\"Client secret\"></p>\n<h3 id=\"thetenantnameandid\">The tenant name and id</h3>\n<p>The name is the easiest part, if you haven't got any custom domain names this is the domain name you used for the &quot;Active Directory&quot;. So in this example it is &quot;fsharptest.onmicrosoft.com&quot; (no http before the name).</p>\n<p>The id is a little bit harder to find. There are probably many ways to find the id, one is to visit the url: <a href=\"https://login.windows.net/fsharptest.onmicrosoft.com/FederationMetadata/2007-06/FederationMetadata.xml\">https://login.windows.net/fsharptest.onmicrosoft.com/FederationMetadata/2007-06/FederationMetadata.xml</a>, but change my domain with yours. The tenant id is the guid in the &quot;entityId&quot; url on the first node in the document.</p>\n<p>Another way to find the tenant id is to expand the &quot;ENABLE USERS TO SIGN ON&quot; and there it is in the magic url:<br>\n<img src=\"/content/images/2015/01/TenantId.PNG\" alt=\"Tenant id\"></p>\n<h2 id=\"timetocode\">Time to code</h2>\n<p>Since I do prefer F# when I can the coding part will be done in F#, but you can easily translate the code to C# if you want to. I'll write everything in one file in a console app. To get started the easiest way is to use to nuget packages:</p>\n<ul>\n<li><code>Microsoft.IdentityModel.Clients.ActiveDirectory</code> (I used version 2.13.112191810)</li>\n<li><code>Microsoft.Azure.ActiveDirectory.GraphClient</code> (I used version 2.0.3)</li>\n</ul>\n<p>The first part of the files are the settings, I like to keep those in a separate module:</p>\n<pre><code>[&lt;AutoOpen&gt;]\nmodule AdSettings = \n    let clientId = &quot;the client id guid&quot;\n    let clientSecret = &quot;the key from azure&quot;\n    let tenantId = &quot;the tenant id&quot;\n    let tenantName = &quot;the domain&quot;\n    let resourceUrl = &quot;https://graph.windows.net&quot;\n    let authString = &quot;https://login.windows.net/&quot; + tenantName\n</code></pre>\n<p>Then we have the actual client code, the code is not intended for production use I'm just showing of the AD integration.</p>\n<pre><code>[&lt;AutoOpen&gt;]\nmodule AdClient = \n    open Microsoft.Azure.ActiveDirectory.GraphClient\n    open Microsoft.IdentityModel.Clients.ActiveDirectory\n    open System\n    open System.Linq.Expressions\n    open System.Threading.Tasks\n    open Microsoft.FSharp.Linq.RuntimeHelpers\n\n    let getAuthenticationToken (clientId:string) (clientSecret:string) tenantName = \n        let authenticationContext = AuthenticationContext(authString, false)\n        let credentials = ClientCredential(clientId, clientSecret)\n        let authenticationResult = authenticationContext.AcquireToken(resourceUrl, credentials)\n        authenticationResult.AccessToken\n\n    let activeDirectoryClient (tenantId:string) token = \n        let serviceRoot = Uri(resourceUrl + &quot;/&quot; + tenantId)\n        let tokenTask = Func&lt;Task&lt;string&gt;&gt;(fun() -&gt;Task.Factory.StartNew&lt;string&gt;(fun() -&gt; token))\n        let activeDirectoryClient = ActiveDirectoryClient(serviceRoot, tokenTask)\n        activeDirectoryClient\n\n    let client = getAuthenticationToken clientId clientSecret tenantName |&gt; activeDirectoryClient tenantId\n\n    let toExpression&lt;'a&gt; quotationExpression = quotationExpression |&gt; LeafExpressionConverter.QuotationToExpression |&gt; unbox&lt;Expression&lt;'a&gt;&gt;\n\n    let getGroup groupName = \n        let matchExpression = &lt;@Func&lt;IGroup,bool&gt;(fun (group:IGroup) -&gt; group.DisplayName = groupName) @&gt;\n        let filter = toExpression&lt;Func&lt;IGroup,bool&gt;&gt; matchExpression\n        let groups = client\n                        .Groups\n                        .Where(filter)\n                        .ExecuteAsync()\n                        .Result\n                        .CurrentPage\n                        |&gt; List.ofSeq\n        match groups with\n        | [] -&gt; None\n        | x::[] -&gt; Some (x :?&gt; Group)\n        | _ -&gt; raise (Exception(&quot;more than one group exists&quot;))\n\n    let addGroup groupName = \n        match getGroup groupName with\n        | None -&gt;\n            let group = Group()\n            group.DisplayName &lt;- groupName\n            group.Description &lt;- groupName\n            group.MailNickname &lt;- groupName\n            group.MailEnabled &lt;- Nullable(false)\n            group.SecurityEnabled &lt;- Nullable(true)\n            client.Groups.AddGroupAsync(group).Wait()\n            Some group\n        | Some x -&gt; Some (x :?&gt; Group)\n\n    let getUser userName = \n        let matchExpression = &lt;@Func&lt;IUser,bool&gt;(fun (user:IUser) -&gt; user.DisplayName = userName) @&gt;\n        let filter = toExpression&lt;Func&lt;IUser,bool&gt;&gt; matchExpression\n        let users = client\n                        .Users\n                        .Where(filter)\n                        .ExecuteAsync()\n                        .Result\n                        .CurrentPage\n                        |&gt; List.ofSeq\n        match users with\n        | [] -&gt; None\n        | x::[] -&gt; Some x\n        | _ -&gt; raise (Exception(&quot;more than one user exists with that name&quot;))\n\n    let addUser userName = \n        match getUser userName with\n        | None -&gt;\n            let passwordProfile() =\n                let passwd = PasswordProfile()\n                passwd.ForceChangePasswordNextLogin &lt;- Nullable(true)\n                passwd.Password &lt;- &quot;Ch@ng3NoW!&quot;\n                passwd\n            let user = User()\n            user.PasswordProfile &lt;- passwordProfile()\n            user.DisplayName &lt;- userName\n            user.UserPrincipalName &lt;- userName + &quot;@fsharptest.onmicrosoft.com&quot;\n            user.AccountEnabled &lt;- Nullable(true)\n            user.MailNickname &lt;- userName\n            client.Users.AddUserAsync(user).Wait()\n            Some user\n        | Some x -&gt; Some (x :?&gt; User)\n\n    let getMembers (group:Group) = \n        let groupFetcher = (group :&gt; IGroupFetcher)\n        let members = groupFetcher.Members.ExecuteAsync().Result\n        members.CurrentPage\n\n    let groupContainsUser (group:Group) (user:User) = \n        group |&gt; getMembers |&gt; Seq.map (fun o -&gt; (o :?&gt; User).DisplayName) |&gt; Seq.exists (fun s -&gt; s = user.DisplayName)\n\n    let addUserToGroup (group:Group) (user:User) = \n        match groupContainsUser (group:Group) (user:User) with\n        | false -&gt;\n            group.Members.Add(user)\n            group.UpdateAsync().Wait()\n            group\n        | true -&gt;\n            group\n</code></pre>\n<p>The client code is sort of straightforward. The first part, down to the <code>client</code> declaration is just about connecting to the API. The names of the functions after that explains them self. To do a query we need to create an expression which we do by creating a code quotation which we translate to a <code>Func</code>. The code here is not by any means what you should have in production, but it show you some part of the API and what you can do.</p>\n<p>The last part is a small program that uses the module we just created:</p>\n<pre><code>open System\nopen Microsoft.Azure.ActiveDirectory.GraphClient\n[&lt;EntryPoint&gt;]\nlet main argv = \n    try\n        let group = addGroup &quot;newgroup&quot; |&gt; Option.get\n        let user = addUser &quot;charlie&quot; |&gt; Option.get\n        user |&gt; addUserToGroup group |&gt; ignore\n        let group2 = getGroup &quot;newgroup&quot; |&gt; Option.get\n\n        printfn &quot;Group name: %s&quot; group2.DisplayName\n        let membersOfGroup = getMembers group\n        let members = membersOfGroup |&gt; Seq.map (fun o -&gt; (o :?&gt; User).DisplayName) |&gt; String.concat &quot;, &quot;\n        printfn &quot;Members: %s&quot; members\n        Console.ReadLine() |&gt; ignore\n    with\n    | :? Exception as ex -&gt; printfn &quot;%s&quot; ex.Message\n    0 \n</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"46","plaintext":"I've been playing around with both Azure and F# lately and thought that I would\nshare my knowledge about programming the Azure Active Directory using F#. It\ntook some time to find some good example code, but after some searching I did\nfind this project: https://github.com/AzureADSamples/ConsoleApp-GraphAPI-DotNet.\nThe project shows most of the basic functionality, but I thought I would show it\nin F# and at the same time try to make it easier for others to understand. So\nlet's started.\n\nTL;DR;\nI've put the code on github, https://github.com/mastoj/ADTutorial, so you can\ndownload it easier. This will get you started quite easy and manipulating Active\nDirectory in no time at all. When working with this short post/tutorial I had\nsome issues with connecting to the AD's to start with, but the problem just went\na way. My guess is that it might have taken some time from the creation of the\nActive Directory and when it was accessible by code, but that's just a guess.\n\nCreating the Active Directory in Azure\nWe will create a new directory that we can play with. So to create the directory\nchoose \"CUSTOM CREATE\" that is available after following the onscreen menu in\nthe picture below.\n\n\nEnter some information in the \"Add directory\" (sample data below).\n\n\nIn the active directory list click the arrow for your new directory to get to\nthe details of this directory.\n\n\nTo connect to the Active Directory, which are going to do through a console\napplication, we need to add an application to the Active Directory. So go to the\n\"Application\" tab and then click \"ADD\" at the bottom of the screen. In the\ndialogue choose \"Add an application my organization is developing\" and then \"WEB\nAPPLICATION AND/OR WEB API\", as \"REDIRECT URI\" and just choose a unique for \"APP\nID URI\" use \"http://localhost\".\n\n\nCollecting and configuring the data needed to connect\nFor the things we are going to do later there are some data we need to collect\nto be able to connect to the active directory. The things we need are:\n\n * Client Id\n * Client secret\n * Tenant name\n * Tenant id\n\nI had problem finding this information, so I'll try to make it as clear as I\ncan. Before we locate those for things there is also some other configuration\nsettings which is more static.\n\n * Resource url (url to the resource we acces to configure AD): \n   https://graph.windows.net\n * Authentication string: https://login.windows.net/\n\nThe last thing we need to do is configure the permissions of the application so\nwe can access the active directory.\n\nThe client id\nThe client id is found under the \"CONFIGURE\" tab.\n\n\nThe client secret\nThis wasn't as straightforward as the id, but with secret they often mean \"key\".\nSo on the \"CONFIGURE\" tab we need to generate a 1 or 2 year key that we can use.\nThe key will only be visible after you save and only that time.\n\n\nThe tenant name and id\nThe name is the easiest part, if you haven't got any custom domain names this is\nthe domain name you used for the \"Active Directory\". So in this example it is\n\"fsharptest.onmicrosoft.com\" (no http before the name).\n\nThe id is a little bit harder to find. There are probably many ways to find the\nid, one is to visit the url: \nhttps://login.windows.net/fsharptest.onmicrosoft.com/FederationMetadata/2007-06/FederationMetadata.xml\n, but change my domain with yours. The tenant id is the guid in the \"entityId\"\nurl on the first node in the document.\n\nAnother way to find the tenant id is to expand the \"ENABLE USERS TO SIGN ON\" and\nthere it is in the magic url:\n\n\nTime to code\nSince I do prefer F# when I can the coding part will be done in F#, but you can\neasily translate the code to C# if you want to. I'll write everything in one\nfile in a console app. To get started the easiest way is to use to nuget\npackages:\n\n * Microsoft.IdentityModel.Clients.ActiveDirectory (I used version\n   2.13.112191810)\n * Microsoft.Azure.ActiveDirectory.GraphClient (I used version 2.0.3)\n\nThe first part of the files are the settings, I like to keep those in a separate\nmodule:\n\n[<AutoOpen>]\nmodule AdSettings = \n    let clientId = \"the client id guid\"\n    let clientSecret = \"the key from azure\"\n    let tenantId = \"the tenant id\"\n    let tenantName = \"the domain\"\n    let resourceUrl = \"https://graph.windows.net\"\n    let authString = \"https://login.windows.net/\" + tenantName\n\n\nThen we have the actual client code, the code is not intended for production use\nI'm just showing of the AD integration.\n\n[<AutoOpen>]\nmodule AdClient = \n    open Microsoft.Azure.ActiveDirectory.GraphClient\n    open Microsoft.IdentityModel.Clients.ActiveDirectory\n    open System\n    open System.Linq.Expressions\n    open System.Threading.Tasks\n    open Microsoft.FSharp.Linq.RuntimeHelpers\n\n    let getAuthenticationToken (clientId:string) (clientSecret:string) tenantName = \n        let authenticationContext = AuthenticationContext(authString, false)\n        let credentials = ClientCredential(clientId, clientSecret)\n        let authenticationResult = authenticationContext.AcquireToken(resourceUrl, credentials)\n        authenticationResult.AccessToken\n\n    let activeDirectoryClient (tenantId:string) token = \n        let serviceRoot = Uri(resourceUrl + \"/\" + tenantId)\n        let tokenTask = Func<Task<string>>(fun() ->Task.Factory.StartNew<string>(fun() -> token))\n        let activeDirectoryClient = ActiveDirectoryClient(serviceRoot, tokenTask)\n        activeDirectoryClient\n\n    let client = getAuthenticationToken clientId clientSecret tenantName |> activeDirectoryClient tenantId\n\n    let toExpression<'a> quotationExpression = quotationExpression |> LeafExpressionConverter.QuotationToExpression |> unbox<Expression<'a>>\n\n    let getGroup groupName = \n        let matchExpression = <@Func<IGroup,bool>(fun (group:IGroup) -> group.DisplayName = groupName) @>\n        let filter = toExpression<Func<IGroup,bool>> matchExpression\n        let groups = client\n                        .Groups\n                        .Where(filter)\n                        .ExecuteAsync()\n                        .Result\n                        .CurrentPage\n                        |> List.ofSeq\n        match groups with\n        | [] -> None\n        | x::[] -> Some (x :?> Group)\n        | _ -> raise (Exception(\"more than one group exists\"))\n\n    let addGroup groupName = \n        match getGroup groupName with\n        | None ->\n            let group = Group()\n            group.DisplayName <- groupName\n            group.Description <- groupName\n            group.MailNickname <- groupName\n            group.MailEnabled <- Nullable(false)\n            group.SecurityEnabled <- Nullable(true)\n            client.Groups.AddGroupAsync(group).Wait()\n            Some group\n        | Some x -> Some (x :?> Group)\n\n    let getUser userName = \n        let matchExpression = <@Func<IUser,bool>(fun (user:IUser) -> user.DisplayName = userName) @>\n        let filter = toExpression<Func<IUser,bool>> matchExpression\n        let users = client\n                        .Users\n                        .Where(filter)\n                        .ExecuteAsync()\n                        .Result\n                        .CurrentPage\n                        |> List.ofSeq\n        match users with\n        | [] -> None\n        | x::[] -> Some x\n        | _ -> raise (Exception(\"more than one user exists with that name\"))\n\n    let addUser userName = \n        match getUser userName with\n        | None ->\n            let passwordProfile() =\n                let passwd = PasswordProfile()\n                passwd.ForceChangePasswordNextLogin <- Nullable(true)\n                passwd.Password <- \"Ch@ng3NoW!\"\n                passwd\n            let user = User()\n            user.PasswordProfile <- passwordProfile()\n            user.DisplayName <- userName\n            user.UserPrincipalName <- userName + \"@fsharptest.onmicrosoft.com\"\n            user.AccountEnabled <- Nullable(true)\n            user.MailNickname <- userName\n            client.Users.AddUserAsync(user).Wait()\n            Some user\n        | Some x -> Some (x :?> User)\n\n    let getMembers (group:Group) = \n        let groupFetcher = (group :> IGroupFetcher)\n        let members = groupFetcher.Members.ExecuteAsync().Result\n        members.CurrentPage\n\n    let groupContainsUser (group:Group) (user:User) = \n        group |> getMembers |> Seq.map (fun o -> (o :?> User).DisplayName) |> Seq.exists (fun s -> s = user.DisplayName)\n\n    let addUserToGroup (group:Group) (user:User) = \n        match groupContainsUser (group:Group) (user:User) with\n        | false ->\n            group.Members.Add(user)\n            group.UpdateAsync().Wait()\n            group\n        | true ->\n            group\n\n\nThe client code is sort of straightforward. The first part, down to the client \ndeclaration is just about connecting to the API. The names of the functions\nafter that explains them self. To do a query we need to create an expression\nwhich we do by creating a code quotation which we translate to a Func. The code\nhere is not by any means what you should have in production, but it show you\nsome part of the API and what you can do.\n\nThe last part is a small program that uses the module we just created:\n\nopen System\nopen Microsoft.Azure.ActiveDirectory.GraphClient\n[<EntryPoint>]\nlet main argv = \n    try\n        let group = addGroup \"newgroup\" |> Option.get\n        let user = addUser \"charlie\" |> Option.get\n        user |> addUserToGroup group |> ignore\n        let group2 = getGroup \"newgroup\" |> Option.get\n\n        printfn \"Group name: %s\" group2.DisplayName\n        let membersOfGroup = getMembers group\n        let members = membersOfGroup |> Seq.map (fun o -> (o :?> User).DisplayName) |> String.concat \", \"\n        printfn \"Members: %s\" members\n        Console.ReadLine() |> ignore\n    with\n    | :? Exception as ex -> printfn \"%s\" ex.Message\n    0","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-01-13T19:25:17.000Z","updated_at":"2015-01-16T18:47:07.000Z","published_at":"2015-01-16T18:47:07.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe062","uuid":"8c0b842d-bd04-4e84-a3ec-16ea2171c090","title":"ASP.NET 5: The pipeline","slug":"asp-net-5-the-pipeline","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## Introduction\\n\\nASP.NET 5 has taken a lot of inspiration from other web framework out there like ruby, nodejs and and the [Open Web Interface for .NET](http://owin.org/), OWIN. Microsoft made an implementation of the OWIN specification in the project called [Katana](http://katanaproject.codeplex.com/documentation) as an experiment. With experience gained from the Katana project Microsoft, and the ASP.NET team, began implementing ASP.NET 5. There are \\\"adapters\\\" available to plugin OWIN components in the pipeline for ASP.NET 5, even though there are some major differences between ASP.NET 5 and OWIN.\\n\\n## What is this pipeline?\\n\\nI'm not sure if pipeline is the \\\"official\\\" way to describe it but that's a mental model that I can relate to. \\n\\n{<1>}![The pipeline](/content/images/2015/04/Pipeline.png)\\n\\nThe way it works is that the browser makes a request to the server, the server forwards the request to the host listen to that port. The hosting environment could be a IIS on Windows or self host environemnt on Linux or Windows. The hosting environment start to execute the application pipeline starting with the first middleware. A piece of middleware can do two things as I see it:\\n\\n* Respond directly and not execute the rest of the pipeline (it will still execute the middleware that's before it in the pipeline)\\n* Manipulate the response in one way or another, that could be authentication or add some extra data to a response\\n\\nUsually application frameworks like ASP.NET MVC falls under the first category and frameworks that are cross cutting like authentication falls under the second one. This might be a simplified view of it but that's how I see it.\\n\\n## The simplest possible thing\\n\\nTo understand The first thing we are going to do is create a simple \\\"Hello world\\\" application that we kan use directly from Visual Studio and the console. I won't cover cross platform development here, but the application will be able to run on other platforms. Also, I'm using a pre-release version of Visual Studio 2015 and ASP.NET 5 so things might change, but it is quite stable now I think.\\n\\nThe goal of the application we are developing is a simple blog engine for a single person, it doesn't make much sense but it will make it possible to show of the features I want to demonstrate. \\n\\n## Creating the application\\n\\nOpen visual studio and choose to create a new \\\"ASP.NET Web Application\\\", I called my application \\\"OneManBlog\\\". In the next step choose \\\"ASP.NET 5 Preview Empty\\\", this will most likely change name to something else later on. The main point is that it should be empty and it is targeting the new framework.\\n\\n## Hello world!\\n\\nTo get started we open up the \\\"Startup.cs\\\" file and change the content to:\\n\\n    public class Startup\\n    {\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n        }\\n\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello world!\\\");\\n            });\\n        }\\n    }\\n    \\nThe code above is all you need to get your \\\"Hello world!\\\" message. You also got the pipeline started. If we want to add another step to our pipeline we just copy the `app.Use` code like such:\\n\\n    public class Startup\\n    {\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n        }\\n\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello world!\\\");\\n                await next();\\n            });\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"This is sort of an echo Hello world!\\\");\\n            });\\n        }\\n    }\\n\\nAlso notice that I updated the first function call to make a call to `next`, otherwise the next step in the pipeline wouldn't execute.\\n\\nThat is all you need to actually get started writing web applications in ASP.NET 5. If you look through the `context` variable in the `Func` that's passed to the `Use` method you see that you basically have access to the whole request and response and can start play with that. If you want to play around with this piece of code you can try to update it to only respond to a specific url, and/or a special header is in the request.\\n\\n## Summary\\n\\nThis was the first post in how to get started with ASP.NET 5. The goal was to show you the simplest building block available for ASP.NET 5. In the next post I'll add MVC to the application since it will be difficult to manage and application if all you do is writing `Func` expressions as those above, and adding an abstraction like MVC will help you.\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>ASP.NET 5 has taken a lot of inspiration from other web framework out there like ruby, nodejs and and the <a href=\"http://owin.org/\">Open Web Interface for .NET</a>, OWIN. Microsoft made an implementation of the OWIN specification in the project called <a href=\"http://katanaproject.codeplex.com/documentation\">Katana</a> as an experiment. With experience gained from the Katana project Microsoft, and the ASP.NET team, began implementing ASP.NET 5. There are &quot;adapters&quot; available to plugin OWIN components in the pipeline for ASP.NET 5, even though there are some major differences between ASP.NET 5 and OWIN.</p>\n<h2 id=\"whatisthispipeline\">What is this pipeline?</h2>\n<p>I'm not sure if pipeline is the &quot;official&quot; way to describe it but that's a mental model that I can relate to.</p>\n<p>{&lt;1&gt;}<img src=\"/content/images/2015/04/Pipeline.png\" alt=\"The pipeline\"></p>\n<p>The way it works is that the browser makes a request to the server, the server forwards the request to the host listen to that port. The hosting environment could be a IIS on Windows or self host environemnt on Linux or Windows. The hosting environment start to execute the application pipeline starting with the first middleware. A piece of middleware can do two things as I see it:</p>\n<ul>\n<li>Respond directly and not execute the rest of the pipeline (it will still execute the middleware that's before it in the pipeline)</li>\n<li>Manipulate the response in one way or another, that could be authentication or add some extra data to a response</li>\n</ul>\n<p>Usually application frameworks like ASP.NET MVC falls under the first category and frameworks that are cross cutting like authentication falls under the second one. This might be a simplified view of it but that's how I see it.</p>\n<h2 id=\"thesimplestpossiblething\">The simplest possible thing</h2>\n<p>To understand The first thing we are going to do is create a simple &quot;Hello world&quot; application that we kan use directly from Visual Studio and the console. I won't cover cross platform development here, but the application will be able to run on other platforms. Also, I'm using a pre-release version of Visual Studio 2015 and ASP.NET 5 so things might change, but it is quite stable now I think.</p>\n<p>The goal of the application we are developing is a simple blog engine for a single person, it doesn't make much sense but it will make it possible to show of the features I want to demonstrate.</p>\n<h2 id=\"creatingtheapplication\">Creating the application</h2>\n<p>Open visual studio and choose to create a new &quot;ASP.NET Web Application&quot;, I called my application &quot;OneManBlog&quot;. In the next step choose &quot;ASP.NET 5 Preview Empty&quot;, this will most likely change name to something else later on. The main point is that it should be empty and it is targeting the new framework.</p>\n<h2 id=\"helloworld\">Hello world!</h2>\n<p>To get started we open up the &quot;Startup.cs&quot; file and change the content to:</p>\n<pre><code>public class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello world!&quot;);\n        });\n    }\n}\n</code></pre>\n<p>The code above is all you need to get your &quot;Hello world!&quot; message. You also got the pipeline started. If we want to add another step to our pipeline we just copy the <code>app.Use</code> code like such:</p>\n<pre><code>public class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello world!&quot;);\n            await next();\n        });\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;This is sort of an echo Hello world!&quot;);\n        });\n    }\n}\n</code></pre>\n<p>Also notice that I updated the first function call to make a call to <code>next</code>, otherwise the next step in the pipeline wouldn't execute.</p>\n<p>That is all you need to actually get started writing web applications in ASP.NET 5. If you look through the <code>context</code> variable in the <code>Func</code> that's passed to the <code>Use</code> method you see that you basically have access to the whole request and response and can start play with that. If you want to play around with this piece of code you can try to update it to only respond to a specific url, and/or a special header is in the request.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This was the first post in how to get started with ASP.NET 5. The goal was to show you the simplest building block available for ASP.NET 5. In the next post I'll add MVC to the application since it will be difficult to manage and application if all you do is writing <code>Func</code> expressions as those above, and adding an abstraction like MVC will help you.</p>\n<!--kg-card-end: markdown-->","comment_id":"47","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nIntroduction\nASP.NET 5 has taken a lot of inspiration from other web framework out there like\nruby, nodejs and and the Open Web Interface for .NET [http://owin.org/], OWIN.\nMicrosoft made an implementation of the OWIN specification in the project called \nKatana [http://katanaproject.codeplex.com/documentation] as an experiment. With\nexperience gained from the Katana project Microsoft, and the ASP.NET team, began\nimplementing ASP.NET 5. There are \"adapters\" available to plugin OWIN components\nin the pipeline for ASP.NET 5, even though there are some major differences\nbetween ASP.NET 5 and OWIN.\n\nWhat is this pipeline?\nI'm not sure if pipeline is the \"official\" way to describe it but that's a\nmental model that I can relate to.\n\n{<1>}\n\nThe way it works is that the browser makes a request to the server, the server\nforwards the request to the host listen to that port. The hosting environment\ncould be a IIS on Windows or self host environemnt on Linux or Windows. The\nhosting environment start to execute the application pipeline starting with the\nfirst middleware. A piece of middleware can do two things as I see it:\n\n * Respond directly and not execute the rest of the pipeline (it will still\n   execute the middleware that's before it in the pipeline)\n * Manipulate the response in one way or another, that could be authentication\n   or add some extra data to a response\n\nUsually application frameworks like ASP.NET MVC falls under the first category\nand frameworks that are cross cutting like authentication falls under the second\none. This might be a simplified view of it but that's how I see it.\n\nThe simplest possible thing\nTo understand The first thing we are going to do is create a simple \"Hello\nworld\" application that we kan use directly from Visual Studio and the console.\nI won't cover cross platform development here, but the application will be able\nto run on other platforms. Also, I'm using a pre-release version of Visual\nStudio 2015 and ASP.NET 5 so things might change, but it is quite stable now I\nthink.\n\nThe goal of the application we are developing is a simple blog engine for a\nsingle person, it doesn't make much sense but it will make it possible to show\nof the features I want to demonstrate.\n\nCreating the application\nOpen visual studio and choose to create a new \"ASP.NET Web Application\", I\ncalled my application \"OneManBlog\". In the next step choose \"ASP.NET 5 Preview\nEmpty\", this will most likely change name to something else later on. The main\npoint is that it should be empty and it is targeting the new framework.\n\nHello world!\nTo get started we open up the \"Startup.cs\" file and change the content to:\n\npublic class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"Hello world!\");\n        });\n    }\n}\n\n\nThe code above is all you need to get your \"Hello world!\" message. You also got\nthe pipeline started. If we want to add another step to our pipeline we just\ncopy the app.Use code like such:\n\npublic class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"Hello world!\");\n            await next();\n        });\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"This is sort of an echo Hello world!\");\n        });\n    }\n}\n\n\nAlso notice that I updated the first function call to make a call to next,\notherwise the next step in the pipeline wouldn't execute.\n\nThat is all you need to actually get started writing web applications in ASP.NET\n5. If you look through the context variable in the Func that's passed to the Use \nmethod you see that you basically have access to the whole request and response\nand can start play with that. If you want to play around with this piece of code\nyou can try to update it to only respond to a specific url, and/or a special\nheader is in the request.\n\nSummary\nThis was the first post in how to get started with ASP.NET 5. The goal was to\nshow you the simplest building block available for ASP.NET 5. In the next post\nI'll add MVC to the application since it will be difficult to manage and\napplication if all you do is writing Func expressions as those above, and adding\nan abstraction like MVC will help you.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-12T16:29:35.000Z","updated_at":"2016-01-15T22:22:11.000Z","published_at":"2015-04-21T11:46:07.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe063","uuid":"bd8e47c4-85d0-416d-b7c2-d0c6ef1c3563","title":"ASP.NET 5: Adding MVC to an application","slug":"asp-net-5-adding-mvc-to-an-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## Introducing the project.json file\\n\\nThe \\\"project.json\\\" file is basically your new csproj-file. There you store all the references for your project as well as other project specific features. You will have intellisense \\\"towards\\\" nuget while editing dependencies, which make it almost easier to edit this file instead of using the UI. I won't cover what every little detail is in the file, only what needed to finish the next step. So to add MVC to the project just modify the depdencies property to look like this:\\n\\n    \\\"dependencies\\\": {\\n        \\\"Microsoft.AspNet.Server.IIS\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Mvc\\\": \\\"6.0.0-beta4\\\"\\n    },\\n\\nThat's version of MVC that's available while writing this post.\\n\\n## Configure the application (updating the pipeline)\\n\\nTo start using the MVC \\\"middleware\\\" we need to add it to the pipeline. If a route matches one that MVC will handle that will end the pipeline, otherwise the request will just pass right through. Most middleware that Microsoft implement and other framework most likely will implement extension methods for the `IApplicationBuilder` interface to make it easier to add the middleware. So to add MVC to the pipeline the `Configure` method must be updated like so:\\n\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            app.UseMvc(rb =>\\n            {\\n                rb.MapRoute(\\n                    name: \\\"default\\\",\\n                    template: \\\"{controller}/{action}/{id?}\\\",\\n                    defaults: new {controller = \\\"Home\\\", action = \\\"Index\\\"});\\n            });\\n\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello world!\\\");\\n            });\\n        }\\n\\nAll we're doing in the `Action` in the `UseMvc` method is setting up the routes as we would with an old MVC app, nothing has changed there.\\n\\n## Adding Controller and View\\n\\nThis step is basically exactly the same as with the old Mvc, so I won't cover it in detail. Just add the `HomeController`:\\n\\n    public class HomeController : Controller\\n    {\\n        public IActionResult Home()\\n        {\\n            return View();\\n        }\\n    }\\n\\nand a `Index` view:\\n\\n    @{\\n        Layout = null;\\n    }\\n\\n    <!DOCTYPE html>\\n\\n    <html>\\n    <head>\\n        <title>Hello</title>\\n    </head>\\n    <body>\\n        <div>\\n            <h1>Hello from MVC</h1>\\n        </div>\\n    </body>\\n    </html>\\n\\nIf you try to run the application now you'll get an error like:\\n\\n    Unable to find the required services. Please add all the required services by calling 'IServiceCollection.AddMvc()' inside the call to 'IApplicationBuilder.UseServices(...)' or 'IApplicationBuilder.UseMvc(...)' in the application startup code.\\n\\nWhat that means is that we have added MVC to the pipeline, but we haven't set up all internals of the middleware. And that is the next step.\\n\\n## Configure services\\n\\nTo \\\"hook\\\" anything in the dependency resolving in ASP.NET 5 you need to add what you're trying to resolve to a `IServiceCollection`, and you do that in the `ConfigureServices` method in the \\\"Startup.cs\\\" file. Again we have extension methods, which I guess will be provided by most middleware creators, for the `IServiceCollection` interface. So to configure MVC, all you need to do is:\\n\\n    public class Startup\\n    {\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n            services.AddMvc();\\n        }\\n\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            app.UseMvc(rb =>\\n            {\\n                rb.MapRoute(\\n                    name: \\\"default\\\",\\n                    template: \\\"{controller}/{action}/{id?}\\\",\\n                    defaults: new {controller = \\\"Home\\\", action = \\\"Index\\\"});\\n            });\\n\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello world!\\\");\\n            });\\n        }\\n    }\\n\\n## Attribute routing\\n\\nNow we have a working MVC application, but I thought we would make use of attribute routing which is new for MVC but has been available for Web API some time. You can mix standard routing and attribute routing if you want, but I like to use just attribute routing. So let's first remove the old routing:\\n\\n    public class Startup\\n    {\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n            services.AddMvc();\\n        }\\n\\n        public void Configure(IApplicationBuilder app)\\n        {\\n            app.UseMvc();\\n\\n            app.Use(async (context, next) =>\\n            {\\n                await context.Response.WriteAsync(\\\"Hello world!\\\");\\n            });\\n        }\\n    }\\n\\nNow nothing will work, so let's fix that by adding attribute routing to the `HomeController`:\\n\\n    [Route(\\\"[controller]\\\"), Route(\\\"/\\\")]\\n    public class HomeController : Controller\\n    {\\n        [Route(\\\"[action]\\\"), Route(\\\"\\\")]\\n        public IActionResult Index()\\n        {\\n            return View();\\n        }\\n    }\\n\\nI've basically added the same routing here but as attributes instead. For the controller I've added two routes, the default `\\\"/\\\"` and also one that matches the name of the controller `\\\"[controller]\\\"`. I did the same thing for the action, but with the action instead. If you start the application now and go to \\\"/\\\", \\\"/Home\\\" or \\\"/Home/Index\\\" the `Index` action will be called.\\n\\n## Summary\\n\\nThis was the end of post two in this serie of posts. In the next step I'll add some simple functionality and try to style it using bootstrap. To be able to use bootstrap we must set up our client side build steps, and for that [Gulp](http://gulpjs.com/) will be used.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"introducingtheprojectjsonfile\">Introducing the project.json file</h2>\n<p>The &quot;project.json&quot; file is basically your new csproj-file. There you store all the references for your project as well as other project specific features. You will have intellisense &quot;towards&quot; nuget while editing dependencies, which make it almost easier to edit this file instead of using the UI. I won't cover what every little detail is in the file, only what needed to finish the next step. So to add MVC to the project just modify the depdencies property to look like this:</p>\n<pre><code>&quot;dependencies&quot;: {\n    &quot;Microsoft.AspNet.Server.IIS&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Mvc&quot;: &quot;6.0.0-beta4&quot;\n},\n</code></pre>\n<p>That's version of MVC that's available while writing this post.</p>\n<h2 id=\"configuretheapplicationupdatingthepipeline\">Configure the application (updating the pipeline)</h2>\n<p>To start using the MVC &quot;middleware&quot; we need to add it to the pipeline. If a route matches one that MVC will handle that will end the pipeline, otherwise the request will just pass right through. Most middleware that Microsoft implement and other framework most likely will implement extension methods for the <code>IApplicationBuilder</code> interface to make it easier to add the middleware. So to add MVC to the pipeline the <code>Configure</code> method must be updated like so:</p>\n<pre><code>    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc(rb =&gt;\n        {\n            rb.MapRoute(\n                name: &quot;default&quot;,\n                template: &quot;{controller}/{action}/{id?}&quot;,\n                defaults: new {controller = &quot;Home&quot;, action = &quot;Index&quot;});\n        });\n\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello world!&quot;);\n        });\n    }\n</code></pre>\n<p>All we're doing in the <code>Action</code> in the <code>UseMvc</code> method is setting up the routes as we would with an old MVC app, nothing has changed there.</p>\n<h2 id=\"addingcontrollerandview\">Adding Controller and View</h2>\n<p>This step is basically exactly the same as with the old Mvc, so I won't cover it in detail. Just add the <code>HomeController</code>:</p>\n<pre><code>public class HomeController : Controller\n{\n    public IActionResult Home()\n    {\n        return View();\n    }\n}\n</code></pre>\n<p>and a <code>Index</code> view:</p>\n<pre><code>@{\n    Layout = null;\n}\n\n&lt;!DOCTYPE html&gt;\n\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Hello&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div&gt;\n        &lt;h1&gt;Hello from MVC&lt;/h1&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>If you try to run the application now you'll get an error like:</p>\n<pre><code>Unable to find the required services. Please add all the required services by calling 'IServiceCollection.AddMvc()' inside the call to 'IApplicationBuilder.UseServices(...)' or 'IApplicationBuilder.UseMvc(...)' in the application startup code.\n</code></pre>\n<p>What that means is that we have added MVC to the pipeline, but we haven't set up all internals of the middleware. And that is the next step.</p>\n<h2 id=\"configureservices\">Configure services</h2>\n<p>To &quot;hook&quot; anything in the dependency resolving in ASP.NET 5 you need to add what you're trying to resolve to a <code>IServiceCollection</code>, and you do that in the <code>ConfigureServices</code> method in the &quot;Startup.cs&quot; file. Again we have extension methods, which I guess will be provided by most middleware creators, for the <code>IServiceCollection</code> interface. So to configure MVC, all you need to do is:</p>\n<pre><code>public class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc(rb =&gt;\n        {\n            rb.MapRoute(\n                name: &quot;default&quot;,\n                template: &quot;{controller}/{action}/{id?}&quot;,\n                defaults: new {controller = &quot;Home&quot;, action = &quot;Index&quot;});\n        });\n\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello world!&quot;);\n        });\n    }\n}\n</code></pre>\n<h2 id=\"attributerouting\">Attribute routing</h2>\n<p>Now we have a working MVC application, but I thought we would make use of attribute routing which is new for MVC but has been available for Web API some time. You can mix standard routing and attribute routing if you want, but I like to use just attribute routing. So let's first remove the old routing:</p>\n<pre><code>public class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc();\n\n        app.Use(async (context, next) =&gt;\n        {\n            await context.Response.WriteAsync(&quot;Hello world!&quot;);\n        });\n    }\n}\n</code></pre>\n<p>Now nothing will work, so let's fix that by adding attribute routing to the <code>HomeController</code>:</p>\n<pre><code>[Route(&quot;[controller]&quot;), Route(&quot;/&quot;)]\npublic class HomeController : Controller\n{\n    [Route(&quot;[action]&quot;), Route(&quot;&quot;)]\n    public IActionResult Index()\n    {\n        return View();\n    }\n}\n</code></pre>\n<p>I've basically added the same routing here but as attributes instead. For the controller I've added two routes, the default <code>&quot;/&quot;</code> and also one that matches the name of the controller <code>&quot;[controller]&quot;</code>. I did the same thing for the action, but with the action instead. If you start the application now and go to &quot;/&quot;, &quot;/Home&quot; or &quot;/Home/Index&quot; the <code>Index</code> action will be called.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This was the end of post two in this serie of posts. In the next step I'll add some simple functionality and try to style it using bootstrap. To be able to use bootstrap we must set up our client side build steps, and for that <a href=\"http://gulpjs.com/\">Gulp</a> will be used.</p>\n<!--kg-card-end: markdown-->","comment_id":"48","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nIntroducing the project.json file\nThe \"project.json\" file is basically your new csproj-file. There you store all\nthe references for your project as well as other project specific features. You\nwill have intellisense \"towards\" nuget while editing dependencies, which make it\nalmost easier to edit this file instead of using the UI. I won't cover what\nevery little detail is in the file, only what needed to finish the next step. So\nto add MVC to the project just modify the depdencies property to look like this:\n\n\"dependencies\": {\n    \"Microsoft.AspNet.Server.IIS\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.Mvc\": \"6.0.0-beta4\"\n},\n\n\nThat's version of MVC that's available while writing this post.\n\nConfigure the application (updating the pipeline)\nTo start using the MVC \"middleware\" we need to add it to the pipeline. If a\nroute matches one that MVC will handle that will end the pipeline, otherwise the\nrequest will just pass right through. Most middleware that Microsoft implement\nand other framework most likely will implement extension methods for the \nIApplicationBuilder interface to make it easier to add the middleware. So to add\nMVC to the pipeline the Configure method must be updated like so:\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc(rb =>\n        {\n            rb.MapRoute(\n                name: \"default\",\n                template: \"{controller}/{action}/{id?}\",\n                defaults: new {controller = \"Home\", action = \"Index\"});\n        });\n\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"Hello world!\");\n        });\n    }\n\n\nAll we're doing in the Action in the UseMvc method is setting up the routes as\nwe would with an old MVC app, nothing has changed there.\n\nAdding Controller and View\nThis step is basically exactly the same as with the old Mvc, so I won't cover it\nin detail. Just add the HomeController:\n\npublic class HomeController : Controller\n{\n    public IActionResult Home()\n    {\n        return View();\n    }\n}\n\n\nand a Index view:\n\n@{\n    Layout = null;\n}\n\n<!DOCTYPE html>\n\n<html>\n<head>\n    <title>Hello</title>\n</head>\n<body>\n    <div>\n        <h1>Hello from MVC</h1>\n    </div>\n</body>\n</html>\n\n\nIf you try to run the application now you'll get an error like:\n\nUnable to find the required services. Please add all the required services by calling 'IServiceCollection.AddMvc()' inside the call to 'IApplicationBuilder.UseServices(...)' or 'IApplicationBuilder.UseMvc(...)' in the application startup code.\n\n\nWhat that means is that we have added MVC to the pipeline, but we haven't set up\nall internals of the middleware. And that is the next step.\n\nConfigure services\nTo \"hook\" anything in the dependency resolving in ASP.NET 5 you need to add what\nyou're trying to resolve to a IServiceCollection, and you do that in the \nConfigureServices method in the \"Startup.cs\" file. Again we have extension\nmethods, which I guess will be provided by most middleware creators, for the \nIServiceCollection interface. So to configure MVC, all you need to do is:\n\npublic class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc(rb =>\n        {\n            rb.MapRoute(\n                name: \"default\",\n                template: \"{controller}/{action}/{id?}\",\n                defaults: new {controller = \"Home\", action = \"Index\"});\n        });\n\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"Hello world!\");\n        });\n    }\n}\n\n\nAttribute routing\nNow we have a working MVC application, but I thought we would make use of\nattribute routing which is new for MVC but has been available for Web API some\ntime. You can mix standard routing and attribute routing if you want, but I like\nto use just attribute routing. So let's first remove the old routing:\n\npublic class Startup\n{\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n    }\n\n    public void Configure(IApplicationBuilder app)\n    {\n        app.UseMvc();\n\n        app.Use(async (context, next) =>\n        {\n            await context.Response.WriteAsync(\"Hello world!\");\n        });\n    }\n}\n\n\nNow nothing will work, so let's fix that by adding attribute routing to the \nHomeController:\n\n[Route(\"[controller]\"), Route(\"/\")]\npublic class HomeController : Controller\n{\n    [Route(\"[action]\"), Route(\"\")]\n    public IActionResult Index()\n    {\n        return View();\n    }\n}\n\n\nI've basically added the same routing here but as attributes instead. For the\ncontroller I've added two routes, the default \"/\" and also one that matches the\nname of the controller \"[controller]\". I did the same thing for the action, but\nwith the action instead. If you start the application now and go to \"/\", \"/Home\"\nor \"/Home/Index\" the Index action will be called.\n\nSummary\nThis was the end of post two in this serie of posts. In the next step I'll add\nsome simple functionality and try to style it using bootstrap. To be able to use\nbootstrap we must set up our client side build steps, and for that Gulp\n[http://gulpjs.com/] will be used.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-12T19:38:33.000Z","updated_at":"2015-05-18T18:50:15.000Z","published_at":"2015-04-26T14:58:46.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe064","uuid":"d02ba63b-c517-4e59-9e0c-b917513ef47c","title":"ASP.NET 5: Setting up frontend build (with grunt)","slug":"asp-net-5-setting-up-frontend-build-with-grunt","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## Adding some functionality\\n\\nBefore we go on to the main purpose of this post, let's add some simple functionality to the application we're building.\\n\\n### Add an in-memory database\\n\\nThe goal of the application is a simple one man blog, so we need some kind of data storage. I settled on this simple model:\\n\\n    public class PostModel\\n    {\\n        public string Slug { get; set; }\\n        public string Content { get; set; }\\n    }\\n\\nand with that I created this in-memory database:\\n\\n    public class Data\\n    {\\n        private Dictionary<string, PostModel> _posts = new Dictionary<string, PostModel>();\\n\\n        public void Add(PostModel post)\\n        {\\n            _posts.Add(post.Slug, post);\\n        }\\n\\n        public PostModel Get(string id)\\n        {\\n            return _posts[id];\\n        }\\n\\n        public IEnumerable<PostModel> GetPosts()\\n        {\\n            return _posts.Values;\\n        }\\n    }\\n\\n### Updating the Home/Index view\\n\\nThe `Index` view of the `Home` controller will be where you enter the blog posts. After that you'll be redirected to the post. The updated view looks like:\\n\\n    <html>\\n    <head>\\n        <title>\\n            Welcome to you!\\n        </title>\\n        <link href=\\\"/lib/bootstrap/css/bootstrap.css\\\" rel=\\\"stylesheet\\\" />\\n        <link href=\\\"/css/site.css\\\" rel=\\\"stylesheet\\\" />\\n    </head>\\n    <body>\\n        <div class=\\\"container\\\">\\n            <h1>This is my new blog</h1>\\n            Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\\n\\n            @using (Html.BeginForm(\\\"Create\\\", \\\"Post\\\", FormMethod.Post))\\n            {\\n                <div class=\\\"form-group\\\">\\n                    <label for=\\\"slug\\\">Slug</label>\\n                    <input type=\\\"text\\\" name=\\\"slug\\\" id=\\\"name\\\" value=\\\"\\\" class=\\\"form-control\\\" />\\n                </div>\\n                <div class=\\\"form-group\\\">\\n                    <label for=\\\"content\\\">Content</label>\\n                    <textarea id=\\\"content\\\" name=\\\"content\\\" class=\\\"form-control\\\"></textarea>\\n                </div>\\n                <input type=\\\"submit\\\" value=\\\"Save\\\" class=\\\"btn btn-default\\\" />\\n            }\\n        </div>\\n        <script src=\\\"/lib/jquery/jquery.js\\\"></script>\\n        <script src=\\\"/lib/bootstrap/js/bootstrap.js\\\"></script>\\n    </body>\\n    </html>\\n\\n### Handling the posts\\n\\nAs you can see in the `Index` view there is a `POST` to the `Create` action in the `Post` controller. So let's add the controller:\\n\\n    [Route(\\\"[controller]\\\")]\\n    public class PostController : Controller\\n    {\\n        public static Data _data = new Data();\\n\\n        [Route(\\\"{slug}\\\", Name = \\\"GetPost\\\")]\\n        public IActionResult Index(string slug)\\n        {\\n            return View(_data.Get(slug));\\n        }\\n\\n        [Route(\\\"[action]\\\")]\\n        public IActionResult Create(PostModel model)\\n        {\\n            _data.Add(model);\\n            return RedirectToAction(\\\"Index\\\", new {slug = model.Slug});\\n        }\\n    }\\n\\n### Creating the post view\\n\\nThe `Create` action results in a redirect to the `Index` action in the same controller, so we must add also add the view for the `Index` action:\\n\\n    @model OneManBlog.Model.PostModel\\n\\n    <html>\\n    <head>\\n        <title>\\n            @Model.Slug\\n        </title>\\n        <link href=\\\"/lib/bootstrap/css/bootstrap.css\\\" rel=\\\"stylesheet\\\" />\\n        <link href=\\\"/css/site.css\\\" rel=\\\"stylesheet\\\" />\\n    </head>\\n    <body>\\n        <div class=\\\"container\\\">\\n            <h1>@Model.Slug</h1>\\n            <div>@Model.Content</div>\\n        </div>\\n        <script src=\\\"/lib/jquery/jquery.js\\\"></script>\\n        <script src=\\\"/lib/bootstrap/js/bootstrap.js\\\"></script>\\n    </body>\\n    </html>\\n\\n### Notes about the application\\n\\nI don't have error handling or anything like that since all I want to do is create a sample application to work with. As you can see in the views, there are references to `jquery` and `bootstrap`, but we haven't added that yet. So the next step is to add a frontend build process that handles all the frontend dependencies, it sounds more difficult than it is.\\n\\n## Adding the frontend dependencies\\n\\nAs you see in the markup above we need [bootstrap](http://getbootstrap.com/) to make it look prettier. So the next step is to add the script and css to the project. The way you are supposed to add frontend script to ASP.NET project in ASP.NET 5 is to use [bower](http://bower.io/) to add the frontend script. (The fact is that you can probably use whatever you want, but bower is integrated in VS15).\\n\\nThere is one simple step you need to do, and that is add a \\\"bower.json\\\" file. Note that you have \\\"intellisense\\\" in the file. The file should have the following content:\\n\\n    {\\n        \\\"name\\\": \\\"OneManBlog\\\",\\n        \\\"private\\\": true,\\n        \\\"dependencies\\\": {\\n            \\\"bootstrap\\\": \\\"3.3.2\\\"\\n        },\\n        \\\"exportsOverride\\\": {\\n            \\\"bootstrap\\\": {\\n                \\\"js\\\":  \\\"dist/js/*.*\\\",\\n                \\\"css\\\":  \\\"dist/css/*.*\\\",\\n                \\\"fonts\\\":  \\\"dist/fonts/*.*\\\"\\n            }\\n        }\\n    }\\n\\nYou can read about the options here: http://bower.io/docs/creating-packages/. The `exportsOverride` defines how we want to extract the files from the package during build which will be describe in the next subsection.\\n\\nIf everything works as expected your solution explorer should look like this:\\n\\n![Bower in Visual Studio](/content/images/2015/04/Bower.PNG)\\n\\nThere you see `bootstrap` is installed in the project with version `3.3.2`, and you can also see the dependencies for bootstrap if you click the small \\\"arrow\\\" left to `bootstrap`. \\n\\nThere has been some issues with the CTP version of Visual Studio and running the bower script. One issue that I had was due to git and a workaround can be found on [Stack Overflow]( http://stackoverflow.com/questions/28725727/vs-2015-bower-does-not-work-behind-firewall/29605933#29605933).\\n\\n## Adding frontend build dependencies\\n\\nAfter you've added `bootstrap` you might get tempted to add a reference to the scripts and css files directly to where they are, but that is not the way you should do it. The way to serve static files in ASP.NET 5 is to move to static files to the `wwwroot`, the web root. You can see the web root in the picture above right under references. Moving files is a repetitive task that humans are generally fails to do over and over again, so instead we should set up a frontend build step. This is nothing new for the web community in general, but it has not been the recommended way to solve this before in the .NET community.\\n\\nWe are going to use [gruntjs](http://gruntjs.com/) to do the actual frontend build, but this is pluggable and if you prefer to use gulpjs](http://gulpjs.com/) you can. All these tools run on [nodejs](https://nodejs.org/), but everything is nicely integrated in Visual Studio so there are not that hard to get it up and running.\\n\\n### Adding grunt\\n\\nTo install grunt in the solution [NPM](http://npmjs.org/) is used, which is sort of the same thing as Nuget but for javascript packages. Also similar to bower, but bower targets mainly frontend frameworks. To add NPM packages we need to add a file \\\"package.json\\\" with the following content:\\n\\n    {\\n        \\\"version\\\": \\\"0.0.0\\\",\\n        \\\"name\\\": \\\"\\\",\\n        \\\"devDependencies\\\": {\\n            \\\"grunt\\\": \\\"0.4.5\\\",\\n            \\\"grunt-bower-task\\\": \\\"0.4.0\\\"\\n        }\\n    }\\n\\nThe difference here compared to \\\"bower.json\\\" is that we use `devDependencies` and that is because gulp is used to build the frontend application and a dependency to the actual applicaiton. You can see that there are two packages included, one is the grunt package so we can run grunt and the second package is an extension for grunt to deal with bower packages.\\n\\nThe solution should now look like:\\n\\n![Solution with NPM packages](/content/images/2015/04/NPM-1.PNG)\\n\\nIf it says \\\"not installed\\\" after the NPM packages you can just right click the NPM folder and select \\\"Restore packages\\\".\\n\\n## Configuring grunt\\n\\nNow we have everything setup to configure the actual frontend build. The build is configured in the file \\\"gruntfile.js\\\" so that should be added to the solution with the following content: \\n\\n    module.exports = function (grunt) {\\n        grunt.initConfig({\\n            bower: {\\n                install: {\\n                    options: {\\n                        targetDir: \\\"wwwroot/lib\\\",\\n                        layout: \\\"byComponent\\\",\\n                        cleanTargetDir: false\\n                    }\\n                }\\n            }\\n        });\\n        grunt.registerTask(\\\"default\\\", [\\\"bower:install\\\"]);\\n        grunt.loadNpmTasks(\\\"grunt-bower-task\\\");\\n    };\\n\\nThe `grunt.initConfig` is where we defined our front end build step. To be able to run the `bower` task we need to add a `bower` section to the config where we configure how to `install` bower. `byComponent` means that each component will get its own section in webroot. `targetDir` is simply the target folder when we move the files. `grunt.registerTask` is used to set up different tasks and alias for them so we can trigger different build task if we need to. `grunt.loadNpmTasks` is how we load the plugins.\\n\\n### The \\\"Task Runner Explorer\\\" window\\n\\nTo configure when and how this build is run we have a new View in Visual Studio. If you right click the \\\"gruntfile.js\\\" file you have an option called \\\"Task Runner Explorer\\\". If you open up the \\\"Task Runner Explorer\\\" you get a window that looks like:\\n\\n![Task Runner Explorer](/content/images/2015/04/TaskRunnerExplorer.PNG)\\n\\nThere you can see all tasks defined in the \\\"gruntfile.js\\\" which you can run by right click the file and choose run. You can also bind the task to be run before or after a build for example. I like to have run the bower task after build so that's how I bind it usually. If you set up a binding you can see it under the binding tab. If you run the `default` task the solution view should be updated with content under the web root and it should look something like: \\n\\n![Web root](/content/images/2015/04/FrontendInstalled.PNG)\\n\\nThere you can see that all the bower packages, bootstrap and jquery which bootstrap depends on, are grouped by component as defined.\\n\\n## Setting up static file handling\\n\\nIf you try to run the solution as it is now it will look like nothing has happened. But that's because we haven't added support for static file handling and all the requests for the scripts and css will match hour \\\"catch all\\\" that will just output \\\"Hello world!\\\". \\n\\nTo solve that you need to do two things:\\n\\n1. Add the static file handling package to your `project.json` file, making the `dependencies` section look like:\\n\\n        \\\"dependencies\\\": {\\n            \\\"Microsoft.AspNet.Mvc\\\": \\\"6.0.0-beta4\\\",\\n            \\\"Microsoft.AspNet.Server.IIS\\\": \\\"1.0.0-beta4\\\",\\n            \\\"Microsoft.AspNet.StaticFiles\\\": \\\"1.0.0-beta4\\\"\\n        },\\n\\n2. Update the `Startup` class so static file handling is included in the pipeline:\\n\\n        public class Startup\\n        {\\n            public void ConfigureServices(IServiceCollection services)\\n            {\\n                services.AddMvc();\\n            }\\n            public void Configure(IApplicationBuilder app)\\n            {\\n                app.UseStaticFiles();\\n                app.UseMvc(rb =>\\n                {\\n                    rb.MapRoute(\\n                        name: \\\"default\\\",\\n                        template: \\\"{controller}/{action}/{id?}\\\",\\n                        defaults: new {controller = \\\"Home\\\", action = \\\"Index\\\"});\\n                });\\n                app.Use(async (context, next) =>\\n                {\\n                    await context.Response.WriteAsync(\\\"Hello world!\\\");\\n                });\\n            }\\n        }\\n\\n## Summary\\n\\nWe now have the building blocks for adding basically any component we want to our frontend application. If we wanted to add [less](http://lesscss.org/) support we would just need to add the `grunt-contrib-less` package to our `package.json` file and update the `gruntfile.js` to compile the less files to css. That finishes this part blog post, next will be a little more about dependency injection.\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"addingsomefunctionality\">Adding some functionality</h2>\n<p>Before we go on to the main purpose of this post, let's add some simple functionality to the application we're building.</p>\n<h3 id=\"addaninmemorydatabase\">Add an in-memory database</h3>\n<p>The goal of the application is a simple one man blog, so we need some kind of data storage. I settled on this simple model:</p>\n<pre><code>public class PostModel\n{\n    public string Slug { get; set; }\n    public string Content { get; set; }\n}\n</code></pre>\n<p>and with that I created this in-memory database:</p>\n<pre><code>public class Data\n{\n    private Dictionary&lt;string, PostModel&gt; _posts = new Dictionary&lt;string, PostModel&gt;();\n\n    public void Add(PostModel post)\n    {\n        _posts.Add(post.Slug, post);\n    }\n\n    public PostModel Get(string id)\n    {\n        return _posts[id];\n    }\n\n    public IEnumerable&lt;PostModel&gt; GetPosts()\n    {\n        return _posts.Values;\n    }\n}\n</code></pre>\n<h3 id=\"updatingthehomeindexview\">Updating the Home/Index view</h3>\n<p>The <code>Index</code> view of the <code>Home</code> controller will be where you enter the blog posts. After that you'll be redirected to the post. The updated view looks like:</p>\n<pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;\n        Welcome to you!\n    &lt;/title&gt;\n    &lt;link href=&quot;/lib/bootstrap/css/bootstrap.css&quot; rel=&quot;stylesheet&quot; /&gt;\n    &lt;link href=&quot;/css/site.css&quot; rel=&quot;stylesheet&quot; /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=&quot;container&quot;&gt;\n        &lt;h1&gt;This is my new blog&lt;/h1&gt;\n        Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\n\n        @using (Html.BeginForm(&quot;Create&quot;, &quot;Post&quot;, FormMethod.Post))\n        {\n            &lt;div class=&quot;form-group&quot;&gt;\n                &lt;label for=&quot;slug&quot;&gt;Slug&lt;/label&gt;\n                &lt;input type=&quot;text&quot; name=&quot;slug&quot; id=&quot;name&quot; value=&quot;&quot; class=&quot;form-control&quot; /&gt;\n            &lt;/div&gt;\n            &lt;div class=&quot;form-group&quot;&gt;\n                &lt;label for=&quot;content&quot;&gt;Content&lt;/label&gt;\n                &lt;textarea id=&quot;content&quot; name=&quot;content&quot; class=&quot;form-control&quot;&gt;&lt;/textarea&gt;\n            &lt;/div&gt;\n            &lt;input type=&quot;submit&quot; value=&quot;Save&quot; class=&quot;btn btn-default&quot; /&gt;\n        }\n    &lt;/div&gt;\n    &lt;script src=&quot;/lib/jquery/jquery.js&quot;&gt;&lt;/script&gt;\n    &lt;script src=&quot;/lib/bootstrap/js/bootstrap.js&quot;&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h3 id=\"handlingtheposts\">Handling the posts</h3>\n<p>As you can see in the <code>Index</code> view there is a <code>POST</code> to the <code>Create</code> action in the <code>Post</code> controller. So let's add the controller:</p>\n<pre><code>[Route(&quot;[controller]&quot;)]\npublic class PostController : Controller\n{\n    public static Data _data = new Data();\n\n    [Route(&quot;{slug}&quot;, Name = &quot;GetPost&quot;)]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(&quot;[action]&quot;)]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(&quot;Index&quot;, new {slug = model.Slug});\n    }\n}\n</code></pre>\n<h3 id=\"creatingthepostview\">Creating the post view</h3>\n<p>The <code>Create</code> action results in a redirect to the <code>Index</code> action in the same controller, so we must add also add the view for the <code>Index</code> action:</p>\n<pre><code>@model OneManBlog.Model.PostModel\n\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;\n        @Model.Slug\n    &lt;/title&gt;\n    &lt;link href=&quot;/lib/bootstrap/css/bootstrap.css&quot; rel=&quot;stylesheet&quot; /&gt;\n    &lt;link href=&quot;/css/site.css&quot; rel=&quot;stylesheet&quot; /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=&quot;container&quot;&gt;\n        &lt;h1&gt;@Model.Slug&lt;/h1&gt;\n        &lt;div&gt;@Model.Content&lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script src=&quot;/lib/jquery/jquery.js&quot;&gt;&lt;/script&gt;\n    &lt;script src=&quot;/lib/bootstrap/js/bootstrap.js&quot;&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h3 id=\"notesabouttheapplication\">Notes about the application</h3>\n<p>I don't have error handling or anything like that since all I want to do is create a sample application to work with. As you can see in the views, there are references to <code>jquery</code> and <code>bootstrap</code>, but we haven't added that yet. So the next step is to add a frontend build process that handles all the frontend dependencies, it sounds more difficult than it is.</p>\n<h2 id=\"addingthefrontenddependencies\">Adding the frontend dependencies</h2>\n<p>As you see in the markup above we need <a href=\"http://getbootstrap.com/\">bootstrap</a> to make it look prettier. So the next step is to add the script and css to the project. The way you are supposed to add frontend script to ASP.NET project in ASP.NET 5 is to use <a href=\"http://bower.io/\">bower</a> to add the frontend script. (The fact is that you can probably use whatever you want, but bower is integrated in VS15).</p>\n<p>There is one simple step you need to do, and that is add a &quot;bower.json&quot; file. Note that you have &quot;intellisense&quot; in the file. The file should have the following content:</p>\n<pre><code>{\n    &quot;name&quot;: &quot;OneManBlog&quot;,\n    &quot;private&quot;: true,\n    &quot;dependencies&quot;: {\n        &quot;bootstrap&quot;: &quot;3.3.2&quot;\n    },\n    &quot;exportsOverride&quot;: {\n        &quot;bootstrap&quot;: {\n            &quot;js&quot;:  &quot;dist/js/*.*&quot;,\n            &quot;css&quot;:  &quot;dist/css/*.*&quot;,\n            &quot;fonts&quot;:  &quot;dist/fonts/*.*&quot;\n        }\n    }\n}\n</code></pre>\n<p>You can read about the options here: <a href=\"http://bower.io/docs/creating-packages/\">http://bower.io/docs/creating-packages/</a>. The <code>exportsOverride</code> defines how we want to extract the files from the package during build which will be describe in the next subsection.</p>\n<p>If everything works as expected your solution explorer should look like this:</p>\n<p><img src=\"/content/images/2015/04/Bower.PNG\" alt=\"Bower in Visual Studio\"></p>\n<p>There you see <code>bootstrap</code> is installed in the project with version <code>3.3.2</code>, and you can also see the dependencies for bootstrap if you click the small &quot;arrow&quot; left to <code>bootstrap</code>.</p>\n<p>There has been some issues with the CTP version of Visual Studio and running the bower script. One issue that I had was due to git and a workaround can be found on <a href=\"http://stackoverflow.com/questions/28725727/vs-2015-bower-does-not-work-behind-firewall/29605933#29605933\">Stack Overflow</a>.</p>\n<h2 id=\"addingfrontendbuilddependencies\">Adding frontend build dependencies</h2>\n<p>After you've added <code>bootstrap</code> you might get tempted to add a reference to the scripts and css files directly to where they are, but that is not the way you should do it. The way to serve static files in ASP.NET 5 is to move to static files to the <code>wwwroot</code>, the web root. You can see the web root in the picture above right under references. Moving files is a repetitive task that humans are generally fails to do over and over again, so instead we should set up a frontend build step. This is nothing new for the web community in general, but it has not been the recommended way to solve this before in the .NET community.</p>\n<p>We are going to use <a href=\"http://gruntjs.com/\">gruntjs</a> to do the actual frontend build, but this is pluggable and if you prefer to use gulpjs](<a href=\"http://gulpjs.com/\">http://gulpjs.com/</a>) you can. All these tools run on <a href=\"https://nodejs.org/\">nodejs</a>, but everything is nicely integrated in Visual Studio so there are not that hard to get it up and running.</p>\n<h3 id=\"addinggrunt\">Adding grunt</h3>\n<p>To install grunt in the solution <a href=\"http://npmjs.org/\">NPM</a> is used, which is sort of the same thing as Nuget but for javascript packages. Also similar to bower, but bower targets mainly frontend frameworks. To add NPM packages we need to add a file &quot;package.json&quot; with the following content:</p>\n<pre><code>{\n    &quot;version&quot;: &quot;0.0.0&quot;,\n    &quot;name&quot;: &quot;&quot;,\n    &quot;devDependencies&quot;: {\n        &quot;grunt&quot;: &quot;0.4.5&quot;,\n        &quot;grunt-bower-task&quot;: &quot;0.4.0&quot;\n    }\n}\n</code></pre>\n<p>The difference here compared to &quot;bower.json&quot; is that we use <code>devDependencies</code> and that is because gulp is used to build the frontend application and a dependency to the actual applicaiton. You can see that there are two packages included, one is the grunt package so we can run grunt and the second package is an extension for grunt to deal with bower packages.</p>\n<p>The solution should now look like:</p>\n<p><img src=\"/content/images/2015/04/NPM-1.PNG\" alt=\"Solution with NPM packages\"></p>\n<p>If it says &quot;not installed&quot; after the NPM packages you can just right click the NPM folder and select &quot;Restore packages&quot;.</p>\n<h2 id=\"configuringgrunt\">Configuring grunt</h2>\n<p>Now we have everything setup to configure the actual frontend build. The build is configured in the file &quot;gruntfile.js&quot; so that should be added to the solution with the following content:</p>\n<pre><code>module.exports = function (grunt) {\n    grunt.initConfig({\n        bower: {\n            install: {\n                options: {\n                    targetDir: &quot;wwwroot/lib&quot;,\n                    layout: &quot;byComponent&quot;,\n                    cleanTargetDir: false\n                }\n            }\n        }\n    });\n    grunt.registerTask(&quot;default&quot;, [&quot;bower:install&quot;]);\n    grunt.loadNpmTasks(&quot;grunt-bower-task&quot;);\n};\n</code></pre>\n<p>The <code>grunt.initConfig</code> is where we defined our front end build step. To be able to run the <code>bower</code> task we need to add a <code>bower</code> section to the config where we configure how to <code>install</code> bower. <code>byComponent</code> means that each component will get its own section in webroot. <code>targetDir</code> is simply the target folder when we move the files. <code>grunt.registerTask</code> is used to set up different tasks and alias for them so we can trigger different build task if we need to. <code>grunt.loadNpmTasks</code> is how we load the plugins.</p>\n<h3 id=\"thetaskrunnerexplorerwindow\">The &quot;Task Runner Explorer&quot; window</h3>\n<p>To configure when and how this build is run we have a new View in Visual Studio. If you right click the &quot;gruntfile.js&quot; file you have an option called &quot;Task Runner Explorer&quot;. If you open up the &quot;Task Runner Explorer&quot; you get a window that looks like:</p>\n<p><img src=\"/content/images/2015/04/TaskRunnerExplorer.PNG\" alt=\"Task Runner Explorer\"></p>\n<p>There you can see all tasks defined in the &quot;gruntfile.js&quot; which you can run by right click the file and choose run. You can also bind the task to be run before or after a build for example. I like to have run the bower task after build so that's how I bind it usually. If you set up a binding you can see it under the binding tab. If you run the <code>default</code> task the solution view should be updated with content under the web root and it should look something like:</p>\n<p><img src=\"/content/images/2015/04/FrontendInstalled.PNG\" alt=\"Web root\"></p>\n<p>There you can see that all the bower packages, bootstrap and jquery which bootstrap depends on, are grouped by component as defined.</p>\n<h2 id=\"settingupstaticfilehandling\">Setting up static file handling</h2>\n<p>If you try to run the solution as it is now it will look like nothing has happened. But that's because we haven't added support for static file handling and all the requests for the scripts and css will match hour &quot;catch all&quot; that will just output &quot;Hello world!&quot;.</p>\n<p>To solve that you need to do two things:</p>\n<ol>\n<li>\n<p>Add the static file handling package to your <code>project.json</code> file, making the <code>dependencies</code> section look like:</p>\n<pre><code> &quot;dependencies&quot;: {\n     &quot;Microsoft.AspNet.Mvc&quot;: &quot;6.0.0-beta4&quot;,\n     &quot;Microsoft.AspNet.Server.IIS&quot;: &quot;1.0.0-beta4&quot;,\n     &quot;Microsoft.AspNet.StaticFiles&quot;: &quot;1.0.0-beta4&quot;\n },\n</code></pre>\n</li>\n<li>\n<p>Update the <code>Startup</code> class so static file handling is included in the pipeline:</p>\n<pre><code> public class Startup\n {\n     public void ConfigureServices(IServiceCollection services)\n     {\n         services.AddMvc();\n     }\n     public void Configure(IApplicationBuilder app)\n     {\n         app.UseStaticFiles();\n         app.UseMvc(rb =&gt;\n         {\n             rb.MapRoute(\n                 name: &quot;default&quot;,\n                 template: &quot;{controller}/{action}/{id?}&quot;,\n                 defaults: new {controller = &quot;Home&quot;, action = &quot;Index&quot;});\n         });\n         app.Use(async (context, next) =&gt;\n         {\n             await context.Response.WriteAsync(&quot;Hello world!&quot;);\n         });\n     }\n }\n</code></pre>\n</li>\n</ol>\n<h2 id=\"summary\">Summary</h2>\n<p>We now have the building blocks for adding basically any component we want to our frontend application. If we wanted to add <a href=\"http://lesscss.org/\">less</a> support we would just need to add the <code>grunt-contrib-less</code> package to our <code>package.json</code> file and update the <code>gruntfile.js</code> to compile the less files to css. That finishes this part blog post, next will be a little more about dependency injection.</p>\n<!--kg-card-end: markdown-->","comment_id":"49","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nAdding some functionality\nBefore we go on to the main purpose of this post, let's add some simple\nfunctionality to the application we're building.\n\nAdd an in-memory database\nThe goal of the application is a simple one man blog, so we need some kind of\ndata storage. I settled on this simple model:\n\npublic class PostModel\n{\n    public string Slug { get; set; }\n    public string Content { get; set; }\n}\n\n\nand with that I created this in-memory database:\n\npublic class Data\n{\n    private Dictionary<string, PostModel> _posts = new Dictionary<string, PostModel>();\n\n    public void Add(PostModel post)\n    {\n        _posts.Add(post.Slug, post);\n    }\n\n    public PostModel Get(string id)\n    {\n        return _posts[id];\n    }\n\n    public IEnumerable<PostModel> GetPosts()\n    {\n        return _posts.Values;\n    }\n}\n\n\nUpdating the Home/Index view\nThe Index view of the Home controller will be where you enter the blog posts.\nAfter that you'll be redirected to the post. The updated view looks like:\n\n<html>\n<head>\n    <title>\n        Welcome to you!\n    </title>\n    <link href=\"/lib/bootstrap/css/bootstrap.css\" rel=\"stylesheet\" />\n    <link href=\"/css/site.css\" rel=\"stylesheet\" />\n</head>\n<body>\n    <div class=\"container\">\n        <h1>This is my new blog</h1>\n        Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\n\n        @using (Html.BeginForm(\"Create\", \"Post\", FormMethod.Post))\n        {\n            <div class=\"form-group\">\n                <label for=\"slug\">Slug</label>\n                <input type=\"text\" name=\"slug\" id=\"name\" value=\"\" class=\"form-control\" />\n            </div>\n            <div class=\"form-group\">\n                <label for=\"content\">Content</label>\n                <textarea id=\"content\" name=\"content\" class=\"form-control\"></textarea>\n            </div>\n            <input type=\"submit\" value=\"Save\" class=\"btn btn-default\" />\n        }\n    </div>\n    <script src=\"/lib/jquery/jquery.js\"></script>\n    <script src=\"/lib/bootstrap/js/bootstrap.js\"></script>\n</body>\n</html>\n\n\nHandling the posts\nAs you can see in the Index view there is a POST to the Create action in the \nPost controller. So let's add the controller:\n\n[Route(\"[controller]\")]\npublic class PostController : Controller\n{\n    public static Data _data = new Data();\n\n    [Route(\"{slug}\", Name = \"GetPost\")]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(\"[action]\")]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(\"Index\", new {slug = model.Slug});\n    }\n}\n\n\nCreating the post view\nThe Create action results in a redirect to the Index action in the same\ncontroller, so we must add also add the view for the Index action:\n\n@model OneManBlog.Model.PostModel\n\n<html>\n<head>\n    <title>\n        @Model.Slug\n    </title>\n    <link href=\"/lib/bootstrap/css/bootstrap.css\" rel=\"stylesheet\" />\n    <link href=\"/css/site.css\" rel=\"stylesheet\" />\n</head>\n<body>\n    <div class=\"container\">\n        <h1>@Model.Slug</h1>\n        <div>@Model.Content</div>\n    </div>\n    <script src=\"/lib/jquery/jquery.js\"></script>\n    <script src=\"/lib/bootstrap/js/bootstrap.js\"></script>\n</body>\n</html>\n\n\nNotes about the application\nI don't have error handling or anything like that since all I want to do is\ncreate a sample application to work with. As you can see in the views, there are\nreferences to jquery and bootstrap, but we haven't added that yet. So the next\nstep is to add a frontend build process that handles all the frontend\ndependencies, it sounds more difficult than it is.\n\nAdding the frontend dependencies\nAs you see in the markup above we need bootstrap [http://getbootstrap.com/] to\nmake it look prettier. So the next step is to add the script and css to the\nproject. The way you are supposed to add frontend script to ASP.NET project in\nASP.NET 5 is to use bower [http://bower.io/] to add the frontend script. (The\nfact is that you can probably use whatever you want, but bower is integrated in\nVS15).\n\nThere is one simple step you need to do, and that is add a \"bower.json\" file.\nNote that you have \"intellisense\" in the file. The file should have the\nfollowing content:\n\n{\n    \"name\": \"OneManBlog\",\n    \"private\": true,\n    \"dependencies\": {\n        \"bootstrap\": \"3.3.2\"\n    },\n    \"exportsOverride\": {\n        \"bootstrap\": {\n            \"js\":  \"dist/js/*.*\",\n            \"css\":  \"dist/css/*.*\",\n            \"fonts\":  \"dist/fonts/*.*\"\n        }\n    }\n}\n\n\nYou can read about the options here: http://bower.io/docs/creating-packages/.\nThe exportsOverride defines how we want to extract the files from the package\nduring build which will be describe in the next subsection.\n\nIf everything works as expected your solution explorer should look like this:\n\n\n\nThere you see bootstrap is installed in the project with version 3.3.2, and you\ncan also see the dependencies for bootstrap if you click the small \"arrow\" left\nto bootstrap.\n\nThere has been some issues with the CTP version of Visual Studio and running the\nbower script. One issue that I had was due to git and a workaround can be found\non Stack Overflow\n[http://stackoverflow.com/questions/28725727/vs-2015-bower-does-not-work-behind-firewall/29605933#29605933]\n.\n\nAdding frontend build dependencies\nAfter you've added bootstrap you might get tempted to add a reference to the\nscripts and css files directly to where they are, but that is not the way you\nshould do it. The way to serve static files in ASP.NET 5 is to move to static\nfiles to the wwwroot, the web root. You can see the web root in the picture\nabove right under references. Moving files is a repetitive task that humans are\ngenerally fails to do over and over again, so instead we should set up a\nfrontend build step. This is nothing new for the web community in general, but\nit has not been the recommended way to solve this before in the .NET community.\n\nWe are going to use gruntjs [http://gruntjs.com/] to do the actual frontend\nbuild, but this is pluggable and if you prefer to use gulpjs](http://gulpjs.com/\n) you can. All these tools run on nodejs [https://nodejs.org/], but everything\nis nicely integrated in Visual Studio so there are not that hard to get it up\nand running.\n\nAdding grunt\nTo install grunt in the solution NPM [http://npmjs.org/] is used, which is sort\nof the same thing as Nuget but for javascript packages. Also similar to bower,\nbut bower targets mainly frontend frameworks. To add NPM packages we need to add\na file \"package.json\" with the following content:\n\n{\n    \"version\": \"0.0.0\",\n    \"name\": \"\",\n    \"devDependencies\": {\n        \"grunt\": \"0.4.5\",\n        \"grunt-bower-task\": \"0.4.0\"\n    }\n}\n\n\nThe difference here compared to \"bower.json\" is that we use devDependencies and\nthat is because gulp is used to build the frontend application and a dependency\nto the actual applicaiton. You can see that there are two packages included, one\nis the grunt package so we can run grunt and the second package is an extension\nfor grunt to deal with bower packages.\n\nThe solution should now look like:\n\n\n\nIf it says \"not installed\" after the NPM packages you can just right click the\nNPM folder and select \"Restore packages\".\n\nConfiguring grunt\nNow we have everything setup to configure the actual frontend build. The build\nis configured in the file \"gruntfile.js\" so that should be added to the solution\nwith the following content:\n\nmodule.exports = function (grunt) {\n    grunt.initConfig({\n        bower: {\n            install: {\n                options: {\n                    targetDir: \"wwwroot/lib\",\n                    layout: \"byComponent\",\n                    cleanTargetDir: false\n                }\n            }\n        }\n    });\n    grunt.registerTask(\"default\", [\"bower:install\"]);\n    grunt.loadNpmTasks(\"grunt-bower-task\");\n};\n\n\nThe grunt.initConfig is where we defined our front end build step. To be able to\nrun the bower task we need to add a bower section to the config where we\nconfigure how to install bower. byComponent means that each component will get\nits own section in webroot. targetDir is simply the target folder when we move\nthe files. grunt.registerTask is used to set up different tasks and alias for\nthem so we can trigger different build task if we need to. grunt.loadNpmTasks is\nhow we load the plugins.\n\nThe \"Task Runner Explorer\" window\nTo configure when and how this build is run we have a new View in Visual Studio.\nIf you right click the \"gruntfile.js\" file you have an option called \"Task\nRunner Explorer\". If you open up the \"Task Runner Explorer\" you get a window\nthat looks like:\n\n\n\nThere you can see all tasks defined in the \"gruntfile.js\" which you can run by\nright click the file and choose run. You can also bind the task to be run before\nor after a build for example. I like to have run the bower task after build so\nthat's how I bind it usually. If you set up a binding you can see it under the\nbinding tab. If you run the default task the solution view should be updated\nwith content under the web root and it should look something like:\n\n\n\nThere you can see that all the bower packages, bootstrap and jquery which\nbootstrap depends on, are grouped by component as defined.\n\nSetting up static file handling\nIf you try to run the solution as it is now it will look like nothing has\nhappened. But that's because we haven't added support for static file handling\nand all the requests for the scripts and css will match hour \"catch all\" that\nwill just output \"Hello world!\".\n\nTo solve that you need to do two things:\n\n 1. Add the static file handling package to your project.json file, making the \n    dependencies section look like:\n    \n     \"dependencies\": {\n         \"Microsoft.AspNet.Mvc\": \"6.0.0-beta4\",\n         \"Microsoft.AspNet.Server.IIS\": \"1.0.0-beta4\",\n         \"Microsoft.AspNet.StaticFiles\": \"1.0.0-beta4\"\n     },\n    \n    \n    \n 2. Update the Startup class so static file handling is included in the\n    pipeline:\n    \n     public class Startup\n     {\n         public void ConfigureServices(IServiceCollection services)\n         {\n             services.AddMvc();\n         }\n         public void Configure(IApplicationBuilder app)\n         {\n             app.UseStaticFiles();\n             app.UseMvc(rb =>\n             {\n                 rb.MapRoute(\n                     name: \"default\",\n                     template: \"{controller}/{action}/{id?}\",\n                     defaults: new {controller = \"Home\", action = \"Index\"});\n             });\n             app.Use(async (context, next) =>\n             {\n                 await context.Response.WriteAsync(\"Hello world!\");\n             });\n         }\n     }\n    \n    \n    \n\nSummary\nWe now have the building blocks for adding basically any component we want to\nour frontend application. If we wanted to add less [http://lesscss.org/] support\nwe would just need to add the grunt-contrib-less package to our package.json \nfile and update the gruntfile.js to compile the less files to css. That finishes\nthis part blog post, next will be a little more about dependency injection.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-14T16:28:57.000Z","updated_at":"2015-05-18T18:49:48.000Z","published_at":"2015-04-26T22:15:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe065","uuid":"55244d9d-2e47-4a15-916e-e566a051866f","title":"ASP.NET 5: IoC and dependency injection","slug":"asp-net-5-ioc-and-dependency-injection","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## Constructor injection in the controller\\n\\nThe way the `PostController` is implemented at the moment is not that testable since it has a dependency on a \\\"third party\\\" component, the data handling. An instance of the `Data` class is created directly in the controller, which a really bad design. A better way to do it would be to inject it in the constructor. So instead of having this: \\n\\n    [Route(\\\"[controller]\\\")]\\n    public class PostController : Controller\\n    {\\n        public static Data _data = new Data();\\n\\n        [Route(\\\"{slug}\\\", Name = \\\"GetPost\\\")]\\n        public IActionResult Index(string slug)\\n        {\\n            return View(_data.Get(slug));\\n        }\\n\\n        [Route(\\\"[action]\\\")]\\n        public IActionResult Create(PostModel model)\\n        {\\n            _data.Add(model);\\n            return RedirectToAction(\\\"Index\\\", new {slug = model.Slug});\\n        }\\n    }\\n\\nwe should get it injected in the constructor like this:\\n\\n    [Route(\\\"[controller]\\\")]\\n    public class PostController : Controller\\n    {\\n        private static Data _data;\\n\\n        public PostController(Data data)\\n        {\\n            _data = data;\\n        }\\n\\n        [Route(\\\"{slug}\\\", Name = \\\"GetPost\\\")]\\n        public IActionResult Index(string slug)\\n        {\\n            return View(_data.Get(slug));\\n        }\\n\\n        [Route(\\\"[action]\\\")]\\n        public IActionResult Create(PostModel model)\\n        {\\n            _data.Add(model);\\n            return RedirectToAction(\\\"Index\\\", new {slug = model.Slug});\\n        }\\n    }\\n\\nSo far so good. But the instance of the `Data` class must be created somewhere, and that's done in the `ConfigureServices` method in the `Startup` class:\\n\\n        public void ConfigureServices(IServiceCollection services)\\n        {\\n            services.AddMvc();\\n            services.AddSingleton<Data>();\\n        }\\n\\nMost likely you shouldn't add it as a singleton, but since mine is an in-memory \\\"database\\\" I must have it as a singleton so it is stored between the request. It's not production code, it just shows the feature.\\n\\n## Property injection in the controller\\n\\nYou can also use property injection if you like, but I recommend constructor injection. In beta4 the way to do property injection is to use the `FromServices` attribute, like so: \\n\\n    [FromServices]\\n    public Data Data\\n    {\\n        get;\\n        set;\\n    }\\n\\nIt is important that it is a public property for it to work. Prior to beta4 the attribute name was `Activate`, but that is now `FromServices`. \\n\\n## Injection in the view\\n\\nAnother nice little feature is that you can inject things into the views. For example, let say we want a list in index view: \\n\\n    <div>\\n        <h3>Blog posts</h3>\\n        <ul>\\n            @foreach (var item in Data.GetPosts())\\n            {\\n                <li>@Html.ActionLink(item.Slug, \\\"Index\\\", \\\"Post\\\", new {slug = item.Slug})</li>\\n            }\\n        </ul>\\n    </div>\\n\\nHere it might look like I'm using `Data` directly, but that is an instance. And to inject the instance in the view you use the `inject` keyword in the razor view:\\n\\n    @inject OneManBlog.Model.Data Data\\n\\n## Summary\\n\\nDependency injection is a nice way to decouple your application from third party dependencies, so it is nice that they have thought of it from the very first start in ASP.NET 5. As you see above it is not that hard to get started. The next post will most likely be about creating reusable `ViewComponents`.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"constructorinjectioninthecontroller\">Constructor injection in the controller</h2>\n<p>The way the <code>PostController</code> is implemented at the moment is not that testable since it has a dependency on a &quot;third party&quot; component, the data handling. An instance of the <code>Data</code> class is created directly in the controller, which a really bad design. A better way to do it would be to inject it in the constructor. So instead of having this:</p>\n<pre><code>[Route(&quot;[controller]&quot;)]\npublic class PostController : Controller\n{\n    public static Data _data = new Data();\n\n    [Route(&quot;{slug}&quot;, Name = &quot;GetPost&quot;)]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(&quot;[action]&quot;)]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(&quot;Index&quot;, new {slug = model.Slug});\n    }\n}\n</code></pre>\n<p>we should get it injected in the constructor like this:</p>\n<pre><code>[Route(&quot;[controller]&quot;)]\npublic class PostController : Controller\n{\n    private static Data _data;\n\n    public PostController(Data data)\n    {\n        _data = data;\n    }\n\n    [Route(&quot;{slug}&quot;, Name = &quot;GetPost&quot;)]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(&quot;[action]&quot;)]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(&quot;Index&quot;, new {slug = model.Slug});\n    }\n}\n</code></pre>\n<p>So far so good. But the instance of the <code>Data</code> class must be created somewhere, and that's done in the <code>ConfigureServices</code> method in the <code>Startup</code> class:</p>\n<pre><code>    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n        services.AddSingleton&lt;Data&gt;();\n    }\n</code></pre>\n<p>Most likely you shouldn't add it as a singleton, but since mine is an in-memory &quot;database&quot; I must have it as a singleton so it is stored between the request. It's not production code, it just shows the feature.</p>\n<h2 id=\"propertyinjectioninthecontroller\">Property injection in the controller</h2>\n<p>You can also use property injection if you like, but I recommend constructor injection. In beta4 the way to do property injection is to use the <code>FromServices</code> attribute, like so:</p>\n<pre><code>[FromServices]\npublic Data Data\n{\n    get;\n    set;\n}\n</code></pre>\n<p>It is important that it is a public property for it to work. Prior to beta4 the attribute name was <code>Activate</code>, but that is now <code>FromServices</code>.</p>\n<h2 id=\"injectionintheview\">Injection in the view</h2>\n<p>Another nice little feature is that you can inject things into the views. For example, let say we want a list in index view:</p>\n<pre><code>&lt;div&gt;\n    &lt;h3&gt;Blog posts&lt;/h3&gt;\n    &lt;ul&gt;\n        @foreach (var item in Data.GetPosts())\n        {\n            &lt;li&gt;@Html.ActionLink(item.Slug, &quot;Index&quot;, &quot;Post&quot;, new {slug = item.Slug})&lt;/li&gt;\n        }\n    &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre>\n<p>Here it might look like I'm using <code>Data</code> directly, but that is an instance. And to inject the instance in the view you use the <code>inject</code> keyword in the razor view:</p>\n<pre><code>@inject OneManBlog.Model.Data Data\n</code></pre>\n<h2 id=\"summary\">Summary</h2>\n<p>Dependency injection is a nice way to decouple your application from third party dependencies, so it is nice that they have thought of it from the very first start in ASP.NET 5. As you see above it is not that hard to get started. The next post will most likely be about creating reusable <code>ViewComponents</code>.</p>\n<!--kg-card-end: markdown-->","comment_id":"50","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nConstructor injection in the controller\nThe way the PostController is implemented at the moment is not that testable\nsince it has a dependency on a \"third party\" component, the data handling. An\ninstance of the Data class is created directly in the controller, which a really\nbad design. A better way to do it would be to inject it in the constructor. So\ninstead of having this:\n\n[Route(\"[controller]\")]\npublic class PostController : Controller\n{\n    public static Data _data = new Data();\n\n    [Route(\"{slug}\", Name = \"GetPost\")]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(\"[action]\")]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(\"Index\", new {slug = model.Slug});\n    }\n}\n\n\nwe should get it injected in the constructor like this:\n\n[Route(\"[controller]\")]\npublic class PostController : Controller\n{\n    private static Data _data;\n\n    public PostController(Data data)\n    {\n        _data = data;\n    }\n\n    [Route(\"{slug}\", Name = \"GetPost\")]\n    public IActionResult Index(string slug)\n    {\n        return View(_data.Get(slug));\n    }\n\n    [Route(\"[action]\")]\n    public IActionResult Create(PostModel model)\n    {\n        _data.Add(model);\n        return RedirectToAction(\"Index\", new {slug = model.Slug});\n    }\n}\n\n\nSo far so good. But the instance of the Data class must be created somewhere,\nand that's done in the ConfigureServices method in the Startup class:\n\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddMvc();\n        services.AddSingleton<Data>();\n    }\n\n\nMost likely you shouldn't add it as a singleton, but since mine is an in-memory\n\"database\" I must have it as a singleton so it is stored between the request.\nIt's not production code, it just shows the feature.\n\nProperty injection in the controller\nYou can also use property injection if you like, but I recommend constructor\ninjection. In beta4 the way to do property injection is to use the FromServices \nattribute, like so:\n\n[FromServices]\npublic Data Data\n{\n    get;\n    set;\n}\n\n\nIt is important that it is a public property for it to work. Prior to beta4 the\nattribute name was Activate, but that is now FromServices.\n\nInjection in the view\nAnother nice little feature is that you can inject things into the views. For\nexample, let say we want a list in index view:\n\n<div>\n    <h3>Blog posts</h3>\n    <ul>\n        @foreach (var item in Data.GetPosts())\n        {\n            <li>@Html.ActionLink(item.Slug, \"Index\", \"Post\", new {slug = item.Slug})</li>\n        }\n    </ul>\n</div>\n\n\nHere it might look like I'm using Data directly, but that is an instance. And to\ninject the instance in the view you use the inject keyword in the razor view:\n\n@inject OneManBlog.Model.Data Data\n\n\nSummary\nDependency injection is a nice way to decouple your application from third party\ndependencies, so it is nice that they have thought of it from the very first\nstart in ASP.NET 5. As you see above it is not that hard to get started. The\nnext post will most likely be about creating reusable ViewComponents.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-01T22:24:32.000Z","updated_at":"2016-01-15T22:18:58.000Z","published_at":"2015-05-02T17:42:55.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe066","uuid":"b999f512-7c25-4c47-adda-5e34343aaa15","title":"ASP.NET 5: View Components","slug":"asp-net-5-view-components","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## What is view components?\\n\\nPartial views in MVC has one limitation, which is that they doesn't have a controller for them. Not having controller makes it hard to have more complex logic associated with the view. View components changes that. Each view component consist of a view and a backing class, it's not a controller but almost. In this post we will take the post list which we created by injecting the \\\"repository\\\" into the index view in the [last post](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/) and create a reuseable view component instead which we can use in multiple views.\\n\\n## Creating the view component\\n\\nThe purpose of this view component is to list all the posts for this simple blog, making it much easier to reuse in multiple views. An alternative would have been to use a partial view, but using a partial view would have forced us to get the data for the partial view in the controller for each of the views where we used the partial, but since a view component has a backing class that is not necessary.\\n\\n### The ViewComponent class\\n\\nCreate a folder called `ViewComponents` where you can have all your view components and add a `PostListViewComponent` class in the folder. The implementation of the class should look like this: \\n\\n    public class PostListViewComponent : ViewComponent\\n    {\\n        private readonly Data _data;\\n\\n        public PostListViewComponent(Data data)\\n        {\\n            _data = data;\\n        }\\n\\n        public IViewComponentResult Invoke(string title)\\n        {\\n            ViewBag.Title = title;\\n            return View(_data.GetPosts());\\n        }\\n    }\\n\\nWe are inheriting from the class `ViewComponent` to get some extra help, like the `View` method in the base class. The `Invoke` method is what will be called from the views that adds this view component and it is returning a simple view that show the posts. We are also injecting the `Data` class in the constructor which makes it possible to get the data we need in this class instead of it being sent to it like you do with a partial view. The view we return has a `ViewBag` as a regular view which we take advantage of, and the `Invoke` method can also have arguments which you can pass to the view component when you use it.\\n\\n### The View\\n\\nThe view is really straightforward:\\n\\n    @model IEnumerable<OneManBlog.Model.PostModel>\\n    <div>\\n        <h3>@ViewBag.Title</h3>\\n        <ul>\\n            @foreach (var item in Model)\\n                {\\n                <li>@Html.ActionLink(item.Slug, \\\"Index\\\", \\\"Post\\\", new { slug = item.Slug })</li>\\n            }\\n        </ul>\\n    </div>\\n\\nI want get into any detail what this does, but the hard part is where to put it. To get this to work it must be in the folder `Views\\\\Shared\\\\Components\\\\PostList\\\\` and the file must be called `Default.cshtml`. The important part is from `Components` and down, the first part just scope where it can be used. Since I want this to be used from all over the site I put it in `Shared`. \\n\\nWith these two files added my solution look like this:\\n\\n![View component solution structure](/content/images/2015/05/ViewComponent.PNG)\\n\\n### Using the view component\\n\\nTo use this in any view all I have to do is add this line:\\n\\n    @Component.Invoke(\\\"PostList\\\", \\\"Blog posts\\\")        \\n\\nSo my new `Home\\\\Index.cshtml` container part now looks like this:\\n\\n    <div class=\\\"container\\\">\\n        <h1>This is my new blog</h1>\\n        Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\\n\\n        @using (Html.BeginForm(\\\"Create\\\", \\\"Post\\\", FormMethod.Post))\\n        {\\n            <div class=\\\"form-group\\\">\\n                <label for=\\\"slug\\\">Slug</label>\\n                <input type=\\\"text\\\" name=\\\"slug\\\" id=\\\"name\\\" value=\\\"\\\" class=\\\"form-control\\\" />\\n            </div>\\n            <div class=\\\"form-group\\\">\\n                <label for=\\\"content\\\">Content</label>\\n                <textarea id=\\\"content\\\" name=\\\"content\\\" class=\\\"form-control\\\"></textarea>\\n            </div>\\n            <input type=\\\"submit\\\" value=\\\"Save\\\" class=\\\"btn btn-default\\\" />\\n        }\\n        @Component.Invoke(\\\"PostList\\\", \\\"Blog posts\\\")        \\n    </div>\\n\\nand my new `Post\\\\Index.cshtml` looks like this:\\n\\n    <div class=\\\"container\\\">\\n        <h1>@Model.Slug</h1>\\n        <div>@Model.Content</div>\\n        @Component.Invoke(\\\"PostList\\\", \\\"Other blog posts\\\")\\n    </div>\\n\\nThat's all there is to it. \\n\\n## Summary\\n\\nThis is something I really missed in regular partial views and something that I think will be really useful creating more simple components that is reusable across a site. I look forward to try out this feature in a larger project and so how well it fits in, but so far I think it is really promising.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"whatisviewcomponents\">What is view components?</h2>\n<p>Partial views in MVC has one limitation, which is that they doesn't have a controller for them. Not having controller makes it hard to have more complex logic associated with the view. View components changes that. Each view component consist of a view and a backing class, it's not a controller but almost. In this post we will take the post list which we created by injecting the &quot;repository&quot; into the index view in the <a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">last post</a> and create a reuseable view component instead which we can use in multiple views.</p>\n<h2 id=\"creatingtheviewcomponent\">Creating the view component</h2>\n<p>The purpose of this view component is to list all the posts for this simple blog, making it much easier to reuse in multiple views. An alternative would have been to use a partial view, but using a partial view would have forced us to get the data for the partial view in the controller for each of the views where we used the partial, but since a view component has a backing class that is not necessary.</p>\n<h3 id=\"theviewcomponentclass\">The ViewComponent class</h3>\n<p>Create a folder called <code>ViewComponents</code> where you can have all your view components and add a <code>PostListViewComponent</code> class in the folder. The implementation of the class should look like this:</p>\n<pre><code>public class PostListViewComponent : ViewComponent\n{\n    private readonly Data _data;\n\n    public PostListViewComponent(Data data)\n    {\n        _data = data;\n    }\n\n    public IViewComponentResult Invoke(string title)\n    {\n        ViewBag.Title = title;\n        return View(_data.GetPosts());\n    }\n}\n</code></pre>\n<p>We are inheriting from the class <code>ViewComponent</code> to get some extra help, like the <code>View</code> method in the base class. The <code>Invoke</code> method is what will be called from the views that adds this view component and it is returning a simple view that show the posts. We are also injecting the <code>Data</code> class in the constructor which makes it possible to get the data we need in this class instead of it being sent to it like you do with a partial view. The view we return has a <code>ViewBag</code> as a regular view which we take advantage of, and the <code>Invoke</code> method can also have arguments which you can pass to the view component when you use it.</p>\n<h3 id=\"theview\">The View</h3>\n<p>The view is really straightforward:</p>\n<pre><code>@model IEnumerable&lt;OneManBlog.Model.PostModel&gt;\n&lt;div&gt;\n    &lt;h3&gt;@ViewBag.Title&lt;/h3&gt;\n    &lt;ul&gt;\n        @foreach (var item in Model)\n            {\n            &lt;li&gt;@Html.ActionLink(item.Slug, &quot;Index&quot;, &quot;Post&quot;, new { slug = item.Slug })&lt;/li&gt;\n        }\n    &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre>\n<p>I want get into any detail what this does, but the hard part is where to put it. To get this to work it must be in the folder <code>Views\\Shared\\Components\\PostList\\</code> and the file must be called <code>Default.cshtml</code>. The important part is from <code>Components</code> and down, the first part just scope where it can be used. Since I want this to be used from all over the site I put it in <code>Shared</code>.</p>\n<p>With these two files added my solution look like this:</p>\n<p><img src=\"/content/images/2015/05/ViewComponent.PNG\" alt=\"View component solution structure\"></p>\n<h3 id=\"usingtheviewcomponent\">Using the view component</h3>\n<p>To use this in any view all I have to do is add this line:</p>\n<pre><code>@Component.Invoke(&quot;PostList&quot;, &quot;Blog posts&quot;)        \n</code></pre>\n<p>So my new <code>Home\\Index.cshtml</code> container part now looks like this:</p>\n<pre><code>&lt;div class=&quot;container&quot;&gt;\n    &lt;h1&gt;This is my new blog&lt;/h1&gt;\n    Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\n\n    @using (Html.BeginForm(&quot;Create&quot;, &quot;Post&quot;, FormMethod.Post))\n    {\n        &lt;div class=&quot;form-group&quot;&gt;\n            &lt;label for=&quot;slug&quot;&gt;Slug&lt;/label&gt;\n            &lt;input type=&quot;text&quot; name=&quot;slug&quot; id=&quot;name&quot; value=&quot;&quot; class=&quot;form-control&quot; /&gt;\n        &lt;/div&gt;\n        &lt;div class=&quot;form-group&quot;&gt;\n            &lt;label for=&quot;content&quot;&gt;Content&lt;/label&gt;\n            &lt;textarea id=&quot;content&quot; name=&quot;content&quot; class=&quot;form-control&quot;&gt;&lt;/textarea&gt;\n        &lt;/div&gt;\n        &lt;input type=&quot;submit&quot; value=&quot;Save&quot; class=&quot;btn btn-default&quot; /&gt;\n    }\n    @Component.Invoke(&quot;PostList&quot;, &quot;Blog posts&quot;)        \n&lt;/div&gt;\n</code></pre>\n<p>and my new <code>Post\\Index.cshtml</code> looks like this:</p>\n<pre><code>&lt;div class=&quot;container&quot;&gt;\n    &lt;h1&gt;@Model.Slug&lt;/h1&gt;\n    &lt;div&gt;@Model.Content&lt;/div&gt;\n    @Component.Invoke(&quot;PostList&quot;, &quot;Other blog posts&quot;)\n&lt;/div&gt;\n</code></pre>\n<p>That's all there is to it.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This is something I really missed in regular partial views and something that I think will be really useful creating more simple components that is reusable across a site. I look forward to try out this feature in a larger project and so how well it fits in, but so far I think it is really promising.</p>\n<!--kg-card-end: markdown-->","comment_id":"51","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nWhat is view components?\nPartial views in MVC has one limitation, which is that they doesn't have a\ncontroller for them. Not having controller makes it hard to have more complex\nlogic associated with the view. View components changes that. Each view\ncomponent consist of a view and a backing class, it's not a controller but\nalmost. In this post we will take the post list which we created by injecting\nthe \"repository\" into the index view in the last post\n[http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/] and\ncreate a reuseable view component instead which we can use in multiple views.\n\nCreating the view component\nThe purpose of this view component is to list all the posts for this simple\nblog, making it much easier to reuse in multiple views. An alternative would\nhave been to use a partial view, but using a partial view would have forced us\nto get the data for the partial view in the controller for each of the views\nwhere we used the partial, but since a view component has a backing class that\nis not necessary.\n\nThe ViewComponent class\nCreate a folder called ViewComponents where you can have all your view\ncomponents and add a PostListViewComponent class in the folder. The\nimplementation of the class should look like this:\n\npublic class PostListViewComponent : ViewComponent\n{\n    private readonly Data _data;\n\n    public PostListViewComponent(Data data)\n    {\n        _data = data;\n    }\n\n    public IViewComponentResult Invoke(string title)\n    {\n        ViewBag.Title = title;\n        return View(_data.GetPosts());\n    }\n}\n\n\nWe are inheriting from the class ViewComponent to get some extra help, like the \nView method in the base class. The Invoke method is what will be called from the\nviews that adds this view component and it is returning a simple view that show\nthe posts. We are also injecting the Data class in the constructor which makes\nit possible to get the data we need in this class instead of it being sent to it\nlike you do with a partial view. The view we return has a ViewBag as a regular\nview which we take advantage of, and the Invoke method can also have arguments\nwhich you can pass to the view component when you use it.\n\nThe View\nThe view is really straightforward:\n\n@model IEnumerable<OneManBlog.Model.PostModel>\n<div>\n    <h3>@ViewBag.Title</h3>\n    <ul>\n        @foreach (var item in Model)\n            {\n            <li>@Html.ActionLink(item.Slug, \"Index\", \"Post\", new { slug = item.Slug })</li>\n        }\n    </ul>\n</div>\n\n\nI want get into any detail what this does, but the hard part is where to put it.\nTo get this to work it must be in the folder Views\\Shared\\Components\\PostList\\ \nand the file must be called Default.cshtml. The important part is from \nComponents and down, the first part just scope where it can be used. Since I\nwant this to be used from all over the site I put it in Shared.\n\nWith these two files added my solution look like this:\n\n\n\nUsing the view component\nTo use this in any view all I have to do is add this line:\n\n@Component.Invoke(\"PostList\", \"Blog posts\")        \n\n\nSo my new Home\\Index.cshtml container part now looks like this:\n\n<div class=\"container\">\n    <h1>This is my new blog</h1>\n    Hello from MVC NNUG! asdad sasdasdasd adasd asd sadsa\n\n    @using (Html.BeginForm(\"Create\", \"Post\", FormMethod.Post))\n    {\n        <div class=\"form-group\">\n            <label for=\"slug\">Slug</label>\n            <input type=\"text\" name=\"slug\" id=\"name\" value=\"\" class=\"form-control\" />\n        </div>\n        <div class=\"form-group\">\n            <label for=\"content\">Content</label>\n            <textarea id=\"content\" name=\"content\" class=\"form-control\"></textarea>\n        </div>\n        <input type=\"submit\" value=\"Save\" class=\"btn btn-default\" />\n    }\n    @Component.Invoke(\"PostList\", \"Blog posts\")        \n</div>\n\n\nand my new Post\\Index.cshtml looks like this:\n\n<div class=\"container\">\n    <h1>@Model.Slug</h1>\n    <div>@Model.Content</div>\n    @Component.Invoke(\"PostList\", \"Other blog posts\")\n</div>\n\n\nThat's all there is to it.\n\nSummary\nThis is something I really missed in regular partial views and something that I\nthink will be really useful creating more simple components that is reusable\nacross a site. I look forward to try out this feature in a larger project and so\nhow well it fits in, but so far I think it is really promising.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-09T20:03:17.000Z","updated_at":"2016-01-15T22:18:36.000Z","published_at":"2015-05-09T20:38:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe067","uuid":"15fc2ba5-08fa-485f-8693-c561c1940bb3","title":"ASP.NET 5: Self-hosting the application","slug":"asp-net-5-self-hosting-the-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## Setting up self-hosting\\n\\nIn ASP.NET 5 you are more in charge of how you host the application. So far we've been using IIS (or IIS Express), but I thought I would show how easy it is to add self-hosting to the application. Self-hosting is great if you want to run it as a service instead of in IIS.\\n\\n### Updating project.json\\n\\nThe first step is to add two more packages we need to add: \\n\\n * `Microsoft.AspNet.Hosting` - the hosting infrastructure\\n * `Microsoft.AspNet.Server.WebListener` - the self-hosted web server implementation for Windows\\n\\nThe `dependencies` node in the  `project.json` file should now look like: \\n\\n    \\\"dependencies\\\": {\\n        \\\"Microsoft.AspNet.Mvc\\\": \\\"6.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Server.IIS\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.StaticFiles\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Server.WebListener\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Hosting\\\": \\\"1.0.0-beta4\\\"\\n    },\\n\\nThe last thing we need to do is add a \\\"command\\\" to the `commands` node:\\n\\n    \\\"commands\\\": {\\n        \\\"web\\\": \\\"Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000\\\"\\n    },\\n\\nAfter you added that you will get a new menu item under the debug button called web:\\n\\n![Self-hosting menu item](/content/images/2015/05/Self-hosting.PNG)\\n\\nChoosing that menu item and start the application will start the self-hosted version instead of IIS Express.\\n\\n### DNVM, DNX and DNU\\n\\nIf you want to run this from the command line you first install `DNVM`. To install it you can follow the instructions here: https://github.com/aspnet/home#powershell. You use `DNVM` to manage which version of `DNX` and `DNU` you want to use. After `DNVM` you can run a command like:\\n\\n    dnvm use 1.0.0-beta4 -r coreclr -arch x64\\n\\nThis will select version 1.0.0-beta4 and the core runtime. When we have selected the version you can now use `DNX` to run the application from the command line with the following command: \\n\\n    dnx . web\\n\\nNote that the command above only works is you are in the same folder as the `project.json` file and that `web` must match the name of the command in the `project.json` file. If you need to restore the packages you can run `DNU` before `DNX`:\\n\\n    dnu restore\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"settingupselfhosting\">Setting up self-hosting</h2>\n<p>In ASP.NET 5 you are more in charge of how you host the application. So far we've been using IIS (or IIS Express), but I thought I would show how easy it is to add self-hosting to the application. Self-hosting is great if you want to run it as a service instead of in IIS.</p>\n<h3 id=\"updatingprojectjson\">Updating project.json</h3>\n<p>The first step is to add two more packages we need to add:</p>\n<ul>\n<li><code>Microsoft.AspNet.Hosting</code> - the hosting infrastructure</li>\n<li><code>Microsoft.AspNet.Server.WebListener</code> - the self-hosted web server implementation for Windows</li>\n</ul>\n<p>The <code>dependencies</code> node in the  <code>project.json</code> file should now look like:</p>\n<pre><code>&quot;dependencies&quot;: {\n    &quot;Microsoft.AspNet.Mvc&quot;: &quot;6.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Server.IIS&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.StaticFiles&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Server.WebListener&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Hosting&quot;: &quot;1.0.0-beta4&quot;\n},\n</code></pre>\n<p>The last thing we need to do is add a &quot;command&quot; to the <code>commands</code> node:</p>\n<pre><code>&quot;commands&quot;: {\n    &quot;web&quot;: &quot;Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000&quot;\n},\n</code></pre>\n<p>After you added that you will get a new menu item under the debug button called web:</p>\n<p><img src=\"/content/images/2015/05/Self-hosting.PNG\" alt=\"Self-hosting menu item\"></p>\n<p>Choosing that menu item and start the application will start the self-hosted version instead of IIS Express.</p>\n<h3 id=\"dnvmdnxanddnu\">DNVM, DNX and DNU</h3>\n<p>If you want to run this from the command line you first install <code>DNVM</code>. To install it you can follow the instructions here: <a href=\"https://github.com/aspnet/home#powershell\">https://github.com/aspnet/home#powershell</a>. You use <code>DNVM</code> to manage which version of <code>DNX</code> and <code>DNU</code> you want to use. After <code>DNVM</code> you can run a command like:</p>\n<pre><code>dnvm use 1.0.0-beta4 -r coreclr -arch x64\n</code></pre>\n<p>This will select version 1.0.0-beta4 and the core runtime. When we have selected the version you can now use <code>DNX</code> to run the application from the command line with the following command:</p>\n<pre><code>dnx . web\n</code></pre>\n<p>Note that the command above only works is you are in the same folder as the <code>project.json</code> file and that <code>web</code> must match the name of the command in the <code>project.json</code> file. If you need to restore the packages you can run <code>DNU</code> before <code>DNX</code>:</p>\n<pre><code>dnu restore</code></pre>\n<!--kg-card-end: markdown-->","comment_id":"52","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nSetting up self-hosting\nIn ASP.NET 5 you are more in charge of how you host the application. So far\nwe've been using IIS (or IIS Express), but I thought I would show how easy it is\nto add self-hosting to the application. Self-hosting is great if you want to run\nit as a service instead of in IIS.\n\nUpdating project.json\nThe first step is to add two more packages we need to add:\n\n * Microsoft.AspNet.Hosting - the hosting infrastructure\n * Microsoft.AspNet.Server.WebListener - the self-hosted web server\n   implementation for Windows\n\nThe dependencies node in the project.json file should now look like:\n\n\"dependencies\": {\n    \"Microsoft.AspNet.Mvc\": \"6.0.0-beta4\",\n    \"Microsoft.AspNet.Server.IIS\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.StaticFiles\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.Server.WebListener\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.Hosting\": \"1.0.0-beta4\"\n},\n\n\nThe last thing we need to do is add a \"command\" to the commands node:\n\n\"commands\": {\n    \"web\": \"Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000\"\n},\n\n\nAfter you added that you will get a new menu item under the debug button called\nweb:\n\n\n\nChoosing that menu item and start the application will start the self-hosted\nversion instead of IIS Express.\n\nDNVM, DNX and DNU\nIf you want to run this from the command line you first install DNVM. To install\nit you can follow the instructions here: \nhttps://github.com/aspnet/home#powershell. You use DNVM to manage which version\nof DNX and DNU you want to use. After DNVM you can run a command like:\n\ndnvm use 1.0.0-beta4 -r coreclr -arch x64\n\n\nThis will select version 1.0.0-beta4 and the core runtime. When we have selected\nthe version you can now use DNX to run the application from the command line\nwith the following command:\n\ndnx . web\n\n\nNote that the command above only works is you are in the same folder as the \nproject.json file and that web must match the name of the command in the \nproject.json file. If you need to restore the packages you can run DNU before \nDNX:\n\ndnu restore","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-14T17:30:59.000Z","updated_at":"2016-01-15T22:18:18.000Z","published_at":"2015-05-14T18:12:01.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe068","uuid":"518e2d16-1d25-40e5-ad99-320b64384a3e","title":"ASP.NET 5: Hosting your application in Docker","slug":"asp-net-5-hosting-your-application-in-docker","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.\\n\\nPost in this serie:\\n\\n* [The pipeline](http://blog.tomasjansson.com/asp-net-5-the-pipeline/)\\n* [Adding MVC to an application](http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application)\\n* [Setting up frontend build (with grunt)](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/)\\n* [IoC and dependency injection](http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/)\\n* [View Components](http://blog.tomasjansson.com/asp-net-5-view-components/)\\n* [Self-hosting the application](http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/)\\n* [Hosting your application in docker](http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/)\\n\\nSource code: https://github.com/mastoj/OneManBlog\\n\\n## The next logical step is container hosting\\n\\nMicrosoft stepping in and partnering up with [Docker](https://www.docker.com/) was great news for the future if you ask me. Hosting applications is a container is a nice mix between light weight hosting and isolation between the application. There exist some security issues with hosting in a container that a someone could take advantage of if the container is running on a host that is not isolated enough, but don't let us go into that discussion.\\n\\nThe goal of this post is to show you one way to host your ASP.NET 5 application inside a docker container. I will not go into details about how you should put this container into production, mainly because I need to figure out a nice way to do it myself first. \\n\\n## Docker Machine and Docker\\n\\nTo manage different \\\"hosting machines\\\" I use [Docker Machine](https://docs.docker.com/machine/), not because I have many machines but because it is easy to manage machines with Docker Machine. Installation is straightforward and the documentation is good so I want go into all details about Docker Machine, all we're going to do is create virtual machine that we can use to host our docker containers. To create a machine on VirtualBox you run the following command:\\n\\n    $ docker-machine create --driver virtualbox dev\\n\\nNote that it is recommended to use msysgit when running Docker Machine, on Windows I recommend using [ConEmu (install from Chocolatey)](https://chocolatey.org/packages/ConEmu) and run bash inside ConEmu to use Docker Machine and Docker. To target this machine and start using Docker against it you run the following command:\\n\\n    $ eval \\\"$(docker-machine env dev)\\\"\\n\\nThe last thing before we start defining our docker image is to find the IP of the current machine so we can test it later, execute the following and note down the IP (it is most likely 192.168.99.100):\\n\\n    $ docker-machine ip\\n\\n## Updating the project\\n\\nBefore we create the container image definition file we need to update the project so we can host it in a linux container. We need one more dependency in our `project.json` file and one more command so we can start the application: \\n\\n    \\\"dependencies\\\": {\\n        \\\"Microsoft.AspNet.Mvc\\\": \\\"6.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Server.IIS\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.StaticFiles\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Server.WebListener\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Microsoft.AspNet.Hosting\\\": \\\"1.0.0-beta4\\\",\\n        \\\"Kestrel\\\": \\\"1.0.0-beta4\\\"\\n    },\\n    \\\"commands\\\": {\\n        \\\"web\\\": \\\"Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000\\\",\\n        \\\"kestrel\\\": \\\"Microsoft.AspNet.Hosting --server Kestrel --server.urls http://localhost:5001\\\"\\n    },\\n\\nThe new part is the `Kestrel` package is a web server that runs on ASP.NET 5 applications on linux. To start a web server with `Kestrel` we add the `kestrel` command which we can use with dnx on linux.\\n\\n## Creating the Dockerfile\\n\\nA docker image is sort of starting point for a container. You define images with `Dockerfile`s, which you can base on other images so you get a hiearchy of images basically. A running instance of an image is what is called a container, so they are almost the same thing but one is passive and one is active. We are going to base our image on the official [ASP.NET image](https://registry.hub.docker.com/u/microsoft/aspnet/), but we will add node, grunt and bower so we can build the application when we create the image. The `Dockerfile` we have define look like:\\n\\n    FROM microsoft/aspnet:vs-1.0.0-beta4\\n\\n    COPY . /app\\n    WORKDIR /app\\n\\n    RUN apt-get update -y && apt-get install --no-install-recommends -y -q \\\\\\n        curl \\\\\\n        python \\\\\\n        build-essential \\\\\\n        git \\\\\\n        ca-certificates\\n\\n    RUN mkdir /nodejs && \\\\\\n        curl http://nodejs.org/dist/v0.10.33/node-v0.10.33-linux-x64.tar.gz | \\\\\\n        tar xvzf - -C /nodejs --strip-components=1\\n\\n    ENV PATH $PATH:/nodejs/bin\\n\\n    RUN npm install -g grunt-cli bower\\n\\n    RUN [\\\"dnu\\\", \\\"restore\\\"]\\n    RUN [\\\"npm\\\", \\\"install\\\", \\\".\\\"]\\n    RUN [\\\"grunt\\\", \\\"default\\\"]\\n\\n    EXPOSE 5001\\n\\n    ENTRYPOINT [\\\"dnx\\\", \\\"./\\\", \\\"kestrel\\\"]\\n\\nLet us go through this line by line (almost):\\n\\n* `COPY . /app`, I have put the `Dockerfile` in the same folder as the `project.json` file this command copies all the code on the host into the `/app` folder in the image.\\n* `WORKDIR /app`, just sets the current working directory for when we execute the rest of the commands.\\n* The next two `RUN` commands and the `ENV` command installs and add [nodejs](https://nodejs.org/) to the environment in the image.\\n* When we have `nodejs` installs we can install `grunt` and `bower` using `npm` with the `RUN` command in the image.\\n* `RUN [\\\"dnu\\\", \\\"restore\\\"]` installs all the packages.\\n* `RUN [\\\"npm\\\", \\\"install\\\", \\\".\\\"]` installs all the javascript packages.\\n* `RUN [\\\"grunt\\\", \\\"default\\\"]` execute the `grunt` build steps that I wrote about [here](http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/).\\n* In our command we specified that we will use port 5001 for `kestrel`, so we use `EXPOSE 5001` to expose that port from the image when we run it.\\n* The last row, `ENTRYPOINT [\\\"dnx\\\", \\\"./\\\", \\\"kestrel\\\"]`, specifies that when we run this image (making it a container) we will execute `dnx` and in the current folder passing it the `kestrel` command.\\n\\nNo when we have the `Dockerfile` ready all we need to do is to create the image:\\n\\n    $ docker build -t onemanblog .\\n\\nand then start a container with:\\n\\n    $ docker run -i -t -p 5001:5001 onemanblog\\n\\nThe `-i` and `-t` flags make the container run in interactive mode, which means we see if it crashes and it also listens to `stdin`, that is, we can press enter to stop the application. The `-p 5001:5001` redirects the port 5001 from host to the container.\\n\\n## Summary\\n\\nIf you have followed all the steps above you should now have a running container which you can go to `http://<ip from \\\"docker-machine ip\\\">:5001/` and you should see the same application as we previously ran on Windows.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG Oslo. The presentation wasn't recorded so I thought I just write some blog posts about it insted. This will be a serie of posts where I plan to go through the features that I demonstrated during the presentation, plus some more features that I didn't have time to cover. I'll start with the basic and show one thing at a time and then add features as we go along. So let's get started.</p>\n<p>Post in this serie:</p>\n<ul>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-the-pipeline/\">The pipeline</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application\">Adding MVC to an application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">Setting up frontend build (with grunt)</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/\">IoC and dependency injection</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-view-components/\">View Components</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/\">Self-hosting the application</a></li>\n<li><a href=\"http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/\">Hosting your application in docker</a></li>\n</ul>\n<p>Source code: <a href=\"https://github.com/mastoj/OneManBlog\">https://github.com/mastoj/OneManBlog</a></p>\n<h2 id=\"thenextlogicalstepiscontainerhosting\">The next logical step is container hosting</h2>\n<p>Microsoft stepping in and partnering up with <a href=\"https://www.docker.com/\">Docker</a> was great news for the future if you ask me. Hosting applications is a container is a nice mix between light weight hosting and isolation between the application. There exist some security issues with hosting in a container that a someone could take advantage of if the container is running on a host that is not isolated enough, but don't let us go into that discussion.</p>\n<p>The goal of this post is to show you one way to host your ASP.NET 5 application inside a docker container. I will not go into details about how you should put this container into production, mainly because I need to figure out a nice way to do it myself first.</p>\n<h2 id=\"dockermachineanddocker\">Docker Machine and Docker</h2>\n<p>To manage different &quot;hosting machines&quot; I use <a href=\"https://docs.docker.com/machine/\">Docker Machine</a>, not because I have many machines but because it is easy to manage machines with Docker Machine. Installation is straightforward and the documentation is good so I want go into all details about Docker Machine, all we're going to do is create virtual machine that we can use to host our docker containers. To create a machine on VirtualBox you run the following command:</p>\n<pre><code>$ docker-machine create --driver virtualbox dev\n</code></pre>\n<p>Note that it is recommended to use msysgit when running Docker Machine, on Windows I recommend using <a href=\"https://chocolatey.org/packages/ConEmu\">ConEmu (install from Chocolatey)</a> and run bash inside ConEmu to use Docker Machine and Docker. To target this machine and start using Docker against it you run the following command:</p>\n<pre><code>$ eval &quot;$(docker-machine env dev)&quot;\n</code></pre>\n<p>The last thing before we start defining our docker image is to find the IP of the current machine so we can test it later, execute the following and note down the IP (it is most likely 192.168.99.100):</p>\n<pre><code>$ docker-machine ip\n</code></pre>\n<h2 id=\"updatingtheproject\">Updating the project</h2>\n<p>Before we create the container image definition file we need to update the project so we can host it in a linux container. We need one more dependency in our <code>project.json</code> file and one more command so we can start the application:</p>\n<pre><code>&quot;dependencies&quot;: {\n    &quot;Microsoft.AspNet.Mvc&quot;: &quot;6.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Server.IIS&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.StaticFiles&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Server.WebListener&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Microsoft.AspNet.Hosting&quot;: &quot;1.0.0-beta4&quot;,\n    &quot;Kestrel&quot;: &quot;1.0.0-beta4&quot;\n},\n&quot;commands&quot;: {\n    &quot;web&quot;: &quot;Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000&quot;,\n    &quot;kestrel&quot;: &quot;Microsoft.AspNet.Hosting --server Kestrel --server.urls http://localhost:5001&quot;\n},\n</code></pre>\n<p>The new part is the <code>Kestrel</code> package is a web server that runs on ASP.NET 5 applications on linux. To start a web server with <code>Kestrel</code> we add the <code>kestrel</code> command which we can use with dnx on linux.</p>\n<h2 id=\"creatingthedockerfile\">Creating the Dockerfile</h2>\n<p>A docker image is sort of starting point for a container. You define images with <code>Dockerfile</code>s, which you can base on other images so you get a hiearchy of images basically. A running instance of an image is what is called a container, so they are almost the same thing but one is passive and one is active. We are going to base our image on the official <a href=\"https://registry.hub.docker.com/u/microsoft/aspnet/\">ASP.NET image</a>, but we will add node, grunt and bower so we can build the application when we create the image. The <code>Dockerfile</code> we have define look like:</p>\n<pre><code>FROM microsoft/aspnet:vs-1.0.0-beta4\n\nCOPY . /app\nWORKDIR /app\n\nRUN apt-get update -y &amp;&amp; apt-get install --no-install-recommends -y -q \\\n    curl \\\n    python \\\n    build-essential \\\n    git \\\n    ca-certificates\n\nRUN mkdir /nodejs &amp;&amp; \\\n    curl http://nodejs.org/dist/v0.10.33/node-v0.10.33-linux-x64.tar.gz | \\\n    tar xvzf - -C /nodejs --strip-components=1\n\nENV PATH $PATH:/nodejs/bin\n\nRUN npm install -g grunt-cli bower\n\nRUN [&quot;dnu&quot;, &quot;restore&quot;]\nRUN [&quot;npm&quot;, &quot;install&quot;, &quot;.&quot;]\nRUN [&quot;grunt&quot;, &quot;default&quot;]\n\nEXPOSE 5001\n\nENTRYPOINT [&quot;dnx&quot;, &quot;./&quot;, &quot;kestrel&quot;]\n</code></pre>\n<p>Let us go through this line by line (almost):</p>\n<ul>\n<li><code>COPY . /app</code>, I have put the <code>Dockerfile</code> in the same folder as the <code>project.json</code> file this command copies all the code on the host into the <code>/app</code> folder in the image.</li>\n<li><code>WORKDIR /app</code>, just sets the current working directory for when we execute the rest of the commands.</li>\n<li>The next two <code>RUN</code> commands and the <code>ENV</code> command installs and add <a href=\"https://nodejs.org/\">nodejs</a> to the environment in the image.</li>\n<li>When we have <code>nodejs</code> installs we can install <code>grunt</code> and <code>bower</code> using <code>npm</code> with the <code>RUN</code> command in the image.</li>\n<li><code>RUN [&quot;dnu&quot;, &quot;restore&quot;]</code> installs all the packages.</li>\n<li><code>RUN [&quot;npm&quot;, &quot;install&quot;, &quot;.&quot;]</code> installs all the javascript packages.</li>\n<li><code>RUN [&quot;grunt&quot;, &quot;default&quot;]</code> execute the <code>grunt</code> build steps that I wrote about <a href=\"http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/\">here</a>.</li>\n<li>In our command we specified that we will use port 5001 for <code>kestrel</code>, so we use <code>EXPOSE 5001</code> to expose that port from the image when we run it.</li>\n<li>The last row, <code>ENTRYPOINT [&quot;dnx&quot;, &quot;./&quot;, &quot;kestrel&quot;]</code>, specifies that when we run this image (making it a container) we will execute <code>dnx</code> and in the current folder passing it the <code>kestrel</code> command.</li>\n</ul>\n<p>No when we have the <code>Dockerfile</code> ready all we need to do is to create the image:</p>\n<pre><code>$ docker build -t onemanblog .\n</code></pre>\n<p>and then start a container with:</p>\n<pre><code>$ docker run -i -t -p 5001:5001 onemanblog\n</code></pre>\n<p>The <code>-i</code> and <code>-t</code> flags make the container run in interactive mode, which means we see if it crashes and it also listens to <code>stdin</code>, that is, we can press enter to stop the application. The <code>-p 5001:5001</code> redirects the port 5001 from host to the container.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>If you have followed all the steps above you should now have a running container which you can go to <code>http://&lt;ip from &quot;docker-machine ip&quot;&gt;:5001/</code> and you should see the same application as we previously ran on Windows.</p>\n<!--kg-card-end: markdown-->","comment_id":"53","plaintext":"A couple of weeks ago I had a presentation about ASP.NET 5 and MVC 6 at NNUG\nOslo. The presentation wasn't recorded so I thought I just write some blog posts\nabout it insted. This will be a serie of posts where I plan to go through the\nfeatures that I demonstrated during the presentation, plus some more features\nthat I didn't have time to cover. I'll start with the basic and show one thing\nat a time and then add features as we go along. So let's get started.\n\nPost in this serie:\n\n * The pipeline [http://blog.tomasjansson.com/asp-net-5-the-pipeline/]\n * Adding MVC to an application\n   [http://blog.tomasjansson.com/asp-net-5-adding-mvc-to-an-application]\n * Setting up frontend build (with grunt)\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n * IoC and dependency injection\n   [http://blog.tomasjansson.com/asp-net-5-ioc-and-dependency-injection/]\n * View Components [http://blog.tomasjansson.com/asp-net-5-view-components/]\n * Self-hosting the application\n   [http://blog.tomasjansson.com/asp-net-5-self-hosting-the-application/]\n * Hosting your application in docker\n   [http://blog.tomasjansson.com/asp-net-5-hosting-your-application-in-docker/]\n\nSource code: https://github.com/mastoj/OneManBlog\n\nThe next logical step is container hosting\nMicrosoft stepping in and partnering up with Docker [https://www.docker.com/] \nwas great news for the future if you ask me. Hosting applications is a container\nis a nice mix between light weight hosting and isolation between the\napplication. There exist some security issues with hosting in a container that a\nsomeone could take advantage of if the container is running on a host that is\nnot isolated enough, but don't let us go into that discussion.\n\nThe goal of this post is to show you one way to host your ASP.NET 5 application\ninside a docker container. I will not go into details about how you should put\nthis container into production, mainly because I need to figure out a nice way\nto do it myself first.\n\nDocker Machine and Docker\nTo manage different \"hosting machines\" I use Docker Machine\n[https://docs.docker.com/machine/], not because I have many machines but because\nit is easy to manage machines with Docker Machine. Installation is\nstraightforward and the documentation is good so I want go into all details\nabout Docker Machine, all we're going to do is create virtual machine that we\ncan use to host our docker containers. To create a machine on VirtualBox you run\nthe following command:\n\n$ docker-machine create --driver virtualbox dev\n\n\nNote that it is recommended to use msysgit when running Docker Machine, on\nWindows I recommend using ConEmu (install from Chocolatey)\n[https://chocolatey.org/packages/ConEmu] and run bash inside ConEmu to use\nDocker Machine and Docker. To target this machine and start using Docker against\nit you run the following command:\n\n$ eval \"$(docker-machine env dev)\"\n\n\nThe last thing before we start defining our docker image is to find the IP of\nthe current machine so we can test it later, execute the following and note down\nthe IP (it is most likely 192.168.99.100):\n\n$ docker-machine ip\n\n\nUpdating the project\nBefore we create the container image definition file we need to update the\nproject so we can host it in a linux container. We need one more dependency in\nour project.json file and one more command so we can start the application:\n\n\"dependencies\": {\n    \"Microsoft.AspNet.Mvc\": \"6.0.0-beta4\",\n    \"Microsoft.AspNet.Server.IIS\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.StaticFiles\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.Server.WebListener\": \"1.0.0-beta4\",\n    \"Microsoft.AspNet.Hosting\": \"1.0.0-beta4\",\n    \"Kestrel\": \"1.0.0-beta4\"\n},\n\"commands\": {\n    \"web\": \"Microsoft.AspNet.Hosting --server Microsoft.AspNet.Server.WebListener --server.urls http://localhost:5000\",\n    \"kestrel\": \"Microsoft.AspNet.Hosting --server Kestrel --server.urls http://localhost:5001\"\n},\n\n\nThe new part is the Kestrel package is a web server that runs on ASP.NET 5\napplications on linux. To start a web server with Kestrel we add the kestrel \ncommand which we can use with dnx on linux.\n\nCreating the Dockerfile\nA docker image is sort of starting point for a container. You define images with \nDockerfiles, which you can base on other images so you get a hiearchy of images\nbasically. A running instance of an image is what is called a container, so they\nare almost the same thing but one is passive and one is active. We are going to\nbase our image on the official ASP.NET image\n[https://registry.hub.docker.com/u/microsoft/aspnet/], but we will add node,\ngrunt and bower so we can build the application when we create the image. The \nDockerfile we have define look like:\n\nFROM microsoft/aspnet:vs-1.0.0-beta4\n\nCOPY . /app\nWORKDIR /app\n\nRUN apt-get update -y && apt-get install --no-install-recommends -y -q \\\n    curl \\\n    python \\\n    build-essential \\\n    git \\\n    ca-certificates\n\nRUN mkdir /nodejs && \\\n    curl http://nodejs.org/dist/v0.10.33/node-v0.10.33-linux-x64.tar.gz | \\\n    tar xvzf - -C /nodejs --strip-components=1\n\nENV PATH $PATH:/nodejs/bin\n\nRUN npm install -g grunt-cli bower\n\nRUN [\"dnu\", \"restore\"]\nRUN [\"npm\", \"install\", \".\"]\nRUN [\"grunt\", \"default\"]\n\nEXPOSE 5001\n\nENTRYPOINT [\"dnx\", \"./\", \"kestrel\"]\n\n\nLet us go through this line by line (almost):\n\n * COPY . /app, I have put the Dockerfile in the same folder as the project.json \n   file this command copies all the code on the host into the /app folder in the\n   image.\n * WORKDIR /app, just sets the current working directory for when we execute the\n   rest of the commands.\n * The next two RUN commands and the ENV command installs and add nodejs\n   [https://nodejs.org/] to the environment in the image.\n * When we have nodejs installs we can install grunt and bower using npm with\n   the RUN command in the image.\n * RUN [\"dnu\", \"restore\"] installs all the packages.\n * RUN [\"npm\", \"install\", \".\"] installs all the javascript packages.\n * RUN [\"grunt\", \"default\"] execute the grunt build steps that I wrote about \n   here\n   [http://blog.tomasjansson.com/asp-net-5-setting-up-frontend-build-with-grunt/]\n   .\n * In our command we specified that we will use port 5001 for kestrel, so we use \n   EXPOSE 5001 to expose that port from the image when we run it.\n * The last row, ENTRYPOINT [\"dnx\", \"./\", \"kestrel\"], specifies that when we run\n   this image (making it a container) we will execute dnx and in the current\n   folder passing it the kestrel command.\n\nNo when we have the Dockerfile ready all we need to do is to create the image:\n\n$ docker build -t onemanblog .\n\n\nand then start a container with:\n\n$ docker run -i -t -p 5001:5001 onemanblog\n\n\nThe -i and -t flags make the container run in interactive mode, which means we\nsee if it crashes and it also listens to stdin, that is, we can press enter to\nstop the application. The -p 5001:5001 redirects the port 5001 from host to the\ncontainer.\n\nSummary\nIf you have followed all the steps above you should now have a running container\nwhich you can go to http://<ip from \"docker-machine ip\">:5001/ and you should\nsee the same application as we previously ran on Windows.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-18T15:50:13.000Z","updated_at":"2016-01-15T22:18:01.000Z","published_at":"2015-05-18T18:46:57.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe069","uuid":"42b57d3a-9f89-4188-90ec-368fccf8bed1","title":"NDC Oslo 2015 wrap up","slug":"ndc-oslo-2015-wrap-up","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"At NDC Oslo 2015 I had the opportunity to speak, and it was a fun experience. My talk, F# as our day job by 2016, was part of the functional track and it was a popular track. I had well over 100 people in the audience for my talk even though it was the second last talk of the conference, and it is really nice to see a growing interest in the functional programming world. I stayed most of the time in the functional track room, and here are some of my takeaways.\\n\\n## We are becoming more functional!\\n\\nI think this is the third or fourth time there is a functional track at NDC, if you count NDC London as well, and I have never seen this much people for the tracks as I did this year. It started of great with a packed room for [Bryan Hunter's](https://twitter.com/bryan_hunter) \\\"Lean and Functional Programming\\\" talk, a talk I really enjoyed. Even though the language you use is not the biggest problem, but it is one way that you can improve yourself, and choosing functional will most likely help with that. \\n\\nAnother talk that I really like was [Yan Cui's](https://twitter.com/theburningmonk) \\\"A tour of the language landscape\\\". Important parts here was that you need to put in the hours, and it is enough with 20 hours dedicated learning, to get of a good start. Of course 20 hours want make you a master, but after that you will most likely be able to start producing.\\n\\nIf you want to see a general talk about why should start doing functional I recommend [Venkat Subramaniam's](https://twitter.com/venkat_s) \\\"Learning from Haskell\\\". He explained to us things like \\\"C++ is not strongly typed, it is sadly typed\\\", and why that is a problem. If you have a good strongly typed functional language with great type inference it will most likely feel as you are in a dynamic language, and this is the nature of languages like Haskell and also F#.\\n\\nIf you are a C# developer and want is interested in F# I do recommend the talk by [Phillip Trelford](https://twitter.com/ptrelford) (or Sean's dad) where he showed \\\"F# for C# Developers\\\". I can also pitch my own talk \\\"F# as our day job by 2016\\\" where I try to show some arguments to why I like F# more than C# and also some facts to back that up. That might be a good resource if you want to start using F# in your project.\\n\\nIf you're more into functional design I do recommend [Scott Wlaschin's](https://twitter.com/ScottWlaschin) \\\"Enterprise Tic-Tac-Toc -- A functional approach\\\", which was entertaining and a lot of useful information, as always when Scott presents). The \\\"Type-Driven Development\\\" by [Mark Seeman](https://twitter.com/ploeh) i also a great talk for this topic.\\n\\n\\n## What will I try after the conference\\n\\nEven though I have done some F# I haven't tried idiomatic web development with F# and that is something I definitely will try out after I saw [Tomas Petricek](https://twitter.com/tomaspetricek) demoing [suave](http://suave.io/) on stage. He basically implemented two applications and also deployed one of the to azure and heroku in about 45 minutes. Of course he had some small things prepared for building, but he basically implemented the whole backend in that time.\\n\\nThe next thing I'll looking at is [elixir](http://elixir-lang.org/) and [Phoenix](http://www.phoenixframework.org/). The creator of elixir, [José Valim](https://twitter.com/josevalim), was at the conference and had a great talk about idioms for building distributed applications in elixir. The main reasons that is possible is because of the erlang vm which you use to run elixir. Elixir supposed to have nicer syntax and a great metaprogramming model to extend the language for your domain. The metaprogramming model was something [Chris McCord](https://twitter.com/chris_mccord), the author of Phoenix, demonstrated in one of his great technical talks. The other talk he had was an introduction to Phoenix which I also recommend.\\n\\n## Don't wait for the community, be the community!\\n\\nMy talk was almost the last talk of the conference it was supposed to be a \\\"call to action\\\" talk for functional developers in Norway. Now is the time to start doing functional programming if you haven't done it before. Of course it might slow you down for a period of time, but thinking like that will not make you in the long run. Improvement doesn't happen over night, you have to practice it and you will see that the new knowledge you get will arm you with new ways of thinking about problem.\\n\\nIf you don't have a community were you live, don't let that stop you instead you should **be the community!**. There are ton of material online and I promise you that there are other people where you live that are interested in functional programming in general and specifically F#. If it is a F# community you want to grow you can always reach out to the people at the [F# foundation](http://fsharp.org/) or people on twitter and you will get help.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>At NDC Oslo 2015 I had the opportunity to speak, and it was a fun experience. My talk, F# as our day job by 2016, was part of the functional track and it was a popular track. I had well over 100 people in the audience for my talk even though it was the second last talk of the conference, and it is really nice to see a growing interest in the functional programming world. I stayed most of the time in the functional track room, and here are some of my takeaways.</p>\n<h2 id=\"wearebecomingmorefunctional\">We are becoming more functional!</h2>\n<p>I think this is the third or fourth time there is a functional track at NDC, if you count NDC London as well, and I have never seen this much people for the tracks as I did this year. It started of great with a packed room for <a href=\"https://twitter.com/bryan_hunter\">Bryan Hunter's</a> &quot;Lean and Functional Programming&quot; talk, a talk I really enjoyed. Even though the language you use is not the biggest problem, but it is one way that you can improve yourself, and choosing functional will most likely help with that.</p>\n<p>Another talk that I really like was <a href=\"https://twitter.com/theburningmonk\">Yan Cui's</a> &quot;A tour of the language landscape&quot;. Important parts here was that you need to put in the hours, and it is enough with 20 hours dedicated learning, to get of a good start. Of course 20 hours want make you a master, but after that you will most likely be able to start producing.</p>\n<p>If you want to see a general talk about why should start doing functional I recommend <a href=\"https://twitter.com/venkat_s\">Venkat Subramaniam's</a> &quot;Learning from Haskell&quot;. He explained to us things like &quot;C++ is not strongly typed, it is sadly typed&quot;, and why that is a problem. If you have a good strongly typed functional language with great type inference it will most likely feel as you are in a dynamic language, and this is the nature of languages like Haskell and also F#.</p>\n<p>If you are a C# developer and want is interested in F# I do recommend the talk by <a href=\"https://twitter.com/ptrelford\">Phillip Trelford</a> (or Sean's dad) where he showed &quot;F# for C# Developers&quot;. I can also pitch my own talk &quot;F# as our day job by 2016&quot; where I try to show some arguments to why I like F# more than C# and also some facts to back that up. That might be a good resource if you want to start using F# in your project.</p>\n<p>If you're more into functional design I do recommend <a href=\"https://twitter.com/ScottWlaschin\">Scott Wlaschin's</a> &quot;Enterprise Tic-Tac-Toc -- A functional approach&quot;, which was entertaining and a lot of useful information, as always when Scott presents). The &quot;Type-Driven Development&quot; by <a href=\"https://twitter.com/ploeh\">Mark Seeman</a> i also a great talk for this topic.</p>\n<h2 id=\"whatwillitryaftertheconference\">What will I try after the conference</h2>\n<p>Even though I have done some F# I haven't tried idiomatic web development with F# and that is something I definitely will try out after I saw <a href=\"https://twitter.com/tomaspetricek\">Tomas Petricek</a> demoing <a href=\"http://suave.io/\">suave</a> on stage. He basically implemented two applications and also deployed one of the to azure and heroku in about 45 minutes. Of course he had some small things prepared for building, but he basically implemented the whole backend in that time.</p>\n<p>The next thing I'll looking at is <a href=\"http://elixir-lang.org/\">elixir</a> and <a href=\"http://www.phoenixframework.org/\">Phoenix</a>. The creator of elixir, <a href=\"https://twitter.com/josevalim\">José Valim</a>, was at the conference and had a great talk about idioms for building distributed applications in elixir. The main reasons that is possible is because of the erlang vm which you use to run elixir. Elixir supposed to have nicer syntax and a great metaprogramming model to extend the language for your domain. The metaprogramming model was something <a href=\"https://twitter.com/chris_mccord\">Chris McCord</a>, the author of Phoenix, demonstrated in one of his great technical talks. The other talk he had was an introduction to Phoenix which I also recommend.</p>\n<h2 id=\"dontwaitforthecommunitybethecommunity\">Don't wait for the community, be the community!</h2>\n<p>My talk was almost the last talk of the conference it was supposed to be a &quot;call to action&quot; talk for functional developers in Norway. Now is the time to start doing functional programming if you haven't done it before. Of course it might slow you down for a period of time, but thinking like that will not make you in the long run. Improvement doesn't happen over night, you have to practice it and you will see that the new knowledge you get will arm you with new ways of thinking about problem.</p>\n<p>If you don't have a community were you live, don't let that stop you instead you should <strong>be the community!</strong>. There are ton of material online and I promise you that there are other people where you live that are interested in functional programming in general and specifically F#. If it is a F# community you want to grow you can always reach out to the people at the <a href=\"http://fsharp.org/\">F# foundation</a> or people on twitter and you will get help.</p>\n<!--kg-card-end: markdown-->","comment_id":"54","plaintext":"At NDC Oslo 2015 I had the opportunity to speak, and it was a fun experience. My\ntalk, F# as our day job by 2016, was part of the functional track and it was a\npopular track. I had well over 100 people in the audience for my talk even\nthough it was the second last talk of the conference, and it is really nice to\nsee a growing interest in the functional programming world. I stayed most of the\ntime in the functional track room, and here are some of my takeaways.\n\nWe are becoming more functional!\nI think this is the third or fourth time there is a functional track at NDC, if\nyou count NDC London as well, and I have never seen this much people for the\ntracks as I did this year. It started of great with a packed room for Bryan\nHunter's [https://twitter.com/bryan_hunter] \"Lean and Functional Programming\"\ntalk, a talk I really enjoyed. Even though the language you use is not the\nbiggest problem, but it is one way that you can improve yourself, and choosing\nfunctional will most likely help with that.\n\nAnother talk that I really like was Yan Cui's\n[https://twitter.com/theburningmonk] \"A tour of the language landscape\".\nImportant parts here was that you need to put in the hours, and it is enough\nwith 20 hours dedicated learning, to get of a good start. Of course 20 hours\nwant make you a master, but after that you will most likely be able to start\nproducing.\n\nIf you want to see a general talk about why should start doing functional I\nrecommend Venkat Subramaniam's [https://twitter.com/venkat_s] \"Learning from\nHaskell\". He explained to us things like \"C++ is not strongly typed, it is sadly\ntyped\", and why that is a problem. If you have a good strongly typed functional\nlanguage with great type inference it will most likely feel as you are in a\ndynamic language, and this is the nature of languages like Haskell and also F#.\n\nIf you are a C# developer and want is interested in F# I do recommend the talk\nby Phillip Trelford [https://twitter.com/ptrelford] (or Sean's dad) where he\nshowed \"F# for C# Developers\". I can also pitch my own talk \"F# as our day job\nby 2016\" where I try to show some arguments to why I like F# more than C# and\nalso some facts to back that up. That might be a good resource if you want to\nstart using F# in your project.\n\nIf you're more into functional design I do recommend Scott Wlaschin's\n[https://twitter.com/ScottWlaschin] \"Enterprise Tic-Tac-Toc -- A functional\napproach\", which was entertaining and a lot of useful information, as always\nwhen Scott presents). The \"Type-Driven Development\" by Mark Seeman\n[https://twitter.com/ploeh] i also a great talk for this topic.\n\nWhat will I try after the conference\nEven though I have done some F# I haven't tried idiomatic web development with\nF# and that is something I definitely will try out after I saw Tomas Petricek\n[https://twitter.com/tomaspetricek] demoing suave [http://suave.io/] on stage.\nHe basically implemented two applications and also deployed one of the to azure\nand heroku in about 45 minutes. Of course he had some small things prepared for\nbuilding, but he basically implemented the whole backend in that time.\n\nThe next thing I'll looking at is elixir [http://elixir-lang.org/] and Phoenix\n[http://www.phoenixframework.org/]. The creator of elixir, José Valim\n[https://twitter.com/josevalim], was at the conference and had a great talk\nabout idioms for building distributed applications in elixir. The main reasons\nthat is possible is because of the erlang vm which you use to run elixir. Elixir\nsupposed to have nicer syntax and a great metaprogramming model to extend the\nlanguage for your domain. The metaprogramming model was something Chris McCord\n[https://twitter.com/chris_mccord], the author of Phoenix, demonstrated in one\nof his great technical talks. The other talk he had was an introduction to\nPhoenix which I also recommend.\n\nDon't wait for the community, be the community!\nMy talk was almost the last talk of the conference it was supposed to be a \"call\nto action\" talk for functional developers in Norway. Now is the time to start\ndoing functional programming if you haven't done it before. Of course it might\nslow you down for a period of time, but thinking like that will not make you in\nthe long run. Improvement doesn't happen over night, you have to practice it and\nyou will see that the new knowledge you get will arm you with new ways of\nthinking about problem.\n\nIf you don't have a community were you live, don't let that stop you instead you\nshould be the community!. There are ton of material online and I promise you\nthat there are other people where you live that are interested in functional\nprogramming in general and specifically F#. If it is a F# community you want to\ngrow you can always reach out to the people at the F# foundation\n[http://fsharp.org/] or people on twitter and you will get help.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-20T11:24:23.000Z","updated_at":"2015-06-20T12:02:55.000Z","published_at":"2015-06-20T12:02:55.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe06a","uuid":"da456681-3121-4009-8054-51f86dc35965","title":"Topshelf F# api improved","slug":"topshelf-fharp-api-improved","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"My plan was never to improve the F# api for [Topshelf](http://topshelf-project.com/), [Topshelf.FSharp](https://github.com/haf/Topshelf.FSharp), but [Henrik Feldt](https://twitter.com/henrikfeldt) asked me to when he saw what I was doing when working with a [Suave](http://suave.io/), which Henrik is a core contributor of, demo where I host the application in `Topshelf`. I implemented a simple Topshelf wrapper that had a nice, at least I think so, fluent api. The `Topshelf.FSharp` also had a fluent api, but it was somewhat more complicated so what I was asked to do was implement my concept in the existing `Topshelf.FSharp` project, and so I did (released as version 2.0.1 of the package).\\n\\n## The API\\n\\nThe already existing API was good, but there some things that could be improved. The major plus with the existing function was that it already had existing functions for basically do every possible configuration of the service, so all I had to do was to find a nicer way to improve the fluent part of the API. The goal I had in mind, and what I implemented in my demo was an API looking somewhat like this:\\n\\n    Service.Default\\n    |> with_start start\\n    |> with_recovery (ServiceRecovery.Default |> restart (min 10))\\n    |> with_stop stop\\n    |> run\\n\\nIt's really easy to follow and true to F#. There are many more functions you could execute before run to configure the service, but the most important ones are `with_start`, `with_stop` and `run`. The start and stop functions just take a single functions which are executed on start and stop and returns a bool and the run function is what executes the service and returns an `int` as expected. I won't cover any more details, but just look at the github repo if you want to know what configuration functions there is.\\n\\n## Under the hood\\n\\nSo how do one build this type of API on top of another more OO oriented framework and one answer to this is to use a kind of builder pattern. All the functions before `run` function is executed just creates a specification of the service and what should happen when `run` is executed. The specification I ended up implementing for `Topshelf` look like this:\\n\\n    type Service =\\n      { Start: HostControl -> bool\\n        Stop: HostControl -> bool\\n        HostConfiguration: (HostConfigurator -> HostConfigurator) list }\\n      static member Default =\\n          { Start = (fun _ -> true)\\n            Stop = (fun _ -> true)\\n            HostConfiguration = [] }\\n\\nFor a service to work you need a start and stop function, and that is what `with_start` and `with_stop` do. All the functions, except from `run`, take the `Service` type as the last parameter as well as returning a new instance of a `Service` making it possible to pipe the specification between all the configuration steps. The static `Default` member makes it easy to start the configuration. To configure the service all the configuration functions add a function to the list of `HostConfiguration` describing what should be done when the service is instantiated. This is done by a base function `add_host_configuration_step` which all the configuration functions partially applies like below:\\n\\n    let add_host_configuration_step step service =\\n        {service with HostConfiguration = step::service.HostConfiguration}\\n\\n    let enable_pause_and_continue =\\n        add_host_configuration_step (fun c -> c.EnablePauseAndContinue();c)\\n\\nTo start the service there are a couple of things we need to do, but first the code and then the description of it\\n\\n    let toAction1 f = new Action<_>(f)\\n    let toFunc f = new Func<_>(f)\\n\\n    let service_control (start : HostControl -> bool) (stop : HostControl -> bool) () =\\n      { new ServiceControl with\\n        member x.Start hc =\\n          start hc\\n        member x.Stop hc =\\n          stop hc }\\n\\n    let create_service (hc:HostConfigurator) service_func =\\n      hc.Service<ServiceControl>(service_func |> toFunc) |> ignore\\n\\n    let run service =\\n      let hostFactoryConfigurator hc =\\n          let createdHc = service.HostConfiguration |> List.fold (fun chc x -> x chc) hc\\n          service_control service.Start service.Stop\\n          |> create_service createdHc\\n\\n      hostFactoryConfigurator |> toAction1 |> HostFactory.Run |> int\\n\\nFirst we need to figure out what the we need to run the service, and the `Topshelf` `HostFactory.Run` method takes an `Action<HostConfigurator>`. To create action from a F# lambda I have a simple helper function, `toAction1`. And to create the actual lambda that takes a `HostConfigurator` we just create a function that has a single parameter `let hostFactoryConfigurator hc`, and now we can send this function to `toAction1` and we have what we need to run the service. To run the actual configuration in the `hostFactoryConfigurator` we just left fold over the `HostConfiguration` list with the first `HostConfiguration` as initial state and apply the functions. When we've done that we can just create our `ServiceControl` from the final `HostConfigurator`, after left fold, and pass in the start and stop functions and we're done.\\n\\n## Improvements\\n\\nThe api can be improved a little bit, but now it works in a nice way. The major improvements that can be added is validation and create separate types for start and stop so we don't mix them. The validation wasn't in the API when I started implemented the change so I didn't add them now either and I don't think they are crucial either.\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>My plan was never to improve the F# api for <a href=\"http://topshelf-project.com/\">Topshelf</a>, <a href=\"https://github.com/haf/Topshelf.FSharp\">Topshelf.FSharp</a>, but <a href=\"https://twitter.com/henrikfeldt\">Henrik Feldt</a> asked me to when he saw what I was doing when working with a <a href=\"http://suave.io/\">Suave</a>, which Henrik is a core contributor of, demo where I host the application in <code>Topshelf</code>. I implemented a simple Topshelf wrapper that had a nice, at least I think so, fluent api. The <code>Topshelf.FSharp</code> also had a fluent api, but it was somewhat more complicated so what I was asked to do was implement my concept in the existing <code>Topshelf.FSharp</code> project, and so I did (released as version 2.0.1 of the package).</p>\n<h2 id=\"theapi\">The API</h2>\n<p>The already existing API was good, but there some things that could be improved. The major plus with the existing function was that it already had existing functions for basically do every possible configuration of the service, so all I had to do was to find a nicer way to improve the fluent part of the API. The goal I had in mind, and what I implemented in my demo was an API looking somewhat like this:</p>\n<pre><code>Service.Default\n|&gt; with_start start\n|&gt; with_recovery (ServiceRecovery.Default |&gt; restart (min 10))\n|&gt; with_stop stop\n|&gt; run\n</code></pre>\n<p>It's really easy to follow and true to F#. There are many more functions you could execute before run to configure the service, but the most important ones are <code>with_start</code>, <code>with_stop</code> and <code>run</code>. The start and stop functions just take a single functions which are executed on start and stop and returns a bool and the run function is what executes the service and returns an <code>int</code> as expected. I won't cover any more details, but just look at the github repo if you want to know what configuration functions there is.</p>\n<h2 id=\"underthehood\">Under the hood</h2>\n<p>So how do one build this type of API on top of another more OO oriented framework and one answer to this is to use a kind of builder pattern. All the functions before <code>run</code> function is executed just creates a specification of the service and what should happen when <code>run</code> is executed. The specification I ended up implementing for <code>Topshelf</code> look like this:</p>\n<pre><code>type Service =\n  { Start: HostControl -&gt; bool\n    Stop: HostControl -&gt; bool\n    HostConfiguration: (HostConfigurator -&gt; HostConfigurator) list }\n  static member Default =\n      { Start = (fun _ -&gt; true)\n        Stop = (fun _ -&gt; true)\n        HostConfiguration = [] }\n</code></pre>\n<p>For a service to work you need a start and stop function, and that is what <code>with_start</code> and <code>with_stop</code> do. All the functions, except from <code>run</code>, take the <code>Service</code> type as the last parameter as well as returning a new instance of a <code>Service</code> making it possible to pipe the specification between all the configuration steps. The static <code>Default</code> member makes it easy to start the configuration. To configure the service all the configuration functions add a function to the list of <code>HostConfiguration</code> describing what should be done when the service is instantiated. This is done by a base function <code>add_host_configuration_step</code> which all the configuration functions partially applies like below:</p>\n<pre><code>let add_host_configuration_step step service =\n    {service with HostConfiguration = step::service.HostConfiguration}\n\nlet enable_pause_and_continue =\n    add_host_configuration_step (fun c -&gt; c.EnablePauseAndContinue();c)\n</code></pre>\n<p>To start the service there are a couple of things we need to do, but first the code and then the description of it</p>\n<pre><code>let toAction1 f = new Action&lt;_&gt;(f)\nlet toFunc f = new Func&lt;_&gt;(f)\n\nlet service_control (start : HostControl -&gt; bool) (stop : HostControl -&gt; bool) () =\n  { new ServiceControl with\n    member x.Start hc =\n      start hc\n    member x.Stop hc =\n      stop hc }\n\nlet create_service (hc:HostConfigurator) service_func =\n  hc.Service&lt;ServiceControl&gt;(service_func |&gt; toFunc) |&gt; ignore\n\nlet run service =\n  let hostFactoryConfigurator hc =\n      let createdHc = service.HostConfiguration |&gt; List.fold (fun chc x -&gt; x chc) hc\n      service_control service.Start service.Stop\n      |&gt; create_service createdHc\n\n  hostFactoryConfigurator |&gt; toAction1 |&gt; HostFactory.Run |&gt; int\n</code></pre>\n<p>First we need to figure out what the we need to run the service, and the <code>Topshelf</code> <code>HostFactory.Run</code> method takes an <code>Action&lt;HostConfigurator&gt;</code>. To create action from a F# lambda I have a simple helper function, <code>toAction1</code>. And to create the actual lambda that takes a <code>HostConfigurator</code> we just create a function that has a single parameter <code>let hostFactoryConfigurator hc</code>, and now we can send this function to <code>toAction1</code> and we have what we need to run the service. To run the actual configuration in the <code>hostFactoryConfigurator</code> we just left fold over the <code>HostConfiguration</code> list with the first <code>HostConfiguration</code> as initial state and apply the functions. When we've done that we can just create our <code>ServiceControl</code> from the final <code>HostConfigurator</code>, after left fold, and pass in the start and stop functions and we're done.</p>\n<h2 id=\"improvements\">Improvements</h2>\n<p>The api can be improved a little bit, but now it works in a nice way. The major improvements that can be added is validation and create separate types for start and stop so we don't mix them. The validation wasn't in the API when I started implemented the change so I didn't add them now either and I don't think they are crucial either.</p>\n<!--kg-card-end: markdown-->","comment_id":"55","plaintext":"My plan was never to improve the F# api for Topshelf\n[http://topshelf-project.com/], Topshelf.FSharp\n[https://github.com/haf/Topshelf.FSharp], but Henrik Feldt\n[https://twitter.com/henrikfeldt] asked me to when he saw what I was doing when\nworking with a Suave [http://suave.io/], which Henrik is a core contributor of,\ndemo where I host the application in Topshelf. I implemented a simple Topshelf\nwrapper that had a nice, at least I think so, fluent api. The Topshelf.FSharp \nalso had a fluent api, but it was somewhat more complicated so what I was asked\nto do was implement my concept in the existing Topshelf.FSharp project, and so I\ndid (released as version 2.0.1 of the package).\n\nThe API\nThe already existing API was good, but there some things that could be improved.\nThe major plus with the existing function was that it already had existing\nfunctions for basically do every possible configuration of the service, so all I\nhad to do was to find a nicer way to improve the fluent part of the API. The\ngoal I had in mind, and what I implemented in my demo was an API looking\nsomewhat like this:\n\nService.Default\n|> with_start start\n|> with_recovery (ServiceRecovery.Default |> restart (min 10))\n|> with_stop stop\n|> run\n\n\nIt's really easy to follow and true to F#. There are many more functions you\ncould execute before run to configure the service, but the most important ones\nare with_start, with_stop and run. The start and stop functions just take a\nsingle functions which are executed on start and stop and returns a bool and the\nrun function is what executes the service and returns an int as expected. I\nwon't cover any more details, but just look at the github repo if you want to\nknow what configuration functions there is.\n\nUnder the hood\nSo how do one build this type of API on top of another more OO oriented\nframework and one answer to this is to use a kind of builder pattern. All the\nfunctions before run function is executed just creates a specification of the\nservice and what should happen when run is executed. The specification I ended\nup implementing for Topshelf look like this:\n\ntype Service =\n  { Start: HostControl -> bool\n    Stop: HostControl -> bool\n    HostConfiguration: (HostConfigurator -> HostConfigurator) list }\n  static member Default =\n      { Start = (fun _ -> true)\n        Stop = (fun _ -> true)\n        HostConfiguration = [] }\n\n\nFor a service to work you need a start and stop function, and that is what \nwith_start and with_stop do. All the functions, except from run, take the \nService type as the last parameter as well as returning a new instance of a \nService making it possible to pipe the specification between all the\nconfiguration steps. The static Default member makes it easy to start the\nconfiguration. To configure the service all the configuration functions add a\nfunction to the list of HostConfiguration describing what should be done when\nthe service is instantiated. This is done by a base function \nadd_host_configuration_step which all the configuration functions partially\napplies like below:\n\nlet add_host_configuration_step step service =\n    {service with HostConfiguration = step::service.HostConfiguration}\n\nlet enable_pause_and_continue =\n    add_host_configuration_step (fun c -> c.EnablePauseAndContinue();c)\n\n\nTo start the service there are a couple of things we need to do, but first the\ncode and then the description of it\n\nlet toAction1 f = new Action<_>(f)\nlet toFunc f = new Func<_>(f)\n\nlet service_control (start : HostControl -> bool) (stop : HostControl -> bool) () =\n  { new ServiceControl with\n    member x.Start hc =\n      start hc\n    member x.Stop hc =\n      stop hc }\n\nlet create_service (hc:HostConfigurator) service_func =\n  hc.Service<ServiceControl>(service_func |> toFunc) |> ignore\n\nlet run service =\n  let hostFactoryConfigurator hc =\n      let createdHc = service.HostConfiguration |> List.fold (fun chc x -> x chc) hc\n      service_control service.Start service.Stop\n      |> create_service createdHc\n\n  hostFactoryConfigurator |> toAction1 |> HostFactory.Run |> int\n\n\nFirst we need to figure out what the we need to run the service, and the \nTopshelf HostFactory.Run method takes an Action<HostConfigurator>. To create\naction from a F# lambda I have a simple helper function, toAction1. And to\ncreate the actual lambda that takes a HostConfigurator we just create a function\nthat has a single parameter let hostFactoryConfigurator hc, and now we can send\nthis function to toAction1 and we have what we need to run the service. To run\nthe actual configuration in the hostFactoryConfigurator we just left fold over\nthe HostConfiguration list with the first HostConfiguration as initial state and\napply the functions. When we've done that we can just create our ServiceControl \nfrom the final HostConfigurator, after left fold, and pass in the start and stop\nfunctions and we're done.\n\nImprovements\nThe api can be improved a little bit, but now it works in a nice way. The major\nimprovements that can be added is validation and create separate types for start\nand stop so we don't mix them. The validation wasn't in the API when I started\nimplemented the change so I didn't add them now either and I don't think they\nare crucial either.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-28T17:00:37.000Z","updated_at":"2015-06-28T17:05:01.000Z","published_at":"2015-06-28T17:05:01.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe06b","uuid":"e68ca7b5-f7c1-4787-9505-9da387073859","title":"Suave as a service with Topshelf","slug":"suave-as-a-service-with-topshelf","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As you can read in [Topshelf F# api improved](http://blog.tomasjansson.com/topshelf-fharp-api-improved/) I started working on a demo, but it magically grown to include a change to the `Topshelf.FSharp` project. The good part with this is that to run `Suave` with `Topshelf` as a Windows service have never been easier than now. The example code will be made available official examples for Suave, I hope, but meanwhile you can find the code on [github](https://github.com/mastoj/SuaveTopShelfDemo).\\n\\n## The code <tl;dr;>\\n\\n```\\nopen Suave\\nopen Suave.Http.Successful\\nopen Suave.Web \\nopen Suave.Http\\nopen Suave.Http.Applicatives\\nopen Suave.Http.Successful\\nopen Topshelf\\nopen System\\nopen System.Threading\\n\\n[<EntryPoint>]\\nlet main argv = \\n    printfn \\\"%A\\\" argv\\n\\n    let cancellationTokenSource = ref None\\n\\n    let home = choose [path \\\"/\\\" >>= GET >>= OK \\\"Hello world\\\"]\\n    let mind = choose [path \\\"/mind\\\" >>= GET >>= OK \\\"Where is my mind?\\\"]\\n    let app = choose [ home; mind ]\\n\\n    let start hc = \\n        let cts = new CancellationTokenSource()\\n        let token = cts.Token\\n        let config = { defaultConfig with cancellationToken = token}\\n\\n        startWebServerAsync config app\\n        |> snd\\n        |> Async.StartAsTask \\n        |> ignore\\n\\n        cancellationTokenSource := Some cts\\n        true\\n\\n    let stop hc = \\n        match !cancellationTokenSource with\\n        | Some cts -> cts.Cancel()\\n        | None -> ()\\n        true\\n\\n    Service.Default \\n    |> display_name \\\"ServiceDisplayName\\\"\\n    |> instance_name \\\"ServiceName\\\"\\n    |> with_start start\\n    |> with_stop stop\\n    |> with_topshelf\\n```\\n\\n## What is going on?\\n\\nI don't think the code need much explanation, but here are some lines. First we create a `CancellationToken` which we pass to the `start` function. The `stop` can then use the `CancellationTokenSource` to cancel the `async` operation that we start in the `start` function. Right before `start` we define our `Suave` app, it consist of two web parts, `home` and `mind` which are combined to one app in `app`. When the `start` and `stop` functions are defined it is trivial to use the new updated version of `Topshelf.FSharp` to run the suave application as a service.\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>As you can read in <a href=\"http://blog.tomasjansson.com/topshelf-fharp-api-improved/\">Topshelf F# api improved</a> I started working on a demo, but it magically grown to include a change to the <code>Topshelf.FSharp</code> project. The good part with this is that to run <code>Suave</code> with <code>Topshelf</code> as a Windows service have never been easier than now. The example code will be made available official examples for Suave, I hope, but meanwhile you can find the code on <a href=\"https://github.com/mastoj/SuaveTopShelfDemo\">github</a>.</p>\n<h2 id=\"thecodetldr\">The code &lt;tl;dr;&gt;</h2>\n<pre><code>open Suave\nopen Suave.Http.Successful\nopen Suave.Web \nopen Suave.Http\nopen Suave.Http.Applicatives\nopen Suave.Http.Successful\nopen Topshelf\nopen System\nopen System.Threading\n\n[&lt;EntryPoint&gt;]\nlet main argv = \n    printfn &quot;%A&quot; argv\n\n    let cancellationTokenSource = ref None\n\n    let home = choose [path &quot;/&quot; &gt;&gt;= GET &gt;&gt;= OK &quot;Hello world&quot;]\n    let mind = choose [path &quot;/mind&quot; &gt;&gt;= GET &gt;&gt;= OK &quot;Where is my mind?&quot;]\n    let app = choose [ home; mind ]\n\n    let start hc = \n        let cts = new CancellationTokenSource()\n        let token = cts.Token\n        let config = { defaultConfig with cancellationToken = token}\n\n        startWebServerAsync config app\n        |&gt; snd\n        |&gt; Async.StartAsTask \n        |&gt; ignore\n\n        cancellationTokenSource := Some cts\n        true\n\n    let stop hc = \n        match !cancellationTokenSource with\n        | Some cts -&gt; cts.Cancel()\n        | None -&gt; ()\n        true\n\n    Service.Default \n    |&gt; display_name &quot;ServiceDisplayName&quot;\n    |&gt; instance_name &quot;ServiceName&quot;\n    |&gt; with_start start\n    |&gt; with_stop stop\n    |&gt; with_topshelf\n</code></pre>\n<h2 id=\"whatisgoingon\">What is going on?</h2>\n<p>I don't think the code need much explanation, but here are some lines. First we create a <code>CancellationToken</code> which we pass to the <code>start</code> function. The <code>stop</code> can then use the <code>CancellationTokenSource</code> to cancel the <code>async</code> operation that we start in the <code>start</code> function. Right before <code>start</code> we define our <code>Suave</code> app, it consist of two web parts, <code>home</code> and <code>mind</code> which are combined to one app in <code>app</code>. When the <code>start</code> and <code>stop</code> functions are defined it is trivial to use the new updated version of <code>Topshelf.FSharp</code> to run the suave application as a service.</p>\n<!--kg-card-end: markdown-->","comment_id":"56","plaintext":"As you can read in Topshelf F# api improved\n[http://blog.tomasjansson.com/topshelf-fharp-api-improved/] I started working on\na demo, but it magically grown to include a change to the Topshelf.FSharp \nproject. The good part with this is that to run Suave with Topshelf as a Windows\nservice have never been easier than now. The example code will be made available\nofficial examples for Suave, I hope, but meanwhile you can find the code on \ngithub [https://github.com/mastoj/SuaveTopShelfDemo].\n\nThe code <tl;dr;>\nopen Suave\nopen Suave.Http.Successful\nopen Suave.Web \nopen Suave.Http\nopen Suave.Http.Applicatives\nopen Suave.Http.Successful\nopen Topshelf\nopen System\nopen System.Threading\n\n[<EntryPoint>]\nlet main argv = \n    printfn \"%A\" argv\n\n    let cancellationTokenSource = ref None\n\n    let home = choose [path \"/\" >>= GET >>= OK \"Hello world\"]\n    let mind = choose [path \"/mind\" >>= GET >>= OK \"Where is my mind?\"]\n    let app = choose [ home; mind ]\n\n    let start hc = \n        let cts = new CancellationTokenSource()\n        let token = cts.Token\n        let config = { defaultConfig with cancellationToken = token}\n\n        startWebServerAsync config app\n        |> snd\n        |> Async.StartAsTask \n        |> ignore\n\n        cancellationTokenSource := Some cts\n        true\n\n    let stop hc = \n        match !cancellationTokenSource with\n        | Some cts -> cts.Cancel()\n        | None -> ()\n        true\n\n    Service.Default \n    |> display_name \"ServiceDisplayName\"\n    |> instance_name \"ServiceName\"\n    |> with_start start\n    |> with_stop stop\n    |> with_topshelf\n\n\nWhat is going on?\nI don't think the code need much explanation, but here are some lines. First we\ncreate a CancellationToken which we pass to the start function. The stop can\nthen use the CancellationTokenSource to cancel the async operation that we start\nin the start function. Right before start we define our Suave app, it consist of\ntwo web parts, home and mind which are combined to one app in app. When the \nstart and stop functions are defined it is trivial to use the new updated\nversion of Topshelf.FSharp to run the suave application as a service.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-28T17:12:11.000Z","updated_at":"2015-06-30T06:38:58.000Z","published_at":"2015-06-28T17:14:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe06d","uuid":"b6dd6257-aa94-4443-bcea-c768b88039b7","title":"Constraints in FSharp","slug":"constraints-in-fsharp","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Constraints in F# is a really powerful feature, but it is also an area that I think is missing some clear documentation about how to use it. This will be a short write up for future me when I need it, and I also it will help some lost souls out there. \\n\\n## Inline functions\\n\\nF# will most of the time handle the types for you, also generics when it can do so. A simple example:\\n\\n```language-fsharp    \\nlet add x y = x + y\\n```\\nThis will when it stands all by itself resolve to the have the type: \\n\\n   val add : x:int -> y:int -> int\\n\\nThat all the compiler can do when it is not provided any more information. If you instead write\\n\\n```language-fsharp\\nlet add x y = x + y\\nadd \\\"Hello \\\" \\\"world\\\"\\n```\\n\\n`add` will get the type\\n\\n```language-fsharp\\nval add : x:string -> y:string -> string\\n```\\nsince you on line two specify that you will use it with strings. What should you do if you want to add it with any arguments that supports the `+` operator? This is where the `inline` keyword is useful (it can also be used in some optimization scenarios). So let us define the function again and adding the `inline` keyword:\\n\\n```language-fsharp\\nlet inline add x y = x + y\\n```\\n\\nThe type for this version of add is a little bit more complicated:\\n\\n```language-fsharp\\nval inline add :\\n  x: ^a -> y: ^b ->  ^c\\n    when ( ^a or  ^b) : (static member ( + ) :  ^a *  ^b ->  ^c)\\n```\\n\\nWhat does this mean? \\n\\n* `x: ^a -> y: ^b ->  ^c` specify that the function takes two arguments and returns something. The argument has types `^a` and `^b` and the result have type `^c`\\n* `when ( ^a or  ^b) : (static member ( + ) :  ^a *  ^b ->  ^c)` is adding a constraint on the types `^a`, `^b` and `^c`. The constraint says that it must exist a static operator `+` that takes a tuple of type `^a *  ^b` and returns type `^c`.\\n\\nWe got all that by adding the keyword `inline`, but why? When adding the `inline` keyword you basically say that whenever you see the function name somewhere in the code replace that name with this function definition. This basically gives you one new implementation of the function every time you use it. If you don't use `inline` you will only have one implementation and that is why you don't get it as generic as you would like. This is my simplified explanation, someone on the compiler team can probably explain it a lot more in detail.\\n\\n\\n## Constraining on interfaces\\n\\nI won't go on with you can constrain the types on interfaces because it quite simple and there is more interesting constraint in the next section. I'll just throw the code at you\\n\\n```language-fsharp\\ntype ISimpleInterface =\\n    abstract member Add: int -> int -> int\\n\\ntype SimpleClassA() =\\n    interface ISimpleInterface with\\n        member this.Add x y = x + y\\n\\ntype SimpleClassB() =\\n    interface ISimpleInterface with\\n        member this.Add x y = x + y\\n\\nlet doSimple<'T when 'T :> ISimpleInterface>  (x: 'T) = x.Add 5 5\\nlet doSimple2 (x: ISimpleInterface) = x.Add 5 5\\n\\nlet a = new SimpleClassA()\\nlet b = new SimpleClassB()\\ndoSimple a\\ndoSimple b\\n```\\n\\nHere first define an interface and then two implementations of that interface. After that I defines two functions `doSimple` and `doSimple2`, which are basically identical. I prefer to use the second variant when possible. \\n\\n## If it walks like a duck and quack like a duck\\n\\n[Duck typing](https://en.wikipedia.org/wiki/Duck_typing) can be achieved in F# by using constraint. This is really powerful since you can write functions that take in arguments and as long as the types of the arguments support doesn't violate the constraint you can use them without the need of an interface. Why would you want to do this you might ask? The reason I started to look into this was actually because I needed it at work. I am using the excellent [`SqlClient` type provider](http://fsprojects.github.io/FSharp.Data.SqlClient/), and wanted to use the `SqlProgrammabilityProvider` to read and update multiple tables using the pipe operator. The problem is that the `Update` method is defined on the generated table type and not as a static `Update` method. A simplified version of what I had looks somewhat like this:\\n\\n```language-fsharp\\ntype SomeClass1() =\\n    member\\n       this.Add(a:int, b:int) = a + b\\n\\ntype SomeClass2() =\\n    member\\n        this.Add(a:int, b:int) = a + b\\n```\\n\\nand I wanted an `add` method that could be applied to either `SomeClass1` or `SomeClass2` so I could write `someClassInstance |> add 2 3`. To achieve that you use member constraints like this:\\n\\n```language-fsharp\\ntype SomeClass1() =\\n    member\\n        this.Add(a:int, b:int) = a + b\\n\\ntype SomeClass2() =\\n    member\\n        this.Add(a:int, b:int) = a + b\\n\\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int*int->int)) =\\n    (^T : (member Add : int*int->int) (x,y,z))\\n\\nSomeClass1() |> add 2 3\\nSomeClass2() |> add 2 3\\n```\\n\\nIn the function definition I define that I the argument `x` must have an operator called `Add` that has type `int*int->int`. In the body of the function I specify that I will call the `Add` function on variable `x` with the input of `y` and `z`. It looks complicated until you get your head around it. It is also important to specify the method as `inline`. One thing that is good to know is that it doesn't work for curried functions even though you don't get a compile time error. \\n\\nThe case I had was a slightly more complicated since the `Update` method I wanted to use had some optional arguments. If you know that optional arguments is represented as `Option` types in F# the code isn't that surprising:\\n\\n```language-fsharp\\ntype SomeClass1Option() =\\n    member\\n        this.Add(?a:int, ?b:int) = \\n            match a,b with\\n            | Some x, Some y -> x + y\\n            | _, _ -> 0\\n\\ntype SomeClass2Option() =\\n    member\\n        this.Add(?a:int, ?b:int) = \\n            match a,b with\\n            | Some x, Some y -> x + y\\n            | _, _ -> 0\\n\\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int option*int option->int)) =\\n    (^T : (member Add : int option*int option->int) (x,Some y,Some z))\\n\\nSomeClass1Option() |> add 2 3\\nSomeClass2Option() |> add 2 3\\n```\\n\\nGiven all this I ended up writing an `update` method that looks like this:\\n\\n```language-fsharp\\nlet inline updateTable(table: ^T when ^T : (member Update : SqlConnection option*SqlTransaction option*int option -> int)) = \\n    (^T : (member Update : SqlConnection option*SqlTransaction option*int option-> int) (table, None, None, None))\\n```\\n\\nThat method can be used with piping to call the `Update` method on any table generated with `SqlProgrammabilityProvider` with default arguments.\\n\\nYou can also write constraint on static operators, but I won't cover that. The documentation for constraints is found here: https://msdn.microsoft.com/en-us/library/dd233203.aspx. \\n \\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Constraints in F# is a really powerful feature, but it is also an area that I think is missing some clear documentation about how to use it. This will be a short write up for future me when I need it, and I also it will help some lost souls out there.</p>\n<h2 id=\"inlinefunctions\">Inline functions</h2>\n<p>F# will most of the time handle the types for you, also generics when it can do so. A simple example:</p>\n<pre><code class=\"language-language-fsharp\">let add x y = x + y\n</code></pre>\n<p>This will when it stands all by itself resolve to the have the type:</p>\n<p>val add : x:int -&gt; y:int -&gt; int</p>\n<p>That all the compiler can do when it is not provided any more information. If you instead write</p>\n<pre><code class=\"language-language-fsharp\">let add x y = x + y\nadd &quot;Hello &quot; &quot;world&quot;\n</code></pre>\n<p><code>add</code> will get the type</p>\n<pre><code class=\"language-language-fsharp\">val add : x:string -&gt; y:string -&gt; string\n</code></pre>\n<p>since you on line two specify that you will use it with strings. What should you do if you want to add it with any arguments that supports the <code>+</code> operator? This is where the <code>inline</code> keyword is useful (it can also be used in some optimization scenarios). So let us define the function again and adding the <code>inline</code> keyword:</p>\n<pre><code class=\"language-language-fsharp\">let inline add x y = x + y\n</code></pre>\n<p>The type for this version of add is a little bit more complicated:</p>\n<pre><code class=\"language-language-fsharp\">val inline add :\n  x: ^a -&gt; y: ^b -&gt;  ^c\n    when ( ^a or  ^b) : (static member ( + ) :  ^a *  ^b -&gt;  ^c)\n</code></pre>\n<p>What does this mean?</p>\n<ul>\n<li><code>x: ^a -&gt; y: ^b -&gt;  ^c</code> specify that the function takes two arguments and returns something. The argument has types <code>^a</code> and <code>^b</code> and the result have type <code>^c</code></li>\n<li><code>when ( ^a or  ^b) : (static member ( + ) :  ^a *  ^b -&gt;  ^c)</code> is adding a constraint on the types <code>^a</code>, <code>^b</code> and <code>^c</code>. The constraint says that it must exist a static operator <code>+</code> that takes a tuple of type <code>^a *  ^b</code> and returns type <code>^c</code>.</li>\n</ul>\n<p>We got all that by adding the keyword <code>inline</code>, but why? When adding the <code>inline</code> keyword you basically say that whenever you see the function name somewhere in the code replace that name with this function definition. This basically gives you one new implementation of the function every time you use it. If you don't use <code>inline</code> you will only have one implementation and that is why you don't get it as generic as you would like. This is my simplified explanation, someone on the compiler team can probably explain it a lot more in detail.</p>\n<h2 id=\"constrainingoninterfaces\">Constraining on interfaces</h2>\n<p>I won't go on with you can constrain the types on interfaces because it quite simple and there is more interesting constraint in the next section. I'll just throw the code at you</p>\n<pre><code class=\"language-language-fsharp\">type ISimpleInterface =\n    abstract member Add: int -&gt; int -&gt; int\n\ntype SimpleClassA() =\n    interface ISimpleInterface with\n        member this.Add x y = x + y\n\ntype SimpleClassB() =\n    interface ISimpleInterface with\n        member this.Add x y = x + y\n\nlet doSimple&lt;'T when 'T :&gt; ISimpleInterface&gt;  (x: 'T) = x.Add 5 5\nlet doSimple2 (x: ISimpleInterface) = x.Add 5 5\n\nlet a = new SimpleClassA()\nlet b = new SimpleClassB()\ndoSimple a\ndoSimple b\n</code></pre>\n<p>Here first define an interface and then two implementations of that interface. After that I defines two functions <code>doSimple</code> and <code>doSimple2</code>, which are basically identical. I prefer to use the second variant when possible.</p>\n<h2 id=\"ifitwalkslikeaduckandquacklikeaduck\">If it walks like a duck and quack like a duck</h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Duck_typing\">Duck typing</a> can be achieved in F# by using constraint. This is really powerful since you can write functions that take in arguments and as long as the types of the arguments support doesn't violate the constraint you can use them without the need of an interface. Why would you want to do this you might ask? The reason I started to look into this was actually because I needed it at work. I am using the excellent <a href=\"http://fsprojects.github.io/FSharp.Data.SqlClient/\"><code>SqlClient</code> type provider</a>, and wanted to use the <code>SqlProgrammabilityProvider</code> to read and update multiple tables using the pipe operator. The problem is that the <code>Update</code> method is defined on the generated table type and not as a static <code>Update</code> method. A simplified version of what I had looks somewhat like this:</p>\n<pre><code class=\"language-language-fsharp\">type SomeClass1() =\n    member\n       this.Add(a:int, b:int) = a + b\n\ntype SomeClass2() =\n    member\n        this.Add(a:int, b:int) = a + b\n</code></pre>\n<p>and I wanted an <code>add</code> method that could be applied to either <code>SomeClass1</code> or <code>SomeClass2</code> so I could write <code>someClassInstance |&gt; add 2 3</code>. To achieve that you use member constraints like this:</p>\n<pre><code class=\"language-language-fsharp\">type SomeClass1() =\n    member\n        this.Add(a:int, b:int) = a + b\n\ntype SomeClass2() =\n    member\n        this.Add(a:int, b:int) = a + b\n\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int*int-&gt;int)) =\n    (^T : (member Add : int*int-&gt;int) (x,y,z))\n\nSomeClass1() |&gt; add 2 3\nSomeClass2() |&gt; add 2 3\n</code></pre>\n<p>In the function definition I define that I the argument <code>x</code> must have an operator called <code>Add</code> that has type <code>int*int-&gt;int</code>. In the body of the function I specify that I will call the <code>Add</code> function on variable <code>x</code> with the input of <code>y</code> and <code>z</code>. It looks complicated until you get your head around it. It is also important to specify the method as <code>inline</code>. One thing that is good to know is that it doesn't work for curried functions even though you don't get a compile time error.</p>\n<p>The case I had was a slightly more complicated since the <code>Update</code> method I wanted to use had some optional arguments. If you know that optional arguments is represented as <code>Option</code> types in F# the code isn't that surprising:</p>\n<pre><code class=\"language-language-fsharp\">type SomeClass1Option() =\n    member\n        this.Add(?a:int, ?b:int) = \n            match a,b with\n            | Some x, Some y -&gt; x + y\n            | _, _ -&gt; 0\n\ntype SomeClass2Option() =\n    member\n        this.Add(?a:int, ?b:int) = \n            match a,b with\n            | Some x, Some y -&gt; x + y\n            | _, _ -&gt; 0\n\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int option*int option-&gt;int)) =\n    (^T : (member Add : int option*int option-&gt;int) (x,Some y,Some z))\n\nSomeClass1Option() |&gt; add 2 3\nSomeClass2Option() |&gt; add 2 3\n</code></pre>\n<p>Given all this I ended up writing an <code>update</code> method that looks like this:</p>\n<pre><code class=\"language-language-fsharp\">let inline updateTable(table: ^T when ^T : (member Update : SqlConnection option*SqlTransaction option*int option -&gt; int)) = \n    (^T : (member Update : SqlConnection option*SqlTransaction option*int option-&gt; int) (table, None, None, None))\n</code></pre>\n<p>That method can be used with piping to call the <code>Update</code> method on any table generated with <code>SqlProgrammabilityProvider</code> with default arguments.</p>\n<p>You can also write constraint on static operators, but I won't cover that. The documentation for constraints is found here: <a href=\"https://msdn.microsoft.com/en-us/library/dd233203.aspx\">https://msdn.microsoft.com/en-us/library/dd233203.aspx</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"58","plaintext":"Constraints in F# is a really powerful feature, but it is also an area that I\nthink is missing some clear documentation about how to use it. This will be a\nshort write up for future me when I need it, and I also it will help some lost\nsouls out there.\n\nInline functions\nF# will most of the time handle the types for you, also generics when it can do\nso. A simple example:\n\nlet add x y = x + y\n\n\nThis will when it stands all by itself resolve to the have the type:\n\nval add : x:int -> y:int -> int\n\nThat all the compiler can do when it is not provided any more information. If\nyou instead write\n\nlet add x y = x + y\nadd \"Hello \" \"world\"\n\n\nadd will get the type\n\nval add : x:string -> y:string -> string\n\n\nsince you on line two specify that you will use it with strings. What should you\ndo if you want to add it with any arguments that supports the + operator? This\nis where the inline keyword is useful (it can also be used in some optimization\nscenarios). So let us define the function again and adding the inline keyword:\n\nlet inline add x y = x + y\n\n\nThe type for this version of add is a little bit more complicated:\n\nval inline add :\n  x: ^a -> y: ^b ->  ^c\n    when ( ^a or  ^b) : (static member ( + ) :  ^a *  ^b ->  ^c)\n\n\nWhat does this mean?\n\n * x: ^a -> y: ^b -> ^c specify that the function takes two arguments and\n   returns something. The argument has types ^a and ^b and the result have type \n   ^c\n * when ( ^a or ^b) : (static member ( + ) : ^a * ^b -> ^c) is adding a\n   constraint on the types ^a, ^b and ^c. The constraint says that it must exist\n   a static operator + that takes a tuple of type ^a * ^b and returns type ^c.\n\nWe got all that by adding the keyword inline, but why? When adding the inline \nkeyword you basically say that whenever you see the function name somewhere in\nthe code replace that name with this function definition. This basically gives\nyou one new implementation of the function every time you use it. If you don't\nuse inline you will only have one implementation and that is why you don't get\nit as generic as you would like. This is my simplified explanation, someone on\nthe compiler team can probably explain it a lot more in detail.\n\nConstraining on interfaces\nI won't go on with you can constrain the types on interfaces because it quite\nsimple and there is more interesting constraint in the next section. I'll just\nthrow the code at you\n\ntype ISimpleInterface =\n    abstract member Add: int -> int -> int\n\ntype SimpleClassA() =\n    interface ISimpleInterface with\n        member this.Add x y = x + y\n\ntype SimpleClassB() =\n    interface ISimpleInterface with\n        member this.Add x y = x + y\n\nlet doSimple<'T when 'T :> ISimpleInterface>  (x: 'T) = x.Add 5 5\nlet doSimple2 (x: ISimpleInterface) = x.Add 5 5\n\nlet a = new SimpleClassA()\nlet b = new SimpleClassB()\ndoSimple a\ndoSimple b\n\n\nHere first define an interface and then two implementations of that interface.\nAfter that I defines two functions doSimple and doSimple2, which are basically\nidentical. I prefer to use the second variant when possible.\n\nIf it walks like a duck and quack like a duck\nDuck typing [https://en.wikipedia.org/wiki/Duck_typing] can be achieved in F# by\nusing constraint. This is really powerful since you can write functions that\ntake in arguments and as long as the types of the arguments support doesn't\nviolate the constraint you can use them without the need of an interface. Why\nwould you want to do this you might ask? The reason I started to look into this\nwas actually because I needed it at work. I am using the excellent SqlClient\ntype provider [http://fsprojects.github.io/FSharp.Data.SqlClient/], and wanted\nto use the SqlProgrammabilityProvider to read and update multiple tables using\nthe pipe operator. The problem is that the Update method is defined on the\ngenerated table type and not as a static Update method. A simplified version of\nwhat I had looks somewhat like this:\n\ntype SomeClass1() =\n    member\n       this.Add(a:int, b:int) = a + b\n\ntype SomeClass2() =\n    member\n        this.Add(a:int, b:int) = a + b\n\n\nand I wanted an add method that could be applied to either SomeClass1 or \nSomeClass2 so I could write someClassInstance |> add 2 3. To achieve that you\nuse member constraints like this:\n\ntype SomeClass1() =\n    member\n        this.Add(a:int, b:int) = a + b\n\ntype SomeClass2() =\n    member\n        this.Add(a:int, b:int) = a + b\n\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int*int->int)) =\n    (^T : (member Add : int*int->int) (x,y,z))\n\nSomeClass1() |> add 2 3\nSomeClass2() |> add 2 3\n\n\nIn the function definition I define that I the argument x must have an operator\ncalled Add that has type int*int->int. In the body of the function I specify\nthat I will call the Add function on variable x with the input of y and z. It\nlooks complicated until you get your head around it. It is also important to\nspecify the method as inline. One thing that is good to know is that it doesn't\nwork for curried functions even though you don't get a compile time error.\n\nThe case I had was a slightly more complicated since the Update method I wanted\nto use had some optional arguments. If you know that optional arguments is\nrepresented as Option types in F# the code isn't that surprising:\n\ntype SomeClass1Option() =\n    member\n        this.Add(?a:int, ?b:int) = \n            match a,b with\n            | Some x, Some y -> x + y\n            | _, _ -> 0\n\ntype SomeClass2Option() =\n    member\n        this.Add(?a:int, ?b:int) = \n            match a,b with\n            | Some x, Some y -> x + y\n            | _, _ -> 0\n\nlet inline add (y:int) (z:int) (x: ^T when ^T : (member Add : int option*int option->int)) =\n    (^T : (member Add : int option*int option->int) (x,Some y,Some z))\n\nSomeClass1Option() |> add 2 3\nSomeClass2Option() |> add 2 3\n\n\nGiven all this I ended up writing an update method that looks like this:\n\nlet inline updateTable(table: ^T when ^T : (member Update : SqlConnection option*SqlTransaction option*int option -> int)) = \n    (^T : (member Update : SqlConnection option*SqlTransaction option*int option-> int) (table, None, None, None))\n\n\nThat method can be used with piping to call the Update method on any table\ngenerated with SqlProgrammabilityProvider with default arguments.\n\nYou can also write constraint on static operators, but I won't cover that. The\ndocumentation for constraints is found here: \nhttps://msdn.microsoft.com/en-us/library/dd233203.aspx.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-10-17T10:01:02.000Z","updated_at":"2016-01-15T22:17:37.000Z","published_at":"2015-10-18T06:57:19.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe06e","uuid":"060918d9-5d07-4a5d-bc9a-ae5add6ae41e","title":"Take control of your build, CI and deployment with FSharp FAKE","slug":"take-control-of-your-build-ci-and-deployment-with-fsharp-fake","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I've been using [TeamCity](https://www.jetbrains.com/teamcity/) for quite a while and trusted it to do the right thing when it comes to building. It has a lot of built in feature to do so, but the moment you start to define your build inside TeamCity you have painted yourself into a corner. I still think TeamCity is a great tooling triggering my builds, but I have come to realize that I should start to put the actual definition of the builds outside of TeamCity, or whatever CI tool I am using.\\n\\n## Why build scripts?\\n\\nMany people argue that it works just fine with using TeamCity and other CI tools to define the way you build, and it is when you have a set of smaller projects I think. The moment when you need to do more things in your build it make sense to put it in a separate script file and here are some of my reasons:\\n\\n* You got your actual build process under source control together with your code instead of defined in a CI tool\\n* It is easier to handle versioning\\n* The way you build on the server is identical to the way you build locally\\n* You will not be locked in to one CI tool, if you use TeamCity today it is quite easy to switch to AppVeyor tomorrow and TravisCI the day after that.\\n\\n## What is FAKE\\n\\n[FAKE](http://fsharp.github.io/FAKE/) is a domain specific language, [dsl](https://en.wikipedia.org/wiki/Domain-specific_language), for build tasks implemented in [F#](http://fsharp.org/). That it is implemented in F# doesn't mean you can only build F# projects, in fact you should event be able to build java project if you would like to. F#, along with many functional programming languages, is terrific when you want to create a dsl, since functional programming languages often are more expressive and not as verbose as OO languages. This is not an introduction to FAKE, and the documentation is [here](http://fsharp.github.io/FAKE/) if you want to check it out. Later in the post I will go through what my FAKE script look like to solve the problems I imagined.\\n\\n## The demo\\n\\nThe thing I wanted to try was this:\\n\\n* Simple .NET Web app that includes some js building\\n* GitHub as source control\\n* AppVeyor as a CI engine to execute build\\n* Octopus Deploy to handle deployment\\n* Deploy to Azure Web App\\n* Most of it configured in FAKE\\n\\nIllustration of the flow from code to deployment:\\n\\n![The build deploy process](/content/images/2015/10/CodeToDeploy.JPG)\\n\\nI think the end result is really good since it is the first time I used FAKEand AppVeyor. If you want to see the sample app and also the builds scripts they are in this repo at github: https://github.com/mastoj/FAKESimpleDemo\\n\\n### Appveyor\\n\\nI could as well used TeamCity, but I thought I would try [Appveyor](http://www.appveyor.com/) out. The goal of AppVeyor is:\\n\\n> AppVeyor aims to give powerful Continuous Integration and Deployment tools to every .NET developer without the hassle of setting up and maintaining their own build server. - [About AppVeyor](http://www.appveyor.com/about)\\n\\nTheir goal is the reason I think AppVeyor is interesting and I wanted to take it for a spin. The experience has been truly amazing with really fast build times and easy to get started, and another really positive aspect is that I don't have to think about maintaining the build server.\\n\\nAppVeyor can do a lot more than I did, one might even argue that I should use AppVeyor instead of Octopus Deploy for deployment, but this is a proof of concept for a scenario were I still will have Octopus Deploy and I really like Octopus Deploy :). \\n\\nWhen you configure AppVeyor you can do so in the UI or with a yml-file. Of course I choose to use a yml-file since I do want as much as I can in source control. The configuration file I created is minimal since I have FAKE that take care of the actual build process:\\n\\n```language-yml\\nenvironment:\\n  version: 1.0.0\\nassembly_info:\\n  patch: false\\nbuild_script:\\n  build.cmd\\ntest: off\\n```\\n\\nThis specifies a environment variable `version` and that AppVeyor should not run tests or patch the assmebly information file. To start the build the `build.cmd default` commmand should be used. I can still run test and patch the assembly information file, but that is something I want to do from FAKE and not define in AppVeyor.\\n\\nI alos added some environment settings in the UI like this:\\n\\n![AppVeyor environment settings](/content/images/2015/10/AppVeyorEnv.PNG)\\n\\nThe reason I added them there and not in source control is because that is not I want to have on GitHub. The same would apply if I needed some other \\\"secret\\\" settings during build, then I would add that a a environment setting in the UI. I do the same thing if I use TeamCity.\\n\\n### Octopus Deploy\\n\\nI'm not going to go through what Octopus Deploy and what you can do with it, all you nned to know is that it is a great tool for handling deployments to multiple different environments locally as well as in the cloud. \\n\\nTo deploy to an Azure Web App you need to create an account under Azure that is an Azure account. You create accounts in `Environment->Accounts`. To create an Azure account you will need a Management Certificate (.pfx) and your subscriptiong id. I don't remember which guide I followed to generate mine, but this might work: https://azure.microsoft.com/en-us/blog/obtaining-a-certificate-for-use-with-windows-azure-web-sites-waws/\\n\\nWhen that is done you just add your project and creates a deployment step of type `Deploy an Azure Web App` and you are good to go. The step configuration should like something like: \\n\\n![Octopus deploy configuration](/content/images/2015/10/OctopusDeployStep.PNG)\\n\\n### FAKE\\n\\nThis is were all the magic happens. I will try to break down this file: https://github.com/mastoj/FAKESimpleDemo/blob/master/build.fsx and explain the different parts of it. I expect you to know the minimum basics of FAKE, that is, what is a target and how you set up a build process with the `==>` operator.\\n\\n#### Bootstrap\\n\\nTo get FAKE running easily I also have a bootstrap file [`build.cmd`](https://github.com/mastoj/FAKESimpleDemo/blob/master/build.cmd). \\n\\n```\\n@echo off\\ncls\\nNuGet.exe \\\"Install\\\" \\\"FAKE\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"OctopusTools\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"Node.js\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"Npm.js\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\n\\\"packages\\\\FAKE\\\\tools\\\\Fake.exe\\\" build.fsx %*\\n```\\n\\nAll this file does is installing the tools I need to build the project, and then calls the build script with the arguments provided.\\n\\nI separated the build script into some modules to make it easier to follow and I thought I would go through this module by module.\\n\\n#### Npm module\\n\\nThe demo application had a simple js-app (that alerted \\\"Hello\\\") to show that it is possible to build js-apps with FAKE. I would be stupid if I did not use the tools from the js-community to do the heavy lifting here, so that is exactly what I did. You can find the [`package.json` here](https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/package.json) and the [`gulpfile.js` here](https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/gulpfile.js). I won't cover those since it is a different topic. When npm and gulp was configured all I needed to do was trigger it from FAKE and to do so I wrote a simple `Npm` wrapper. (I might extract this and try to submit a PR to FAKE later). The code is quite simple:\\n\\n```language-fsharp\\nmodule Npm =\\n  open System\\n\\n  let npmFileName =\\n    match isUnix with\\n      | true -> \\\"/usr/local/bin/npm\\\"\\n      | _ -> \\\"./packages/Npm.js/tools/npm.cmd\\\"\\n\\n  type InstallArgs =\\n    | Standard\\n    | Forced\\n\\n  type NpmCommand =\\n    | Install of InstallArgs\\n    | Run of string\\n\\n  type NpmParams = {\\n    Src: string\\n    NpmFilePath: string\\n    WorkingDirectory: string\\n    Command: NpmCommand\\n    Timeout: TimeSpan\\n  }\\n\\n  let npmParams = {\\n    Src = \\\"\\\"\\n    NpmFilePath = npmFileName\\n    Command = Install Standard\\n    WorkingDirectory = \\\".\\\"\\n    Timeout = TimeSpan.MaxValue\\n  }\\n\\n  let parseInsallArgs = function\\n    | Standard -> \\\"\\\"\\n    | Forced -> \\\" --force\\\"\\n\\n  let parse command =\\n    match command with\\n    | Install installArgs -> sprintf \\\"install%s\\\" (installArgs |> parseInsallArgs)\\n    | Run str -> sprintf \\\"run %s\\\" str\\n\\n  let run npmParams =\\n    let npmPath = Path.GetFullPath(npmParams.NpmFilePath)\\n    let arguments = npmParams.Command |> parse\\n    let result = ExecProcess (\\n                  fun info ->\\n                    info.FileName <- npmPath\\n                    info.WorkingDirectory <- npmParams.WorkingDirectory\\n                    info.Arguments <- arguments\\n                  ) npmParams.Timeout\\n    if result <> 0 then failwith (sprintf \\\"'npm %s' failed\\\" arguments)\\n\\n  let Npm f =\\n    npmParams |> f |> run\\n```\\n\\nThe `Npm` function is the important part. There I take the default arguments, pass it through `f` which adds changes to the arguments and then that is passed to `run`. The `run` method parse the `Command` property and execute `Npm`. It doesn't get simpler than that.\\n\\n#### OctoHelpers module\\n\\nThere were two things I wanted to do with Octopus Deploy; create release and create deploy. This helper module is a wrapper around the `Octo` module in FAKE since there were a lot of similarities between the steps.\\n\\n```language-fsharp\\nmodule OctoHelpers =\\n  let executeOcto command =\\n    let serverName = environVar \\\"OCTO_SERVER\\\"\\n    let apiKey = environVar \\\"OCTO_KEY\\\"\\n    let server = { Server = serverName; ApiKey = apiKey }\\n    Octo (fun octoParams ->\\n        { octoParams with\\n            ToolPath = \\\"./packages/octopustools\\\"\\n            Server   = server\\n            Command  = command }\\n    )\\n```\\n\\nAll that is going on here is that I read some things from environment variables that should be configured on AppVeyor (or the CI you use) and then execute an action against Octopus Deploy. \\n\\n#### AppVeyorHelper module\\n\\nI extracted the things that dealt with AppVeyor integration to a separate module to keep my targets clean (I get to the targets soon). It is actually one thing I'm doing on AppVeyor and that is publishing the artifacts to the nuget feed hosted by AppVeyor.\\n\\n```language-fsharp\\nmodule AppVeyorHelpers =\\n  let execOnAppveyor arguments =\\n    let result =\\n      ExecProcess (fun info ->\\n        info.FileName <- \\\"appveyor\\\"\\n        info.Arguments <- arguments\\n        ) (System.TimeSpan.FromMinutes 2.0)\\n    if result <> 0 then failwith (sprintf \\\"Failed to execute appveyor command: %s\\\" arguments)\\n    trace \\\"Published packages\\\"\\n\\n  let publishOnAppveyor folder =\\n    !! (folder + \\\"*.nupkg\\\")\\n    |> Seq.iter (fun artifact -> execOnAppveyor (sprintf \\\"PushArtifact %s\\\" artifact))\\n```\\n\\nThe `publishOnAppveyor` function takes a folder and finds all the nuget packages in it. After that it executes the `appveyor` command to publish every package to the feed.\\n\\n#### Settings module\\n\\nThe module name isn't perfect, but what is. It contains all the variables that are used in the targets as well as two three helper functions:\\n\\n```language-fsharp\\nmodule Settings =\\n  let buildDir = \\\"./.build/\\\"\\n  let packagingDir = buildDir + \\\"FAKESimple.Web/_PublishedWebsites/FAKESimple.Web\\\"\\n  let deployDir = \\\"./.deploy/\\\"\\n  let testDir = \\\"./.test/\\\"\\n  let projects = !! \\\"src/**/*.csproj\\\" -- \\\"src/**/*.Tests.csproj\\\"\\n  let testProjects = !! \\\"src/**/*.Tests.csproj\\\"\\n  let packages = !! \\\"./**/packages.config\\\"\\n\\n  let getOutputDir proj =\\n    let folderName = Directory.GetParent(proj).Name\\n    sprintf \\\"%s%s/\\\" buildDir folderName\\n\\n  let build proj =\\n    let outputDir = proj |> getOutputDir\\n    MSBuildRelease outputDir \\\"ResolveReferences;Build\\\" [proj] |> ignore\\n\\n  let getVersion() =\\n    let buildCandidate = (environVar \\\"APPVEYOR_BUILD_NUMBER\\\")\\n    if buildCandidate = \\\"\\\" || buildCandidate = null then \\\"1.0.0\\\" else (sprintf \\\"1.0.0.%s\\\" buildCandidate)\\n```\\n\\nThe `getOutputDir` is used to get the name of the folder of a file, it says `proj` but it could be any file. I'm using it to create one output folder per project instead of everything ending up in the same folder or under `bin/release` as it usually does when building from VS. It is using the convention that the parent folder name of a project file is the name of the project. The `build` function is a wrapper to build one single project at a time to be able to specify the output folder per project. I don't bother to explain the `getVersion` function.\\n\\n#### Targets module\\n\\nThis is where I define all the separate steps. When I have all my helpers settled it is quite straighforward:\\n\\n```language-fsharp\\nmodule Targets =\\n  Target \\\"Clean\\\" (fun() ->\\n    CleanDirs [buildDir; deployDir; testDir]\\n  )\\n\\n  Target \\\"RestorePackages\\\" (fun _ ->\\n    packages\\n    |> Seq.iter (RestorePackage (fun p -> {p with OutputPath = \\\"./src/packages\\\"}))\\n  )\\n\\n  Target \\\"Build\\\" (fun() ->\\n    projects\\n    |> Seq.iter build\\n  )\\n\\n  Target \\\"Web\\\" (fun _ ->\\n    Npm (fun p ->\\n      { p with\\n          Command = Install Standard\\n          WorkingDirectory = \\\"./src/FAKESimple.Web/\\\"\\n      })\\n\\n    Npm (fun p ->\\n      { p with\\n          Command = (Run \\\"build\\\")\\n          WorkingDirectory = \\\"./src/FAKESimple.Web/\\\"\\n      })\\n  )\\n\\n  Target \\\"CopyWeb\\\" (fun _ ->\\n    let targetDir = packagingDir @@ \\\"dist\\\"\\n    let sourceDir = \\\"./src/FAKESimple.Web/dist\\\"\\n    CopyDir targetDir sourceDir (fun x -> true)\\n  )\\n\\n  Target \\\"BuildTest\\\" (fun() ->\\n    testProjects\\n    |> MSBuildDebug testDir \\\"Build\\\"\\n    |> ignore\\n  )\\n\\n  Target \\\"Test\\\" (fun() ->\\n    !! (testDir + \\\"/*.Tests.dll\\\")\\n        |> xUnit2 (fun p ->\\n            {p with\\n                ShadowCopy = false;\\n                HtmlOutputPath = Some (testDir @@ \\\"xunit.html\\\");\\n                XmlOutputPath = Some (testDir @@ \\\"xunit.xml\\\");\\n            })\\n  )\\n\\n  Target \\\"Package\\\" (fun _ ->\\n    trace \\\"Packing the web\\\"\\n    let version = getVersion()\\n    NuGet (fun p ->\\n          {p with\\n              Authors = [\\\"Tomas Jansson\\\"]\\n              Project = \\\"FAKESimple.Web\\\"\\n              Description = \\\"Demoing FAKE\\\"\\n              OutputPath = deployDir\\n              Summary = \\\"Does this work\\\"\\n              WorkingDir = packagingDir\\n              Version = version\\n              Publish = false })\\n              (packagingDir + \\\"/FAKESimple.Web.nuspec\\\")\\n  )\\n\\n  Target \\\"Publish\\\" (fun _ ->\\n    match buildServer with\\n    | BuildServer.AppVeyor ->\\n        publishOnAppveyor deployDir\\n    | _ -> ()\\n  )\\n\\n  Target \\\"Create release\\\" (fun _ ->\\n    let version = getVersion()\\n    let release = CreateRelease({ releaseOptions with Project = \\\"FAKESimple.Web\\\"; Version = version }, None)\\n    executeOcto release\\n  )\\n\\n  Target \\\"Deploy\\\" (fun _ ->\\n    let version = getVersion()\\n    let deploy = DeployRelease(\\n                  { deployOptions with\\n                      Project = \\\"FAKESimple.Web\\\"\\n                      Version = version\\n                      DeployTo = \\\"Prod\\\"\\n                      WaitForDeployment = true})\\n    executeOcto deploy\\n  )\\n\\n  Target \\\"Default\\\" (fun _ ->\\n    ()\\n  )\\n```\\n\\nThe responsibility of each target is as follows:\\n\\n* `Clean` - removes all the output files so I get a clean build\\n* `RestorePackages` - gets all the packages from nuget\\n* `Build` - take my definition of project files and runs the build helper for each one\\n* `Web` - restore all the node modules and then builds the js-app using `Npm`\\n* `CopyWeb` - copy the js-app to the web application so it can be added to the package for deployment\\n* `BuildTest` - same as `Build` but for the test projects\\n* `Test` - execute the tests\\n* `Package` - packages the web application to a nuget package that I can use for deployment\\n* `Publish` - publishes all the packages to the nuget feed, in this case AppVeyor\\n* `Create release` - creates a release on Octopus Deploy\\n* `Deploy` - deploy the release created\\n* `Default` - empty default step that I can always use to run everything\\n\\nThere were many steps there, but I think it is nice to have that separation in place. I could probably combine some of the steps, but I like it as it is since it is easier to move things around if I want to.\\n\\n#### The build process\\n\\nThe last part is to defined the dependencies between all the targets, and here it is:\\n\\n```language-fsharp\\n\\\"Clean\\\"\\n==> \\\"RestorePackages\\\"\\n==> \\\"Build\\\"\\n==> \\\"Web\\\"\\n==> \\\"CopyWeb\\\"\\n==> \\\"BuildTest\\\"\\n==> \\\"Test\\\"\\n==> \\\"Package\\\"\\n==> \\\"Publish\\\"\\n==> \\\"Create release\\\"\\n==> \\\"Deploy\\\"\\n==> \\\"Default\\\"\\n\\nRunTargetOrDefault \\\"Default\\\"\\n```\\n\\nI most likely only want to run down to `Test` locally and need to make some adjustments to run everything locally, but it is doable. The important part is that I can create an actual artifact locally, and that I'm doing it the same way as I would on the build server.\\n\\n\\n## Summary\\n\\nThis post became longer than I thought, but I hope you see the use of using FAKE. Once more I think F# shows that it is a good fit for many different problems, not just science and math. If I start a new .NET-project today I would definitely add FAKE as one of the first things. That gives me a reliable build that executes the same way on the server and locally as well as version control of the build process compared to having it all configured in the CI server.\\n\\nIf you find any improvements, please comment here or on GitHub. You should be able to clone the [repo](https://github.com/mastoj/FAKESimpleDemo) and just execute `build.cmd Test` to get started.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I've been using <a href=\"https://www.jetbrains.com/teamcity/\">TeamCity</a> for quite a while and trusted it to do the right thing when it comes to building. It has a lot of built in feature to do so, but the moment you start to define your build inside TeamCity you have painted yourself into a corner. I still think TeamCity is a great tooling triggering my builds, but I have come to realize that I should start to put the actual definition of the builds outside of TeamCity, or whatever CI tool I am using.</p>\n<h2 id=\"whybuildscripts\">Why build scripts?</h2>\n<p>Many people argue that it works just fine with using TeamCity and other CI tools to define the way you build, and it is when you have a set of smaller projects I think. The moment when you need to do more things in your build it make sense to put it in a separate script file and here are some of my reasons:</p>\n<ul>\n<li>You got your actual build process under source control together with your code instead of defined in a CI tool</li>\n<li>It is easier to handle versioning</li>\n<li>The way you build on the server is identical to the way you build locally</li>\n<li>You will not be locked in to one CI tool, if you use TeamCity today it is quite easy to switch to AppVeyor tomorrow and TravisCI the day after that.</li>\n</ul>\n<h2 id=\"whatisfake\">What is FAKE</h2>\n<p><a href=\"http://fsharp.github.io/FAKE/\">FAKE</a> is a domain specific language, <a href=\"https://en.wikipedia.org/wiki/Domain-specific_language\">dsl</a>, for build tasks implemented in <a href=\"http://fsharp.org/\">F#</a>. That it is implemented in F# doesn't mean you can only build F# projects, in fact you should event be able to build java project if you would like to. F#, along with many functional programming languages, is terrific when you want to create a dsl, since functional programming languages often are more expressive and not as verbose as OO languages. This is not an introduction to FAKE, and the documentation is <a href=\"http://fsharp.github.io/FAKE/\">here</a> if you want to check it out. Later in the post I will go through what my FAKE script look like to solve the problems I imagined.</p>\n<h2 id=\"thedemo\">The demo</h2>\n<p>The thing I wanted to try was this:</p>\n<ul>\n<li>Simple .NET Web app that includes some js building</li>\n<li>GitHub as source control</li>\n<li>AppVeyor as a CI engine to execute build</li>\n<li>Octopus Deploy to handle deployment</li>\n<li>Deploy to Azure Web App</li>\n<li>Most of it configured in FAKE</li>\n</ul>\n<p>Illustration of the flow from code to deployment:</p>\n<p><img src=\"/content/images/2015/10/CodeToDeploy.JPG\" alt=\"The build deploy process\"></p>\n<p>I think the end result is really good since it is the first time I used FAKEand AppVeyor. If you want to see the sample app and also the builds scripts they are in this repo at github: <a href=\"https://github.com/mastoj/FAKESimpleDemo\">https://github.com/mastoj/FAKESimpleDemo</a></p>\n<h3 id=\"appveyor\">Appveyor</h3>\n<p>I could as well used TeamCity, but I thought I would try <a href=\"http://www.appveyor.com/\">Appveyor</a> out. The goal of AppVeyor is:</p>\n<blockquote>\n<p>AppVeyor aims to give powerful Continuous Integration and Deployment tools to every .NET developer without the hassle of setting up and maintaining their own build server. - <a href=\"http://www.appveyor.com/about\">About AppVeyor</a></p>\n</blockquote>\n<p>Their goal is the reason I think AppVeyor is interesting and I wanted to take it for a spin. The experience has been truly amazing with really fast build times and easy to get started, and another really positive aspect is that I don't have to think about maintaining the build server.</p>\n<p>AppVeyor can do a lot more than I did, one might even argue that I should use AppVeyor instead of Octopus Deploy for deployment, but this is a proof of concept for a scenario were I still will have Octopus Deploy and I really like Octopus Deploy :).</p>\n<p>When you configure AppVeyor you can do so in the UI or with a yml-file. Of course I choose to use a yml-file since I do want as much as I can in source control. The configuration file I created is minimal since I have FAKE that take care of the actual build process:</p>\n<pre><code class=\"language-language-yml\">environment:\n  version: 1.0.0\nassembly_info:\n  patch: false\nbuild_script:\n  build.cmd\ntest: off\n</code></pre>\n<p>This specifies a environment variable <code>version</code> and that AppVeyor should not run tests or patch the assmebly information file. To start the build the <code>build.cmd default</code> commmand should be used. I can still run test and patch the assembly information file, but that is something I want to do from FAKE and not define in AppVeyor.</p>\n<p>I alos added some environment settings in the UI like this:</p>\n<p><img src=\"/content/images/2015/10/AppVeyorEnv.PNG\" alt=\"AppVeyor environment settings\"></p>\n<p>The reason I added them there and not in source control is because that is not I want to have on GitHub. The same would apply if I needed some other &quot;secret&quot; settings during build, then I would add that a a environment setting in the UI. I do the same thing if I use TeamCity.</p>\n<h3 id=\"octopusdeploy\">Octopus Deploy</h3>\n<p>I'm not going to go through what Octopus Deploy and what you can do with it, all you nned to know is that it is a great tool for handling deployments to multiple different environments locally as well as in the cloud.</p>\n<p>To deploy to an Azure Web App you need to create an account under Azure that is an Azure account. You create accounts in <code>Environment-&gt;Accounts</code>. To create an Azure account you will need a Management Certificate (.pfx) and your subscriptiong id. I don't remember which guide I followed to generate mine, but this might work: <a href=\"https://azure.microsoft.com/en-us/blog/obtaining-a-certificate-for-use-with-windows-azure-web-sites-waws/\">https://azure.microsoft.com/en-us/blog/obtaining-a-certificate-for-use-with-windows-azure-web-sites-waws/</a></p>\n<p>When that is done you just add your project and creates a deployment step of type <code>Deploy an Azure Web App</code> and you are good to go. The step configuration should like something like:</p>\n<p><img src=\"/content/images/2015/10/OctopusDeployStep.PNG\" alt=\"Octopus deploy configuration\"></p>\n<h3 id=\"fake\">FAKE</h3>\n<p>This is were all the magic happens. I will try to break down this file: <a href=\"https://github.com/mastoj/FAKESimpleDemo/blob/master/build.fsx\">https://github.com/mastoj/FAKESimpleDemo/blob/master/build.fsx</a> and explain the different parts of it. I expect you to know the minimum basics of FAKE, that is, what is a target and how you set up a build process with the <code>==&gt;</code> operator.</p>\n<h4 id=\"bootstrap\">Bootstrap</h4>\n<p>To get FAKE running easily I also have a bootstrap file <a href=\"https://github.com/mastoj/FAKESimpleDemo/blob/master/build.cmd\"><code>build.cmd</code></a>.</p>\n<pre><code>@echo off\ncls\nNuGet.exe &quot;Install&quot; &quot;FAKE&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;OctopusTools&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;Node.js&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;Npm.js&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\n&quot;packages\\FAKE\\tools\\Fake.exe&quot; build.fsx %*\n</code></pre>\n<p>All this file does is installing the tools I need to build the project, and then calls the build script with the arguments provided.</p>\n<p>I separated the build script into some modules to make it easier to follow and I thought I would go through this module by module.</p>\n<h4 id=\"npmmodule\">Npm module</h4>\n<p>The demo application had a simple js-app (that alerted &quot;Hello&quot;) to show that it is possible to build js-apps with FAKE. I would be stupid if I did not use the tools from the js-community to do the heavy lifting here, so that is exactly what I did. You can find the <a href=\"https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/package.json\"><code>package.json</code> here</a> and the <a href=\"https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/gulpfile.js\"><code>gulpfile.js</code> here</a>. I won't cover those since it is a different topic. When npm and gulp was configured all I needed to do was trigger it from FAKE and to do so I wrote a simple <code>Npm</code> wrapper. (I might extract this and try to submit a PR to FAKE later). The code is quite simple:</p>\n<pre><code class=\"language-language-fsharp\">module Npm =\n  open System\n\n  let npmFileName =\n    match isUnix with\n      | true -&gt; &quot;/usr/local/bin/npm&quot;\n      | _ -&gt; &quot;./packages/Npm.js/tools/npm.cmd&quot;\n\n  type InstallArgs =\n    | Standard\n    | Forced\n\n  type NpmCommand =\n    | Install of InstallArgs\n    | Run of string\n\n  type NpmParams = {\n    Src: string\n    NpmFilePath: string\n    WorkingDirectory: string\n    Command: NpmCommand\n    Timeout: TimeSpan\n  }\n\n  let npmParams = {\n    Src = &quot;&quot;\n    NpmFilePath = npmFileName\n    Command = Install Standard\n    WorkingDirectory = &quot;.&quot;\n    Timeout = TimeSpan.MaxValue\n  }\n\n  let parseInsallArgs = function\n    | Standard -&gt; &quot;&quot;\n    | Forced -&gt; &quot; --force&quot;\n\n  let parse command =\n    match command with\n    | Install installArgs -&gt; sprintf &quot;install%s&quot; (installArgs |&gt; parseInsallArgs)\n    | Run str -&gt; sprintf &quot;run %s&quot; str\n\n  let run npmParams =\n    let npmPath = Path.GetFullPath(npmParams.NpmFilePath)\n    let arguments = npmParams.Command |&gt; parse\n    let result = ExecProcess (\n                  fun info -&gt;\n                    info.FileName &lt;- npmPath\n                    info.WorkingDirectory &lt;- npmParams.WorkingDirectory\n                    info.Arguments &lt;- arguments\n                  ) npmParams.Timeout\n    if result &lt;&gt; 0 then failwith (sprintf &quot;'npm %s' failed&quot; arguments)\n\n  let Npm f =\n    npmParams |&gt; f |&gt; run\n</code></pre>\n<p>The <code>Npm</code> function is the important part. There I take the default arguments, pass it through <code>f</code> which adds changes to the arguments and then that is passed to <code>run</code>. The <code>run</code> method parse the <code>Command</code> property and execute <code>Npm</code>. It doesn't get simpler than that.</p>\n<h4 id=\"octohelpersmodule\">OctoHelpers module</h4>\n<p>There were two things I wanted to do with Octopus Deploy; create release and create deploy. This helper module is a wrapper around the <code>Octo</code> module in FAKE since there were a lot of similarities between the steps.</p>\n<pre><code class=\"language-language-fsharp\">module OctoHelpers =\n  let executeOcto command =\n    let serverName = environVar &quot;OCTO_SERVER&quot;\n    let apiKey = environVar &quot;OCTO_KEY&quot;\n    let server = { Server = serverName; ApiKey = apiKey }\n    Octo (fun octoParams -&gt;\n        { octoParams with\n            ToolPath = &quot;./packages/octopustools&quot;\n            Server   = server\n            Command  = command }\n    )\n</code></pre>\n<p>All that is going on here is that I read some things from environment variables that should be configured on AppVeyor (or the CI you use) and then execute an action against Octopus Deploy.</p>\n<h4 id=\"appveyorhelpermodule\">AppVeyorHelper module</h4>\n<p>I extracted the things that dealt with AppVeyor integration to a separate module to keep my targets clean (I get to the targets soon). It is actually one thing I'm doing on AppVeyor and that is publishing the artifacts to the nuget feed hosted by AppVeyor.</p>\n<pre><code class=\"language-language-fsharp\">module AppVeyorHelpers =\n  let execOnAppveyor arguments =\n    let result =\n      ExecProcess (fun info -&gt;\n        info.FileName &lt;- &quot;appveyor&quot;\n        info.Arguments &lt;- arguments\n        ) (System.TimeSpan.FromMinutes 2.0)\n    if result &lt;&gt; 0 then failwith (sprintf &quot;Failed to execute appveyor command: %s&quot; arguments)\n    trace &quot;Published packages&quot;\n\n  let publishOnAppveyor folder =\n    !! (folder + &quot;*.nupkg&quot;)\n    |&gt; Seq.iter (fun artifact -&gt; execOnAppveyor (sprintf &quot;PushArtifact %s&quot; artifact))\n</code></pre>\n<p>The <code>publishOnAppveyor</code> function takes a folder and finds all the nuget packages in it. After that it executes the <code>appveyor</code> command to publish every package to the feed.</p>\n<h4 id=\"settingsmodule\">Settings module</h4>\n<p>The module name isn't perfect, but what is. It contains all the variables that are used in the targets as well as two three helper functions:</p>\n<pre><code class=\"language-language-fsharp\">module Settings =\n  let buildDir = &quot;./.build/&quot;\n  let packagingDir = buildDir + &quot;FAKESimple.Web/_PublishedWebsites/FAKESimple.Web&quot;\n  let deployDir = &quot;./.deploy/&quot;\n  let testDir = &quot;./.test/&quot;\n  let projects = !! &quot;src/**/*.csproj&quot; -- &quot;src/**/*.Tests.csproj&quot;\n  let testProjects = !! &quot;src/**/*.Tests.csproj&quot;\n  let packages = !! &quot;./**/packages.config&quot;\n\n  let getOutputDir proj =\n    let folderName = Directory.GetParent(proj).Name\n    sprintf &quot;%s%s/&quot; buildDir folderName\n\n  let build proj =\n    let outputDir = proj |&gt; getOutputDir\n    MSBuildRelease outputDir &quot;ResolveReferences;Build&quot; [proj] |&gt; ignore\n\n  let getVersion() =\n    let buildCandidate = (environVar &quot;APPVEYOR_BUILD_NUMBER&quot;)\n    if buildCandidate = &quot;&quot; || buildCandidate = null then &quot;1.0.0&quot; else (sprintf &quot;1.0.0.%s&quot; buildCandidate)\n</code></pre>\n<p>The <code>getOutputDir</code> is used to get the name of the folder of a file, it says <code>proj</code> but it could be any file. I'm using it to create one output folder per project instead of everything ending up in the same folder or under <code>bin/release</code> as it usually does when building from VS. It is using the convention that the parent folder name of a project file is the name of the project. The <code>build</code> function is a wrapper to build one single project at a time to be able to specify the output folder per project. I don't bother to explain the <code>getVersion</code> function.</p>\n<h4 id=\"targetsmodule\">Targets module</h4>\n<p>This is where I define all the separate steps. When I have all my helpers settled it is quite straighforward:</p>\n<pre><code class=\"language-language-fsharp\">module Targets =\n  Target &quot;Clean&quot; (fun() -&gt;\n    CleanDirs [buildDir; deployDir; testDir]\n  )\n\n  Target &quot;RestorePackages&quot; (fun _ -&gt;\n    packages\n    |&gt; Seq.iter (RestorePackage (fun p -&gt; {p with OutputPath = &quot;./src/packages&quot;}))\n  )\n\n  Target &quot;Build&quot; (fun() -&gt;\n    projects\n    |&gt; Seq.iter build\n  )\n\n  Target &quot;Web&quot; (fun _ -&gt;\n    Npm (fun p -&gt;\n      { p with\n          Command = Install Standard\n          WorkingDirectory = &quot;./src/FAKESimple.Web/&quot;\n      })\n\n    Npm (fun p -&gt;\n      { p with\n          Command = (Run &quot;build&quot;)\n          WorkingDirectory = &quot;./src/FAKESimple.Web/&quot;\n      })\n  )\n\n  Target &quot;CopyWeb&quot; (fun _ -&gt;\n    let targetDir = packagingDir @@ &quot;dist&quot;\n    let sourceDir = &quot;./src/FAKESimple.Web/dist&quot;\n    CopyDir targetDir sourceDir (fun x -&gt; true)\n  )\n\n  Target &quot;BuildTest&quot; (fun() -&gt;\n    testProjects\n    |&gt; MSBuildDebug testDir &quot;Build&quot;\n    |&gt; ignore\n  )\n\n  Target &quot;Test&quot; (fun() -&gt;\n    !! (testDir + &quot;/*.Tests.dll&quot;)\n        |&gt; xUnit2 (fun p -&gt;\n            {p with\n                ShadowCopy = false;\n                HtmlOutputPath = Some (testDir @@ &quot;xunit.html&quot;);\n                XmlOutputPath = Some (testDir @@ &quot;xunit.xml&quot;);\n            })\n  )\n\n  Target &quot;Package&quot; (fun _ -&gt;\n    trace &quot;Packing the web&quot;\n    let version = getVersion()\n    NuGet (fun p -&gt;\n          {p with\n              Authors = [&quot;Tomas Jansson&quot;]\n              Project = &quot;FAKESimple.Web&quot;\n              Description = &quot;Demoing FAKE&quot;\n              OutputPath = deployDir\n              Summary = &quot;Does this work&quot;\n              WorkingDir = packagingDir\n              Version = version\n              Publish = false })\n              (packagingDir + &quot;/FAKESimple.Web.nuspec&quot;)\n  )\n\n  Target &quot;Publish&quot; (fun _ -&gt;\n    match buildServer with\n    | BuildServer.AppVeyor -&gt;\n        publishOnAppveyor deployDir\n    | _ -&gt; ()\n  )\n\n  Target &quot;Create release&quot; (fun _ -&gt;\n    let version = getVersion()\n    let release = CreateRelease({ releaseOptions with Project = &quot;FAKESimple.Web&quot;; Version = version }, None)\n    executeOcto release\n  )\n\n  Target &quot;Deploy&quot; (fun _ -&gt;\n    let version = getVersion()\n    let deploy = DeployRelease(\n                  { deployOptions with\n                      Project = &quot;FAKESimple.Web&quot;\n                      Version = version\n                      DeployTo = &quot;Prod&quot;\n                      WaitForDeployment = true})\n    executeOcto deploy\n  )\n\n  Target &quot;Default&quot; (fun _ -&gt;\n    ()\n  )\n</code></pre>\n<p>The responsibility of each target is as follows:</p>\n<ul>\n<li><code>Clean</code> - removes all the output files so I get a clean build</li>\n<li><code>RestorePackages</code> - gets all the packages from nuget</li>\n<li><code>Build</code> - take my definition of project files and runs the build helper for each one</li>\n<li><code>Web</code> - restore all the node modules and then builds the js-app using <code>Npm</code></li>\n<li><code>CopyWeb</code> - copy the js-app to the web application so it can be added to the package for deployment</li>\n<li><code>BuildTest</code> - same as <code>Build</code> but for the test projects</li>\n<li><code>Test</code> - execute the tests</li>\n<li><code>Package</code> - packages the web application to a nuget package that I can use for deployment</li>\n<li><code>Publish</code> - publishes all the packages to the nuget feed, in this case AppVeyor</li>\n<li><code>Create release</code> - creates a release on Octopus Deploy</li>\n<li><code>Deploy</code> - deploy the release created</li>\n<li><code>Default</code> - empty default step that I can always use to run everything</li>\n</ul>\n<p>There were many steps there, but I think it is nice to have that separation in place. I could probably combine some of the steps, but I like it as it is since it is easier to move things around if I want to.</p>\n<h4 id=\"thebuildprocess\">The build process</h4>\n<p>The last part is to defined the dependencies between all the targets, and here it is:</p>\n<pre><code class=\"language-language-fsharp\">&quot;Clean&quot;\n==&gt; &quot;RestorePackages&quot;\n==&gt; &quot;Build&quot;\n==&gt; &quot;Web&quot;\n==&gt; &quot;CopyWeb&quot;\n==&gt; &quot;BuildTest&quot;\n==&gt; &quot;Test&quot;\n==&gt; &quot;Package&quot;\n==&gt; &quot;Publish&quot;\n==&gt; &quot;Create release&quot;\n==&gt; &quot;Deploy&quot;\n==&gt; &quot;Default&quot;\n\nRunTargetOrDefault &quot;Default&quot;\n</code></pre>\n<p>I most likely only want to run down to <code>Test</code> locally and need to make some adjustments to run everything locally, but it is doable. The important part is that I can create an actual artifact locally, and that I'm doing it the same way as I would on the build server.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>This post became longer than I thought, but I hope you see the use of using FAKE. Once more I think F# shows that it is a good fit for many different problems, not just science and math. If I start a new .NET-project today I would definitely add FAKE as one of the first things. That gives me a reliable build that executes the same way on the server and locally as well as version control of the build process compared to having it all configured in the CI server.</p>\n<p>If you find any improvements, please comment here or on GitHub. You should be able to clone the <a href=\"https://github.com/mastoj/FAKESimpleDemo\">repo</a> and just execute <code>build.cmd Test</code> to get started.</p>\n<!--kg-card-end: markdown-->","comment_id":"59","plaintext":"I've been using TeamCity [https://www.jetbrains.com/teamcity/] for quite a while\nand trusted it to do the right thing when it comes to building. It has a lot of\nbuilt in feature to do so, but the moment you start to define your build inside\nTeamCity you have painted yourself into a corner. I still think TeamCity is a\ngreat tooling triggering my builds, but I have come to realize that I should\nstart to put the actual definition of the builds outside of TeamCity, or\nwhatever CI tool I am using.\n\nWhy build scripts?\nMany people argue that it works just fine with using TeamCity and other CI tools\nto define the way you build, and it is when you have a set of smaller projects I\nthink. The moment when you need to do more things in your build it make sense to\nput it in a separate script file and here are some of my reasons:\n\n * You got your actual build process under source control together with your\n   code instead of defined in a CI tool\n * It is easier to handle versioning\n * The way you build on the server is identical to the way you build locally\n * You will not be locked in to one CI tool, if you use TeamCity today it is\n   quite easy to switch to AppVeyor tomorrow and TravisCI the day after that.\n\nWhat is FAKE\nFAKE [http://fsharp.github.io/FAKE/] is a domain specific language, dsl\n[https://en.wikipedia.org/wiki/Domain-specific_language], for build tasks\nimplemented in F# [http://fsharp.org/]. That it is implemented in F# doesn't\nmean you can only build F# projects, in fact you should event be able to build\njava project if you would like to. F#, along with many functional programming\nlanguages, is terrific when you want to create a dsl, since functional\nprogramming languages often are more expressive and not as verbose as OO\nlanguages. This is not an introduction to FAKE, and the documentation is here\n[http://fsharp.github.io/FAKE/] if you want to check it out. Later in the post I\nwill go through what my FAKE script look like to solve the problems I imagined.\n\nThe demo\nThe thing I wanted to try was this:\n\n * Simple .NET Web app that includes some js building\n * GitHub as source control\n * AppVeyor as a CI engine to execute build\n * Octopus Deploy to handle deployment\n * Deploy to Azure Web App\n * Most of it configured in FAKE\n\nIllustration of the flow from code to deployment:\n\n\n\nI think the end result is really good since it is the first time I used FAKEand\nAppVeyor. If you want to see the sample app and also the builds scripts they are\nin this repo at github: https://github.com/mastoj/FAKESimpleDemo\n\nAppveyor\nI could as well used TeamCity, but I thought I would try Appveyor\n[http://www.appveyor.com/] out. The goal of AppVeyor is:\n\n> AppVeyor aims to give powerful Continuous Integration and Deployment tools to\nevery .NET developer without the hassle of setting up and maintaining their own\nbuild server. - About AppVeyor [http://www.appveyor.com/about]\n\n\nTheir goal is the reason I think AppVeyor is interesting and I wanted to take it\nfor a spin. The experience has been truly amazing with really fast build times\nand easy to get started, and another really positive aspect is that I don't have\nto think about maintaining the build server.\n\nAppVeyor can do a lot more than I did, one might even argue that I should use\nAppVeyor instead of Octopus Deploy for deployment, but this is a proof of\nconcept for a scenario were I still will have Octopus Deploy and I really like\nOctopus Deploy :).\n\nWhen you configure AppVeyor you can do so in the UI or with a yml-file. Of\ncourse I choose to use a yml-file since I do want as much as I can in source\ncontrol. The configuration file I created is minimal since I have FAKE that take\ncare of the actual build process:\n\nenvironment:\n  version: 1.0.0\nassembly_info:\n  patch: false\nbuild_script:\n  build.cmd\ntest: off\n\n\nThis specifies a environment variable version and that AppVeyor should not run\ntests or patch the assmebly information file. To start the build the build.cmd\ndefault commmand should be used. I can still run test and patch the assembly\ninformation file, but that is something I want to do from FAKE and not define in\nAppVeyor.\n\nI alos added some environment settings in the UI like this:\n\n\n\nThe reason I added them there and not in source control is because that is not I\nwant to have on GitHub. The same would apply if I needed some other \"secret\"\nsettings during build, then I would add that a a environment setting in the UI.\nI do the same thing if I use TeamCity.\n\nOctopus Deploy\nI'm not going to go through what Octopus Deploy and what you can do with it, all\nyou nned to know is that it is a great tool for handling deployments to multiple\ndifferent environments locally as well as in the cloud.\n\nTo deploy to an Azure Web App you need to create an account under Azure that is\nan Azure account. You create accounts in Environment->Accounts. To create an\nAzure account you will need a Management Certificate (.pfx) and your\nsubscriptiong id. I don't remember which guide I followed to generate mine, but\nthis might work: \nhttps://azure.microsoft.com/en-us/blog/obtaining-a-certificate-for-use-with-windows-azure-web-sites-waws/\n\nWhen that is done you just add your project and creates a deployment step of\ntype Deploy an Azure Web App and you are good to go. The step configuration\nshould like something like:\n\n\n\nFAKE\nThis is were all the magic happens. I will try to break down this file: \nhttps://github.com/mastoj/FAKESimpleDemo/blob/master/build.fsx and explain the\ndifferent parts of it. I expect you to know the minimum basics of FAKE, that is,\nwhat is a target and how you set up a build process with the ==> operator.\n\nBootstrap\nTo get FAKE running easily I also have a bootstrap file build.cmd\n[https://github.com/mastoj/FAKESimpleDemo/blob/master/build.cmd].\n\n@echo off\ncls\nNuGet.exe \"Install\" \"FAKE\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"OctopusTools\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"Node.js\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"Npm.js\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\n\"packages\\FAKE\\tools\\Fake.exe\" build.fsx %*\n\n\nAll this file does is installing the tools I need to build the project, and then\ncalls the build script with the arguments provided.\n\nI separated the build script into some modules to make it easier to follow and I\nthought I would go through this module by module.\n\nNpm module\nThe demo application had a simple js-app (that alerted \"Hello\") to show that it\nis possible to build js-apps with FAKE. I would be stupid if I did not use the\ntools from the js-community to do the heavy lifting here, so that is exactly\nwhat I did. You can find the package.json here\n[https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/package.json] \nand the gulpfile.js here\n[https://github.com/mastoj/FAKESimpleDemo/blob/master/src/FAKESimple.Web/gulpfile.js]\n. I won't cover those since it is a different topic. When npm and gulp was\nconfigured all I needed to do was trigger it from FAKE and to do so I wrote a\nsimple Npm wrapper. (I might extract this and try to submit a PR to FAKE later).\nThe code is quite simple:\n\nmodule Npm =\n  open System\n\n  let npmFileName =\n    match isUnix with\n      | true -> \"/usr/local/bin/npm\"\n      | _ -> \"./packages/Npm.js/tools/npm.cmd\"\n\n  type InstallArgs =\n    | Standard\n    | Forced\n\n  type NpmCommand =\n    | Install of InstallArgs\n    | Run of string\n\n  type NpmParams = {\n    Src: string\n    NpmFilePath: string\n    WorkingDirectory: string\n    Command: NpmCommand\n    Timeout: TimeSpan\n  }\n\n  let npmParams = {\n    Src = \"\"\n    NpmFilePath = npmFileName\n    Command = Install Standard\n    WorkingDirectory = \".\"\n    Timeout = TimeSpan.MaxValue\n  }\n\n  let parseInsallArgs = function\n    | Standard -> \"\"\n    | Forced -> \" --force\"\n\n  let parse command =\n    match command with\n    | Install installArgs -> sprintf \"install%s\" (installArgs |> parseInsallArgs)\n    | Run str -> sprintf \"run %s\" str\n\n  let run npmParams =\n    let npmPath = Path.GetFullPath(npmParams.NpmFilePath)\n    let arguments = npmParams.Command |> parse\n    let result = ExecProcess (\n                  fun info ->\n                    info.FileName <- npmPath\n                    info.WorkingDirectory <- npmParams.WorkingDirectory\n                    info.Arguments <- arguments\n                  ) npmParams.Timeout\n    if result <> 0 then failwith (sprintf \"'npm %s' failed\" arguments)\n\n  let Npm f =\n    npmParams |> f |> run\n\n\nThe Npm function is the important part. There I take the default arguments, pass\nit through f which adds changes to the arguments and then that is passed to run.\nThe run method parse the Command property and execute Npm. It doesn't get\nsimpler than that.\n\nOctoHelpers module\nThere were two things I wanted to do with Octopus Deploy; create release and\ncreate deploy. This helper module is a wrapper around the Octo module in FAKE\nsince there were a lot of similarities between the steps.\n\nmodule OctoHelpers =\n  let executeOcto command =\n    let serverName = environVar \"OCTO_SERVER\"\n    let apiKey = environVar \"OCTO_KEY\"\n    let server = { Server = serverName; ApiKey = apiKey }\n    Octo (fun octoParams ->\n        { octoParams with\n            ToolPath = \"./packages/octopustools\"\n            Server   = server\n            Command  = command }\n    )\n\n\nAll that is going on here is that I read some things from environment variables\nthat should be configured on AppVeyor (or the CI you use) and then execute an\naction against Octopus Deploy.\n\nAppVeyorHelper module\nI extracted the things that dealt with AppVeyor integration to a separate module\nto keep my targets clean (I get to the targets soon). It is actually one thing\nI'm doing on AppVeyor and that is publishing the artifacts to the nuget feed\nhosted by AppVeyor.\n\nmodule AppVeyorHelpers =\n  let execOnAppveyor arguments =\n    let result =\n      ExecProcess (fun info ->\n        info.FileName <- \"appveyor\"\n        info.Arguments <- arguments\n        ) (System.TimeSpan.FromMinutes 2.0)\n    if result <> 0 then failwith (sprintf \"Failed to execute appveyor command: %s\" arguments)\n    trace \"Published packages\"\n\n  let publishOnAppveyor folder =\n    !! (folder + \"*.nupkg\")\n    |> Seq.iter (fun artifact -> execOnAppveyor (sprintf \"PushArtifact %s\" artifact))\n\n\nThe publishOnAppveyor function takes a folder and finds all the nuget packages\nin it. After that it executes the appveyor command to publish every package to\nthe feed.\n\nSettings module\nThe module name isn't perfect, but what is. It contains all the variables that\nare used in the targets as well as two three helper functions:\n\nmodule Settings =\n  let buildDir = \"./.build/\"\n  let packagingDir = buildDir + \"FAKESimple.Web/_PublishedWebsites/FAKESimple.Web\"\n  let deployDir = \"./.deploy/\"\n  let testDir = \"./.test/\"\n  let projects = !! \"src/**/*.csproj\" -- \"src/**/*.Tests.csproj\"\n  let testProjects = !! \"src/**/*.Tests.csproj\"\n  let packages = !! \"./**/packages.config\"\n\n  let getOutputDir proj =\n    let folderName = Directory.GetParent(proj).Name\n    sprintf \"%s%s/\" buildDir folderName\n\n  let build proj =\n    let outputDir = proj |> getOutputDir\n    MSBuildRelease outputDir \"ResolveReferences;Build\" [proj] |> ignore\n\n  let getVersion() =\n    let buildCandidate = (environVar \"APPVEYOR_BUILD_NUMBER\")\n    if buildCandidate = \"\" || buildCandidate = null then \"1.0.0\" else (sprintf \"1.0.0.%s\" buildCandidate)\n\n\nThe getOutputDir is used to get the name of the folder of a file, it says proj \nbut it could be any file. I'm using it to create one output folder per project\ninstead of everything ending up in the same folder or under bin/release as it\nusually does when building from VS. It is using the convention that the parent\nfolder name of a project file is the name of the project. The build function is\na wrapper to build one single project at a time to be able to specify the output\nfolder per project. I don't bother to explain the getVersion function.\n\nTargets module\nThis is where I define all the separate steps. When I have all my helpers\nsettled it is quite straighforward:\n\nmodule Targets =\n  Target \"Clean\" (fun() ->\n    CleanDirs [buildDir; deployDir; testDir]\n  )\n\n  Target \"RestorePackages\" (fun _ ->\n    packages\n    |> Seq.iter (RestorePackage (fun p -> {p with OutputPath = \"./src/packages\"}))\n  )\n\n  Target \"Build\" (fun() ->\n    projects\n    |> Seq.iter build\n  )\n\n  Target \"Web\" (fun _ ->\n    Npm (fun p ->\n      { p with\n          Command = Install Standard\n          WorkingDirectory = \"./src/FAKESimple.Web/\"\n      })\n\n    Npm (fun p ->\n      { p with\n          Command = (Run \"build\")\n          WorkingDirectory = \"./src/FAKESimple.Web/\"\n      })\n  )\n\n  Target \"CopyWeb\" (fun _ ->\n    let targetDir = packagingDir @@ \"dist\"\n    let sourceDir = \"./src/FAKESimple.Web/dist\"\n    CopyDir targetDir sourceDir (fun x -> true)\n  )\n\n  Target \"BuildTest\" (fun() ->\n    testProjects\n    |> MSBuildDebug testDir \"Build\"\n    |> ignore\n  )\n\n  Target \"Test\" (fun() ->\n    !! (testDir + \"/*.Tests.dll\")\n        |> xUnit2 (fun p ->\n            {p with\n                ShadowCopy = false;\n                HtmlOutputPath = Some (testDir @@ \"xunit.html\");\n                XmlOutputPath = Some (testDir @@ \"xunit.xml\");\n            })\n  )\n\n  Target \"Package\" (fun _ ->\n    trace \"Packing the web\"\n    let version = getVersion()\n    NuGet (fun p ->\n          {p with\n              Authors = [\"Tomas Jansson\"]\n              Project = \"FAKESimple.Web\"\n              Description = \"Demoing FAKE\"\n              OutputPath = deployDir\n              Summary = \"Does this work\"\n              WorkingDir = packagingDir\n              Version = version\n              Publish = false })\n              (packagingDir + \"/FAKESimple.Web.nuspec\")\n  )\n\n  Target \"Publish\" (fun _ ->\n    match buildServer with\n    | BuildServer.AppVeyor ->\n        publishOnAppveyor deployDir\n    | _ -> ()\n  )\n\n  Target \"Create release\" (fun _ ->\n    let version = getVersion()\n    let release = CreateRelease({ releaseOptions with Project = \"FAKESimple.Web\"; Version = version }, None)\n    executeOcto release\n  )\n\n  Target \"Deploy\" (fun _ ->\n    let version = getVersion()\n    let deploy = DeployRelease(\n                  { deployOptions with\n                      Project = \"FAKESimple.Web\"\n                      Version = version\n                      DeployTo = \"Prod\"\n                      WaitForDeployment = true})\n    executeOcto deploy\n  )\n\n  Target \"Default\" (fun _ ->\n    ()\n  )\n\n\nThe responsibility of each target is as follows:\n\n * Clean - removes all the output files so I get a clean build\n * RestorePackages - gets all the packages from nuget\n * Build - take my definition of project files and runs the build helper for\n   each one\n * Web - restore all the node modules and then builds the js-app using Npm\n * CopyWeb - copy the js-app to the web application so it can be added to the\n   package for deployment\n * BuildTest - same as Build but for the test projects\n * Test - execute the tests\n * Package - packages the web application to a nuget package that I can use for\n   deployment\n * Publish - publishes all the packages to the nuget feed, in this case AppVeyor\n * Create release - creates a release on Octopus Deploy\n * Deploy - deploy the release created\n * Default - empty default step that I can always use to run everything\n\nThere were many steps there, but I think it is nice to have that separation in\nplace. I could probably combine some of the steps, but I like it as it is since\nit is easier to move things around if I want to.\n\nThe build process\nThe last part is to defined the dependencies between all the targets, and here\nit is:\n\n\"Clean\"\n==> \"RestorePackages\"\n==> \"Build\"\n==> \"Web\"\n==> \"CopyWeb\"\n==> \"BuildTest\"\n==> \"Test\"\n==> \"Package\"\n==> \"Publish\"\n==> \"Create release\"\n==> \"Deploy\"\n==> \"Default\"\n\nRunTargetOrDefault \"Default\"\n\n\nI most likely only want to run down to Test locally and need to make some\nadjustments to run everything locally, but it is doable. The important part is\nthat I can create an actual artifact locally, and that I'm doing it the same way\nas I would on the build server.\n\nSummary\nThis post became longer than I thought, but I hope you see the use of using\nFAKE. Once more I think F# shows that it is a good fit for many different\nproblems, not just science and math. If I start a new .NET-project today I would\ndefinitely add FAKE as one of the first things. That gives me a reliable build\nthat executes the same way on the server and locally as well as version control\nof the build process compared to having it all configured in the CI server.\n\nIf you find any improvements, please comment here or on GitHub. You should be\nable to clone the repo [https://github.com/mastoj/FAKESimpleDemo] and just\nexecute build.cmd Test to get started.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-10-24T07:38:08.000Z","updated_at":"2015-10-26T07:49:45.000Z","published_at":"2015-10-25T22:43:46.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe070","uuid":"8cdb9462-2e58-4bc0-82fe-36e12a993cb3","title":"Npm build with F# FAKE","slug":"npm-build-with-fsharp-fake","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Most web project to day has some javascript in them, and you should really build the javascript to minify them and also to find stupid errors. It would be stupid to implement the build part all over again in F#, instead you should use the tooling that already exists, like `node` and `npm`. Even though `npm` is used to build the javascript application I still want to control the overall build flow with FAKE, and for that reason I created the [FAKE NpmHelper](http://fsharp.github.io/FAKE/apidocs/fake-npmhelper.html).\\n\\n## Configure FAKE\\n\\nThe easiest way to get started is to install `node` and `npm` with `nuget` as part of the `build.cmd` before calling `build.fsx`. This will add `npm` to the default paths that is used by the helper. Don't worry, it is possible to override which `npm` file that should be used. A sample `build.cmd` can be found in my FAKE [sample](https://github.com/mastoj/FAKESimpleDemo) and looks like this:\\n\\n```\\necho off\\ncls\\nNuGet.exe \\\"Install\\\" \\\"FAKE\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"OctopusTools\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"Node.js\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\nNuGet.exe \\\"Install\\\" \\\"Npm.js\\\" \\\"-OutputDirectory\\\" \\\"packages\\\" \\\"-ExcludeVersion\\\"\\n\\\"packages\\\\FAKE\\\\tools\\\\Fake.exe\\\" build.fsx %*\\n```\\n\\n## Supported commands\\n\\nThere are only two supported commands where you get some type check, `Install` and `Run`. Below is the simplest possible sample to use those two.\\n\\n```language-fsharp\\nNpm (fun p ->\\n  { p with\\n      Command = Install Standard\\n      WorkingDirectory = \\\"./src/FAKESimple.Web/\\\"\\n  })\\n\\nNpm (fun p ->\\n  { p with\\n      Command = (Run \\\"build\\\")\\n      WorkingDirectory = \\\"./src/FAKESimple.Web/\\\"\\n  })\\n```\\n\\nI figured those two commands are the one you usually would like to run in this kind of scenario, but if you do want to run any of the other `npm` commands you can do so by using the `Custom` command parameter and just pass in the string you like. Or if it is something you think is commonly used send a PR or ping me about it :).\\n\\nThat's all, let me know if you have any questions.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Most web project to day has some javascript in them, and you should really build the javascript to minify them and also to find stupid errors. It would be stupid to implement the build part all over again in F#, instead you should use the tooling that already exists, like <code>node</code> and <code>npm</code>. Even though <code>npm</code> is used to build the javascript application I still want to control the overall build flow with FAKE, and for that reason I created the <a href=\"http://fsharp.github.io/FAKE/apidocs/fake-npmhelper.html\">FAKE NpmHelper</a>.</p>\n<h2 id=\"configurefake\">Configure FAKE</h2>\n<p>The easiest way to get started is to install <code>node</code> and <code>npm</code> with <code>nuget</code> as part of the <code>build.cmd</code> before calling <code>build.fsx</code>. This will add <code>npm</code> to the default paths that is used by the helper. Don't worry, it is possible to override which <code>npm</code> file that should be used. A sample <code>build.cmd</code> can be found in my FAKE <a href=\"https://github.com/mastoj/FAKESimpleDemo\">sample</a> and looks like this:</p>\n<pre><code>echo off\ncls\nNuGet.exe &quot;Install&quot; &quot;FAKE&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;OctopusTools&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;Node.js&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\nNuGet.exe &quot;Install&quot; &quot;Npm.js&quot; &quot;-OutputDirectory&quot; &quot;packages&quot; &quot;-ExcludeVersion&quot;\n&quot;packages\\FAKE\\tools\\Fake.exe&quot; build.fsx %*\n</code></pre>\n<h2 id=\"supportedcommands\">Supported commands</h2>\n<p>There are only two supported commands where you get some type check, <code>Install</code> and <code>Run</code>. Below is the simplest possible sample to use those two.</p>\n<pre><code class=\"language-language-fsharp\">Npm (fun p -&gt;\n  { p with\n      Command = Install Standard\n      WorkingDirectory = &quot;./src/FAKESimple.Web/&quot;\n  })\n\nNpm (fun p -&gt;\n  { p with\n      Command = (Run &quot;build&quot;)\n      WorkingDirectory = &quot;./src/FAKESimple.Web/&quot;\n  })\n</code></pre>\n<p>I figured those two commands are the one you usually would like to run in this kind of scenario, but if you do want to run any of the other <code>npm</code> commands you can do so by using the <code>Custom</code> command parameter and just pass in the string you like. Or if it is something you think is commonly used send a PR or ping me about it :).</p>\n<p>That's all, let me know if you have any questions.</p>\n<!--kg-card-end: markdown-->","comment_id":"62","plaintext":"Most web project to day has some javascript in them, and you should really build\nthe javascript to minify them and also to find stupid errors. It would be stupid\nto implement the build part all over again in F#, instead you should use the\ntooling that already exists, like node and npm. Even though npm is used to build\nthe javascript application I still want to control the overall build flow with\nFAKE, and for that reason I created the FAKE NpmHelper\n[http://fsharp.github.io/FAKE/apidocs/fake-npmhelper.html].\n\nConfigure FAKE\nThe easiest way to get started is to install node and npm with nuget as part of\nthe build.cmd before calling build.fsx. This will add npm to the default paths\nthat is used by the helper. Don't worry, it is possible to override which npm \nfile that should be used. A sample build.cmd can be found in my FAKE sample\n[https://github.com/mastoj/FAKESimpleDemo] and looks like this:\n\necho off\ncls\nNuGet.exe \"Install\" \"FAKE\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"OctopusTools\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"Node.js\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\nNuGet.exe \"Install\" \"Npm.js\" \"-OutputDirectory\" \"packages\" \"-ExcludeVersion\"\n\"packages\\FAKE\\tools\\Fake.exe\" build.fsx %*\n\n\nSupported commands\nThere are only two supported commands where you get some type check, Install and \nRun. Below is the simplest possible sample to use those two.\n\nNpm (fun p ->\n  { p with\n      Command = Install Standard\n      WorkingDirectory = \"./src/FAKESimple.Web/\"\n  })\n\nNpm (fun p ->\n  { p with\n      Command = (Run \"build\")\n      WorkingDirectory = \"./src/FAKESimple.Web/\"\n  })\n\n\nI figured those two commands are the one you usually would like to run in this\nkind of scenario, but if you do want to run any of the other npm commands you\ncan do so by using the Custom command parameter and just pass in the string you\nlike. Or if it is something you think is commonly used send a PR or ping me\nabout it :).\n\nThat's all, let me know if you have any questions.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-11-09T08:38:11.000Z","updated_at":"2015-11-15T22:35:39.000Z","published_at":"2015-11-15T22:34:03.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe071","uuid":"e18a2c22-8bc3-40c2-a62a-56f1af8f4d0f","title":"React hot load and ASP.NET 5","slug":"react-hot-load-and-asp-net-5","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"One or two months ago I saw a [presentation](https://www.youtube.com/watch?v=xsSnOQynTHs) by [Dan Abramov](https://twitter.com/dan_abramov) about hot loading and react. I have't done much react and I haven't done much webpack which is used to do this, but I know I wanted a sample for ASP.NET 5 and now I finally had the time to try it out. Even though the titles says ASP.NET 5 the solution can be used for basically any web project running on Windows.\\n\\n## TL;DR\\n\\nI've put together a small sample with react hot load and ASP.NET 5. You can find the code on [github](https://github.com/mastoj/ReactHotLoadAspNet/).\\n\\n## What is it?\\n\\n[React hot load](http://gaearon.github.io/react-hot-loader/) let you change the style files (css/less/scss) and javascript files on the server and see the result directly in the browser while keeping the browser state. This make it really easy to try things out and see the result directly instead of reloading the browser after every minor change. An example is when working with styling, instead of tweaking around in the dev tools you can just change the style file directly. Another example is if you're working with a javascript wizard, then it might be annoying to go through every step when you find an error, hot loading the javascript lets you continue where you are after you change the javascript. \\n\\nBelow is a demo showing the result. I use atom for changing less and jsx files since the support for jsx+ES6 in Visual Studio isn't the best yet.\\n\\n![Demo of react hot load](/content/images/2015/11/Demo.gif)\\n\\n## How it sort of works (my version)\\n\\nI won't cover the details of how hot loading works but the short version of how it works is something like this. Instead of reading the javascript files you read them from a node server, the node server \\\"injects\\\" a \\\"wrapper\\\" between the physical file on disc and the one you pointed too from your web page that routes your javascript command to the file that is currently up to date. If you make a change to a file the wrapper will see this and make a rebuild swapping out what has changed while keeping the state. Keeping the state is possible because of the architecture of react.\\n\\n## Gotchas\\n\\nMy goal was to get a sample up and running with both javascript and less since that is what I most likely will use in a real project, I also wanted everything to run with the latest version of everything. This caused some problems during the setup and here are a list of things that you need to do to your dev environment to get things working:\\n\\n* I wanted to run regular npm tasks and to do so in Visual Studio you need the extension [NPM Scripts Task Runner](https://visualstudiogallery.msdn.microsoft.com/8f2f2cbc-4da5-43ba-9de2-c9d08ade4941)\\n* For some reason `node 5` was needed, so install that.\\n* To get `node 5` to play nice in Visual Studio you need to add the path for External Web Tools. You find that setting under `Tools->Options->Projects and Solutions->External Web Tools`. Just add the path to `node 5` above the line that points to external tools. It looks like `node 5` installs node modules directly under modules folder and not in a hierarchy as before. This makes Visual Studio to think that a lot of the packages are extraneous, but you can just ignore that.\\n* One of the packages I used got some weird error if python wasn't installed, so python needs to be installed. I couldn't use version 3 of python so I installed version 2 from [chocolatey](https://chocolatey.org/packages/python2).\\n* This last part is a little bit tricky and only works because I have both VS2013 and VS2015 I think. I got some error which I found the answer on at [stackoverflow](http://stackoverflow.com/questions/33183161/node-gyp-error-tracker-error-trk0005-failed-to-locate-cl-exe-the-system-c). `Npm` needed to run `cl.exe` for some task, but it didn't find it for VS2015 so changing `Npm` to use VS2013 helped as the SO answer says. The command to do so is `npm config set msvs_version 2013 --global`.\\n\\n## The setup\\n\\nI won't cover the whole application and what it does, I'll just cover the pieces to get this up and running.\\n\\n### The package.json file\\n\\nThe file I ended up with looks like this\\n\\n```language-json\\n{\\n  \\\"version\\\": \\\"0.0.0\\\",\\n  \\\"name\\\": \\\"\\\",\\n  \\\"scripts\\\": {\\n    \\\"start\\\": \\\"node server.js\\\",\\n    \\\"build\\\": \\\"set NODE_ENV=production && webpack -p --progress --colors\\\"\\n  },\\n  \\\"devDependencies\\\": {\\n    \\\"babel-core\\\": \\\"^6.1.2\\\",\\n    \\\"babel-loader\\\": \\\"^6.0.1\\\",\\n    \\\"babel-preset-es2015\\\": \\\"^6.1.2\\\",\\n    \\\"babel-preset-react\\\": \\\"^6.1.2\\\",\\n    \\\"css-loader\\\": \\\"^0.22.0\\\",\\n    \\\"less\\\": \\\"^2.5.3\\\",\\n    \\\"less-loader\\\": \\\"^2.2.1\\\",\\n    \\\"react-hot-loader\\\": \\\"^1.3.0\\\",\\n    \\\"style-loader\\\": \\\"^0.13.0\\\",\\n    \\\"webpack\\\": \\\"^1.12.2\\\",\\n    \\\"webpack-dev-server\\\": \\\"^1.12.1\\\"\\n  },\\n  \\\"dependencies\\\": {\\n    \\\"jquery\\\": \\\"2.1.4\\\",\\n    \\\"marked\\\": \\\"^0.3.5\\\",\\n    \\\"react\\\": \\\"^0.14.0\\\",\\n    \\\"react-dom\\\": \\\"^0.14.0\\\"\\n  }\\n}\\n```\\n\\nI've added to `npm` scripts to the file, `build` that should be used when packaging for production on a build server and `start` that is used to start the server for development. This only works with [webpack](https://webpack.github.io/) so I installed that and all the loaders I needed to build `jsx` and `less` files.\\n\\n### The server.js\\n\\nThe `server.js` is a small node server that will host the javascript for us during development. It is started with the `start` script task defined in `package.json`. \\n\\n```language-javascript\\nvar webpack = require('webpack');\\nvar WebpackDevServer = require('webpack-dev-server');\\nvar config = require('./webpack.config');\\n\\nnew WebpackDevServer(webpack(config), {\\n    publicPath: config.output.publicPath,\\n    hot: true,\\n    historyApiFallback: true,\\n    headers: { 'Access-Control-Allow-Origin': '*' }\\n}).listen(3000, 'localhost', function (err, result) {\\n    if (err) {\\n        console.log(err);\\n    }\\n\\n    console.log('Listening at localhost:3000');\\n});\\n```\\n\\nThe server is based on this [boilerplate code](https://github.com/gaearon/react-hot-boilerplate/blob/master/server.js) but I added `headers: { 'Access-Control-Allow-Origin': '*' }`. When I upgraded everything to the latest bits `CORS` was required.\\n\\n## Webpack configuration\\n\\nYou can run `webpack` directly from the command line, but you usually use a configuration file to do so. This is my first time using `webpack` so it is most likely a guide to how you should do it, more a sample of how I got it to do what I wanted it to do. The `webpack.config.js` I ended up with looks like this:\\n\\n```language-javascript\\nvar webpack = require('webpack');\\nvar path = require('path');\\nvar outFolder = path.resolve(__dirname, \\\"./wwwroot/app\\\");\\nvar isProduction = process.env.NODE_ENV === 'production ';\\nvar jsxLoaders = isProduction ?\\n    ['babel?presets[]=es2015,presets[]=react'] :\\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\\nvar entryPoint = './content/app.jsx';\\nvar app = isProduction ? [entryPoint] : [\\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\\n    'webpack/hot/only-dev-server', // \\\"only\\\" prevents reload on syntax errors\\n    entryPoint\\n];\\n\\nmodule.exports = {\\n    entry: {\\n        app: app\\n    },\\n    output: {\\n        path: outFolder,\\n        filename: \\\"[name].js\\\",\\n        publicPath: 'http://localhost:3000/static/'\\n    },\\n    devtool: \\\"source-map\\\",\\n    minimize: true,\\n    module: {\\n        loaders: [{\\n            test: /\\\\.(js|jsx)$/,\\n            loaders: jsxLoaders,\\n            exclude: /node_modules/\\n        },\\n        {\\n            test: /\\\\.(css|less)$/,\\n            loaders: ['style','css','less']\\n        }]\\n    },\\n    plugins: [\\n      new webpack.HotModuleReplacementPlugin()\\n    ],\\n    resolve: {\\n        extensions: [\\\"\\\", \\\".webpack.js\\\", \\\".web.js\\\", \\\".js\\\", \\\".jsx\\\"]\\n    },\\n    devServer: {\\n        headers: { \\\"Access-Control-Allow-Origin\\\": \\\"*\\\" }\\n    }\\n};\\n```\\n\\nFirst I define some settings that differs depending on environment. The environment is set as an environment variable, see the `build` script task in the `package.json` file. One important part is this one: \\n\\n```language-javascript\\nvar jsxLoaders = isProduction ?\\n    ['babel?presets[]=es2015,presets[]=react'] :\\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\\nvar entryPoint = './content/app.jsx';\\nvar app = isProduction ? [entryPoint] : [\\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\\n    'webpack/hot/only-dev-server', // \\\"only\\\" prevents reload on syntax errors\\n    entryPoint\\n];\\n```\\n\\nThis is what make the actual server running, I used to port 3000 to host the files. I also needed to specify the `publicPath` under `output`, that's because the files are not served from the same application as the consumer of the files. As you can see, if we are doing a production build, by running `npm rum build`, we will only use the actual `app.jsx` as entry point. Also, we won't add `react-hot` (alias for `react-hot-loader`) to the list of `jsxLoaders` since I don't want hot loading enabled in production.\\n\\nI won't try to cover `webpack` in more depth since all this is sort of new to me. \\n\\n### The ASP.NET part\\n\\nIf you haven't figured it out by now, the ASP.NET solution stays mainly the same to get this working. The trick is actually just to fire up a node server to host your static content and then point the `script` tags in your solution to that server. So my simple index page looks like this:\\n\\n```language-aspnet\\n@{\\n    // ViewBag.Title = \\\"Home Page\\\";\\n}\\n<html>\\n<head>\\n    <title>Sample hot load demo</title>\\n    <link href=\\\"/static/\\\"/>\\n</head>\\n<body>\\n    <div id=\\\"content\\\"></div>\\n    @*<script src=\\\"/static/app.js\\\"></script>*@\\n    <script src=\\\"http://localhost:3000/static/app.js\\\"></script>\\n</body>\\n</html>\\n```\\n\\nAs you can see I'm pointing to `localhost:3000` instead of directly to disk, this is what makes everything above work. In production probably want to point to the file to disk, and that could probably be solved by tag helpers in ASP.NET 5, or using server side variables based on environment in any other version of ASP.NET.\\n\\n## Running everything\\n\\nIf you have cloned the [repository](https://github.com/mastoj/ReactHotLoadAspNet) and want to try it out you can now either run start from the `Task Runner Explorer` if you have the `NPM Scripts Task Runner` installed, or you can run `npm run start` from the command line in the root of the web project. \\n\\n![Task Runner Explorer](/content/images/2015/11/TaskRunner.PNG)\\n\\nThis will start the node server for you. When the node server is up and running you can start the ASP.NET application. Now you can start to interact with the application in the browser and then try to change the `jsx` or `less` files, save and see the changes appear in the browser with no refresh of the page.\\n\\n## Summary\\n\\nReact hot load looks to me like an awesome way to get fast feedback while doing web development with react. There was a little bit of hazzle to get it up and running on Windows but it is doable, and you probably only need to feel that pain once :). Let me know if you have any questions.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>One or two months ago I saw a <a href=\"https://www.youtube.com/watch?v=xsSnOQynTHs\">presentation</a> by <a href=\"https://twitter.com/dan_abramov\">Dan Abramov</a> about hot loading and react. I have't done much react and I haven't done much webpack which is used to do this, but I know I wanted a sample for ASP.NET 5 and now I finally had the time to try it out. Even though the titles says ASP.NET 5 the solution can be used for basically any web project running on Windows.</p>\n<h2 id=\"tldr\">TL;DR</h2>\n<p>I've put together a small sample with react hot load and ASP.NET 5. You can find the code on <a href=\"https://github.com/mastoj/ReactHotLoadAspNet/\">github</a>.</p>\n<h2 id=\"whatisit\">What is it?</h2>\n<p><a href=\"http://gaearon.github.io/react-hot-loader/\">React hot load</a> let you change the style files (css/less/scss) and javascript files on the server and see the result directly in the browser while keeping the browser state. This make it really easy to try things out and see the result directly instead of reloading the browser after every minor change. An example is when working with styling, instead of tweaking around in the dev tools you can just change the style file directly. Another example is if you're working with a javascript wizard, then it might be annoying to go through every step when you find an error, hot loading the javascript lets you continue where you are after you change the javascript.</p>\n<p>Below is a demo showing the result. I use atom for changing less and jsx files since the support for jsx+ES6 in Visual Studio isn't the best yet.</p>\n<p><img src=\"/content/images/2015/11/Demo.gif\" alt=\"Demo of react hot load\"></p>\n<h2 id=\"howitsortofworksmyversion\">How it sort of works (my version)</h2>\n<p>I won't cover the details of how hot loading works but the short version of how it works is something like this. Instead of reading the javascript files you read them from a node server, the node server &quot;injects&quot; a &quot;wrapper&quot; between the physical file on disc and the one you pointed too from your web page that routes your javascript command to the file that is currently up to date. If you make a change to a file the wrapper will see this and make a rebuild swapping out what has changed while keeping the state. Keeping the state is possible because of the architecture of react.</p>\n<h2 id=\"gotchas\">Gotchas</h2>\n<p>My goal was to get a sample up and running with both javascript and less since that is what I most likely will use in a real project, I also wanted everything to run with the latest version of everything. This caused some problems during the setup and here are a list of things that you need to do to your dev environment to get things working:</p>\n<ul>\n<li>I wanted to run regular npm tasks and to do so in Visual Studio you need the extension <a href=\"https://visualstudiogallery.msdn.microsoft.com/8f2f2cbc-4da5-43ba-9de2-c9d08ade4941\">NPM Scripts Task Runner</a></li>\n<li>For some reason <code>node 5</code> was needed, so install that.</li>\n<li>To get <code>node 5</code> to play nice in Visual Studio you need to add the path for External Web Tools. You find that setting under <code>Tools-&gt;Options-&gt;Projects and Solutions-&gt;External Web Tools</code>. Just add the path to <code>node 5</code> above the line that points to external tools. It looks like <code>node 5</code> installs node modules directly under modules folder and not in a hierarchy as before. This makes Visual Studio to think that a lot of the packages are extraneous, but you can just ignore that.</li>\n<li>One of the packages I used got some weird error if python wasn't installed, so python needs to be installed. I couldn't use version 3 of python so I installed version 2 from <a href=\"https://chocolatey.org/packages/python2\">chocolatey</a>.</li>\n<li>This last part is a little bit tricky and only works because I have both VS2013 and VS2015 I think. I got some error which I found the answer on at <a href=\"http://stackoverflow.com/questions/33183161/node-gyp-error-tracker-error-trk0005-failed-to-locate-cl-exe-the-system-c\">stackoverflow</a>. <code>Npm</code> needed to run <code>cl.exe</code> for some task, but it didn't find it for VS2015 so changing <code>Npm</code> to use VS2013 helped as the SO answer says. The command to do so is <code>npm config set msvs_version 2013 --global</code>.</li>\n</ul>\n<h2 id=\"thesetup\">The setup</h2>\n<p>I won't cover the whole application and what it does, I'll just cover the pieces to get this up and running.</p>\n<h3 id=\"thepackagejsonfile\">The package.json file</h3>\n<p>The file I ended up with looks like this</p>\n<pre><code class=\"language-language-json\">{\n  &quot;version&quot;: &quot;0.0.0&quot;,\n  &quot;name&quot;: &quot;&quot;,\n  &quot;scripts&quot;: {\n    &quot;start&quot;: &quot;node server.js&quot;,\n    &quot;build&quot;: &quot;set NODE_ENV=production &amp;&amp; webpack -p --progress --colors&quot;\n  },\n  &quot;devDependencies&quot;: {\n    &quot;babel-core&quot;: &quot;^6.1.2&quot;,\n    &quot;babel-loader&quot;: &quot;^6.0.1&quot;,\n    &quot;babel-preset-es2015&quot;: &quot;^6.1.2&quot;,\n    &quot;babel-preset-react&quot;: &quot;^6.1.2&quot;,\n    &quot;css-loader&quot;: &quot;^0.22.0&quot;,\n    &quot;less&quot;: &quot;^2.5.3&quot;,\n    &quot;less-loader&quot;: &quot;^2.2.1&quot;,\n    &quot;react-hot-loader&quot;: &quot;^1.3.0&quot;,\n    &quot;style-loader&quot;: &quot;^0.13.0&quot;,\n    &quot;webpack&quot;: &quot;^1.12.2&quot;,\n    &quot;webpack-dev-server&quot;: &quot;^1.12.1&quot;\n  },\n  &quot;dependencies&quot;: {\n    &quot;jquery&quot;: &quot;2.1.4&quot;,\n    &quot;marked&quot;: &quot;^0.3.5&quot;,\n    &quot;react&quot;: &quot;^0.14.0&quot;,\n    &quot;react-dom&quot;: &quot;^0.14.0&quot;\n  }\n}\n</code></pre>\n<p>I've added to <code>npm</code> scripts to the file, <code>build</code> that should be used when packaging for production on a build server and <code>start</code> that is used to start the server for development. This only works with <a href=\"https://webpack.github.io/\">webpack</a> so I installed that and all the loaders I needed to build <code>jsx</code> and <code>less</code> files.</p>\n<h3 id=\"theserverjs\">The server.js</h3>\n<p>The <code>server.js</code> is a small node server that will host the javascript for us during development. It is started with the <code>start</code> script task defined in <code>package.json</code>.</p>\n<pre><code class=\"language-language-javascript\">var webpack = require('webpack');\nvar WebpackDevServer = require('webpack-dev-server');\nvar config = require('./webpack.config');\n\nnew WebpackDevServer(webpack(config), {\n    publicPath: config.output.publicPath,\n    hot: true,\n    historyApiFallback: true,\n    headers: { 'Access-Control-Allow-Origin': '*' }\n}).listen(3000, 'localhost', function (err, result) {\n    if (err) {\n        console.log(err);\n    }\n\n    console.log('Listening at localhost:3000');\n});\n</code></pre>\n<p>The server is based on this <a href=\"https://github.com/gaearon/react-hot-boilerplate/blob/master/server.js\">boilerplate code</a> but I added <code>headers: { 'Access-Control-Allow-Origin': '*' }</code>. When I upgraded everything to the latest bits <code>CORS</code> was required.</p>\n<h2 id=\"webpackconfiguration\">Webpack configuration</h2>\n<p>You can run <code>webpack</code> directly from the command line, but you usually use a configuration file to do so. This is my first time using <code>webpack</code> so it is most likely a guide to how you should do it, more a sample of how I got it to do what I wanted it to do. The <code>webpack.config.js</code> I ended up with looks like this:</p>\n<pre><code class=\"language-language-javascript\">var webpack = require('webpack');\nvar path = require('path');\nvar outFolder = path.resolve(__dirname, &quot;./wwwroot/app&quot;);\nvar isProduction = process.env.NODE_ENV === 'production ';\nvar jsxLoaders = isProduction ?\n    ['babel?presets[]=es2015,presets[]=react'] :\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\nvar entryPoint = './content/app.jsx';\nvar app = isProduction ? [entryPoint] : [\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\n    'webpack/hot/only-dev-server', // &quot;only&quot; prevents reload on syntax errors\n    entryPoint\n];\n\nmodule.exports = {\n    entry: {\n        app: app\n    },\n    output: {\n        path: outFolder,\n        filename: &quot;[name].js&quot;,\n        publicPath: 'http://localhost:3000/static/'\n    },\n    devtool: &quot;source-map&quot;,\n    minimize: true,\n    module: {\n        loaders: [{\n            test: /\\.(js|jsx)$/,\n            loaders: jsxLoaders,\n            exclude: /node_modules/\n        },\n        {\n            test: /\\.(css|less)$/,\n            loaders: ['style','css','less']\n        }]\n    },\n    plugins: [\n      new webpack.HotModuleReplacementPlugin()\n    ],\n    resolve: {\n        extensions: [&quot;&quot;, &quot;.webpack.js&quot;, &quot;.web.js&quot;, &quot;.js&quot;, &quot;.jsx&quot;]\n    },\n    devServer: {\n        headers: { &quot;Access-Control-Allow-Origin&quot;: &quot;*&quot; }\n    }\n};\n</code></pre>\n<p>First I define some settings that differs depending on environment. The environment is set as an environment variable, see the <code>build</code> script task in the <code>package.json</code> file. One important part is this one:</p>\n<pre><code class=\"language-language-javascript\">var jsxLoaders = isProduction ?\n    ['babel?presets[]=es2015,presets[]=react'] :\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\nvar entryPoint = './content/app.jsx';\nvar app = isProduction ? [entryPoint] : [\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\n    'webpack/hot/only-dev-server', // &quot;only&quot; prevents reload on syntax errors\n    entryPoint\n];\n</code></pre>\n<p>This is what make the actual server running, I used to port 3000 to host the files. I also needed to specify the <code>publicPath</code> under <code>output</code>, that's because the files are not served from the same application as the consumer of the files. As you can see, if we are doing a production build, by running <code>npm rum build</code>, we will only use the actual <code>app.jsx</code> as entry point. Also, we won't add <code>react-hot</code> (alias for <code>react-hot-loader</code>) to the list of <code>jsxLoaders</code> since I don't want hot loading enabled in production.</p>\n<p>I won't try to cover <code>webpack</code> in more depth since all this is sort of new to me.</p>\n<h3 id=\"theaspnetpart\">The ASP.NET part</h3>\n<p>If you haven't figured it out by now, the ASP.NET solution stays mainly the same to get this working. The trick is actually just to fire up a node server to host your static content and then point the <code>script</code> tags in your solution to that server. So my simple index page looks like this:</p>\n<pre><code class=\"language-language-aspnet\">@{\n    // ViewBag.Title = &quot;Home Page&quot;;\n}\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Sample hot load demo&lt;/title&gt;\n    &lt;link href=&quot;/static/&quot;/&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div id=&quot;content&quot;&gt;&lt;/div&gt;\n    @*&lt;script src=&quot;/static/app.js&quot;&gt;&lt;/script&gt;*@\n    &lt;script src=&quot;http://localhost:3000/static/app.js&quot;&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>As you can see I'm pointing to <code>localhost:3000</code> instead of directly to disk, this is what makes everything above work. In production probably want to point to the file to disk, and that could probably be solved by tag helpers in ASP.NET 5, or using server side variables based on environment in any other version of ASP.NET.</p>\n<h2 id=\"runningeverything\">Running everything</h2>\n<p>If you have cloned the <a href=\"https://github.com/mastoj/ReactHotLoadAspNet\">repository</a> and want to try it out you can now either run start from the <code>Task Runner Explorer</code> if you have the <code>NPM Scripts Task Runner</code> installed, or you can run <code>npm run start</code> from the command line in the root of the web project.</p>\n<p><img src=\"/content/images/2015/11/TaskRunner.PNG\" alt=\"Task Runner Explorer\"></p>\n<p>This will start the node server for you. When the node server is up and running you can start the ASP.NET application. Now you can start to interact with the application in the browser and then try to change the <code>jsx</code> or <code>less</code> files, save and see the changes appear in the browser with no refresh of the page.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>React hot load looks to me like an awesome way to get fast feedback while doing web development with react. There was a little bit of hazzle to get it up and running on Windows but it is doable, and you probably only need to feel that pain once :). Let me know if you have any questions.</p>\n<!--kg-card-end: markdown-->","comment_id":"63","plaintext":"One or two months ago I saw a presentation\n[https://www.youtube.com/watch?v=xsSnOQynTHs] by Dan Abramov\n[https://twitter.com/dan_abramov] about hot loading and react. I have't done\nmuch react and I haven't done much webpack which is used to do this, but I know\nI wanted a sample for ASP.NET 5 and now I finally had the time to try it out.\nEven though the titles says ASP.NET 5 the solution can be used for basically any\nweb project running on Windows.\n\nTL;DR\nI've put together a small sample with react hot load and ASP.NET 5. You can find\nthe code on github [https://github.com/mastoj/ReactHotLoadAspNet/].\n\nWhat is it?\nReact hot load [http://gaearon.github.io/react-hot-loader/] let you change the\nstyle files (css/less/scss) and javascript files on the server and see the\nresult directly in the browser while keeping the browser state. This make it\nreally easy to try things out and see the result directly instead of reloading\nthe browser after every minor change. An example is when working with styling,\ninstead of tweaking around in the dev tools you can just change the style file\ndirectly. Another example is if you're working with a javascript wizard, then it\nmight be annoying to go through every step when you find an error, hot loading\nthe javascript lets you continue where you are after you change the javascript.\n\nBelow is a demo showing the result. I use atom for changing less and jsx files\nsince the support for jsx+ES6 in Visual Studio isn't the best yet.\n\n\n\nHow it sort of works (my version)\nI won't cover the details of how hot loading works but the short version of how\nit works is something like this. Instead of reading the javascript files you\nread them from a node server, the node server \"injects\" a \"wrapper\" between the\nphysical file on disc and the one you pointed too from your web page that routes\nyour javascript command to the file that is currently up to date. If you make a\nchange to a file the wrapper will see this and make a rebuild swapping out what\nhas changed while keeping the state. Keeping the state is possible because of\nthe architecture of react.\n\nGotchas\nMy goal was to get a sample up and running with both javascript and less since\nthat is what I most likely will use in a real project, I also wanted everything\nto run with the latest version of everything. This caused some problems during\nthe setup and here are a list of things that you need to do to your dev\nenvironment to get things working:\n\n * I wanted to run regular npm tasks and to do so in Visual Studio you need the\n   extension NPM Scripts Task Runner\n   [https://visualstudiogallery.msdn.microsoft.com/8f2f2cbc-4da5-43ba-9de2-c9d08ade4941]\n * For some reason node 5 was needed, so install that.\n * To get node 5 to play nice in Visual Studio you need to add the path for\n   External Web Tools. You find that setting under Tools->Options->Projects and\n   Solutions->External Web Tools. Just add the path to node 5 above the line\n   that points to external tools. It looks like node 5 installs node modules\n   directly under modules folder and not in a hierarchy as before. This makes\n   Visual Studio to think that a lot of the packages are extraneous, but you can\n   just ignore that.\n * One of the packages I used got some weird error if python wasn't installed,\n   so python needs to be installed. I couldn't use version 3 of python so I\n   installed version 2 from chocolatey [https://chocolatey.org/packages/python2]\n   .\n * This last part is a little bit tricky and only works because I have both\n   VS2013 and VS2015 I think. I got some error which I found the answer on at \n   stackoverflow\n   [http://stackoverflow.com/questions/33183161/node-gyp-error-tracker-error-trk0005-failed-to-locate-cl-exe-the-system-c]\n   . Npm needed to run cl.exe for some task, but it didn't find it for VS2015 so\n   changing Npm to use VS2013 helped as the SO answer says. The command to do so\n   is npm config set msvs_version 2013 --global.\n\nThe setup\nI won't cover the whole application and what it does, I'll just cover the pieces\nto get this up and running.\n\nThe package.json file\nThe file I ended up with looks like this\n\n{\n  \"version\": \"0.0.0\",\n  \"name\": \"\",\n  \"scripts\": {\n    \"start\": \"node server.js\",\n    \"build\": \"set NODE_ENV=production && webpack -p --progress --colors\"\n  },\n  \"devDependencies\": {\n    \"babel-core\": \"^6.1.2\",\n    \"babel-loader\": \"^6.0.1\",\n    \"babel-preset-es2015\": \"^6.1.2\",\n    \"babel-preset-react\": \"^6.1.2\",\n    \"css-loader\": \"^0.22.0\",\n    \"less\": \"^2.5.3\",\n    \"less-loader\": \"^2.2.1\",\n    \"react-hot-loader\": \"^1.3.0\",\n    \"style-loader\": \"^0.13.0\",\n    \"webpack\": \"^1.12.2\",\n    \"webpack-dev-server\": \"^1.12.1\"\n  },\n  \"dependencies\": {\n    \"jquery\": \"2.1.4\",\n    \"marked\": \"^0.3.5\",\n    \"react\": \"^0.14.0\",\n    \"react-dom\": \"^0.14.0\"\n  }\n}\n\n\nI've added to npm scripts to the file, build that should be used when packaging\nfor production on a build server and start that is used to start the server for\ndevelopment. This only works with webpack [https://webpack.github.io/] so I\ninstalled that and all the loaders I needed to build jsx and less files.\n\nThe server.js\nThe server.js is a small node server that will host the javascript for us during\ndevelopment. It is started with the start script task defined in package.json.\n\nvar webpack = require('webpack');\nvar WebpackDevServer = require('webpack-dev-server');\nvar config = require('./webpack.config');\n\nnew WebpackDevServer(webpack(config), {\n    publicPath: config.output.publicPath,\n    hot: true,\n    historyApiFallback: true,\n    headers: { 'Access-Control-Allow-Origin': '*' }\n}).listen(3000, 'localhost', function (err, result) {\n    if (err) {\n        console.log(err);\n    }\n\n    console.log('Listening at localhost:3000');\n});\n\n\nThe server is based on this boilerplate code\n[https://github.com/gaearon/react-hot-boilerplate/blob/master/server.js] but I\nadded headers: { 'Access-Control-Allow-Origin': '*' }. When I upgraded\neverything to the latest bits CORS was required.\n\nWebpack configuration\nYou can run webpack directly from the command line, but you usually use a\nconfiguration file to do so. This is my first time using webpack so it is most\nlikely a guide to how you should do it, more a sample of how I got it to do what\nI wanted it to do. The webpack.config.js I ended up with looks like this:\n\nvar webpack = require('webpack');\nvar path = require('path');\nvar outFolder = path.resolve(__dirname, \"./wwwroot/app\");\nvar isProduction = process.env.NODE_ENV === 'production ';\nvar jsxLoaders = isProduction ?\n    ['babel?presets[]=es2015,presets[]=react'] :\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\nvar entryPoint = './content/app.jsx';\nvar app = isProduction ? [entryPoint] : [\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\n    'webpack/hot/only-dev-server', // \"only\" prevents reload on syntax errors\n    entryPoint\n];\n\nmodule.exports = {\n    entry: {\n        app: app\n    },\n    output: {\n        path: outFolder,\n        filename: \"[name].js\",\n        publicPath: 'http://localhost:3000/static/'\n    },\n    devtool: \"source-map\",\n    minimize: true,\n    module: {\n        loaders: [{\n            test: /\\.(js|jsx)$/,\n            loaders: jsxLoaders,\n            exclude: /node_modules/\n        },\n        {\n            test: /\\.(css|less)$/,\n            loaders: ['style','css','less']\n        }]\n    },\n    plugins: [\n      new webpack.HotModuleReplacementPlugin()\n    ],\n    resolve: {\n        extensions: [\"\", \".webpack.js\", \".web.js\", \".js\", \".jsx\"]\n    },\n    devServer: {\n        headers: { \"Access-Control-Allow-Origin\": \"*\" }\n    }\n};\n\n\nFirst I define some settings that differs depending on environment. The\nenvironment is set as an environment variable, see the build script task in the \npackage.json file. One important part is this one:\n\nvar jsxLoaders = isProduction ?\n    ['babel?presets[]=es2015,presets[]=react'] :\n    ['react-hot', 'babel?presets[]=es2015,presets[]=react']; // only react hot load in debug build\nvar entryPoint = './content/app.jsx';\nvar app = isProduction ? [entryPoint] : [\n    'webpack-dev-server/client?http://0.0.0.0:3000', // WebpackDevServer host and port\n    'webpack/hot/only-dev-server', // \"only\" prevents reload on syntax errors\n    entryPoint\n];\n\n\nThis is what make the actual server running, I used to port 3000 to host the\nfiles. I also needed to specify the publicPath under output, that's because the\nfiles are not served from the same application as the consumer of the files. As\nyou can see, if we are doing a production build, by running npm rum build, we\nwill only use the actual app.jsx as entry point. Also, we won't add react-hot \n(alias for react-hot-loader) to the list of jsxLoaders since I don't want hot\nloading enabled in production.\n\nI won't try to cover webpack in more depth since all this is sort of new to me.\n\nThe ASP.NET part\nIf you haven't figured it out by now, the ASP.NET solution stays mainly the same\nto get this working. The trick is actually just to fire up a node server to host\nyour static content and then point the script tags in your solution to that\nserver. So my simple index page looks like this:\n\n@{\n    // ViewBag.Title = \"Home Page\";\n}\n<html>\n<head>\n    <title>Sample hot load demo</title>\n    <link href=\"/static/\"/>\n</head>\n<body>\n    <div id=\"content\"></div>\n    @*<script src=\"/static/app.js\"></script>*@\n    <script src=\"http://localhost:3000/static/app.js\"></script>\n</body>\n</html>\n\n\nAs you can see I'm pointing to localhost:3000 instead of directly to disk, this\nis what makes everything above work. In production probably want to point to the\nfile to disk, and that could probably be solved by tag helpers in ASP.NET 5, or\nusing server side variables based on environment in any other version of\nASP.NET.\n\nRunning everything\nIf you have cloned the repository [https://github.com/mastoj/ReactHotLoadAspNet] \nand want to try it out you can now either run start from the Task Runner\nExplorer if you have the NPM Scripts Task Runner installed, or you can run npm\nrun start from the command line in the root of the web project.\n\n\n\nThis will start the node server for you. When the node server is up and running\nyou can start the ASP.NET application. Now you can start to interact with the\napplication in the browser and then try to change the jsx or less files, save\nand see the changes appear in the browser with no refresh of the page.\n\nSummary\nReact hot load looks to me like an awesome way to get fast feedback while doing\nweb development with react. There was a little bit of hazzle to get it up and\nrunning on Windows but it is doable, and you probably only need to feel that\npain once :). Let me know if you have any questions.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-11-11T21:53:54.000Z","updated_at":"2015-11-15T22:10:23.000Z","published_at":"2015-11-15T22:09:39.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe072","uuid":"97510e6b-25c2-409b-8d43-81f71918dfb2","title":"F#, event sourcing and CQRS tutorial... and agents","slug":"fsharp-event-sourcing-and-cqrs-tutorial-and-agents","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Last year I wrote a post about [Pure Functional Application](http://blog.2mas.xyz/pure-application-in-fsharp/) for the F# advent calendar, I really think it is a great initiative so I signed up again. This is my contribution to this year's F# advent calendar, you can find all the other excellent posts on [@sergey_tihon](https://twitter.com/sergey_tihon)'s blog: https://sergeytihon.wordpress.com/tag/fsadvent/\\n\\nOne would expect that I would write a totally different post this year, but instead I decided to make my last year's post more concrete. With that I mean I would like to introduce a tutorial for you to follow. I won't cover the whole tutorial in this post since it is described in the tutorial which you can find on [github](https://github.com/mastoj/LibAAS). The tutorial covers how it might be to work in a project where you have put in the time to set up the boilerplate for a project using CQRS and event sourcing in F#. It might not be production ready either, but it might give you some inspiration of how you can approach application development.\\n\\nCovering the exercises in the tutorial here would be a little bit boring since they are covered in the tutorial so instead I thought I would explain how the in-memory event store is implemented using an F# agent. You can find the code I will cover in this folder on github: https://github.com/mastoj/LibAAS/tree/master/ex4/done/LibAAS.Infrastructure.\\n\\n## Agents\\n\\nI hope there are some people out there not that familiar with F# that follow along in this calendar since it is a great opportunity to learn some F#. I'll try to make this post understandable for most developers out there and that is why I'll write a short section about agents. Agents in F# is usually used as alias for `MailboxProcessor`, so you often see something like this in code where agents are used:\\n\\n```language-fsharp\\ntype Agent<'TMessage> = MailboxProcessor<'TMessage>\\n```\\n\\nThe way I see agents is like in-process workers that can keep some kind of state. You can compare it to actors in the [actor model](https://en.wikipedia.org/wiki/Actor_model) but much simpler. They are really great if all you need is a async worker inside your process or a nice way of storing state in your application.\\n\\nAgents work with an inbox to which you can send messages, when a message arrives in the inbox the agent will read it an act upon it. The type of the message must be defined before hand and it can be of any type, a discriminated union is often used as message type. You can use simple types like `string` in this example\\n\\n```language-fsharp\\ntype Agent<'T> = MailboxProcessor<'T>\\n\\nlet agent = Agent.Start(fun (inbox:Agent<string>) ->\\n    let rec loop() =\\n        async {\\n            let! msg = inbox.Receive()\\n            match msg with\\n            | \\\"\\\" -> \\n                printfn \\\"Stopping agent\\\"\\n            | _ -> \\n                printfn \\\"Message recieved: %s\\\" msg\\n                return! loop() \\n        }\\n    loop() \\n)\\n\\nlet post (agent:Agent<'T>) message = agent.Post message\\n\\n\\\"hello\\\" |> post agent\\n\\\"\\\" |> post agent\\n\\\"hello\\\" |> post agent\\n```\\n\\nWe first make an alias for the `MailboxProcessor` type. When the alias is created it can be used to start the agent with `Agent.Start`. The `Start` function takes a function as argument and this is the body of the agent. The structure you see in this simple example is probably the most common one as far as I know. The body is usually a recursive function in which you listen to new messages with the `inbox.Receive`, if you want to continue process after a message you just make a recursive call. You can define the recursive function to take a state parameter to keep track inside the agent between messages. I also defined a simple helper so it is easier to post messages to agents with the pipe operator. If you run the code above you should see two messages printed, the last one will not be printed since we are not doing a recursive call on empty messages and that stops the agent.\\n\\n## Event store\\n\\nWhat is an event store? Short answer: a data store that store events. It is almost that simple. The simplest possible event store need two functions:\\n\\n* Get events given a stream id\\n* Save events given a stream id, expected version and events\\n\\nThe first function should return the list of events for the given stream id. A stream is just a way to group events that belong together.\\n\\nThe second function should append the events to a given stream with the stream id given that the version of the stream is the same as the expected version. A version of a stream is basically the number of events in the stream, this prevents concurrency issues and is also the transaction boundary when working against the event store.\\n\\nThat was a short introduction to what an event store is, there is plenty of more information online, but feel free to ask here if you have questions. Next up is the implementation of the event store in F#.\\n\\n## Event store implementation\\n\\nThe implementation I will describe here is using agents, mainly because it is a nice way to abstract away the basics of an event store. With that in place you can easily create different types of event stores by changing two functions as you'll see. \\n\\n### The messages\\n\\nFirst let's define some simple helpers for our agent:\\n\\n```language-fsharp\\nmodule AgentHelper\\n\\ntype Agent<'T> = MailboxProcessor<'T>\\nlet post (agent:Agent<'T>) message = agent.Post message\\nlet postAsyncReply (agent:Agent<'T>) messageConstr = agent.PostAndAsyncReply(messageConstr)\\n```\\n\\nNow when we got that out of our way we can start with the acutal implementation. We will start with the messages and some types that help us stay out of trouble.\\n\\n```language-fsharp\\ntype StreamId = StreamId of int\\ntype StreamVersion = StreamVersion of int\\n\\ntype SaveResult = \\n    | Ok\\n    | VersionConflict\\n\\ntype Messages<'T> = \\n    | GetEvents of StreamId * AsyncReplyChannel<'T list option>\\n    | SaveEvents of StreamId * StreamVersion * 'T list * AsyncReplyChannel<SaveResult>\\n    | AddSubscriber of string * (StreamId * 'T list -> unit)\\n    | RemoveSubscriber of string\\n```\\n\\nJust by reading this type definitions you can almost understand how the event store is supposed to work. We have a generic `Messages` type, where the generic parameter defines the type of event that we want to store in the event store. We have four actions we will be able to do against the event store:\\n\\n1. Get the events for a stream.\\n2. Save events for a stream.  \\n    a. When saving you can have version conflict and to indicate that we use the `SaveResult` type.\\n3. You can add multiple subscribers, where the first string is an id of the subscriber (should probably be wrapped in a type). A subscriber will be called every time some events have been saved.\\n4. You can remove an existing subscriber based on the string id.\\n\\n### State format\\n\\nTo make the agent flexible we need to keep an internal state that can be provided when creating the agent. The definition of the state type looks like this:\\n\\n```language-fsharp\\ntype internal EventStoreState<'TEvent,'THandler> = \\n    {\\n        EventHandler: 'THandler\\n        GetEvents: 'THandler -> StreamId -> ('TEvent list option * 'THandler) \\n        SaveEvents: 'THandler -> StreamId -> StreamVersion -> 'TEvent list -> (SaveResult * 'THandler)\\n        Subscribers: Map<string, (StreamId * 'TEvent list -> unit)>\\n    }\\n```\\n\\nWhat I call `EventHandler` here is the the \\\"thing\\\" that stores the actual events, it can be an internal map or a connection to an external db. The methods `GetEvents` and `SaveEvents` uses the `EventHandler` to get or save events. The last thing in the state is the subscribers which we also need to keep track of.\\n\\n### Agent body\\n\\nNext up is the actual implementation of the agent. I give you the code right away and then walk you through it:\\n\\n```language-fsharp\\nlet eventSourcingAgent<'T, 'TEventHandler> (eventHandler:'TEventHandler) getEvents saveEvents (inbox:Agent<Messages<'T>>) = \\n    let initState = \\n        {\\n            EventHandler = eventHandler\\n            Subscribers = Map.empty\\n            GetEvents = getEvents\\n            SaveEvents = saveEvents\\n        }\\n    let rec loop state = \\n        async {\\n            let! msg = inbox.Receive()\\n            match msg with\\n            | GetEvents (id, replyChannel) ->\\n                let (events, newHandler) = state.GetEvents state.EventHandler id\\n                replyChannel.Reply(events)\\n                return! loop {state with EventHandler = newHandler}\\n            | SaveEvents (id, expectedVersion, events, replyChannel) ->\\n                let (result, newHandler) = state.SaveEvents state.EventHandler id expectedVersion events\\n                if result = Ok then state.Subscribers |> Map.iter (fun _ sub -> sub(id, events)) else ()\\n                replyChannel.Reply(result)\\n                return! loop {state with EventHandler = newHandler}\\n            | AddSubscriber (subId, subFunction) ->\\n                let newState = {state with Subscribers = (state.Subscribers |> Map.add subId subFunction)}\\n                return! loop newState\\n            | RemoveSubscriber subId ->\\n                let newState = {state with Subscribers = (state.Subscribers |> Map.remove subId )}\\n                return! loop newState\\n        }\\n    loop initState\\n```\\n\\nTo create the agent we need the `eventHandler` a function to `getEvents` and `saveEvents`, nothing to fancy about that. The first thing we do in the function is to create the `initState` with the input and an empty `Map` for our subscribers. Next up is the recursive loop (remember it from the section above?). We first wait until there is a new message in the inbox, when we get one we match on the message type. \\n\\nFor `GetEvents` we use the `GetEvents` method on the state passing in the `EventHandler` and the id of the string. When we have the events we reply back to the callee and finish it of with a recursive call with the new state (if it has changed).\\n\\n`SaveEvents` works almost the same way, with the addition of notifying the subscribers if we manage to save the events. We also reply back to the callee with the result of the save operation before making the recursive call to wait for the next message.\\n\\nThe implementation of `AddSubscriber` and `RemoveSubscriber` do what you would expect them to, it adds or removes a subscriber for the `Subscribers` map we have in the state and make a recursive call to wait for the next message.\\n\\n### In-memory implementation\\n\\nTo make it a little bit easier for a user to work with an agent it make sense to hide it behind some kind of type, which also make it easier to swap for another implementation later, and that type looks like this:\\n\\n```language-fsharp\\ntype EventStore<'TEvent, 'TError> = \\n    {\\n        GetEvents: StreamId -> Result<StreamVersion*'TEvent list, 'TError>\\n        SaveEvents: StreamId -> StreamVersion -> 'TEvent list -> Result<'TEvent list, 'TError>\\n        AddSubscriber: string -> (StreamId * 'TEvent list -> unit) -> unit\\n        RemoveSubscriber: string -> unit\\n    }\\n```\\n\\nThe `SaveEvents` and `GetEvents` method returns something of type `Result`, and that is taken from [Railway Oriented Programming](http://fsharpforfunandprofit.com/rop/) which is a really nice way to handle errors in an application without introducing side effects like exceptions. The `Result` type is defined as:\\n\\n```language-fsharp\\n[<AutoOpen>]\\nmodule ErrorHandling\\n \\ntype Result<'TResult, 'TError> = \\n    | Success of 'TResult\\n    | Failure of 'TError\\n\\nlet ok x = Success x\\nlet fail x = Failure x\\n```\\n\\nTogether with the type we have defined two helpers `ok` and `fail` to make it easier to create a `Result` through piping. \\n\\nWe also need a function to create a wrapper around an agent that create an instance of an `EventStore`.\\n\\n```language-fsharp\\nlet createEventStore<'TEvent, 'TError> (versionError:'TError) agent =\\n    let getEvents streamId : Result<StreamVersion*'TEvent list, 'TError> = \\n        let result = (fun r -> GetEvents (streamId, r)) |> postAsyncReply agent |> Async.RunSynchronously\\n        match result with\\n        | Some events -> (StreamVersion (events |> List.length), events) |> ok\\n        | None -> (StreamVersion 0, []) |> ok\\n\\n    let saveEvents streamId expectedVersion events : Result<'TEvent list, 'TError> = \\n        let result = (fun r -> SaveEvents(streamId, expectedVersion, events, r)) |> postAsyncReply agent |> Async.RunSynchronously\\n        match result with\\n        | Ok -> events |> ok\\n        | VersionConflict -> versionError |> fail\\n\\n    let addSubscriber subId subscriber = \\n        (subId,subscriber) |> AddSubscriber |> post agent\\n\\n    let removeSubscriber subId = \\n        subId |> RemoveSubscriber |> post agent\\n\\n    { GetEvents = getEvents; SaveEvents = saveEvents; AddSubscriber = addSubscriber; RemoveSubscriber = removeSubscriber}\\n```\\n\\nIt is nothing to complicated going on, the `getEvents` function takes a stream id and wrap it in a `GetEvents` message which is sent to the agent. After sending the message we wait for the reply and return the events together with the current version of the stream wrapped in a `Result` type. The `saveEvents` method works almost the same way, that is, we wrap the input in a `SaveEvents` message and pass it to the agent and wait for the reply. If we get a `VersionConflict` back we translate it to the provided error to keep this code isolated from other code.\\n\\nNow we have all the pieces to put together our in-memory event store. The in-memory event store will use a simple map as a storage for the events for easy lookup. \\n\\n```language-fsharp\\nlet createInMemoryEventStore<'TEvent, 'TError> (versionError:'TError) =\\n    let initState : Map<StreamId, 'TEvent list> = Map.empty\\n\\n    let saveEventsInMap map id expectedVersion events = \\n        match map |> Map.tryFind id with\\n        | None -> \\n            (Ok, map |> Map.add id events)\\n        | Some existingEvents ->\\n            let currentVersion = existingEvents |> List.length |> StreamVersion\\n            match currentVersion = expectedVersion with\\n            | true -> \\n                (Ok, map |> Map.add id (existingEvents@events))\\n            | false -> \\n                (VersionConflict, map)\\n\\n    let getEventsInMap map id = Map.tryFind id map, map\\n\\n    let agent = createEventStoreAgent initState getEventsInMap saveEventsInMap\\n    createEventStore<'TEvent, 'TError> versionError agent\\n```\\n\\n* The `initState` is of course an empty map since we don't have any events when we start. \\n* The `saveEventsInMap` uses the `id` argument to lookup in the `map` argument (current state), if the result is `None` the entry is added to the map with the `events`. If the entry already exist we check the version before appending the `events` to the stream.\\n* The `getEventsInMap` will just do a lookup in the `map` and returning an `Option` type together with the new map which is the same as the input.\\n\\nWith these three functions we can now call the `createEventStore` function to create our in-memory event store and we are done.\\n\\n## Taking it out for a spin\\n\\nThe simplest way to actually try the event store out is to use it in a fsharp script. So in the same folder as the I have the files for the implementation I also have a simple script with the following content:\\n\\n```language-fsharp\\n#load \\\"AgentHelper.fs\\\"\\n#load \\\"ErrorHandling.fs\\\"\\n#load \\\"EventStore.fs\\\"\\n\\nopen EventStore\\n\\nlet inMemoryEventStore = createInMemoryEventStore<string,string> \\\"This is a version error\\\"\\ninMemoryEventStore.AddSubscriber \\\"FirstSubscriber\\\" (printfn \\\"%A\\\")\\nlet res0 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 0) [\\\"Hello\\\";\\\"World\\\"]\\nlet res1 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 1) [\\\"Hello2\\\";\\\"World2\\\"]\\nlet res2 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 2) [\\\"Hello2\\\";\\\"World2\\\"]\\n\\n[res0;res1;res2] |> List.mapi (fun i v -> printfn \\\"%i: %A\\\" i v)\\n```\\n\\nWe keep it really simple and only storing strings, as well as using a string as our error indicator. Executing this code with mono `fsharpi --exec Script.fsx` or on Windows `fsi --exec Script.fsx` should give the following output:\\n\\n```\\n(StreamId 1, [\\\"Hello\\\"; \\\"World\\\"])\\n(StreamId 1, [\\\"Hello2\\\"; \\\"World2\\\"])\\n0: Success [\\\"Hello\\\"; \\\"World\\\"]\\n1: Failure \\\"This is a version error\\\"\\n2: Success [\\\"Hello2\\\"; \\\"World2\\\"]\\n```\\n\\nThe first two lines are from the subscriber and last in the script I print all the results.\\n\\n## Now it is your turn\\n\\nThere is room for a lot of improvement here I guess, but it is a good starting point. Feel free to try the tutorial and also come with suggestion to what can simplify the infrastructure part. The goal of this implementation was to make it easy to use in a tutorial, and I think I manage that since the user only need to use code like the one in the last script.\\n\\nWith all this in place it shouldn't be that hard to implement an agent that is using [eventstore](https://geteventstore.com/) or a event simple one backed by a SQL database. All you need to do is send in the connection as the `EventHandler` and then implement the `GetEvents` and `SaveEvents` method accepting the connection (`EventHandler`) as an argument and returning the result for these two methods together with the new `EventHandler` state, the state could be the same as the input to the function.\\n\\nAnd this finishes of my contribution to this year's F# calendar. I hope you enjoyed the read and learned something. Let me know if you have any questions!\\n\\nMerry Christmas!\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>Last year I wrote a post about <a href=\"http://blog.2mas.xyz/pure-application-in-fsharp/\">Pure Functional Application</a> for the F# advent calendar, I really think it is a great initiative so I signed up again. This is my contribution to this year's F# advent calendar, you can find all the other excellent posts on <a href=\"https://twitter.com/sergey_tihon\">@sergey_tihon</a>'s blog: <a href=\"https://sergeytihon.wordpress.com/tag/fsadvent/\">https://sergeytihon.wordpress.com/tag/fsadvent/</a></p>\n<p>One would expect that I would write a totally different post this year, but instead I decided to make my last year's post more concrete. With that I mean I would like to introduce a tutorial for you to follow. I won't cover the whole tutorial in this post since it is described in the tutorial which you can find on <a href=\"https://github.com/mastoj/LibAAS\">github</a>. The tutorial covers how it might be to work in a project where you have put in the time to set up the boilerplate for a project using CQRS and event sourcing in F#. It might not be production ready either, but it might give you some inspiration of how you can approach application development.</p>\n<p>Covering the exercises in the tutorial here would be a little bit boring since they are covered in the tutorial so instead I thought I would explain how the in-memory event store is implemented using an F# agent. You can find the code I will cover in this folder on github: <a href=\"https://github.com/mastoj/LibAAS/tree/master/ex4/done/LibAAS.Infrastructure\">https://github.com/mastoj/LibAAS/tree/master/ex4/done/LibAAS.Infrastructure</a>.</p>\n<h2 id=\"agents\">Agents</h2>\n<p>I hope there are some people out there not that familiar with F# that follow along in this calendar since it is a great opportunity to learn some F#. I'll try to make this post understandable for most developers out there and that is why I'll write a short section about agents. Agents in F# is usually used as alias for <code>MailboxProcessor</code>, so you often see something like this in code where agents are used:</p>\n<pre><code class=\"language-language-fsharp\">type Agent&lt;'TMessage&gt; = MailboxProcessor&lt;'TMessage&gt;\n</code></pre>\n<p>The way I see agents is like in-process workers that can keep some kind of state. You can compare it to actors in the <a href=\"https://en.wikipedia.org/wiki/Actor_model\">actor model</a> but much simpler. They are really great if all you need is a async worker inside your process or a nice way of storing state in your application.</p>\n<p>Agents work with an inbox to which you can send messages, when a message arrives in the inbox the agent will read it an act upon it. The type of the message must be defined before hand and it can be of any type, a discriminated union is often used as message type. You can use simple types like <code>string</code> in this example</p>\n<pre><code class=\"language-language-fsharp\">type Agent&lt;'T&gt; = MailboxProcessor&lt;'T&gt;\n\nlet agent = Agent.Start(fun (inbox:Agent&lt;string&gt;) -&gt;\n    let rec loop() =\n        async {\n            let! msg = inbox.Receive()\n            match msg with\n            | &quot;&quot; -&gt; \n                printfn &quot;Stopping agent&quot;\n            | _ -&gt; \n                printfn &quot;Message recieved: %s&quot; msg\n                return! loop() \n        }\n    loop() \n)\n\nlet post (agent:Agent&lt;'T&gt;) message = agent.Post message\n\n&quot;hello&quot; |&gt; post agent\n&quot;&quot; |&gt; post agent\n&quot;hello&quot; |&gt; post agent\n</code></pre>\n<p>We first make an alias for the <code>MailboxProcessor</code> type. When the alias is created it can be used to start the agent with <code>Agent.Start</code>. The <code>Start</code> function takes a function as argument and this is the body of the agent. The structure you see in this simple example is probably the most common one as far as I know. The body is usually a recursive function in which you listen to new messages with the <code>inbox.Receive</code>, if you want to continue process after a message you just make a recursive call. You can define the recursive function to take a state parameter to keep track inside the agent between messages. I also defined a simple helper so it is easier to post messages to agents with the pipe operator. If you run the code above you should see two messages printed, the last one will not be printed since we are not doing a recursive call on empty messages and that stops the agent.</p>\n<h2 id=\"eventstore\">Event store</h2>\n<p>What is an event store? Short answer: a data store that store events. It is almost that simple. The simplest possible event store need two functions:</p>\n<ul>\n<li>Get events given a stream id</li>\n<li>Save events given a stream id, expected version and events</li>\n</ul>\n<p>The first function should return the list of events for the given stream id. A stream is just a way to group events that belong together.</p>\n<p>The second function should append the events to a given stream with the stream id given that the version of the stream is the same as the expected version. A version of a stream is basically the number of events in the stream, this prevents concurrency issues and is also the transaction boundary when working against the event store.</p>\n<p>That was a short introduction to what an event store is, there is plenty of more information online, but feel free to ask here if you have questions. Next up is the implementation of the event store in F#.</p>\n<h2 id=\"eventstoreimplementation\">Event store implementation</h2>\n<p>The implementation I will describe here is using agents, mainly because it is a nice way to abstract away the basics of an event store. With that in place you can easily create different types of event stores by changing two functions as you'll see.</p>\n<h3 id=\"themessages\">The messages</h3>\n<p>First let's define some simple helpers for our agent:</p>\n<pre><code class=\"language-language-fsharp\">module AgentHelper\n\ntype Agent&lt;'T&gt; = MailboxProcessor&lt;'T&gt;\nlet post (agent:Agent&lt;'T&gt;) message = agent.Post message\nlet postAsyncReply (agent:Agent&lt;'T&gt;) messageConstr = agent.PostAndAsyncReply(messageConstr)\n</code></pre>\n<p>Now when we got that out of our way we can start with the acutal implementation. We will start with the messages and some types that help us stay out of trouble.</p>\n<pre><code class=\"language-language-fsharp\">type StreamId = StreamId of int\ntype StreamVersion = StreamVersion of int\n\ntype SaveResult = \n    | Ok\n    | VersionConflict\n\ntype Messages&lt;'T&gt; = \n    | GetEvents of StreamId * AsyncReplyChannel&lt;'T list option&gt;\n    | SaveEvents of StreamId * StreamVersion * 'T list * AsyncReplyChannel&lt;SaveResult&gt;\n    | AddSubscriber of string * (StreamId * 'T list -&gt; unit)\n    | RemoveSubscriber of string\n</code></pre>\n<p>Just by reading this type definitions you can almost understand how the event store is supposed to work. We have a generic <code>Messages</code> type, where the generic parameter defines the type of event that we want to store in the event store. We have four actions we will be able to do against the event store:</p>\n<ol>\n<li>Get the events for a stream.</li>\n<li>Save events for a stream.<br>\na. When saving you can have version conflict and to indicate that we use the <code>SaveResult</code> type.</li>\n<li>You can add multiple subscribers, where the first string is an id of the subscriber (should probably be wrapped in a type). A subscriber will be called every time some events have been saved.</li>\n<li>You can remove an existing subscriber based on the string id.</li>\n</ol>\n<h3 id=\"stateformat\">State format</h3>\n<p>To make the agent flexible we need to keep an internal state that can be provided when creating the agent. The definition of the state type looks like this:</p>\n<pre><code class=\"language-language-fsharp\">type internal EventStoreState&lt;'TEvent,'THandler&gt; = \n    {\n        EventHandler: 'THandler\n        GetEvents: 'THandler -&gt; StreamId -&gt; ('TEvent list option * 'THandler) \n        SaveEvents: 'THandler -&gt; StreamId -&gt; StreamVersion -&gt; 'TEvent list -&gt; (SaveResult * 'THandler)\n        Subscribers: Map&lt;string, (StreamId * 'TEvent list -&gt; unit)&gt;\n    }\n</code></pre>\n<p>What I call <code>EventHandler</code> here is the the &quot;thing&quot; that stores the actual events, it can be an internal map or a connection to an external db. The methods <code>GetEvents</code> and <code>SaveEvents</code> uses the <code>EventHandler</code> to get or save events. The last thing in the state is the subscribers which we also need to keep track of.</p>\n<h3 id=\"agentbody\">Agent body</h3>\n<p>Next up is the actual implementation of the agent. I give you the code right away and then walk you through it:</p>\n<pre><code class=\"language-language-fsharp\">let eventSourcingAgent&lt;'T, 'TEventHandler&gt; (eventHandler:'TEventHandler) getEvents saveEvents (inbox:Agent&lt;Messages&lt;'T&gt;&gt;) = \n    let initState = \n        {\n            EventHandler = eventHandler\n            Subscribers = Map.empty\n            GetEvents = getEvents\n            SaveEvents = saveEvents\n        }\n    let rec loop state = \n        async {\n            let! msg = inbox.Receive()\n            match msg with\n            | GetEvents (id, replyChannel) -&gt;\n                let (events, newHandler) = state.GetEvents state.EventHandler id\n                replyChannel.Reply(events)\n                return! loop {state with EventHandler = newHandler}\n            | SaveEvents (id, expectedVersion, events, replyChannel) -&gt;\n                let (result, newHandler) = state.SaveEvents state.EventHandler id expectedVersion events\n                if result = Ok then state.Subscribers |&gt; Map.iter (fun _ sub -&gt; sub(id, events)) else ()\n                replyChannel.Reply(result)\n                return! loop {state with EventHandler = newHandler}\n            | AddSubscriber (subId, subFunction) -&gt;\n                let newState = {state with Subscribers = (state.Subscribers |&gt; Map.add subId subFunction)}\n                return! loop newState\n            | RemoveSubscriber subId -&gt;\n                let newState = {state with Subscribers = (state.Subscribers |&gt; Map.remove subId )}\n                return! loop newState\n        }\n    loop initState\n</code></pre>\n<p>To create the agent we need the <code>eventHandler</code> a function to <code>getEvents</code> and <code>saveEvents</code>, nothing to fancy about that. The first thing we do in the function is to create the <code>initState</code> with the input and an empty <code>Map</code> for our subscribers. Next up is the recursive loop (remember it from the section above?). We first wait until there is a new message in the inbox, when we get one we match on the message type.</p>\n<p>For <code>GetEvents</code> we use the <code>GetEvents</code> method on the state passing in the <code>EventHandler</code> and the id of the string. When we have the events we reply back to the callee and finish it of with a recursive call with the new state (if it has changed).</p>\n<p><code>SaveEvents</code> works almost the same way, with the addition of notifying the subscribers if we manage to save the events. We also reply back to the callee with the result of the save operation before making the recursive call to wait for the next message.</p>\n<p>The implementation of <code>AddSubscriber</code> and <code>RemoveSubscriber</code> do what you would expect them to, it adds or removes a subscriber for the <code>Subscribers</code> map we have in the state and make a recursive call to wait for the next message.</p>\n<h3 id=\"inmemoryimplementation\">In-memory implementation</h3>\n<p>To make it a little bit easier for a user to work with an agent it make sense to hide it behind some kind of type, which also make it easier to swap for another implementation later, and that type looks like this:</p>\n<pre><code class=\"language-language-fsharp\">type EventStore&lt;'TEvent, 'TError&gt; = \n    {\n        GetEvents: StreamId -&gt; Result&lt;StreamVersion*'TEvent list, 'TError&gt;\n        SaveEvents: StreamId -&gt; StreamVersion -&gt; 'TEvent list -&gt; Result&lt;'TEvent list, 'TError&gt;\n        AddSubscriber: string -&gt; (StreamId * 'TEvent list -&gt; unit) -&gt; unit\n        RemoveSubscriber: string -&gt; unit\n    }\n</code></pre>\n<p>The <code>SaveEvents</code> and <code>GetEvents</code> method returns something of type <code>Result</code>, and that is taken from <a href=\"http://fsharpforfunandprofit.com/rop/\">Railway Oriented Programming</a> which is a really nice way to handle errors in an application without introducing side effects like exceptions. The <code>Result</code> type is defined as:</p>\n<pre><code class=\"language-language-fsharp\">[&lt;AutoOpen&gt;]\nmodule ErrorHandling\n \ntype Result&lt;'TResult, 'TError&gt; = \n    | Success of 'TResult\n    | Failure of 'TError\n\nlet ok x = Success x\nlet fail x = Failure x\n</code></pre>\n<p>Together with the type we have defined two helpers <code>ok</code> and <code>fail</code> to make it easier to create a <code>Result</code> through piping.</p>\n<p>We also need a function to create a wrapper around an agent that create an instance of an <code>EventStore</code>.</p>\n<pre><code class=\"language-language-fsharp\">let createEventStore&lt;'TEvent, 'TError&gt; (versionError:'TError) agent =\n    let getEvents streamId : Result&lt;StreamVersion*'TEvent list, 'TError&gt; = \n        let result = (fun r -&gt; GetEvents (streamId, r)) |&gt; postAsyncReply agent |&gt; Async.RunSynchronously\n        match result with\n        | Some events -&gt; (StreamVersion (events |&gt; List.length), events) |&gt; ok\n        | None -&gt; (StreamVersion 0, []) |&gt; ok\n\n    let saveEvents streamId expectedVersion events : Result&lt;'TEvent list, 'TError&gt; = \n        let result = (fun r -&gt; SaveEvents(streamId, expectedVersion, events, r)) |&gt; postAsyncReply agent |&gt; Async.RunSynchronously\n        match result with\n        | Ok -&gt; events |&gt; ok\n        | VersionConflict -&gt; versionError |&gt; fail\n\n    let addSubscriber subId subscriber = \n        (subId,subscriber) |&gt; AddSubscriber |&gt; post agent\n\n    let removeSubscriber subId = \n        subId |&gt; RemoveSubscriber |&gt; post agent\n\n    { GetEvents = getEvents; SaveEvents = saveEvents; AddSubscriber = addSubscriber; RemoveSubscriber = removeSubscriber}\n</code></pre>\n<p>It is nothing to complicated going on, the <code>getEvents</code> function takes a stream id and wrap it in a <code>GetEvents</code> message which is sent to the agent. After sending the message we wait for the reply and return the events together with the current version of the stream wrapped in a <code>Result</code> type. The <code>saveEvents</code> method works almost the same way, that is, we wrap the input in a <code>SaveEvents</code> message and pass it to the agent and wait for the reply. If we get a <code>VersionConflict</code> back we translate it to the provided error to keep this code isolated from other code.</p>\n<p>Now we have all the pieces to put together our in-memory event store. The in-memory event store will use a simple map as a storage for the events for easy lookup.</p>\n<pre><code class=\"language-language-fsharp\">let createInMemoryEventStore&lt;'TEvent, 'TError&gt; (versionError:'TError) =\n    let initState : Map&lt;StreamId, 'TEvent list&gt; = Map.empty\n\n    let saveEventsInMap map id expectedVersion events = \n        match map |&gt; Map.tryFind id with\n        | None -&gt; \n            (Ok, map |&gt; Map.add id events)\n        | Some existingEvents -&gt;\n            let currentVersion = existingEvents |&gt; List.length |&gt; StreamVersion\n            match currentVersion = expectedVersion with\n            | true -&gt; \n                (Ok, map |&gt; Map.add id (existingEvents@events))\n            | false -&gt; \n                (VersionConflict, map)\n\n    let getEventsInMap map id = Map.tryFind id map, map\n\n    let agent = createEventStoreAgent initState getEventsInMap saveEventsInMap\n    createEventStore&lt;'TEvent, 'TError&gt; versionError agent\n</code></pre>\n<ul>\n<li>The <code>initState</code> is of course an empty map since we don't have any events when we start.</li>\n<li>The <code>saveEventsInMap</code> uses the <code>id</code> argument to lookup in the <code>map</code> argument (current state), if the result is <code>None</code> the entry is added to the map with the <code>events</code>. If the entry already exist we check the version before appending the <code>events</code> to the stream.</li>\n<li>The <code>getEventsInMap</code> will just do a lookup in the <code>map</code> and returning an <code>Option</code> type together with the new map which is the same as the input.</li>\n</ul>\n<p>With these three functions we can now call the <code>createEventStore</code> function to create our in-memory event store and we are done.</p>\n<h2 id=\"takingitoutforaspin\">Taking it out for a spin</h2>\n<p>The simplest way to actually try the event store out is to use it in a fsharp script. So in the same folder as the I have the files for the implementation I also have a simple script with the following content:</p>\n<pre><code class=\"language-language-fsharp\">#load &quot;AgentHelper.fs&quot;\n#load &quot;ErrorHandling.fs&quot;\n#load &quot;EventStore.fs&quot;\n\nopen EventStore\n\nlet inMemoryEventStore = createInMemoryEventStore&lt;string,string&gt; &quot;This is a version error&quot;\ninMemoryEventStore.AddSubscriber &quot;FirstSubscriber&quot; (printfn &quot;%A&quot;)\nlet res0 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 0) [&quot;Hello&quot;;&quot;World&quot;]\nlet res1 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 1) [&quot;Hello2&quot;;&quot;World2&quot;]\nlet res2 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 2) [&quot;Hello2&quot;;&quot;World2&quot;]\n\n[res0;res1;res2] |&gt; List.mapi (fun i v -&gt; printfn &quot;%i: %A&quot; i v)\n</code></pre>\n<p>We keep it really simple and only storing strings, as well as using a string as our error indicator. Executing this code with mono <code>fsharpi --exec Script.fsx</code> or on Windows <code>fsi --exec Script.fsx</code> should give the following output:</p>\n<pre><code>(StreamId 1, [&quot;Hello&quot;; &quot;World&quot;])\n(StreamId 1, [&quot;Hello2&quot;; &quot;World2&quot;])\n0: Success [&quot;Hello&quot;; &quot;World&quot;]\n1: Failure &quot;This is a version error&quot;\n2: Success [&quot;Hello2&quot;; &quot;World2&quot;]\n</code></pre>\n<p>The first two lines are from the subscriber and last in the script I print all the results.</p>\n<h2 id=\"nowitisyourturn\">Now it is your turn</h2>\n<p>There is room for a lot of improvement here I guess, but it is a good starting point. Feel free to try the tutorial and also come with suggestion to what can simplify the infrastructure part. The goal of this implementation was to make it easy to use in a tutorial, and I think I manage that since the user only need to use code like the one in the last script.</p>\n<p>With all this in place it shouldn't be that hard to implement an agent that is using <a href=\"https://geteventstore.com/\">eventstore</a> or a event simple one backed by a SQL database. All you need to do is send in the connection as the <code>EventHandler</code> and then implement the <code>GetEvents</code> and <code>SaveEvents</code> method accepting the connection (<code>EventHandler</code>) as an argument and returning the result for these two methods together with the new <code>EventHandler</code> state, the state could be the same as the input to the function.</p>\n<p>And this finishes of my contribution to this year's F# calendar. I hope you enjoyed the read and learned something. Let me know if you have any questions!</p>\n<p>Merry Christmas!</p>\n<!--kg-card-end: markdown-->","comment_id":"64","plaintext":"Last year I wrote a post about Pure Functional Application\n[http://blog.2mas.xyz/pure-application-in-fsharp/] for the F# advent calendar, I\nreally think it is a great initiative so I signed up again. This is my\ncontribution to this year's F# advent calendar, you can find all the other\nexcellent posts on @sergey_tihon [https://twitter.com/sergey_tihon]'s blog: \nhttps://sergeytihon.wordpress.com/tag/fsadvent/\n\nOne would expect that I would write a totally different post this year, but\ninstead I decided to make my last year's post more concrete. With that I mean I\nwould like to introduce a tutorial for you to follow. I won't cover the whole\ntutorial in this post since it is described in the tutorial which you can find\non github [https://github.com/mastoj/LibAAS]. The tutorial covers how it might\nbe to work in a project where you have put in the time to set up the boilerplate\nfor a project using CQRS and event sourcing in F#. It might not be production\nready either, but it might give you some inspiration of how you can approach\napplication development.\n\nCovering the exercises in the tutorial here would be a little bit boring since\nthey are covered in the tutorial so instead I thought I would explain how the\nin-memory event store is implemented using an F# agent. You can find the code I\nwill cover in this folder on github: \nhttps://github.com/mastoj/LibAAS/tree/master/ex4/done/LibAAS.Infrastructure.\n\nAgents\nI hope there are some people out there not that familiar with F# that follow\nalong in this calendar since it is a great opportunity to learn some F#. I'll\ntry to make this post understandable for most developers out there and that is\nwhy I'll write a short section about agents. Agents in F# is usually used as\nalias for MailboxProcessor, so you often see something like this in code where\nagents are used:\n\ntype Agent<'TMessage> = MailboxProcessor<'TMessage>\n\n\nThe way I see agents is like in-process workers that can keep some kind of\nstate. You can compare it to actors in the actor model\n[https://en.wikipedia.org/wiki/Actor_model] but much simpler. They are really\ngreat if all you need is a async worker inside your process or a nice way of\nstoring state in your application.\n\nAgents work with an inbox to which you can send messages, when a message arrives\nin the inbox the agent will read it an act upon it. The type of the message must\nbe defined before hand and it can be of any type, a discriminated union is often\nused as message type. You can use simple types like string in this example\n\ntype Agent<'T> = MailboxProcessor<'T>\n\nlet agent = Agent.Start(fun (inbox:Agent<string>) ->\n    let rec loop() =\n        async {\n            let! msg = inbox.Receive()\n            match msg with\n            | \"\" -> \n                printfn \"Stopping agent\"\n            | _ -> \n                printfn \"Message recieved: %s\" msg\n                return! loop() \n        }\n    loop() \n)\n\nlet post (agent:Agent<'T>) message = agent.Post message\n\n\"hello\" |> post agent\n\"\" |> post agent\n\"hello\" |> post agent\n\n\nWe first make an alias for the MailboxProcessor type. When the alias is created\nit can be used to start the agent with Agent.Start. The Start function takes a\nfunction as argument and this is the body of the agent. The structure you see in\nthis simple example is probably the most common one as far as I know. The body\nis usually a recursive function in which you listen to new messages with the \ninbox.Receive, if you want to continue process after a message you just make a\nrecursive call. You can define the recursive function to take a state parameter\nto keep track inside the agent between messages. I also defined a simple helper\nso it is easier to post messages to agents with the pipe operator. If you run\nthe code above you should see two messages printed, the last one will not be\nprinted since we are not doing a recursive call on empty messages and that stops\nthe agent.\n\nEvent store\nWhat is an event store? Short answer: a data store that store events. It is\nalmost that simple. The simplest possible event store need two functions:\n\n * Get events given a stream id\n * Save events given a stream id, expected version and events\n\nThe first function should return the list of events for the given stream id. A\nstream is just a way to group events that belong together.\n\nThe second function should append the events to a given stream with the stream\nid given that the version of the stream is the same as the expected version. A\nversion of a stream is basically the number of events in the stream, this\nprevents concurrency issues and is also the transaction boundary when working\nagainst the event store.\n\nThat was a short introduction to what an event store is, there is plenty of more\ninformation online, but feel free to ask here if you have questions. Next up is\nthe implementation of the event store in F#.\n\nEvent store implementation\nThe implementation I will describe here is using agents, mainly because it is a\nnice way to abstract away the basics of an event store. With that in place you\ncan easily create different types of event stores by changing two functions as\nyou'll see.\n\nThe messages\nFirst let's define some simple helpers for our agent:\n\nmodule AgentHelper\n\ntype Agent<'T> = MailboxProcessor<'T>\nlet post (agent:Agent<'T>) message = agent.Post message\nlet postAsyncReply (agent:Agent<'T>) messageConstr = agent.PostAndAsyncReply(messageConstr)\n\n\nNow when we got that out of our way we can start with the acutal implementation.\nWe will start with the messages and some types that help us stay out of trouble.\n\ntype StreamId = StreamId of int\ntype StreamVersion = StreamVersion of int\n\ntype SaveResult = \n    | Ok\n    | VersionConflict\n\ntype Messages<'T> = \n    | GetEvents of StreamId * AsyncReplyChannel<'T list option>\n    | SaveEvents of StreamId * StreamVersion * 'T list * AsyncReplyChannel<SaveResult>\n    | AddSubscriber of string * (StreamId * 'T list -> unit)\n    | RemoveSubscriber of string\n\n\nJust by reading this type definitions you can almost understand how the event\nstore is supposed to work. We have a generic Messages type, where the generic\nparameter defines the type of event that we want to store in the event store. We\nhave four actions we will be able to do against the event store:\n\n 1. Get the events for a stream.\n 2. Save events for a stream.\n    a. When saving you can have version conflict and to indicate that we use the \n    SaveResult type.\n 3. You can add multiple subscribers, where the first string is an id of the\n    subscriber (should probably be wrapped in a type). A subscriber will be\n    called every time some events have been saved.\n 4. You can remove an existing subscriber based on the string id.\n\nState format\nTo make the agent flexible we need to keep an internal state that can be\nprovided when creating the agent. The definition of the state type looks like\nthis:\n\ntype internal EventStoreState<'TEvent,'THandler> = \n    {\n        EventHandler: 'THandler\n        GetEvents: 'THandler -> StreamId -> ('TEvent list option * 'THandler) \n        SaveEvents: 'THandler -> StreamId -> StreamVersion -> 'TEvent list -> (SaveResult * 'THandler)\n        Subscribers: Map<string, (StreamId * 'TEvent list -> unit)>\n    }\n\n\nWhat I call EventHandler here is the the \"thing\" that stores the actual events,\nit can be an internal map or a connection to an external db. The methods \nGetEvents and SaveEvents uses the EventHandler to get or save events. The last\nthing in the state is the subscribers which we also need to keep track of.\n\nAgent body\nNext up is the actual implementation of the agent. I give you the code right\naway and then walk you through it:\n\nlet eventSourcingAgent<'T, 'TEventHandler> (eventHandler:'TEventHandler) getEvents saveEvents (inbox:Agent<Messages<'T>>) = \n    let initState = \n        {\n            EventHandler = eventHandler\n            Subscribers = Map.empty\n            GetEvents = getEvents\n            SaveEvents = saveEvents\n        }\n    let rec loop state = \n        async {\n            let! msg = inbox.Receive()\n            match msg with\n            | GetEvents (id, replyChannel) ->\n                let (events, newHandler) = state.GetEvents state.EventHandler id\n                replyChannel.Reply(events)\n                return! loop {state with EventHandler = newHandler}\n            | SaveEvents (id, expectedVersion, events, replyChannel) ->\n                let (result, newHandler) = state.SaveEvents state.EventHandler id expectedVersion events\n                if result = Ok then state.Subscribers |> Map.iter (fun _ sub -> sub(id, events)) else ()\n                replyChannel.Reply(result)\n                return! loop {state with EventHandler = newHandler}\n            | AddSubscriber (subId, subFunction) ->\n                let newState = {state with Subscribers = (state.Subscribers |> Map.add subId subFunction)}\n                return! loop newState\n            | RemoveSubscriber subId ->\n                let newState = {state with Subscribers = (state.Subscribers |> Map.remove subId )}\n                return! loop newState\n        }\n    loop initState\n\n\nTo create the agent we need the eventHandler a function to getEvents and \nsaveEvents, nothing to fancy about that. The first thing we do in the function\nis to create the initState with the input and an empty Map for our subscribers.\nNext up is the recursive loop (remember it from the section above?). We first\nwait until there is a new message in the inbox, when we get one we match on the\nmessage type.\n\nFor GetEvents we use the GetEvents method on the state passing in the \nEventHandler and the id of the string. When we have the events we reply back to\nthe callee and finish it of with a recursive call with the new state (if it has\nchanged).\n\nSaveEvents works almost the same way, with the addition of notifying the\nsubscribers if we manage to save the events. We also reply back to the callee\nwith the result of the save operation before making the recursive call to wait\nfor the next message.\n\nThe implementation of AddSubscriber and RemoveSubscriber do what you would\nexpect them to, it adds or removes a subscriber for the Subscribers map we have\nin the state and make a recursive call to wait for the next message.\n\nIn-memory implementation\nTo make it a little bit easier for a user to work with an agent it make sense to\nhide it behind some kind of type, which also make it easier to swap for another\nimplementation later, and that type looks like this:\n\ntype EventStore<'TEvent, 'TError> = \n    {\n        GetEvents: StreamId -> Result<StreamVersion*'TEvent list, 'TError>\n        SaveEvents: StreamId -> StreamVersion -> 'TEvent list -> Result<'TEvent list, 'TError>\n        AddSubscriber: string -> (StreamId * 'TEvent list -> unit) -> unit\n        RemoveSubscriber: string -> unit\n    }\n\n\nThe SaveEvents and GetEvents method returns something of type Result, and that\nis taken from Railway Oriented Programming\n[http://fsharpforfunandprofit.com/rop/] which is a really nice way to handle\nerrors in an application without introducing side effects like exceptions. The \nResult type is defined as:\n\n[<AutoOpen>]\nmodule ErrorHandling\n \ntype Result<'TResult, 'TError> = \n    | Success of 'TResult\n    | Failure of 'TError\n\nlet ok x = Success x\nlet fail x = Failure x\n\n\nTogether with the type we have defined two helpers ok and fail to make it easier\nto create a Result through piping.\n\nWe also need a function to create a wrapper around an agent that create an\ninstance of an EventStore.\n\nlet createEventStore<'TEvent, 'TError> (versionError:'TError) agent =\n    let getEvents streamId : Result<StreamVersion*'TEvent list, 'TError> = \n        let result = (fun r -> GetEvents (streamId, r)) |> postAsyncReply agent |> Async.RunSynchronously\n        match result with\n        | Some events -> (StreamVersion (events |> List.length), events) |> ok\n        | None -> (StreamVersion 0, []) |> ok\n\n    let saveEvents streamId expectedVersion events : Result<'TEvent list, 'TError> = \n        let result = (fun r -> SaveEvents(streamId, expectedVersion, events, r)) |> postAsyncReply agent |> Async.RunSynchronously\n        match result with\n        | Ok -> events |> ok\n        | VersionConflict -> versionError |> fail\n\n    let addSubscriber subId subscriber = \n        (subId,subscriber) |> AddSubscriber |> post agent\n\n    let removeSubscriber subId = \n        subId |> RemoveSubscriber |> post agent\n\n    { GetEvents = getEvents; SaveEvents = saveEvents; AddSubscriber = addSubscriber; RemoveSubscriber = removeSubscriber}\n\n\nIt is nothing to complicated going on, the getEvents function takes a stream id\nand wrap it in a GetEvents message which is sent to the agent. After sending the\nmessage we wait for the reply and return the events together with the current\nversion of the stream wrapped in a Result type. The saveEvents method works\nalmost the same way, that is, we wrap the input in a SaveEvents message and pass\nit to the agent and wait for the reply. If we get a VersionConflict back we\ntranslate it to the provided error to keep this code isolated from other code.\n\nNow we have all the pieces to put together our in-memory event store. The\nin-memory event store will use a simple map as a storage for the events for easy\nlookup.\n\nlet createInMemoryEventStore<'TEvent, 'TError> (versionError:'TError) =\n    let initState : Map<StreamId, 'TEvent list> = Map.empty\n\n    let saveEventsInMap map id expectedVersion events = \n        match map |> Map.tryFind id with\n        | None -> \n            (Ok, map |> Map.add id events)\n        | Some existingEvents ->\n            let currentVersion = existingEvents |> List.length |> StreamVersion\n            match currentVersion = expectedVersion with\n            | true -> \n                (Ok, map |> Map.add id (existingEvents@events))\n            | false -> \n                (VersionConflict, map)\n\n    let getEventsInMap map id = Map.tryFind id map, map\n\n    let agent = createEventStoreAgent initState getEventsInMap saveEventsInMap\n    createEventStore<'TEvent, 'TError> versionError agent\n\n\n * The initState is of course an empty map since we don't have any events when\n   we start.\n * The saveEventsInMap uses the id argument to lookup in the map argument\n   (current state), if the result is None the entry is added to the map with the \n   events. If the entry already exist we check the version before appending the \n   events to the stream.\n * The getEventsInMap will just do a lookup in the map and returning an Option \n   type together with the new map which is the same as the input.\n\nWith these three functions we can now call the createEventStore function to\ncreate our in-memory event store and we are done.\n\nTaking it out for a spin\nThe simplest way to actually try the event store out is to use it in a fsharp\nscript. So in the same folder as the I have the files for the implementation I\nalso have a simple script with the following content:\n\n#load \"AgentHelper.fs\"\n#load \"ErrorHandling.fs\"\n#load \"EventStore.fs\"\n\nopen EventStore\n\nlet inMemoryEventStore = createInMemoryEventStore<string,string> \"This is a version error\"\ninMemoryEventStore.AddSubscriber \"FirstSubscriber\" (printfn \"%A\")\nlet res0 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 0) [\"Hello\";\"World\"]\nlet res1 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 1) [\"Hello2\";\"World2\"]\nlet res2 = inMemoryEventStore.SaveEvents (StreamId 1) (StreamVersion 2) [\"Hello2\";\"World2\"]\n\n[res0;res1;res2] |> List.mapi (fun i v -> printfn \"%i: %A\" i v)\n\n\nWe keep it really simple and only storing strings, as well as using a string as\nour error indicator. Executing this code with mono fsharpi --exec Script.fsx or\non Windows fsi --exec Script.fsx should give the following output:\n\n(StreamId 1, [\"Hello\"; \"World\"])\n(StreamId 1, [\"Hello2\"; \"World2\"])\n0: Success [\"Hello\"; \"World\"]\n1: Failure \"This is a version error\"\n2: Success [\"Hello2\"; \"World2\"]\n\n\nThe first two lines are from the subscriber and last in the script I print all\nthe results.\n\nNow it is your turn\nThere is room for a lot of improvement here I guess, but it is a good starting\npoint. Feel free to try the tutorial and also come with suggestion to what can\nsimplify the infrastructure part. The goal of this implementation was to make it\neasy to use in a tutorial, and I think I manage that since the user only need to\nuse code like the one in the last script.\n\nWith all this in place it shouldn't be that hard to implement an agent that is\nusing eventstore [https://geteventstore.com/] or a event simple one backed by a\nSQL database. All you need to do is send in the connection as the EventHandler \nand then implement the GetEvents and SaveEvents method accepting the connection\n(EventHandler) as an argument and returning the result for these two methods\ntogether with the new EventHandler state, the state could be the same as the\ninput to the function.\n\nAnd this finishes of my contribution to this year's F# calendar. I hope you\nenjoyed the read and learned something. Let me know if you have any questions!\n\nMerry Christmas!","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-12-03T22:12:09.000Z","updated_at":"2015-12-14T13:15:26.000Z","published_at":"2015-12-14T13:15:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe073","uuid":"ae5ee69a-90b7-4ad0-897d-fdeb25650380","title":"F# will solve your everyday problem without a headache","slug":"fsharp-will-solve-your-everyday-problem-without-a-headache","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The last couple of days I had two experiences that triggered this post. The first one was a question at work regarding how to model a finite state machine (FSM) in java or a language similar to java. The second one was a [tweet](https://twitter.com/isaac_abraham/status/698178752560418816) by [Isaac Abraham](https://twitter.com/isaac_abraham): \\n\\n<blockquote class=\\\"twitter-tweet\\\" data-lang=\\\"en\\\"><p lang=\\\"en\\\" dir=\\\"ltr\\\">It&#39;s really disappointing in 2016 to read this &quot;F# tends to be used in domains that have a lot of scientific or financial computations.&quot;</p>&mdash; Isaac Abraham (@isaac_abraham) <a href=\\\"https://twitter.com/isaac_abraham/status/698178752560418816\\\">February 12, 2016</a></blockquote>\\n<script async src=\\\"//platform.twitter.com/widgets.js\\\" charset=\\\"utf-8\\\"></script>\\n\\nThe quote was from an [interview](http://ericlippert.com/2016/01/14/functional-style-follow-up/) with Eric Lippert, a former member of the C# team.\\n\\nSo how are these two events related? They are related because the greater community still has a misconception of functional languages, and to find better solutions to problems we need to look further than C# and java. I agree with Isaac 100 % that it is disappointing, and sad, to read and hear this misconception of F# and functional programming. At the same time I do see a change happening for everyone, with a more functional approach in the front-end with libraries like react and more functional aspects in languages like java and C#. What I don't understand is why doesn't people try harder to stay ahead of the game and learn functional programming, even though they do like it when it gets accepted by the greater community. Almost every C# developer I know is now comfortable, and like, lambda expressions. More and more developers discover that immutability isn't that bad after all, but still don't use it as much since it is so easy to use mutability in the languages we use the most. Everyone is now comfortable with the `var` keyword in C#, but still want to write the whole type on the other side of the equal sign.\\n\\nF#, and probably scala and other functional languages that are strongly type handle all this in better ways than we are used to in java and C#. So my answer to the question was how I should define an FSM in F#. This is a problem that is not scientific and it has nothing to do with finance. This is a real problem that anyone of us could have in almost any type of application where you need to model a FSM, and model a FSM is probably something we should do more often. Before we get to my answer to the question, let's look at one of the references in one of the other answers. One colleague linked to [Martin Fowler's](https://twitter.com/martinfowler) book Domain-Specific Languages which you can read an example dealing with FSM here: http://www.informit.com/articles/article.aspx?p=1592379. Reading through the example gave me a headache and almost made my eyes bleed. The java code in the example is probably good java code, and I would probably think that all those fancy patterns where nice a couple of years back when the book was written. What is the problem with the code in the book? As I said, it is probably nice java code, but that doesn't mean it is readable. I took me a while to understand how the State Machine Model worked as described here: http://www.informit.com/articles/article.aspx?p=1592379&seqNum=2. Why was it so? The main reason it was hard for me to understand the sample code there was probably all the noise and extra syntax. The noise and extra syntax distracted me from the actual model which can never be a good thing. Let's go to the actual FSM defined on page 3: http://www.informit.com/articles/article.aspx?p=1592379&seqNum=3. The code here is definitely easier to understand since the level of noise has decreased due to the previous model, but it is still much noise. In fact, Fowler agree since he also provides multiple way to specify the model in other format than java code. There is one xml version, and two other versions. The only reason you need this is due to the fact that java is to verbose and the level of noise is too high.\\n\\n## F# to the rescue\\n\\nThe whole purpose of this post is to show you that it is possible to solve real world problem with F# in more elegant ways than you would in languages like C# and java. I'll show you my code and then explain why I think it is better. I don't say that Fowler's java code is bad, more that the language might not be right tool.\\n\\n<script src=\\\"https://gist.github.com/mastoj/9dfc21848c449fadcc93.js\\\"></script>\\n\\nLet me walk you through this piece of code. First I define a FSM module where all the general logic for implementing the FSM is defined. I define one type that represents the FSM. In the module I have also defined a set of helper functions that helps me create a FSM and also one [function](https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L12) that handles an event given an event and a FSM as input. All the helper functions takes a FSM as the last argument, since that allows me to pipe that argument in making it possible to have a really nice DSL in the language. The helper functions takes the provided FSM and returns a new FSM based on the provided FSM and the extra input. [`registerTransition`](https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L40) takes mapping from one state to another given an event. \\n\\nI defined all the valid events, commands and states on these lines: https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L54-L72. I really don't think they need any explanation. \\n\\nThe creation of the FSM is done here: https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L75-L88. As you can see I'm using the helper functions which defines a really nice DSL for me. \\n\\nWhen I have the FSM defined I can actually try that it works and that is done here: https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L90-L105. First I define a helper infix operator that print the current state before calling the next function in each step. \\n\\nAs I hope you can see in this example there are many advantages compared to the java version:\\n\\n* No nulls\\n* Impossible to represent bad states\\n* The ration of noise vs. relevant code is significantly lower\\n* Shorter code, so it is easier to get the full picture\\n* No need for an external dsl\\n\\n## Wrap up\\n\\nF# (and most likely Scala) is a language you can, and should use, to solve your everyday problem. It will give you more concise code, easier to read code, and easier to maintain code. This will also have the positive side effect of less bugs. I understand that it might be a little bit weird at start since it is a whole new paradigm, but the reward is high on the other side. Learning to program functionally will help you write better programs in any language and you will also be ahead of the game when the functional features comes to java and C#. Note that even though those languages get some functional features they will never be as functional as F# and Scala since the base design of the languages are different. Do your self a favor and learn you some FP :)\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>The last couple of days I had two experiences that triggered this post. The first one was a question at work regarding how to model a finite state machine (FSM) in java or a language similar to java. The second one was a <a href=\"https://twitter.com/isaac_abraham/status/698178752560418816\">tweet</a> by <a href=\"https://twitter.com/isaac_abraham\">Isaac Abraham</a>:</p>\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">It&#39;s really disappointing in 2016 to read this &quot;F# tends to be used in domains that have a lot of scientific or financial computations.&quot;</p>&mdash; Isaac Abraham (@isaac_abraham) <a href=\"https://twitter.com/isaac_abraham/status/698178752560418816\">February 12, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n<p>The quote was from an <a href=\"http://ericlippert.com/2016/01/14/functional-style-follow-up/\">interview</a> with Eric Lippert, a former member of the C# team.</p>\n<p>So how are these two events related? They are related because the greater community still has a misconception of functional languages, and to find better solutions to problems we need to look further than C# and java. I agree with Isaac 100 % that it is disappointing, and sad, to read and hear this misconception of F# and functional programming. At the same time I do see a change happening for everyone, with a more functional approach in the front-end with libraries like react and more functional aspects in languages like java and C#. What I don't understand is why doesn't people try harder to stay ahead of the game and learn functional programming, even though they do like it when it gets accepted by the greater community. Almost every C# developer I know is now comfortable, and like, lambda expressions. More and more developers discover that immutability isn't that bad after all, but still don't use it as much since it is so easy to use mutability in the languages we use the most. Everyone is now comfortable with the <code>var</code> keyword in C#, but still want to write the whole type on the other side of the equal sign.</p>\n<p>F#, and probably scala and other functional languages that are strongly type handle all this in better ways than we are used to in java and C#. So my answer to the question was how I should define an FSM in F#. This is a problem that is not scientific and it has nothing to do with finance. This is a real problem that anyone of us could have in almost any type of application where you need to model a FSM, and model a FSM is probably something we should do more often. Before we get to my answer to the question, let's look at one of the references in one of the other answers. One colleague linked to <a href=\"https://twitter.com/martinfowler\">Martin Fowler's</a> book Domain-Specific Languages which you can read an example dealing with FSM here: <a href=\"http://www.informit.com/articles/article.aspx?p=1592379\">http://www.informit.com/articles/article.aspx?p=1592379</a>. Reading through the example gave me a headache and almost made my eyes bleed. The java code in the example is probably good java code, and I would probably think that all those fancy patterns where nice a couple of years back when the book was written. What is the problem with the code in the book? As I said, it is probably nice java code, but that doesn't mean it is readable. I took me a while to understand how the State Machine Model worked as described here: <a href=\"http://www.informit.com/articles/article.aspx?p=1592379&amp;seqNum=2\">http://www.informit.com/articles/article.aspx?p=1592379&amp;seqNum=2</a>. Why was it so? The main reason it was hard for me to understand the sample code there was probably all the noise and extra syntax. The noise and extra syntax distracted me from the actual model which can never be a good thing. Let's go to the actual FSM defined on page 3: <a href=\"http://www.informit.com/articles/article.aspx?p=1592379&amp;seqNum=3\">http://www.informit.com/articles/article.aspx?p=1592379&amp;seqNum=3</a>. The code here is definitely easier to understand since the level of noise has decreased due to the previous model, but it is still much noise. In fact, Fowler agree since he also provides multiple way to specify the model in other format than java code. There is one xml version, and two other versions. The only reason you need this is due to the fact that java is to verbose and the level of noise is too high.</p>\n<h2 id=\"ftotherescue\">F# to the rescue</h2>\n<p>The whole purpose of this post is to show you that it is possible to solve real world problem with F# in more elegant ways than you would in languages like C# and java. I'll show you my code and then explain why I think it is better. I don't say that Fowler's java code is bad, more that the language might not be right tool.</p>\n<script src=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93.js\"></script>\n<p>Let me walk you through this piece of code. First I define a FSM module where all the general logic for implementing the FSM is defined. I define one type that represents the FSM. In the module I have also defined a set of helper functions that helps me create a FSM and also one <a href=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L12\">function</a> that handles an event given an event and a FSM as input. All the helper functions takes a FSM as the last argument, since that allows me to pipe that argument in making it possible to have a really nice DSL in the language. The helper functions takes the provided FSM and returns a new FSM based on the provided FSM and the extra input. <a href=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L40\"><code>registerTransition</code></a> takes mapping from one state to another given an event.</p>\n<p>I defined all the valid events, commands and states on these lines: <a href=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L54-L72\">https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L54-L72</a>. I really don't think they need any explanation.</p>\n<p>The creation of the FSM is done here: <a href=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L75-L88\">https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L75-L88</a>. As you can see I'm using the helper functions which defines a really nice DSL for me.</p>\n<p>When I have the FSM defined I can actually try that it works and that is done here: <a href=\"https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L90-L105\">https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L90-L105</a>. First I define a helper infix operator that print the current state before calling the next function in each step.</p>\n<p>As I hope you can see in this example there are many advantages compared to the java version:</p>\n<ul>\n<li>No nulls</li>\n<li>Impossible to represent bad states</li>\n<li>The ration of noise vs. relevant code is significantly lower</li>\n<li>Shorter code, so it is easier to get the full picture</li>\n<li>No need for an external dsl</li>\n</ul>\n<h2 id=\"wrapup\">Wrap up</h2>\n<p>F# (and most likely Scala) is a language you can, and should use, to solve your everyday problem. It will give you more concise code, easier to read code, and easier to maintain code. This will also have the positive side effect of less bugs. I understand that it might be a little bit weird at start since it is a whole new paradigm, but the reward is high on the other side. Learning to program functionally will help you write better programs in any language and you will also be ahead of the game when the functional features comes to java and C#. Note that even though those languages get some functional features they will never be as functional as F# and Scala since the base design of the languages are different. Do your self a favor and learn you some FP :)</p>\n<!--kg-card-end: markdown-->","comment_id":"65","plaintext":"The last couple of days I had two experiences that triggered this post. The\nfirst one was a question at work regarding how to model a finite state machine\n(FSM) in java or a language similar to java. The second one was a tweet\n[https://twitter.com/isaac_abraham/status/698178752560418816] by Isaac Abraham\n[https://twitter.com/isaac_abraham]:\n\n> It's really disappointing in 2016 to read this \"F# tends to be used in domains\nthat have a lot of scientific or financial computations.\"\n\n— Isaac Abraham (@isaac_abraham) February 12, 2016\n[https://twitter.com/isaac_abraham/status/698178752560418816]\nThe quote was from an interview\n[http://ericlippert.com/2016/01/14/functional-style-follow-up/] with Eric\nLippert, a former member of the C# team.\n\nSo how are these two events related? They are related because the greater\ncommunity still has a misconception of functional languages, and to find better\nsolutions to problems we need to look further than C# and java. I agree with\nIsaac 100 % that it is disappointing, and sad, to read and hear this\nmisconception of F# and functional programming. At the same time I do see a\nchange happening for everyone, with a more functional approach in the front-end\nwith libraries like react and more functional aspects in languages like java and\nC#. What I don't understand is why doesn't people try harder to stay ahead of\nthe game and learn functional programming, even though they do like it when it\ngets accepted by the greater community. Almost every C# developer I know is now\ncomfortable, and like, lambda expressions. More and more developers discover\nthat immutability isn't that bad after all, but still don't use it as much since\nit is so easy to use mutability in the languages we use the most. Everyone is\nnow comfortable with the var keyword in C#, but still want to write the whole\ntype on the other side of the equal sign.\n\nF#, and probably scala and other functional languages that are strongly type\nhandle all this in better ways than we are used to in java and C#. So my answer\nto the question was how I should define an FSM in F#. This is a problem that is\nnot scientific and it has nothing to do with finance. This is a real problem\nthat anyone of us could have in almost any type of application where you need to\nmodel a FSM, and model a FSM is probably something we should do more often.\nBefore we get to my answer to the question, let's look at one of the references\nin one of the other answers. One colleague linked to Martin Fowler's\n[https://twitter.com/martinfowler] book Domain-Specific Languages which you can\nread an example dealing with FSM here: \nhttp://www.informit.com/articles/article.aspx?p=1592379. Reading through the\nexample gave me a headache and almost made my eyes bleed. The java code in the\nexample is probably good java code, and I would probably think that all those\nfancy patterns where nice a couple of years back when the book was written. What\nis the problem with the code in the book? As I said, it is probably nice java\ncode, but that doesn't mean it is readable. I took me a while to understand how\nthe State Machine Model worked as described here: \nhttp://www.informit.com/articles/article.aspx?p=1592379&seqNum=2\n[http://www.informit.com/articles/article.aspx?p=1592379&seqNum=2]. Why was it\nso? The main reason it was hard for me to understand the sample code there was\nprobably all the noise and extra syntax. The noise and extra syntax distracted\nme from the actual model which can never be a good thing. Let's go to the actual\nFSM defined on page 3: \nhttp://www.informit.com/articles/article.aspx?p=1592379&seqNum=3\n[http://www.informit.com/articles/article.aspx?p=1592379&seqNum=3]. The code\nhere is definitely easier to understand since the level of noise has decreased\ndue to the previous model, but it is still much noise. In fact, Fowler agree\nsince he also provides multiple way to specify the model in other format than\njava code. There is one xml version, and two other versions. The only reason you\nneed this is due to the fact that java is to verbose and the level of noise is\ntoo high.\n\nF# to the rescue\nThe whole purpose of this post is to show you that it is possible to solve real\nworld problem with F# in more elegant ways than you would in languages like C#\nand java. I'll show you my code and then explain why I think it is better. I\ndon't say that Fowler's java code is bad, more that the language might not be\nright tool.\n\nLet me walk you through this piece of code. First I define a FSM module where\nall the general logic for implementing the FSM is defined. I define one type\nthat represents the FSM. In the module I have also defined a set of helper\nfunctions that helps me create a FSM and also one function\n[https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L12] \nthat handles an event given an event and a FSM as input. All the helper\nfunctions takes a FSM as the last argument, since that allows me to pipe that\nargument in making it possible to have a really nice DSL in the language. The\nhelper functions takes the provided FSM and returns a new FSM based on the\nprovided FSM and the extra input. registerTransition\n[https://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L40] \ntakes mapping from one state to another given an event.\n\nI defined all the valid events, commands and states on these lines: \nhttps://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L54-L72.\nI really don't think they need any explanation.\n\nThe creation of the FSM is done here: \nhttps://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L75-L88.\nAs you can see I'm using the helper functions which defines a really nice DSL\nfor me.\n\nWhen I have the FSM defined I can actually try that it works and that is done\nhere: \nhttps://gist.github.com/mastoj/9dfc21848c449fadcc93#file-fowler_fsm-fsx-L90-L105\n. First I define a helper infix operator that print the current state before\ncalling the next function in each step.\n\nAs I hope you can see in this example there are many advantages compared to the\njava version:\n\n * No nulls\n * Impossible to represent bad states\n * The ration of noise vs. relevant code is significantly lower\n * Shorter code, so it is easier to get the full picture\n * No need for an external dsl\n\nWrap up\nF# (and most likely Scala) is a language you can, and should use, to solve your\neveryday problem. It will give you more concise code, easier to read code, and\neasier to maintain code. This will also have the positive side effect of less\nbugs. I understand that it might be a little bit weird at start since it is a\nwhole new paradigm, but the reward is high on the other side. Learning to\nprogram functionally will help you write better programs in any language and you\nwill also be ahead of the game when the functional features comes to java and\nC#. Note that even though those languages get some functional features they will\nnever be as functional as F# and Scala since the base design of the languages\nare different. Do your self a favor and learn you some FP :)","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-02-12T22:23:53.000Z","updated_at":"2016-02-13T07:06:23.000Z","published_at":"2016-02-12T23:43:57.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe074","uuid":"9fdedd9f-b25e-44ac-b295-6300ade0b34c","title":"F# Suave app on dotnet core on Kubernetes on Google Cloud","slug":"fsharp-suave-app-on-dotnet-core-on-kubernetes-on-google-cloud","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I haven't been doing that much F# dotnet core development, but I think it was time for me to try it out. One of the scenarios that I think it will be used a lot is on [kubernetes](http://kubernetes.io/). I choose to run it on google cloud so I didn't have to set up the infrastructure myself. I could probably have used Azure as well since they now have support for it in preview, but I think that google's implementation looks more mature and easier to use from the command line. So let get this little tutorial started.\\n\\n## Creating the application\\n\\nBefore we need start we need to install the latest dotnet core bits, which we find here: https://www.microsoft.com/net/download/core. Clicking on `Current` tab we will find the latest bits (1.1.0 as of this moment). With dotnet installed we can get going.\\n\\n### Create project\\n\\nThe first is to create the project. Just navigate to a folder where you want your project and run this command to create a new F# project:\\n\\n```\\ndotnet new -l F#\\n```\\n\\nNote that the folder name will be the name of the project, in my case it is `suavecore` and that will also be the name of the dll file created.\\n\\n### Update references\\n\\nI made some minor changes to the [`project.json` file](https://github.com/mastoj/suavecore/blob/master/project.json):\\n\\n```\\n{\\n    \\\"version\\\": \\\"1.0.0-*\\\",\\n    \\\"buildOptions\\\": {\\n        \\\"debugType\\\": \\\"portable\\\",\\n        \\\"emitEntryPoint\\\": true,\\n        \\\"compilerName\\\": \\\"fsc\\\",\\n        \\\"compile\\\": {\\n            \\\"includeFiles\\\": [\\n                \\\"Program.fs\\\"\\n            ]\\n        }\\n    },\\n    \\\"dependencies\\\": {\\n        \\\"Microsoft.FSharp.Core.netcore\\\": \\\"1.0.0-alpha-*\\\",\\n        \\\"Suave\\\": \\\"2.0.0-rc2\\\"\\n    },\\n    \\\"tools\\\": {\\n        \\\"dotnet-compile-fsc\\\": \\\"1.0.0-preview2.1-*\\\"\\n    },\\n    \\\"frameworks\\\": {\\n        \\\"netcoreapp1.0\\\": {\\n            \\\"dependencies\\\": {\\n                \\\"Microsoft.NETCore.App\\\": {\\n                    \\\"type\\\": \\\"platform\\\",\\n                    \\\"version\\\": \\\"1.1.0\\\"\\n                },\\n                \\\"Microsoft.FSharp.Core.netcore\\\": \\\"1.0.0-alpha-161111\\\"\\n            }\\n        }\\n    }\\n}\\n```\\n\\nI basically changed to the latest version of all the packages and added a reference to [Suave](https://suave.io/). \\n\\nAfter the update we need to run\\n\\n```\\ndotnet restore\\n```\\n\\nto install the dependencies.\\n\\n### Implementing the application\\n\\nThe application implemented is really simple, it is a basic `Hello World` application that also prints the host name. It is a single file application and it all fits in [`Program.fs`](https://github.com/mastoj/suavecore/blob/master/Program.fs):\\n\\n```language-fsharp\\nopen Suave\\nopen System.Net\\n\\ntype CmdArgs = { IP: System.Net.IPAddress; Port: Sockets.Port }\\n\\n[<EntryPoint>]\\nlet main argv = \\n\\n    // parse arguments\\n    let args =\\n        let parse f str = match f str with (true, i) -> Some i | _ -> None\\n\\n        let (|Port|_|) = parse System.UInt16.TryParse\\n        let (|IPAddress|_|) = parse System.Net.IPAddress.TryParse\\n\\n        //default bind to 127.0.0.1:8083\\n        let defaultArgs = { IP = System.Net.IPAddress.Loopback; Port = 8083us }\\n\\n        let rec parseArgs b args =\\n            match args with\\n            | [] -> b\\n            | \\\"--ip\\\" :: IPAddress ip :: xs -> parseArgs { b with IP = ip } xs\\n            | \\\"--port\\\" :: Port p :: xs -> parseArgs { b with Port = p } xs\\n            | invalidArgs ->\\n                printfn \\\"error: invalid arguments %A\\\" invalidArgs\\n                printfn \\\"Usage:\\\"\\n                printfn \\\"    --ip ADDRESS   ip address (Default: %O)\\\" defaultArgs.IP\\n                printfn \\\"    --port PORT    port (Default: %i)\\\" defaultArgs.Port\\n                exit 1\\n\\n        argv |> List.ofArray |> parseArgs defaultArgs\\n\\n    let log x = printfn \\\"%A\\\" x; x\\n\\n    let getHostName() = \\n        Dns.GetHostName()\\n\\n    // start suave\\n    startWebServer\\n        { defaultConfig with\\n            bindings = [ HttpBinding.create HTTP args.IP args.Port ] }\\n        (Successful.OK (sprintf \\\"Hello world: %s\\\" (getHostName())))\\n\\n    0\\n```\\n\\nThat is all we need to try the application. If you run \\n\\n```\\ndotnet run\\n```\\n\\nyou will start the application and you can now pay a visit to http://localhost:8083.\\n\\n### Publishing the application\\n\\nThe last thing we need to do is to publish the application, this will create the bits that we will add to our docker container later on. Run\\n\\n```\\ndotnet publish -C Release\\n```\\n\\nto publish the application to `bin/Release/netcoreapp1.0/publish`. If you navigate to that folder it is now possible to run the publish commands by executing \\n\\n```\\ndotnet suavecore.dll\\n```\\n\\nThis will start the web server and you can now navigate to http://localhost:8083 again. Note that `suavecore` is the name of my project, if you have a different name of the project folder your name might differ.\\n\\n## Building the container\\n\\nTo be able to run this on kubernetes later on we will create a docker container. I have [docker beta for OSX](https://beta.docker.com/) installed to build and try out the container. If you are following along I assume you to have that installed.\\n\\n### Creating the Dockerfile\\n\\nThe [Dockerfile](https://github.com/mastoj/suavecore/blob/master/Dockerfile) is based on the official dotnet core image from microsoft and looks like this:\\n\\n```\\nFROM microsoft/dotnet:core\\nCOPY ./bin/Release/netcoreapp1.0/publish /app\\nWORKDIR /app\\nEXPOSE 8083\\nENTRYPOINT [\\\"dotnet\\\", \\\"suavecore.dll\\\"]\\n```\\n\\nIt is quite straightforward what is going on. We base our image on the one from Microsoft as mentioned, then we copy our published app to the `app` folder in the container. We expose port `8083` to be able to access it from the outside and lastly we set the entry point to the command to start the application.\\n\\n### Building the container\\n\\nBuilding a container is as simple as\\n\\n```\\ndocker build . -t mastoj/suavecore:v1.5\\n```\\n\\nThe container is tagged with the name of my repo for this image on [docker hub](https://hub.docker.com/r/mastoj/suavecore/) and a version number so we can access the correct version when publishing to kubernetes later on.\\n\\n### Testing the container\\n\\nBefore we publish the container it might be smart to try it out locally first. So to create a container of the newly created image we run\\n\\n```\\ndocker run -p=8083:8083 --name suave mastoj/suavecore:v1.5 --ip 0.0.0.0\\n```\\n\\nThe command above will start a running container of our image and name it `suave`. It will also map port `8083` on our local machine to port `8083` on the container. Lastly we will pass the arguments `--ip 0.0.0.0` to the application to tell it to listen all request no matter what the IP is.\\n\\nAgain you can try http://localhost:8083, but this time you should get a little bit different response since the host name of the container is probably not the same as your machine.\\n\\n### Publish the container\\n\\nWe are now ready to publish the container. For this tutorial we are using a public repo to keep things simple.\\n\\n```\\ndocker push mastoj/suavecore:v1.5\\n```\\n\\nThis will upload the image to docker hub and making it accessible to the public. \\n\\n## Setting up google cloud\\n\\nWe are now ready to proceed to the google cloud and kubernetes part. The goal is to host the application with three replicas running behind nginx using https. To be able to try it out on [google cloud](https://cloud.google.com/) you need to sign up and create a project. You also need to install the [sdk](https://cloud.google.com/sdk/).\\n\\n### Creating the kubernetes cluster\\n\\nIf you have the sdk installed it is really simple to set up a new basic cluster. To create a cluster named `k1` run the following\\n\\n```\\ngcloud container clusters create k1\\n```\\n\\nWhen you have the cluster up and running you need to install `kubectl`, which is the CLI tool to work with kubernetes.\\n\\n```\\ngcloud components install kubectl\\n```\\n\\nNow you should be all set to operate your google cloud container cluster, hopefully. \\n\\n### Secrets and configmaps\\n\\nFor nginx to run correctly we need to configure it to use `https` and where our application is in the cluster. \\n\\nThe first part is generating `cert.pem` and `key.pem` files for tls to work. If you have `openssl` installed you can run: \\n\\n```\\nopenssl req -newkey rsa:4096 -nodes -sha512 -x509 -days 3650 -nodes -out cert.pem -keyout key.pem\\n```\\n\\nThe result from this have I stored in a folder in the repo: https://github.com/mastoj/suavecore/tree/master/kubernetes/tls. You should probably never publish these files, I'm just doing it for demo purpose.\\n\\nWhen we have the files we can create a secret that we will be able to mount in our containers when they run in the cluster. To create a secret you use `kubectl`\\n\\n```\\nkubectl create secret generic tls-certs --from-file=tls/\\n``` \\n\\nWe will see later how we access the secrets.\\n\\nNext step is to add the nginx configuration. The configuration is a file, that we will mount when the container starts. The file is also in repo with the name [frontend.conf](https://github.com/mastoj/suavecore/blob/master/kubernetes/nginx/frontend.conf):\\n\\n```\\nupstream hello {\\n    server hello.default.svc.cluster.local;\\n}\\nserver {\\n    listen 443;\\n    ssl    on;\\n    ssl_certificate     /etc/tls/cert.pem;\\n    ssl_certificate_key /etc/tls/key.pem;\\n    location / {\\n        proxy_pass http://hello;\\n    }\\n}\\n```\\n\\nThis configuration file will configure nginx to listen to `443` with `ssl` enabled and route the traffic to `hello.default.svc.cluster.local`, which is where our hello nodes will be. You can also see where `nginx` now expect the secrets to be located.\\n\\nWith this done we can now continue on to creating the deployments and services.\\n\\n### Creating the deployments\\n\\nIf you want to know exactly what a deployment is you should read this: http://kubernetes.io/docs/user-guide/deployments/#what-is-a-deployment. I want go into details about all these concepts, just show you how to configure it. \\n\\n#### The frontend deployment\\n\\nThe [`deployments/frontend.yaml`](https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/frontend.yaml) defines the deployment for the frontend, which is the nginx part of our application.\\n\\n```\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: frontend\\n        track: stable\\n    spec:\\n      containers:\\n        - name: nginx\\n          image: \\\"nginx:1.9.14\\\"\\n          lifecycle:\\n            preStop:\\n              exec:\\n                command: [\\\"/usr/sbin/nginx\\\",\\\"-s\\\",\\\"quit\\\"]\\n          volumeMounts:\\n            - name: \\\"nginx-frontend-conf\\\"\\n              mountPath: \\\"/etc/nginx/conf.d\\\"\\n            - name: \\\"tls-certs\\\"\\n              mountPath: \\\"/etc/tls\\\"\\n      volumes:\\n        - name: \\\"tls-certs\\\"\\n          secret:\\n            secretName: \\\"tls-certs\\\"\\n        - name: \\\"nginx-frontend-conf\\\"\\n          configMap:\\n            name: \\\"nginx-frontend-conf\\\"\\n            items:\\n              - key: \\\"frontend.conf\\\"\\n                path: \\\"frontend.conf\\\"\\n```\\n\\nIn the file we first define that it is a `deployment` and some metadata. The interesting part is the `containers` part where we define that we will use `nginx` and also add a correct shutdown command when the container is stopped. In the `volumes` section we define that we want access to our `secret` named `tls-certs`, and the `configMap` named `nginx-frontend-conf`. For the `configMap` we are only interested in the key `frontend.conf` and we are going to name that file the same as the key. When we have defined the `volumens` we can reference them in the `volumeMounts` section of the file and define where they should go. That is it for the frontend deployment.\\n\\nTo create our deployment in the cluster run\\n\\n```\\nkubectl create -f deployments/frontend.yaml\\n```\\n\\n#### Application deployment\\n\\nThe application deployment is defined in [`deployments/hello.yaml`](https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/hello.yaml)\\n\\n```\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: hello\\nspec:\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: hello\\n        track: stable\\n    spec:\\n      containers:\\n        - name: hello\\n          image: \\\"mastoj/suavecore:v1.5\\\"\\n          args: [\\\"--ip\\\", \\\"0.0.0.0\\\"]\\n          ports:\\n            - name: http\\n              containerPort: 8083\\n```\\n\\nThis is a little bit simple deployment. What interesting here is the number of replicas, `3`, and that we are referencing the docker image we have created earlier. To create the deployment of the application we need to run\\n\\n```\\nkubectl create -f deployments/hello.yaml\\n```\\n\\r\\n### Creating services\\n\\nNext up is creating services, which allow us to balance the load requests between our nodes and expose the application to the public.\\n\\n#### The frontend service\\n\\nThe [`services/frontend.yaml`](https://github.com/mastoj/suavecore/blob/master/kubernetes/services/frontend.yaml) is what defines the service for the frontend. \\n\\n```\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: \\\"frontend\\\"\\nspec:\\n  selector:\\n    app: \\\"frontend\\\"\\n  ports:\\n    - protocol: \\\"TCP\\\"\\n      port: 443\\n      targetPort: 443\\n  type: LoadBalancer\\n```\\n\\nWe create the service with\\n\\n```\\nkubectl create -f services/frontend.yaml\\n```\\n\\nWhen the service is created you can run the command\\n\\n```\\nkubectl get services\\n```\\n\\nto check the status. There you will see the public ip when it is available.\\n\\n#### The hello application service\\n\\nThe definition in [`services/hello.yaml`](https://github.com/mastoj/suavecore/blob/master/kubernetes/services/hello.yaml) is similar to the `frontend` definition\\n\\n```\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: \\\"hello\\\"\\nspec:\\n  selector:\\n    app: \\\"hello\\\"\\n  ports:\\n    - protocol: \\\"TCP\\\"\\n      port: 80\\n      targetPort: 8083\\n```\\n\\nThe difference here is that we don't have the `LoadBalancer` part, which means our app will NOT be accessible from the outside, you have to go through our frontend. We also routes the traffic from the service port `80` to the container port `8083` which our containers use. Creating the service is as easy as for the frontend\\n\\n```\\nkubectl create -f services/hello.yaml\\n```\\n\\n## Win\\n\\nEverything should now be configured and up and running. You can find the public IP that you should be able to navigate to be executing\\n\\n```\\nkubectl get services\\n```\\n\\nRemember that it is `https://<your public ip>`, and the cert we are using is self signed so you will probably get a warning about that as well.\\n\\nThe source code for everything is available here: https://github.com/mastoj/suavecore/\\n\\nIf you have any comments or questions, feel free to post them at the comment section.\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>I haven't been doing that much F# dotnet core development, but I think it was time for me to try it out. One of the scenarios that I think it will be used a lot is on <a href=\"http://kubernetes.io/\">kubernetes</a>. I choose to run it on google cloud so I didn't have to set up the infrastructure myself. I could probably have used Azure as well since they now have support for it in preview, but I think that google's implementation looks more mature and easier to use from the command line. So let get this little tutorial started.</p>\n<h2 id=\"creatingtheapplication\">Creating the application</h2>\n<p>Before we need start we need to install the latest dotnet core bits, which we find here: <a href=\"https://www.microsoft.com/net/download/core\">https://www.microsoft.com/net/download/core</a>. Clicking on <code>Current</code> tab we will find the latest bits (1.1.0 as of this moment). With dotnet installed we can get going.</p>\n<h3 id=\"createproject\">Create project</h3>\n<p>The first is to create the project. Just navigate to a folder where you want your project and run this command to create a new F# project:</p>\n<pre><code>dotnet new -l F#\n</code></pre>\n<p>Note that the folder name will be the name of the project, in my case it is <code>suavecore</code> and that will also be the name of the dll file created.</p>\n<h3 id=\"updatereferences\">Update references</h3>\n<p>I made some minor changes to the <a href=\"https://github.com/mastoj/suavecore/blob/master/project.json\"><code>project.json</code> file</a>:</p>\n<pre><code>{\n    &quot;version&quot;: &quot;1.0.0-*&quot;,\n    &quot;buildOptions&quot;: {\n        &quot;debugType&quot;: &quot;portable&quot;,\n        &quot;emitEntryPoint&quot;: true,\n        &quot;compilerName&quot;: &quot;fsc&quot;,\n        &quot;compile&quot;: {\n            &quot;includeFiles&quot;: [\n                &quot;Program.fs&quot;\n            ]\n        }\n    },\n    &quot;dependencies&quot;: {\n        &quot;Microsoft.FSharp.Core.netcore&quot;: &quot;1.0.0-alpha-*&quot;,\n        &quot;Suave&quot;: &quot;2.0.0-rc2&quot;\n    },\n    &quot;tools&quot;: {\n        &quot;dotnet-compile-fsc&quot;: &quot;1.0.0-preview2.1-*&quot;\n    },\n    &quot;frameworks&quot;: {\n        &quot;netcoreapp1.0&quot;: {\n            &quot;dependencies&quot;: {\n                &quot;Microsoft.NETCore.App&quot;: {\n                    &quot;type&quot;: &quot;platform&quot;,\n                    &quot;version&quot;: &quot;1.1.0&quot;\n                },\n                &quot;Microsoft.FSharp.Core.netcore&quot;: &quot;1.0.0-alpha-161111&quot;\n            }\n        }\n    }\n}\n</code></pre>\n<p>I basically changed to the latest version of all the packages and added a reference to <a href=\"https://suave.io/\">Suave</a>.</p>\n<p>After the update we need to run</p>\n<pre><code>dotnet restore\n</code></pre>\n<p>to install the dependencies.</p>\n<h3 id=\"implementingtheapplication\">Implementing the application</h3>\n<p>The application implemented is really simple, it is a basic <code>Hello World</code> application that also prints the host name. It is a single file application and it all fits in <a href=\"https://github.com/mastoj/suavecore/blob/master/Program.fs\"><code>Program.fs</code></a>:</p>\n<pre><code class=\"language-language-fsharp\">open Suave\nopen System.Net\n\ntype CmdArgs = { IP: System.Net.IPAddress; Port: Sockets.Port }\n\n[&lt;EntryPoint&gt;]\nlet main argv = \n\n    // parse arguments\n    let args =\n        let parse f str = match f str with (true, i) -&gt; Some i | _ -&gt; None\n\n        let (|Port|_|) = parse System.UInt16.TryParse\n        let (|IPAddress|_|) = parse System.Net.IPAddress.TryParse\n\n        //default bind to 127.0.0.1:8083\n        let defaultArgs = { IP = System.Net.IPAddress.Loopback; Port = 8083us }\n\n        let rec parseArgs b args =\n            match args with\n            | [] -&gt; b\n            | &quot;--ip&quot; :: IPAddress ip :: xs -&gt; parseArgs { b with IP = ip } xs\n            | &quot;--port&quot; :: Port p :: xs -&gt; parseArgs { b with Port = p } xs\n            | invalidArgs -&gt;\n                printfn &quot;error: invalid arguments %A&quot; invalidArgs\n                printfn &quot;Usage:&quot;\n                printfn &quot;    --ip ADDRESS   ip address (Default: %O)&quot; defaultArgs.IP\n                printfn &quot;    --port PORT    port (Default: %i)&quot; defaultArgs.Port\n                exit 1\n\n        argv |&gt; List.ofArray |&gt; parseArgs defaultArgs\n\n    let log x = printfn &quot;%A&quot; x; x\n\n    let getHostName() = \n        Dns.GetHostName()\n\n    // start suave\n    startWebServer\n        { defaultConfig with\n            bindings = [ HttpBinding.create HTTP args.IP args.Port ] }\n        (Successful.OK (sprintf &quot;Hello world: %s&quot; (getHostName())))\n\n    0\n</code></pre>\n<p>That is all we need to try the application. If you run</p>\n<pre><code>dotnet run\n</code></pre>\n<p>you will start the application and you can now pay a visit to <a href=\"http://localhost:8083\">http://localhost:8083</a>.</p>\n<h3 id=\"publishingtheapplication\">Publishing the application</h3>\n<p>The last thing we need to do is to publish the application, this will create the bits that we will add to our docker container later on. Run</p>\n<pre><code>dotnet publish -C Release\n</code></pre>\n<p>to publish the application to <code>bin/Release/netcoreapp1.0/publish</code>. If you navigate to that folder it is now possible to run the publish commands by executing</p>\n<pre><code>dotnet suavecore.dll\n</code></pre>\n<p>This will start the web server and you can now navigate to <a href=\"http://localhost:8083\">http://localhost:8083</a> again. Note that <code>suavecore</code> is the name of my project, if you have a different name of the project folder your name might differ.</p>\n<h2 id=\"buildingthecontainer\">Building the container</h2>\n<p>To be able to run this on kubernetes later on we will create a docker container. I have <a href=\"https://beta.docker.com/\">docker beta for OSX</a> installed to build and try out the container. If you are following along I assume you to have that installed.</p>\n<h3 id=\"creatingthedockerfile\">Creating the Dockerfile</h3>\n<p>The <a href=\"https://github.com/mastoj/suavecore/blob/master/Dockerfile\">Dockerfile</a> is based on the official dotnet core image from microsoft and looks like this:</p>\n<pre><code>FROM microsoft/dotnet:core\nCOPY ./bin/Release/netcoreapp1.0/publish /app\nWORKDIR /app\nEXPOSE 8083\nENTRYPOINT [&quot;dotnet&quot;, &quot;suavecore.dll&quot;]\n</code></pre>\n<p>It is quite straightforward what is going on. We base our image on the one from Microsoft as mentioned, then we copy our published app to the <code>app</code> folder in the container. We expose port <code>8083</code> to be able to access it from the outside and lastly we set the entry point to the command to start the application.</p>\n<h3 id=\"buildingthecontainer\">Building the container</h3>\n<p>Building a container is as simple as</p>\n<pre><code>docker build . -t mastoj/suavecore:v1.5\n</code></pre>\n<p>The container is tagged with the name of my repo for this image on <a href=\"https://hub.docker.com/r/mastoj/suavecore/\">docker hub</a> and a version number so we can access the correct version when publishing to kubernetes later on.</p>\n<h3 id=\"testingthecontainer\">Testing the container</h3>\n<p>Before we publish the container it might be smart to try it out locally first. So to create a container of the newly created image we run</p>\n<pre><code>docker run -p=8083:8083 --name suave mastoj/suavecore:v1.5 --ip 0.0.0.0\n</code></pre>\n<p>The command above will start a running container of our image and name it <code>suave</code>. It will also map port <code>8083</code> on our local machine to port <code>8083</code> on the container. Lastly we will pass the arguments <code>--ip 0.0.0.0</code> to the application to tell it to listen all request no matter what the IP is.</p>\n<p>Again you can try <a href=\"http://localhost:8083\">http://localhost:8083</a>, but this time you should get a little bit different response since the host name of the container is probably not the same as your machine.</p>\n<h3 id=\"publishthecontainer\">Publish the container</h3>\n<p>We are now ready to publish the container. For this tutorial we are using a public repo to keep things simple.</p>\n<pre><code>docker push mastoj/suavecore:v1.5\n</code></pre>\n<p>This will upload the image to docker hub and making it accessible to the public.</p>\n<h2 id=\"settingupgooglecloud\">Setting up google cloud</h2>\n<p>We are now ready to proceed to the google cloud and kubernetes part. The goal is to host the application with three replicas running behind nginx using https. To be able to try it out on <a href=\"https://cloud.google.com/\">google cloud</a> you need to sign up and create a project. You also need to install the <a href=\"https://cloud.google.com/sdk/\">sdk</a>.</p>\n<h3 id=\"creatingthekubernetescluster\">Creating the kubernetes cluster</h3>\n<p>If you have the sdk installed it is really simple to set up a new basic cluster. To create a cluster named <code>k1</code> run the following</p>\n<pre><code>gcloud container clusters create k1\n</code></pre>\n<p>When you have the cluster up and running you need to install <code>kubectl</code>, which is the CLI tool to work with kubernetes.</p>\n<pre><code>gcloud components install kubectl\n</code></pre>\n<p>Now you should be all set to operate your google cloud container cluster, hopefully.</p>\n<h3 id=\"secretsandconfigmaps\">Secrets and configmaps</h3>\n<p>For nginx to run correctly we need to configure it to use <code>https</code> and where our application is in the cluster.</p>\n<p>The first part is generating <code>cert.pem</code> and <code>key.pem</code> files for tls to work. If you have <code>openssl</code> installed you can run:</p>\n<pre><code>openssl req -newkey rsa:4096 -nodes -sha512 -x509 -days 3650 -nodes -out cert.pem -keyout key.pem\n</code></pre>\n<p>The result from this have I stored in a folder in the repo: <a href=\"https://github.com/mastoj/suavecore/tree/master/kubernetes/tls\">https://github.com/mastoj/suavecore/tree/master/kubernetes/tls</a>. You should probably never publish these files, I'm just doing it for demo purpose.</p>\n<p>When we have the files we can create a secret that we will be able to mount in our containers when they run in the cluster. To create a secret you use <code>kubectl</code></p>\n<pre><code>kubectl create secret generic tls-certs --from-file=tls/\n</code></pre>\n<p>We will see later how we access the secrets.</p>\n<p>Next step is to add the nginx configuration. The configuration is a file, that we will mount when the container starts. The file is also in repo with the name <a href=\"https://github.com/mastoj/suavecore/blob/master/kubernetes/nginx/frontend.conf\">frontend.conf</a>:</p>\n<pre><code>upstream hello {\n    server hello.default.svc.cluster.local;\n}\nserver {\n    listen 443;\n    ssl    on;\n    ssl_certificate     /etc/tls/cert.pem;\n    ssl_certificate_key /etc/tls/key.pem;\n    location / {\n        proxy_pass http://hello;\n    }\n}\n</code></pre>\n<p>This configuration file will configure nginx to listen to <code>443</code> with <code>ssl</code> enabled and route the traffic to <code>hello.default.svc.cluster.local</code>, which is where our hello nodes will be. You can also see where <code>nginx</code> now expect the secrets to be located.</p>\n<p>With this done we can now continue on to creating the deployments and services.</p>\n<h3 id=\"creatingthedeployments\">Creating the deployments</h3>\n<p>If you want to know exactly what a deployment is you should read this: <a href=\"http://kubernetes.io/docs/user-guide/deployments/#what-is-a-deployment\">http://kubernetes.io/docs/user-guide/deployments/#what-is-a-deployment</a>. I want go into details about all these concepts, just show you how to configure it.</p>\n<h4 id=\"thefrontenddeployment\">The frontend deployment</h4>\n<p>The <a href=\"https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/frontend.yaml\"><code>deployments/frontend.yaml</code></a> defines the deployment for the frontend, which is the nginx part of our application.</p>\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n        track: stable\n    spec:\n      containers:\n        - name: nginx\n          image: &quot;nginx:1.9.14&quot;\n          lifecycle:\n            preStop:\n              exec:\n                command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]\n          volumeMounts:\n            - name: &quot;nginx-frontend-conf&quot;\n              mountPath: &quot;/etc/nginx/conf.d&quot;\n            - name: &quot;tls-certs&quot;\n              mountPath: &quot;/etc/tls&quot;\n      volumes:\n        - name: &quot;tls-certs&quot;\n          secret:\n            secretName: &quot;tls-certs&quot;\n        - name: &quot;nginx-frontend-conf&quot;\n          configMap:\n            name: &quot;nginx-frontend-conf&quot;\n            items:\n              - key: &quot;frontend.conf&quot;\n                path: &quot;frontend.conf&quot;\n</code></pre>\n<p>In the file we first define that it is a <code>deployment</code> and some metadata. The interesting part is the <code>containers</code> part where we define that we will use <code>nginx</code> and also add a correct shutdown command when the container is stopped. In the <code>volumes</code> section we define that we want access to our <code>secret</code> named <code>tls-certs</code>, and the <code>configMap</code> named <code>nginx-frontend-conf</code>. For the <code>configMap</code> we are only interested in the key <code>frontend.conf</code> and we are going to name that file the same as the key. When we have defined the <code>volumens</code> we can reference them in the <code>volumeMounts</code> section of the file and define where they should go. That is it for the frontend deployment.</p>\n<p>To create our deployment in the cluster run</p>\n<pre><code>kubectl create -f deployments/frontend.yaml\n</code></pre>\n<h4 id=\"applicationdeployment\">Application deployment</h4>\n<p>The application deployment is defined in <a href=\"https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/hello.yaml\"><code>deployments/hello.yaml</code></a></p>\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: hello\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: hello\n        track: stable\n    spec:\n      containers:\n        - name: hello\n          image: &quot;mastoj/suavecore:v1.5&quot;\n          args: [&quot;--ip&quot;, &quot;0.0.0.0&quot;]\n          ports:\n            - name: http\n              containerPort: 8083\n</code></pre>\n<p>This is a little bit simple deployment. What interesting here is the number of replicas, <code>3</code>, and that we are referencing the docker image we have created earlier. To create the deployment of the application we need to run</p>\n<pre><code>kubectl create -f deployments/hello.yaml\n</code></pre>\n<h3 id=\"creatingservices\">Creating services</h3>\n<p>Next up is creating services, which allow us to balance the load requests between our nodes and expose the application to the public.</p>\n<h4 id=\"thefrontendservice\">The frontend service</h4>\n<p>The <a href=\"https://github.com/mastoj/suavecore/blob/master/kubernetes/services/frontend.yaml\"><code>services/frontend.yaml</code></a> is what defines the service for the frontend.</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: &quot;frontend&quot;\nspec:\n  selector:\n    app: &quot;frontend&quot;\n  ports:\n    - protocol: &quot;TCP&quot;\n      port: 443\n      targetPort: 443\n  type: LoadBalancer\n</code></pre>\n<p>We create the service with</p>\n<pre><code>kubectl create -f services/frontend.yaml\n</code></pre>\n<p>When the service is created you can run the command</p>\n<pre><code>kubectl get services\n</code></pre>\n<p>to check the status. There you will see the public ip when it is available.</p>\n<h4 id=\"thehelloapplicationservice\">The hello application service</h4>\n<p>The definition in <a href=\"https://github.com/mastoj/suavecore/blob/master/kubernetes/services/hello.yaml\"><code>services/hello.yaml</code></a> is similar to the <code>frontend</code> definition</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: &quot;hello&quot;\nspec:\n  selector:\n    app: &quot;hello&quot;\n  ports:\n    - protocol: &quot;TCP&quot;\n      port: 80\n      targetPort: 8083\n</code></pre>\n<p>The difference here is that we don't have the <code>LoadBalancer</code> part, which means our app will NOT be accessible from the outside, you have to go through our frontend. We also routes the traffic from the service port <code>80</code> to the container port <code>8083</code> which our containers use. Creating the service is as easy as for the frontend</p>\n<pre><code>kubectl create -f services/hello.yaml\n</code></pre>\n<h2 id=\"win\">Win</h2>\n<p>Everything should now be configured and up and running. You can find the public IP that you should be able to navigate to be executing</p>\n<pre><code>kubectl get services\n</code></pre>\n<p>Remember that it is <code>https://&lt;your public ip&gt;</code>, and the cert we are using is self signed so you will probably get a warning about that as well.</p>\n<p>The source code for everything is available here: <a href=\"https://github.com/mastoj/suavecore/\">https://github.com/mastoj/suavecore/</a></p>\n<p>If you have any comments or questions, feel free to post them at the comment section.</p>\n<!--kg-card-end: markdown-->","comment_id":"66","plaintext":"I haven't been doing that much F# dotnet core development, but I think it was\ntime for me to try it out. One of the scenarios that I think it will be used a\nlot is on kubernetes [http://kubernetes.io/]. I choose to run it on google cloud\nso I didn't have to set up the infrastructure myself. I could probably have used\nAzure as well since they now have support for it in preview, but I think that\ngoogle's implementation looks more mature and easier to use from the command\nline. So let get this little tutorial started.\n\nCreating the application\nBefore we need start we need to install the latest dotnet core bits, which we\nfind here: https://www.microsoft.com/net/download/core. Clicking on Current tab\nwe will find the latest bits (1.1.0 as of this moment). With dotnet installed we\ncan get going.\n\nCreate project\nThe first is to create the project. Just navigate to a folder where you want\nyour project and run this command to create a new F# project:\n\ndotnet new -l F#\n\n\nNote that the folder name will be the name of the project, in my case it is \nsuavecore and that will also be the name of the dll file created.\n\nUpdate references\nI made some minor changes to the project.json file\n[https://github.com/mastoj/suavecore/blob/master/project.json]:\n\n{\n    \"version\": \"1.0.0-*\",\n    \"buildOptions\": {\n        \"debugType\": \"portable\",\n        \"emitEntryPoint\": true,\n        \"compilerName\": \"fsc\",\n        \"compile\": {\n            \"includeFiles\": [\n                \"Program.fs\"\n            ]\n        }\n    },\n    \"dependencies\": {\n        \"Microsoft.FSharp.Core.netcore\": \"1.0.0-alpha-*\",\n        \"Suave\": \"2.0.0-rc2\"\n    },\n    \"tools\": {\n        \"dotnet-compile-fsc\": \"1.0.0-preview2.1-*\"\n    },\n    \"frameworks\": {\n        \"netcoreapp1.0\": {\n            \"dependencies\": {\n                \"Microsoft.NETCore.App\": {\n                    \"type\": \"platform\",\n                    \"version\": \"1.1.0\"\n                },\n                \"Microsoft.FSharp.Core.netcore\": \"1.0.0-alpha-161111\"\n            }\n        }\n    }\n}\n\n\nI basically changed to the latest version of all the packages and added a\nreference to Suave [https://suave.io/].\n\nAfter the update we need to run\n\ndotnet restore\n\n\nto install the dependencies.\n\nImplementing the application\nThe application implemented is really simple, it is a basic Hello World \napplication that also prints the host name. It is a single file application and\nit all fits in Program.fs\n[https://github.com/mastoj/suavecore/blob/master/Program.fs]:\n\nopen Suave\nopen System.Net\n\ntype CmdArgs = { IP: System.Net.IPAddress; Port: Sockets.Port }\n\n[<EntryPoint>]\nlet main argv = \n\n    // parse arguments\n    let args =\n        let parse f str = match f str with (true, i) -> Some i | _ -> None\n\n        let (|Port|_|) = parse System.UInt16.TryParse\n        let (|IPAddress|_|) = parse System.Net.IPAddress.TryParse\n\n        //default bind to 127.0.0.1:8083\n        let defaultArgs = { IP = System.Net.IPAddress.Loopback; Port = 8083us }\n\n        let rec parseArgs b args =\n            match args with\n            | [] -> b\n            | \"--ip\" :: IPAddress ip :: xs -> parseArgs { b with IP = ip } xs\n            | \"--port\" :: Port p :: xs -> parseArgs { b with Port = p } xs\n            | invalidArgs ->\n                printfn \"error: invalid arguments %A\" invalidArgs\n                printfn \"Usage:\"\n                printfn \"    --ip ADDRESS   ip address (Default: %O)\" defaultArgs.IP\n                printfn \"    --port PORT    port (Default: %i)\" defaultArgs.Port\n                exit 1\n\n        argv |> List.ofArray |> parseArgs defaultArgs\n\n    let log x = printfn \"%A\" x; x\n\n    let getHostName() = \n        Dns.GetHostName()\n\n    // start suave\n    startWebServer\n        { defaultConfig with\n            bindings = [ HttpBinding.create HTTP args.IP args.Port ] }\n        (Successful.OK (sprintf \"Hello world: %s\" (getHostName())))\n\n    0\n\n\nThat is all we need to try the application. If you run\n\ndotnet run\n\n\nyou will start the application and you can now pay a visit to \nhttp://localhost:8083.\n\nPublishing the application\nThe last thing we need to do is to publish the application, this will create the\nbits that we will add to our docker container later on. Run\n\ndotnet publish -C Release\n\n\nto publish the application to bin/Release/netcoreapp1.0/publish. If you navigate\nto that folder it is now possible to run the publish commands by executing\n\ndotnet suavecore.dll\n\n\nThis will start the web server and you can now navigate to http://localhost:8083 \nagain. Note that suavecore is the name of my project, if you have a different\nname of the project folder your name might differ.\n\nBuilding the container\nTo be able to run this on kubernetes later on we will create a docker container.\nI have docker beta for OSX [https://beta.docker.com/] installed to build and try\nout the container. If you are following along I assume you to have that\ninstalled.\n\nCreating the Dockerfile\nThe Dockerfile [https://github.com/mastoj/suavecore/blob/master/Dockerfile] is\nbased on the official dotnet core image from microsoft and looks like this:\n\nFROM microsoft/dotnet:core\nCOPY ./bin/Release/netcoreapp1.0/publish /app\nWORKDIR /app\nEXPOSE 8083\nENTRYPOINT [\"dotnet\", \"suavecore.dll\"]\n\n\nIt is quite straightforward what is going on. We base our image on the one from\nMicrosoft as mentioned, then we copy our published app to the app folder in the\ncontainer. We expose port 8083 to be able to access it from the outside and\nlastly we set the entry point to the command to start the application.\n\nBuilding the container\nBuilding a container is as simple as\n\ndocker build . -t mastoj/suavecore:v1.5\n\n\nThe container is tagged with the name of my repo for this image on docker hub\n[https://hub.docker.com/r/mastoj/suavecore/] and a version number so we can\naccess the correct version when publishing to kubernetes later on.\n\nTesting the container\nBefore we publish the container it might be smart to try it out locally first.\nSo to create a container of the newly created image we run\n\ndocker run -p=8083:8083 --name suave mastoj/suavecore:v1.5 --ip 0.0.0.0\n\n\nThe command above will start a running container of our image and name it suave.\nIt will also map port 8083 on our local machine to port 8083 on the container.\nLastly we will pass the arguments --ip 0.0.0.0 to the application to tell it to\nlisten all request no matter what the IP is.\n\nAgain you can try http://localhost:8083, but this time you should get a little\nbit different response since the host name of the container is probably not the\nsame as your machine.\n\nPublish the container\nWe are now ready to publish the container. For this tutorial we are using a\npublic repo to keep things simple.\n\ndocker push mastoj/suavecore:v1.5\n\n\nThis will upload the image to docker hub and making it accessible to the public.\n\nSetting up google cloud\nWe are now ready to proceed to the google cloud and kubernetes part. The goal is\nto host the application with three replicas running behind nginx using https. To\nbe able to try it out on google cloud [https://cloud.google.com/] you need to\nsign up and create a project. You also need to install the sdk\n[https://cloud.google.com/sdk/].\n\nCreating the kubernetes cluster\nIf you have the sdk installed it is really simple to set up a new basic cluster.\nTo create a cluster named k1 run the following\n\ngcloud container clusters create k1\n\n\nWhen you have the cluster up and running you need to install kubectl, which is\nthe CLI tool to work with kubernetes.\n\ngcloud components install kubectl\n\n\nNow you should be all set to operate your google cloud container cluster,\nhopefully.\n\nSecrets and configmaps\nFor nginx to run correctly we need to configure it to use https and where our\napplication is in the cluster.\n\nThe first part is generating cert.pem and key.pem files for tls to work. If you\nhave openssl installed you can run:\n\nopenssl req -newkey rsa:4096 -nodes -sha512 -x509 -days 3650 -nodes -out cert.pem -keyout key.pem\n\n\nThe result from this have I stored in a folder in the repo: \nhttps://github.com/mastoj/suavecore/tree/master/kubernetes/tls. You should\nprobably never publish these files, I'm just doing it for demo purpose.\n\nWhen we have the files we can create a secret that we will be able to mount in\nour containers when they run in the cluster. To create a secret you use kubectl\n\nkubectl create secret generic tls-certs --from-file=tls/\n\n\nWe will see later how we access the secrets.\n\nNext step is to add the nginx configuration. The configuration is a file, that\nwe will mount when the container starts. The file is also in repo with the name \nfrontend.conf\n[https://github.com/mastoj/suavecore/blob/master/kubernetes/nginx/frontend.conf]\n:\n\nupstream hello {\n    server hello.default.svc.cluster.local;\n}\nserver {\n    listen 443;\n    ssl    on;\n    ssl_certificate     /etc/tls/cert.pem;\n    ssl_certificate_key /etc/tls/key.pem;\n    location / {\n        proxy_pass http://hello;\n    }\n}\n\n\nThis configuration file will configure nginx to listen to 443 with ssl enabled\nand route the traffic to hello.default.svc.cluster.local, which is where our\nhello nodes will be. You can also see where nginx now expect the secrets to be\nlocated.\n\nWith this done we can now continue on to creating the deployments and services.\n\nCreating the deployments\nIf you want to know exactly what a deployment is you should read this: \nhttp://kubernetes.io/docs/user-guide/deployments/#what-is-a-deployment. I want\ngo into details about all these concepts, just show you how to configure it.\n\nThe frontend deployment\nThe deployments/frontend.yaml\n[https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/frontend.yaml] \ndefines the deployment for the frontend, which is the nginx part of our\napplication.\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n        track: stable\n    spec:\n      containers:\n        - name: nginx\n          image: \"nginx:1.9.14\"\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n          volumeMounts:\n            - name: \"nginx-frontend-conf\"\n              mountPath: \"/etc/nginx/conf.d\"\n            - name: \"tls-certs\"\n              mountPath: \"/etc/tls\"\n      volumes:\n        - name: \"tls-certs\"\n          secret:\n            secretName: \"tls-certs\"\n        - name: \"nginx-frontend-conf\"\n          configMap:\n            name: \"nginx-frontend-conf\"\n            items:\n              - key: \"frontend.conf\"\n                path: \"frontend.conf\"\n\n\nIn the file we first define that it is a deployment and some metadata. The\ninteresting part is the containers part where we define that we will use nginx \nand also add a correct shutdown command when the container is stopped. In the \nvolumes section we define that we want access to our secret named tls-certs, and\nthe configMap named nginx-frontend-conf. For the configMap we are only\ninterested in the key frontend.conf and we are going to name that file the same\nas the key. When we have defined the volumens we can reference them in the \nvolumeMounts section of the file and define where they should go. That is it for\nthe frontend deployment.\n\nTo create our deployment in the cluster run\n\nkubectl create -f deployments/frontend.yaml\n\n\nApplication deployment\nThe application deployment is defined in deployments/hello.yaml\n[https://github.com/mastoj/suavecore/blob/master/kubernetes/deployments/hello.yaml]\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: hello\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: hello\n        track: stable\n    spec:\n      containers:\n        - name: hello\n          image: \"mastoj/suavecore:v1.5\"\n          args: [\"--ip\", \"0.0.0.0\"]\n          ports:\n            - name: http\n              containerPort: 8083\n\n\nThis is a little bit simple deployment. What interesting here is the number of\nreplicas, 3, and that we are referencing the docker image we have created\nearlier. To create the deployment of the application we need to run\n\nkubectl create -f deployments/hello.yaml\n\n\nCreating services\nNext up is creating services, which allow us to balance the load requests\nbetween our nodes and expose the application to the public.\n\nThe frontend service\nThe services/frontend.yaml\n[https://github.com/mastoj/suavecore/blob/master/kubernetes/services/frontend.yaml] \nis what defines the service for the frontend.\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: \"frontend\"\nspec:\n  selector:\n    app: \"frontend\"\n  ports:\n    - protocol: \"TCP\"\n      port: 443\n      targetPort: 443\n  type: LoadBalancer\n\n\nWe create the service with\n\nkubectl create -f services/frontend.yaml\n\n\nWhen the service is created you can run the command\n\nkubectl get services\n\n\nto check the status. There you will see the public ip when it is available.\n\nThe hello application service\nThe definition in services/hello.yaml\n[https://github.com/mastoj/suavecore/blob/master/kubernetes/services/hello.yaml] \nis similar to the frontend definition\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: \"hello\"\nspec:\n  selector:\n    app: \"hello\"\n  ports:\n    - protocol: \"TCP\"\n      port: 80\n      targetPort: 8083\n\n\nThe difference here is that we don't have the LoadBalancer part, which means our\napp will NOT be accessible from the outside, you have to go through our\nfrontend. We also routes the traffic from the service port 80 to the container\nport 8083 which our containers use. Creating the service is as easy as for the\nfrontend\n\nkubectl create -f services/hello.yaml\n\n\nWin\nEverything should now be configured and up and running. You can find the public\nIP that you should be able to navigate to be executing\n\nkubectl get services\n\n\nRemember that it is https://<your public ip>, and the cert we are using is self\nsigned so you will probably get a warning about that as well.\n\nThe source code for everything is available here: \nhttps://github.com/mastoj/suavecore/\n\nIf you have any comments or questions, feel free to post them at the comment\nsection.","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-11-16T21:39:21.000Z","updated_at":"2016-11-23T22:40:48.000Z","published_at":"2016-11-17T11:25:50.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe075","uuid":"49adb05b-9ab4-411b-938b-a35e506dc3eb","title":"Setting up Event Store with kubernetes on google cloud","slug":"setting-up-event-store-with-kubernetes-on-google-cloud","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"To prepare myself for my new job, which will involve some kubernetes stuff, I've played around with it somewhat lately, as you could see in this [post](http://blog.2mas.xyz/fsharp-suave-app-on-dotnet-core-on-kubernetes-on-google-cloud/). This post is taking things one step further without making it that much more advanced. The goal for this post is to set up a [Event Store](http://geteventstore.com/) cluster on google container engine with a simple script. A prerequisite to get any of this working is that you have installed [`gcloud`](https://cloud.google.com/sdk/) and [`kubectl`](https://cloud.google.com/container-engine/docs/quickstart).\\n\\nIf you don't want to read the whole post and just go to the code you can look it up on [github](https://github.com/mastoj/eventstore-kubernetes). The [`naïve`](https://github.com/mastoj/eventstore-kubernetes/tree/naive) and [`master`](https://github.com/mastoj/eventstore-kubernetes) branches will be described in this post.\\n\\n## Disclaimer\\n\\nWhat I will describe here will expose the Event Store cluster to the public and is something you **should not** do, I do it to make it easier to test that it works. I haven't done any performance tests or reliability tests on my setup either, which you should probably do before using it in production. \\n\\n## The end goal\\n\\nI did two different approaches, which both will be covered in this post, that had the same end goal. I wanted to have a cluster of Event Store nodes running behind a [headless kubernetes service](http://kubernetes.io/docs/user-guide/services/#headless-services) and nginx on top of that to add access to the public. Using a headless kubernetes service will result in a service registered with a dns registration that resolves to all the IPs for the associated containers, and this is exactly what EventStore needs to to discovery through dns.\\n\\n## Configuring nginx\\n\\nI put this section first since it is the same for both approaches. \\n\\n### Nginx configuraion\\n\\nThe configuration of nginx will be stored in a `configmap` and looks like this, https://github.com/mastoj/eventstore-kubernetes/blob/master/nginx/frontend.conf:\\n\\n```\\nupstream es {\\n    server es.default.svc.cluster.local:2113;\\n}\\nserver {\\n    listen 2113;\\n    location / {\\n        proxy_set_header    X-Real-IP $remote_addr;\\n        proxy_set_header    Host      $http_host;\\n        proxy_pass          http://es;\\n    }\\n}\\n```\\n\\nIf you know nginx, this is just basic configuration. First we create an `upstream` that can be references later on by our `proxy_pass` when someone is visiting the path `/`. The url `es.default.svc.cluster.local` is the dns registration that our Event Store node will get when we get to that point. The `server` section just defines that we should listen on port `80` and proxy the traffic to the `upstream ` defined.\\n\\nTo create the `configmap` we can execute this command\\n\\n```\\nkubectl create configmap nginx-es-frontend-conf --from-file=nginx/frontend.conf\\n```\\n\\n### The nginx deployment\\n\\nThis is basically the same as I used in the previous post, if you read that one. The [specification](https://github.com/mastoj/eventstore-kubernetes/blob/master/deployments/frontend-es.yaml) looks like this:\\n\\n```\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend-es\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: frontend-es\\n        track: stable\\n    spec:\\n      containers:\\n        - name: nginx\\n          image: \\\"nginx:1.9.14\\\"\\n          lifecycle:\\n            preStop:\\n              exec:\\n                command: [\\\"/usr/sbin/nginx\\\",\\\"-s\\\",\\\"quit\\\"]\\n          volumeMounts:\\n            - name: \\\"nginx-es-frontend-conf\\\"\\n              mountPath: \\\"/etc/nginx/conf.d\\\"\\n      volumes:\\n        - name: \\\"nginx-es-frontend-conf\\\"\\n          configMap:\\n            name: \\\"nginx-es-frontend-conf\\\"\\n            items:\\n              - key: \\\"frontend.conf\\\"\\n                path: \\\"frontend.conf\\\"\\n```\\n\\nIt only contains one container, the `nginx` one, which uses the `configmap` created above for configuration. We only need one replica at the moment.\\n\\nTo create it we run the following command\\n\\n```\\nkubectl create -f deployments/frontend-es.yaml\\n```\\n\\n### The nginx service\\n\\nTo create a public IP we need to create a service on top of this deployment. The [specification](https://github.com/mastoj/eventstore-kubernetes/blob/master/services/frontend-es.yaml) for the service looks like this:\\n\\n```\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: \\\"frontend-es\\\"\\nspec:\\n  selector:\\n    app: \\\"frontend-es\\\"\\n  ports:\\n    - protocol: \\\"TCP\\\"\\n      port: 2113\\n      targetPort: 2113\\n  type: LoadBalancer\\n```\\n\\nTo create the service and finish up the nginx part of the post we run the following command\\n\\n```\\nkubectl create -f services/frontend-es.yaml\\n```\\n\\n## First approach - the naïve one\\n\\nThe first thing I wanted to do was to get a cluster up and running without persisting the data to disk, that is, only keep the data in the container. A cluster like that might work for development, but not in production. My grand masterplan was to just add persistent to that cluster after the cluster was up and running, which did not work. How to get persistent will be covered under [*Second approach*](#secondapproachaddingpersistentdata). Before we get into the persistent part, let's get this cluster up and running.\\n\\n### Creating the deployment\\n\\nThe [deployment file](https://github.com/mastoj/eventstore-kubernetes/blob/naive/deployments/eventstore.yaml) is really simple:\\n\\n```\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: es\\nspec:\\n  replicas: 3\\n  template:\\n    metadata:\\n      labels:\\n        app: es\\n    spec:\\n      containers:\\n        - name: es\\n          image: \\\"eventstore/eventstore\\\"\\n          env: \\n            - name: EVENTSTORE_INT_IP\\n              valueFrom:\\n                fieldRef:\\n                  fieldPath: status.podIP\\n            - name: EVENTSTORE_EXT_IP\\n              valueFrom:\\n                fieldRef:\\n                  fieldPath: status.podIP\\n            - name: EVENTSTORE_INT_TCP_PORT\\n              value: \\\"1111\\\"\\n            - name: EVENTSTORE_EXT_TCP_PORT\\n              value: \\\"1112\\\"\\n            - name: EVENTSTORE_INT_HTTP_PORT\\n              value: \\\"2114\\\"\\n            - name: EVENTSTORE_EXT_HTTP_PORT\\n              value: \\\"2113\\\"\\n            - name: EVENTSTORE_CLUSTER_SIZE\\n              value: \\\"3\\\"\\n            - name: EVENTSTORE_CLUSTER_DNS\\n              value: \\\"es.default.svc.cluster.local\\\"\\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\\n              value: \\\"2114\\\"\\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\\n              value: \\\"600000\\\"\\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\\n              value: \\\"http://*:2114/\\\"\\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\\n              value: \\\"http://*:2113/\\\"\\n          ports:\\n            - containerPort: 2113\\n            - containerPort: 2114\\n            - containerPort: 1111\\n            - containerPort: 1112\\n```\\n\\nWe use the Event Store docker image from [docker hub](https://hub.docker.com/r/eventstore/eventstore/). This image doesn't allow command line arguments, so we need to use environment variables to configure it. You can read about Event Store configuration [here](http://docs.geteventstore.com/server/3.9.0/command-line-arguments/). Every container (we are running three here) need to use their own IP during configuration, and with kubernetes we can access that data through `status.podIP` when we configure the environment variables.\\n\\nCreating is as simple as before:\\n\\n```\\nkubectl create -f deployments/eventstore.yaml\\n```\\n\\nThat should create a three nodes, but as of this moment they will fail to find each other, and that is why we need the service.\\n\\n### Creating the service\\n\\nCreating the service is even easier than the deployment. The [specification file](https://github.com/mastoj/eventstore-kubernetes/blob/naive/services/eventstore.yaml) looks like this:\\n\\n```\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: \\\"es\\\"\\nspec:\\n  selector:\\n    app: \\\"es\\\"\\n  ports:\\n    - protocol: \\\"TCP\\\"\\n      port: 2113\\n      targetPort: 2113\\n  clusterIP: None\\n```\\n\\nWe only expose the 2113 port, which means that we will only be able to talk over http to the event store cluster. To the service we name all the containers that has the label `app: \\\"es\\\"`. The last thing to know is that we set the `clusterIP` to `None`, this will not create one single IP in the DNS for this service, instead it will resolve to all the IPs of the containers and that is exactly what Event Store needs to be able to configure itself.\\n\\nAgain we are using `kubectl` to create the service:\\n\\n```\\nkubectl create -f services/eventstore.yaml\\n```\\n\\nWhen this is created we should now be ready to test it.\\n\\n### Test\\n\\nTo test that it works run the following command: \\n\\n```\\nkubectl get services\\n```\\n\\nIn the result from that command you will find an `External IP`. To access Event Store, open the browser and go to `<external ip>:2113`. If everything works as expected you should now have access to Event Store.\\n\\n### Challenges with approach one\\n\\nThe major challenge is persistent data, how do you map one persistent volume to each node in the cluster? Leave a comment on the post if you have any idea. \\n\\nLet us say that you solve the first problem, how do you make sure the nodes get the same persistent volume the next time you restart the cluster?\\n\\nBoth those two problems is what got me started working on the second approach.\\n\\nThere is a third problem, which we won't fix, and that is increasing the number of replicas won't work. The reason be that Event Store doesn't support elastic scaling, so increasing the number of replicas will only add \\\"clones\\\" to the cluster, not increase its size.\\n\\n## Second approach - adding persistent data\\n\\nWe are still going to use deployments, but instead of letting the number of replicas define the number of nodes we will create one deployment with one replica per node. This way we can force each node to get access to the same persistent data volume when it is restarted, and using deployments will also handle restart for us.\\n\\n### Generating deployment\\n\\nThe difference from the first approach here is that we will generate the deployment from a template instead of creating one. For this to work we need both a template and a simple script that generates the deployments. The [template file](https://github.com/mastoj/eventstore-kubernetes/blob/master/templates/es_deployment_template.yaml) looks like this: \\n\\n```\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: es-${nodenumber}\\nspec:\\n  replicas: 1\\n  template:\\n    metadata:\\n      labels:\\n        app: es-${nodenumber}\\n        escluster: es\\n    spec:\\n      containers:\\n        - name: es-${nodenumber}\\n          image: \\\"eventstore/eventstore\\\"\\n          env: \\n            - name: EVENTSTORE_INT_IP\\n              valueFrom:\\n                fieldRef:\\n                  fieldPath: status.podIP\\n            - name: EVENTSTORE_EXT_IP\\n              valueFrom:\\n                fieldRef:\\n                  fieldPath: status.podIP\\n            - name: EVENTSTORE_INT_TCP_PORT\\n              value: \\\"1111\\\"\\n            - name: EVENTSTORE_EXT_TCP_PORT\\n              value: \\\"1112\\\"\\n            - name: EVENTSTORE_INT_HTTP_PORT\\n              value: \\\"2114\\\"\\n            - name: EVENTSTORE_EXT_HTTP_PORT\\n              value: \\\"2113\\\"\\n            - name: EVENTSTORE_CLUSTER_SIZE\\n              value: \\\"${nodecount}\\\"\\n            - name: EVENTSTORE_CLUSTER_DNS\\n              value: \\\"es.default.svc.cluster.local\\\"\\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\\n              value: \\\"2114\\\"\\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\\n              value: \\\"600000\\\"\\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\\n              value: \\\"http://*:2114/\\\"\\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\\n              value: \\\"http://*:2113/\\\"\\n            - name: EVENTSTORE_DB\\n              value: \\\"/usr/data/eventstore/data\\\"\\n            - name: EVENTSTORE_LOG\\n              value: \\\"/usr/data/eventstore/log\\\"\\n          ports:\\n            - containerPort: 2113\\n            - containerPort: 2114\\n            - containerPort: 1111\\n            - containerPort: 1112\\n          volumeMounts:\\n            - mountPath: \\\"/usr/data/eventstore\\\"\\n              name: espd\\n      volumes:\\n        - name: espd\\n          gcePersistentDisk:\\n            pdName: esdisk-${nodenumber}\\n            fsType: ext4\\n```\\n\\nThe template looks almost the same as in the first case, but we have now added two; `nodenumber` and `nodecount`. We have also added a `gcePersistentDisk` which we mount to `usr/data/eventstore`, and that is the parent folder we use for logs and data in the configuration of Event Store, `EVENTSTORE_DB` and `EVENTSTORE_LOGS`. We have also added a new label `escluster` which will be used for the service to identify which nodes that should be included in the service. \\n\\nTo generate the actual deploy files we run a bash script that has the following content: \\n\\n```\\n    for ((c=1; c<=$count; c++ ))\\n    do\\n        cat ../templates/es_deployment_template.yaml | sed -e \\\"s/\\\\${nodenumber}/$c/\\\" | sed -e \\\"s/\\\\${nodecount}/$count/\\\" > .tmp/es_deployment_$c.yaml\\n    done\\n```\\n\\nRunning that will generate one file for each node, and each file has its own volume mounted. The script as a whole can be found [here](https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh).\\n\\nWhen the files has been generated we can then run:\\n\\n```\\n    for ((c=1; c<=$count; c++ ))\\n    do\\n        kubectl apply -f .tmp/es_deployment_$c.yaml\\n    done\\n```\\n\\nThis code will create one deployment per file. Since we are using deployments our pods will be restarted if they crashes.\\n\\n### Creating the service\\n\\nThis is exactly the same as in the first approach, but with a minor change. We will use the `escluster` label to identify the pods to add to the service. The file is [here](https://github.com/mastoj/eventstore-kubernetes/blob/master/services/eventstore.yaml)\\n\\n### The create cluster script\\n\\nIt is not reasonable to execute any of this by hand, so I created this [script](https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh). I will dissect it here:\\n\\n```\\n#!/bin/bash\\n\\nfunction init {\\n    rm -rf .tmp\\n    mkdir -p .tmp\\n}\\n\\nfunction validateInput {\\n    count=$1\\n    re='^[0-9]+$'\\n    if ! [[ $count =~ $re ]] ; then\\n        echo \\\"error: Not a number\\\" >&2; exit 1\\n    fi\\n}\\n```\\n\\nThe first part is just some house keeping and validating input arguments. The plan is that you should be able to create a cluster of any size by running: `./create_cluster.sh <size>`. \\n\\n``` \\nfunction createSpecs {\\n    local count=$1\\n    for ((c=1; c<=$count; c++ ))\\n    do\\n        cat ../templates/es_deployment_template.yaml | sed -e \\\"s/\\\\${nodenumber}/$c/\\\" | sed -e \\\"s/\\\\${nodecount}/$count/\\\" > .tmp/es_deployment_$c.yaml\\n    done\\n}\\n\\nfunction createDeployments {\\n    local count=$1\\n    for ((c=1; c<=$count; c++ ))\\n    do\\n        kubectl apply -f .tmp/es_deployment_$c.yaml\\n    done\\n}\\n```\\n\\nThe next part defines functions to generate the deployment files and how they should be executed.\\n\\n```\\nfunction createEsService {\\n    kubectl create -f ../services/eventstore.yaml\\n}\\n```\\n\\nThis finish of the Event Store part of the script by creating a service on top of the nodes created by deployment.\\n\\n```\\nfunction addNginxConfig {\\n    kubectl create configmap nginx-es-frontend-conf --from-file=../nginx/frontend.conf\\n}\\n\\nfunction createFrontendDeployment {\\n    kubectl create -f ../deployments/frontend-es.yaml\\n}\\n\\nfunction createFrontendService {\\n    kubectl create -f ../services/frontend-es.yaml\\n}\\n```\\n\\nThe next part is basically what we described in the section about `nginx` setup.\\n\\n```\\nfunction createDisks {\\n    local count=$1\\n    for ((c=1; c<=$count; c++ ))\\n    do\\n        if gcloud compute disks list esdisk-$c | grep esdisk-$c; then\\n            echo \\\"creating disk: esdisk-$c\\\" \\n            gcloud compute disks create --size=10GB esdisk-$c\\n        else\\n            echo \\\"disk already exists: esdisk-$c\\\"\\n        fi\\n    done\\n}\\n```\\n\\nA simple helper function to create disks on google cloud.\\n\\n```\\nfunction createEsCluster {\\n    local count=$1\\n    createSpecs $count\\n    createDeployments $count\\n    createEsService\\n}\\n\\nfunction createFrontEnd {\\n    addNginxConfig\\n    createFrontendDeployment\\n    createFrontendService\\n}\\n```\\n\\nThe last two functions is just to make it easier to read what is going on.\\n\\n```\\ninit\\nvalidateInput $1 #sets the variable $count\\ncreateDisks $count\\ncreateEsCluster $count\\ncreateFrontEnd\\n```\\n\\nWith all the functions defined it is quite clear what is going on in this script.\\n\\n### Test\\n\\nThe easier way to test it is to create the cluster:\\n\\n```\\n./create_cluster.sh 5\\n```\\n\\nAdd some data to the cluster. You do that by finding the external IP of the nginx service and then follow the [getting started instructions](http://docs.geteventstore.com/introduction/3.9.0/) for Event Store to add some data.\\n\\nWith that in place you can kill pods as you like to simulate failures with:\\n\\n```\\nkubectl delete pod --now <pod id>\\n```\\n\\nWhen you kill a pod a new should be created, but this time it should use the same persistent disk as the old one.\\n\\nYou can even delete the whole cluster and rebuild it again. I have added [delete cluster script](https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/delete_cluster.sh) that will delete everything but the disks. If you then create the cluster again the data you added should still be there, since we are using the same disks for the cluster.\\n\\nIf you want to change the size of the cluster you can actually do that as well, just delete the cluster and create it again with a larger cluster size and that should work.\\n\\nNote that if you delete and create the cluster you might end up with a new IP.\\n\\n## Summary\\n\\nI wouldn't say that this is perfect, but it definitely a start. One drawback is that it doesn't allow for zero downtime increase of cluster size. It could probably be added, but it is out of scope for this post. I haven't tested the performance either, and that is probably something you should do before actually using it. As I mentioned earlier, in a production environment you shouldn't expose Event Store to the public. \\n\\nThere is probably a lot more comments one can have about this setup, but I leave that up to you. Feel free to come with both positive and negative comments :).\\n\\n\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>To prepare myself for my new job, which will involve some kubernetes stuff, I've played around with it somewhat lately, as you could see in this <a href=\"http://blog.2mas.xyz/fsharp-suave-app-on-dotnet-core-on-kubernetes-on-google-cloud/\">post</a>. This post is taking things one step further without making it that much more advanced. The goal for this post is to set up a <a href=\"http://geteventstore.com/\">Event Store</a> cluster on google container engine with a simple script. A prerequisite to get any of this working is that you have installed <a href=\"https://cloud.google.com/sdk/\"><code>gcloud</code></a> and <a href=\"https://cloud.google.com/container-engine/docs/quickstart\"><code>kubectl</code></a>.</p>\n<p>If you don't want to read the whole post and just go to the code you can look it up on <a href=\"https://github.com/mastoj/eventstore-kubernetes\">github</a>. The <a href=\"https://github.com/mastoj/eventstore-kubernetes/tree/naive\"><code>naïve</code></a> and <a href=\"https://github.com/mastoj/eventstore-kubernetes\"><code>master</code></a> branches will be described in this post.</p>\n<h2 id=\"disclaimer\">Disclaimer</h2>\n<p>What I will describe here will expose the Event Store cluster to the public and is something you <strong>should not</strong> do, I do it to make it easier to test that it works. I haven't done any performance tests or reliability tests on my setup either, which you should probably do before using it in production.</p>\n<h2 id=\"theendgoal\">The end goal</h2>\n<p>I did two different approaches, which both will be covered in this post, that had the same end goal. I wanted to have a cluster of Event Store nodes running behind a <a href=\"http://kubernetes.io/docs/user-guide/services/#headless-services\">headless kubernetes service</a> and nginx on top of that to add access to the public. Using a headless kubernetes service will result in a service registered with a dns registration that resolves to all the IPs for the associated containers, and this is exactly what EventStore needs to to discovery through dns.</p>\n<h2 id=\"configuringnginx\">Configuring nginx</h2>\n<p>I put this section first since it is the same for both approaches.</p>\n<h3 id=\"nginxconfiguraion\">Nginx configuraion</h3>\n<p>The configuration of nginx will be stored in a <code>configmap</code> and looks like this, <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/nginx/frontend.conf:\">https://github.com/mastoj/eventstore-kubernetes/blob/master/nginx/frontend.conf:</a></p>\n<pre><code>upstream es {\n    server es.default.svc.cluster.local:2113;\n}\nserver {\n    listen 2113;\n    location / {\n        proxy_set_header    X-Real-IP $remote_addr;\n        proxy_set_header    Host      $http_host;\n        proxy_pass          http://es;\n    }\n}\n</code></pre>\n<p>If you know nginx, this is just basic configuration. First we create an <code>upstream</code> that can be references later on by our <code>proxy_pass</code> when someone is visiting the path <code>/</code>. The url <code>es.default.svc.cluster.local</code> is the dns registration that our Event Store node will get when we get to that point. The <code>server</code> section just defines that we should listen on port <code>80</code> and proxy the traffic to the <code>upstream </code> defined.</p>\n<p>To create the <code>configmap</code> we can execute this command</p>\n<pre><code>kubectl create configmap nginx-es-frontend-conf --from-file=nginx/frontend.conf\n</code></pre>\n<h3 id=\"thenginxdeployment\">The nginx deployment</h3>\n<p>This is basically the same as I used in the previous post, if you read that one. The <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/deployments/frontend-es.yaml\">specification</a> looks like this:</p>\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend-es\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-es\n        track: stable\n    spec:\n      containers:\n        - name: nginx\n          image: &quot;nginx:1.9.14&quot;\n          lifecycle:\n            preStop:\n              exec:\n                command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]\n          volumeMounts:\n            - name: &quot;nginx-es-frontend-conf&quot;\n              mountPath: &quot;/etc/nginx/conf.d&quot;\n      volumes:\n        - name: &quot;nginx-es-frontend-conf&quot;\n          configMap:\n            name: &quot;nginx-es-frontend-conf&quot;\n            items:\n              - key: &quot;frontend.conf&quot;\n                path: &quot;frontend.conf&quot;\n</code></pre>\n<p>It only contains one container, the <code>nginx</code> one, which uses the <code>configmap</code> created above for configuration. We only need one replica at the moment.</p>\n<p>To create it we run the following command</p>\n<pre><code>kubectl create -f deployments/frontend-es.yaml\n</code></pre>\n<h3 id=\"thenginxservice\">The nginx service</h3>\n<p>To create a public IP we need to create a service on top of this deployment. The <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/services/frontend-es.yaml\">specification</a> for the service looks like this:</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: &quot;frontend-es&quot;\nspec:\n  selector:\n    app: &quot;frontend-es&quot;\n  ports:\n    - protocol: &quot;TCP&quot;\n      port: 2113\n      targetPort: 2113\n  type: LoadBalancer\n</code></pre>\n<p>To create the service and finish up the nginx part of the post we run the following command</p>\n<pre><code>kubectl create -f services/frontend-es.yaml\n</code></pre>\n<h2 id=\"firstapproachthenaveone\">First approach - the naïve one</h2>\n<p>The first thing I wanted to do was to get a cluster up and running without persisting the data to disk, that is, only keep the data in the container. A cluster like that might work for development, but not in production. My grand masterplan was to just add persistent to that cluster after the cluster was up and running, which did not work. How to get persistent will be covered under <a href=\"#secondapproachaddingpersistentdata\"><em>Second approach</em></a>. Before we get into the persistent part, let's get this cluster up and running.</p>\n<h3 id=\"creatingthedeployment\">Creating the deployment</h3>\n<p>The <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/naive/deployments/eventstore.yaml\">deployment file</a> is really simple:</p>\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: es\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: es\n    spec:\n      containers:\n        - name: es\n          image: &quot;eventstore/eventstore&quot;\n          env: \n            - name: EVENTSTORE_INT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_EXT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_INT_TCP_PORT\n              value: &quot;1111&quot;\n            - name: EVENTSTORE_EXT_TCP_PORT\n              value: &quot;1112&quot;\n            - name: EVENTSTORE_INT_HTTP_PORT\n              value: &quot;2114&quot;\n            - name: EVENTSTORE_EXT_HTTP_PORT\n              value: &quot;2113&quot;\n            - name: EVENTSTORE_CLUSTER_SIZE\n              value: &quot;3&quot;\n            - name: EVENTSTORE_CLUSTER_DNS\n              value: &quot;es.default.svc.cluster.local&quot;\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\n              value: &quot;2114&quot;\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\n              value: &quot;600000&quot;\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\n              value: &quot;http://*:2114/&quot;\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\n              value: &quot;http://*:2113/&quot;\n          ports:\n            - containerPort: 2113\n            - containerPort: 2114\n            - containerPort: 1111\n            - containerPort: 1112\n</code></pre>\n<p>We use the Event Store docker image from <a href=\"https://hub.docker.com/r/eventstore/eventstore/\">docker hub</a>. This image doesn't allow command line arguments, so we need to use environment variables to configure it. You can read about Event Store configuration <a href=\"http://docs.geteventstore.com/server/3.9.0/command-line-arguments/\">here</a>. Every container (we are running three here) need to use their own IP during configuration, and with kubernetes we can access that data through <code>status.podIP</code> when we configure the environment variables.</p>\n<p>Creating is as simple as before:</p>\n<pre><code>kubectl create -f deployments/eventstore.yaml\n</code></pre>\n<p>That should create a three nodes, but as of this moment they will fail to find each other, and that is why we need the service.</p>\n<h3 id=\"creatingtheservice\">Creating the service</h3>\n<p>Creating the service is even easier than the deployment. The <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/naive/services/eventstore.yaml\">specification file</a> looks like this:</p>\n<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: &quot;es&quot;\nspec:\n  selector:\n    app: &quot;es&quot;\n  ports:\n    - protocol: &quot;TCP&quot;\n      port: 2113\n      targetPort: 2113\n  clusterIP: None\n</code></pre>\n<p>We only expose the 2113 port, which means that we will only be able to talk over http to the event store cluster. To the service we name all the containers that has the label <code>app: &quot;es&quot;</code>. The last thing to know is that we set the <code>clusterIP</code> to <code>None</code>, this will not create one single IP in the DNS for this service, instead it will resolve to all the IPs of the containers and that is exactly what Event Store needs to be able to configure itself.</p>\n<p>Again we are using <code>kubectl</code> to create the service:</p>\n<pre><code>kubectl create -f services/eventstore.yaml\n</code></pre>\n<p>When this is created we should now be ready to test it.</p>\n<h3 id=\"test\">Test</h3>\n<p>To test that it works run the following command:</p>\n<pre><code>kubectl get services\n</code></pre>\n<p>In the result from that command you will find an <code>External IP</code>. To access Event Store, open the browser and go to <code>&lt;external ip&gt;:2113</code>. If everything works as expected you should now have access to Event Store.</p>\n<h3 id=\"challengeswithapproachone\">Challenges with approach one</h3>\n<p>The major challenge is persistent data, how do you map one persistent volume to each node in the cluster? Leave a comment on the post if you have any idea.</p>\n<p>Let us say that you solve the first problem, how do you make sure the nodes get the same persistent volume the next time you restart the cluster?</p>\n<p>Both those two problems is what got me started working on the second approach.</p>\n<p>There is a third problem, which we won't fix, and that is increasing the number of replicas won't work. The reason be that Event Store doesn't support elastic scaling, so increasing the number of replicas will only add &quot;clones&quot; to the cluster, not increase its size.</p>\n<h2 id=\"secondapproachaddingpersistentdata\">Second approach - adding persistent data</h2>\n<p>We are still going to use deployments, but instead of letting the number of replicas define the number of nodes we will create one deployment with one replica per node. This way we can force each node to get access to the same persistent data volume when it is restarted, and using deployments will also handle restart for us.</p>\n<h3 id=\"generatingdeployment\">Generating deployment</h3>\n<p>The difference from the first approach here is that we will generate the deployment from a template instead of creating one. For this to work we need both a template and a simple script that generates the deployments. The <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/templates/es_deployment_template.yaml\">template file</a> looks like this:</p>\n<pre><code>apiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: es-${nodenumber}\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: es-${nodenumber}\n        escluster: es\n    spec:\n      containers:\n        - name: es-${nodenumber}\n          image: &quot;eventstore/eventstore&quot;\n          env: \n            - name: EVENTSTORE_INT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_EXT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_INT_TCP_PORT\n              value: &quot;1111&quot;\n            - name: EVENTSTORE_EXT_TCP_PORT\n              value: &quot;1112&quot;\n            - name: EVENTSTORE_INT_HTTP_PORT\n              value: &quot;2114&quot;\n            - name: EVENTSTORE_EXT_HTTP_PORT\n              value: &quot;2113&quot;\n            - name: EVENTSTORE_CLUSTER_SIZE\n              value: &quot;${nodecount}&quot;\n            - name: EVENTSTORE_CLUSTER_DNS\n              value: &quot;es.default.svc.cluster.local&quot;\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\n              value: &quot;2114&quot;\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\n              value: &quot;600000&quot;\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\n              value: &quot;http://*:2114/&quot;\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\n              value: &quot;http://*:2113/&quot;\n            - name: EVENTSTORE_DB\n              value: &quot;/usr/data/eventstore/data&quot;\n            - name: EVENTSTORE_LOG\n              value: &quot;/usr/data/eventstore/log&quot;\n          ports:\n            - containerPort: 2113\n            - containerPort: 2114\n            - containerPort: 1111\n            - containerPort: 1112\n          volumeMounts:\n            - mountPath: &quot;/usr/data/eventstore&quot;\n              name: espd\n      volumes:\n        - name: espd\n          gcePersistentDisk:\n            pdName: esdisk-${nodenumber}\n            fsType: ext4\n</code></pre>\n<p>The template looks almost the same as in the first case, but we have now added two; <code>nodenumber</code> and <code>nodecount</code>. We have also added a <code>gcePersistentDisk</code> which we mount to <code>usr/data/eventstore</code>, and that is the parent folder we use for logs and data in the configuration of Event Store, <code>EVENTSTORE_DB</code> and <code>EVENTSTORE_LOGS</code>. We have also added a new label <code>escluster</code> which will be used for the service to identify which nodes that should be included in the service.</p>\n<p>To generate the actual deploy files we run a bash script that has the following content:</p>\n<pre><code>    for ((c=1; c&lt;=$count; c++ ))\n    do\n        cat ../templates/es_deployment_template.yaml | sed -e &quot;s/\\${nodenumber}/$c/&quot; | sed -e &quot;s/\\${nodecount}/$count/&quot; &gt; .tmp/es_deployment_$c.yaml\n    done\n</code></pre>\n<p>Running that will generate one file for each node, and each file has its own volume mounted. The script as a whole can be found <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh\">here</a>.</p>\n<p>When the files has been generated we can then run:</p>\n<pre><code>    for ((c=1; c&lt;=$count; c++ ))\n    do\n        kubectl apply -f .tmp/es_deployment_$c.yaml\n    done\n</code></pre>\n<p>This code will create one deployment per file. Since we are using deployments our pods will be restarted if they crashes.</p>\n<h3 id=\"creatingtheservice\">Creating the service</h3>\n<p>This is exactly the same as in the first approach, but with a minor change. We will use the <code>escluster</code> label to identify the pods to add to the service. The file is <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/services/eventstore.yaml\">here</a></p>\n<h3 id=\"thecreateclusterscript\">The create cluster script</h3>\n<p>It is not reasonable to execute any of this by hand, so I created this <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh\">script</a>. I will dissect it here:</p>\n<pre><code>#!/bin/bash\n\nfunction init {\n    rm -rf .tmp\n    mkdir -p .tmp\n}\n\nfunction validateInput {\n    count=$1\n    re='^[0-9]+$'\n    if ! [[ $count =~ $re ]] ; then\n        echo &quot;error: Not a number&quot; &gt;&amp;2; exit 1\n    fi\n}\n</code></pre>\n<p>The first part is just some house keeping and validating input arguments. The plan is that you should be able to create a cluster of any size by running: <code>./create_cluster.sh &lt;size&gt;</code>.</p>\n<pre><code>function createSpecs {\n    local count=$1\n    for ((c=1; c&lt;=$count; c++ ))\n    do\n        cat ../templates/es_deployment_template.yaml | sed -e &quot;s/\\${nodenumber}/$c/&quot; | sed -e &quot;s/\\${nodecount}/$count/&quot; &gt; .tmp/es_deployment_$c.yaml\n    done\n}\n\nfunction createDeployments {\n    local count=$1\n    for ((c=1; c&lt;=$count; c++ ))\n    do\n        kubectl apply -f .tmp/es_deployment_$c.yaml\n    done\n}\n</code></pre>\n<p>The next part defines functions to generate the deployment files and how they should be executed.</p>\n<pre><code>function createEsService {\n    kubectl create -f ../services/eventstore.yaml\n}\n</code></pre>\n<p>This finish of the Event Store part of the script by creating a service on top of the nodes created by deployment.</p>\n<pre><code>function addNginxConfig {\n    kubectl create configmap nginx-es-frontend-conf --from-file=../nginx/frontend.conf\n}\n\nfunction createFrontendDeployment {\n    kubectl create -f ../deployments/frontend-es.yaml\n}\n\nfunction createFrontendService {\n    kubectl create -f ../services/frontend-es.yaml\n}\n</code></pre>\n<p>The next part is basically what we described in the section about <code>nginx</code> setup.</p>\n<pre><code>function createDisks {\n    local count=$1\n    for ((c=1; c&lt;=$count; c++ ))\n    do\n        if gcloud compute disks list esdisk-$c | grep esdisk-$c; then\n            echo &quot;creating disk: esdisk-$c&quot; \n            gcloud compute disks create --size=10GB esdisk-$c\n        else\n            echo &quot;disk already exists: esdisk-$c&quot;\n        fi\n    done\n}\n</code></pre>\n<p>A simple helper function to create disks on google cloud.</p>\n<pre><code>function createEsCluster {\n    local count=$1\n    createSpecs $count\n    createDeployments $count\n    createEsService\n}\n\nfunction createFrontEnd {\n    addNginxConfig\n    createFrontendDeployment\n    createFrontendService\n}\n</code></pre>\n<p>The last two functions is just to make it easier to read what is going on.</p>\n<pre><code>init\nvalidateInput $1 #sets the variable $count\ncreateDisks $count\ncreateEsCluster $count\ncreateFrontEnd\n</code></pre>\n<p>With all the functions defined it is quite clear what is going on in this script.</p>\n<h3 id=\"test\">Test</h3>\n<p>The easier way to test it is to create the cluster:</p>\n<pre><code>./create_cluster.sh 5\n</code></pre>\n<p>Add some data to the cluster. You do that by finding the external IP of the nginx service and then follow the <a href=\"http://docs.geteventstore.com/introduction/3.9.0/\">getting started instructions</a> for Event Store to add some data.</p>\n<p>With that in place you can kill pods as you like to simulate failures with:</p>\n<pre><code>kubectl delete pod --now &lt;pod id&gt;\n</code></pre>\n<p>When you kill a pod a new should be created, but this time it should use the same persistent disk as the old one.</p>\n<p>You can even delete the whole cluster and rebuild it again. I have added <a href=\"https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/delete_cluster.sh\">delete cluster script</a> that will delete everything but the disks. If you then create the cluster again the data you added should still be there, since we are using the same disks for the cluster.</p>\n<p>If you want to change the size of the cluster you can actually do that as well, just delete the cluster and create it again with a larger cluster size and that should work.</p>\n<p>Note that if you delete and create the cluster you might end up with a new IP.</p>\n<h2 id=\"summary\">Summary</h2>\n<p>I wouldn't say that this is perfect, but it definitely a start. One drawback is that it doesn't allow for zero downtime increase of cluster size. It could probably be added, but it is out of scope for this post. I haven't tested the performance either, and that is probably something you should do before actually using it. As I mentioned earlier, in a production environment you shouldn't expose Event Store to the public.</p>\n<p>There is probably a lot more comments one can have about this setup, but I leave that up to you. Feel free to come with both positive and negative comments :).</p>\n<!--kg-card-end: markdown-->","comment_id":"67","plaintext":"To prepare myself for my new job, which will involve some kubernetes stuff, I've\nplayed around with it somewhat lately, as you could see in this post\n[http://blog.2mas.xyz/fsharp-suave-app-on-dotnet-core-on-kubernetes-on-google-cloud/]\n. This post is taking things one step further without making it that much more\nadvanced. The goal for this post is to set up a Event Store\n[http://geteventstore.com/] cluster on google container engine with a simple\nscript. A prerequisite to get any of this working is that you have installed \ngcloud [https://cloud.google.com/sdk/] and kubectl\n[https://cloud.google.com/container-engine/docs/quickstart].\n\nIf you don't want to read the whole post and just go to the code you can look it\nup on github [https://github.com/mastoj/eventstore-kubernetes]. The naïve\n[https://github.com/mastoj/eventstore-kubernetes/tree/naive] and master\n[https://github.com/mastoj/eventstore-kubernetes] branches will be described in\nthis post.\n\nDisclaimer\nWhat I will describe here will expose the Event Store cluster to the public and\nis something you should not do, I do it to make it easier to test that it works.\nI haven't done any performance tests or reliability tests on my setup either,\nwhich you should probably do before using it in production.\n\nThe end goal\nI did two different approaches, which both will be covered in this post, that\nhad the same end goal. I wanted to have a cluster of Event Store nodes running\nbehind a headless kubernetes service\n[http://kubernetes.io/docs/user-guide/services/#headless-services] and nginx on\ntop of that to add access to the public. Using a headless kubernetes service\nwill result in a service registered with a dns registration that resolves to all\nthe IPs for the associated containers, and this is exactly what EventStore needs\nto to discovery through dns.\n\nConfiguring nginx\nI put this section first since it is the same for both approaches.\n\nNginx configuraion\nThe configuration of nginx will be stored in a configmap and looks like this, \nhttps://github.com/mastoj/eventstore-kubernetes/blob/master/nginx/frontend.conf:\n\nupstream es {\n    server es.default.svc.cluster.local:2113;\n}\nserver {\n    listen 2113;\n    location / {\n        proxy_set_header    X-Real-IP $remote_addr;\n        proxy_set_header    Host      $http_host;\n        proxy_pass          http://es;\n    }\n}\n\n\nIf you know nginx, this is just basic configuration. First we create an upstream \nthat can be references later on by our proxy_pass when someone is visiting the\npath /. The url es.default.svc.cluster.local is the dns registration that our\nEvent Store node will get when we get to that point. The server section just\ndefines that we should listen on port 80 and proxy the traffic to the upstream \ndefined.\n\nTo create the configmap we can execute this command\n\nkubectl create configmap nginx-es-frontend-conf --from-file=nginx/frontend.conf\n\n\nThe nginx deployment\nThis is basically the same as I used in the previous post, if you read that one.\nThe specification\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/deployments/frontend-es.yaml] \nlooks like this:\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend-es\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend-es\n        track: stable\n    spec:\n      containers:\n        - name: nginx\n          image: \"nginx:1.9.14\"\n          lifecycle:\n            preStop:\n              exec:\n                command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n          volumeMounts:\n            - name: \"nginx-es-frontend-conf\"\n              mountPath: \"/etc/nginx/conf.d\"\n      volumes:\n        - name: \"nginx-es-frontend-conf\"\n          configMap:\n            name: \"nginx-es-frontend-conf\"\n            items:\n              - key: \"frontend.conf\"\n                path: \"frontend.conf\"\n\n\nIt only contains one container, the nginx one, which uses the configmap created\nabove for configuration. We only need one replica at the moment.\n\nTo create it we run the following command\n\nkubectl create -f deployments/frontend-es.yaml\n\n\nThe nginx service\nTo create a public IP we need to create a service on top of this deployment. The \nspecification\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/services/frontend-es.yaml] \nfor the service looks like this:\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: \"frontend-es\"\nspec:\n  selector:\n    app: \"frontend-es\"\n  ports:\n    - protocol: \"TCP\"\n      port: 2113\n      targetPort: 2113\n  type: LoadBalancer\n\n\nTo create the service and finish up the nginx part of the post we run the\nfollowing command\n\nkubectl create -f services/frontend-es.yaml\n\n\nFirst approach - the naïve one\nThe first thing I wanted to do was to get a cluster up and running without\npersisting the data to disk, that is, only keep the data in the container. A\ncluster like that might work for development, but not in production. My grand\nmasterplan was to just add persistent to that cluster after the cluster was up\nand running, which did not work. How to get persistent will be covered under \nSecond approach. Before we get into the persistent part, let's get this cluster\nup and running.\n\nCreating the deployment\nThe deployment file\n[https://github.com/mastoj/eventstore-kubernetes/blob/naive/deployments/eventstore.yaml] \nis really simple:\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: es\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: es\n    spec:\n      containers:\n        - name: es\n          image: \"eventstore/eventstore\"\n          env: \n            - name: EVENTSTORE_INT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_EXT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_INT_TCP_PORT\n              value: \"1111\"\n            - name: EVENTSTORE_EXT_TCP_PORT\n              value: \"1112\"\n            - name: EVENTSTORE_INT_HTTP_PORT\n              value: \"2114\"\n            - name: EVENTSTORE_EXT_HTTP_PORT\n              value: \"2113\"\n            - name: EVENTSTORE_CLUSTER_SIZE\n              value: \"3\"\n            - name: EVENTSTORE_CLUSTER_DNS\n              value: \"es.default.svc.cluster.local\"\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\n              value: \"2114\"\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\n              value: \"600000\"\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\n              value: \"http://*:2114/\"\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\n              value: \"http://*:2113/\"\n          ports:\n            - containerPort: 2113\n            - containerPort: 2114\n            - containerPort: 1111\n            - containerPort: 1112\n\n\nWe use the Event Store docker image from docker hub\n[https://hub.docker.com/r/eventstore/eventstore/]. This image doesn't allow\ncommand line arguments, so we need to use environment variables to configure it.\nYou can read about Event Store configuration here\n[http://docs.geteventstore.com/server/3.9.0/command-line-arguments/]. Every\ncontainer (we are running three here) need to use their own IP during\nconfiguration, and with kubernetes we can access that data through status.podIP \nwhen we configure the environment variables.\n\nCreating is as simple as before:\n\nkubectl create -f deployments/eventstore.yaml\n\n\nThat should create a three nodes, but as of this moment they will fail to find\neach other, and that is why we need the service.\n\nCreating the service\nCreating the service is even easier than the deployment. The specification file\n[https://github.com/mastoj/eventstore-kubernetes/blob/naive/services/eventstore.yaml] \nlooks like this:\n\nkind: Service\napiVersion: v1\nmetadata:\n  name: \"es\"\nspec:\n  selector:\n    app: \"es\"\n  ports:\n    - protocol: \"TCP\"\n      port: 2113\n      targetPort: 2113\n  clusterIP: None\n\n\nWe only expose the 2113 port, which means that we will only be able to talk over\nhttp to the event store cluster. To the service we name all the containers that\nhas the label app: \"es\". The last thing to know is that we set the clusterIP to \nNone, this will not create one single IP in the DNS for this service, instead it\nwill resolve to all the IPs of the containers and that is exactly what Event\nStore needs to be able to configure itself.\n\nAgain we are using kubectl to create the service:\n\nkubectl create -f services/eventstore.yaml\n\n\nWhen this is created we should now be ready to test it.\n\nTest\nTo test that it works run the following command:\n\nkubectl get services\n\n\nIn the result from that command you will find an External IP. To access Event\nStore, open the browser and go to <external ip>:2113. If everything works as\nexpected you should now have access to Event Store.\n\nChallenges with approach one\nThe major challenge is persistent data, how do you map one persistent volume to\neach node in the cluster? Leave a comment on the post if you have any idea.\n\nLet us say that you solve the first problem, how do you make sure the nodes get\nthe same persistent volume the next time you restart the cluster?\n\nBoth those two problems is what got me started working on the second approach.\n\nThere is a third problem, which we won't fix, and that is increasing the number\nof replicas won't work. The reason be that Event Store doesn't support elastic\nscaling, so increasing the number of replicas will only add \"clones\" to the\ncluster, not increase its size.\n\nSecond approach - adding persistent data\nWe are still going to use deployments, but instead of letting the number of\nreplicas define the number of nodes we will create one deployment with one\nreplica per node. This way we can force each node to get access to the same\npersistent data volume when it is restarted, and using deployments will also\nhandle restart for us.\n\nGenerating deployment\nThe difference from the first approach here is that we will generate the\ndeployment from a template instead of creating one. For this to work we need\nboth a template and a simple script that generates the deployments. The \ntemplate\nfile\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/templates/es_deployment_template.yaml] \nlooks like this:\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: es-${nodenumber}\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: es-${nodenumber}\n        escluster: es\n    spec:\n      containers:\n        - name: es-${nodenumber}\n          image: \"eventstore/eventstore\"\n          env: \n            - name: EVENTSTORE_INT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_EXT_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.podIP\n            - name: EVENTSTORE_INT_TCP_PORT\n              value: \"1111\"\n            - name: EVENTSTORE_EXT_TCP_PORT\n              value: \"1112\"\n            - name: EVENTSTORE_INT_HTTP_PORT\n              value: \"2114\"\n            - name: EVENTSTORE_EXT_HTTP_PORT\n              value: \"2113\"\n            - name: EVENTSTORE_CLUSTER_SIZE\n              value: \"${nodecount}\"\n            - name: EVENTSTORE_CLUSTER_DNS\n              value: \"es.default.svc.cluster.local\"\n            - name: EVENTSTORE_CLUSTER_GOSSIP_PORT\n              value: \"2114\"\n            - name: EVENTSTORE_GOSSIP_ALLOWED_DIFFERENCE_MS\n              value: \"600000\"\n            - name: EVENTSTORE_INT_HTTP_PREFIXES\n              value: \"http://*:2114/\"\n            - name: EVENTSTORE_EXT_HTTP_PREFIXES\n              value: \"http://*:2113/\"\n            - name: EVENTSTORE_DB\n              value: \"/usr/data/eventstore/data\"\n            - name: EVENTSTORE_LOG\n              value: \"/usr/data/eventstore/log\"\n          ports:\n            - containerPort: 2113\n            - containerPort: 2114\n            - containerPort: 1111\n            - containerPort: 1112\n          volumeMounts:\n            - mountPath: \"/usr/data/eventstore\"\n              name: espd\n      volumes:\n        - name: espd\n          gcePersistentDisk:\n            pdName: esdisk-${nodenumber}\n            fsType: ext4\n\n\nThe template looks almost the same as in the first case, but we have now added\ntwo; nodenumber and nodecount. We have also added a gcePersistentDisk which we\nmount to usr/data/eventstore, and that is the parent folder we use for logs and\ndata in the configuration of Event Store, EVENTSTORE_DB and EVENTSTORE_LOGS. We\nhave also added a new label escluster which will be used for the service to\nidentify which nodes that should be included in the service.\n\nTo generate the actual deploy files we run a bash script that has the following\ncontent:\n\n    for ((c=1; c<=$count; c++ ))\n    do\n        cat ../templates/es_deployment_template.yaml | sed -e \"s/\\${nodenumber}/$c/\" | sed -e \"s/\\${nodecount}/$count/\" > .tmp/es_deployment_$c.yaml\n    done\n\n\nRunning that will generate one file for each node, and each file has its own\nvolume mounted. The script as a whole can be found here\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh]\n.\n\nWhen the files has been generated we can then run:\n\n    for ((c=1; c<=$count; c++ ))\n    do\n        kubectl apply -f .tmp/es_deployment_$c.yaml\n    done\n\n\nThis code will create one deployment per file. Since we are using deployments\nour pods will be restarted if they crashes.\n\nCreating the service\nThis is exactly the same as in the first approach, but with a minor change. We\nwill use the escluster label to identify the pods to add to the service. The\nfile is here\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/services/eventstore.yaml]\n\nThe create cluster script\nIt is not reasonable to execute any of this by hand, so I created this script\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/create_cluster.sh]\n. I will dissect it here:\n\n#!/bin/bash\n\nfunction init {\n    rm -rf .tmp\n    mkdir -p .tmp\n}\n\nfunction validateInput {\n    count=$1\n    re='^[0-9]+$'\n    if ! [[ $count =~ $re ]] ; then\n        echo \"error: Not a number\" >&2; exit 1\n    fi\n}\n\n\nThe first part is just some house keeping and validating input arguments. The\nplan is that you should be able to create a cluster of any size by running: \n./create_cluster.sh <size>.\n\nfunction createSpecs {\n    local count=$1\n    for ((c=1; c<=$count; c++ ))\n    do\n        cat ../templates/es_deployment_template.yaml | sed -e \"s/\\${nodenumber}/$c/\" | sed -e \"s/\\${nodecount}/$count/\" > .tmp/es_deployment_$c.yaml\n    done\n}\n\nfunction createDeployments {\n    local count=$1\n    for ((c=1; c<=$count; c++ ))\n    do\n        kubectl apply -f .tmp/es_deployment_$c.yaml\n    done\n}\n\n\nThe next part defines functions to generate the deployment files and how they\nshould be executed.\n\nfunction createEsService {\n    kubectl create -f ../services/eventstore.yaml\n}\n\n\nThis finish of the Event Store part of the script by creating a service on top\nof the nodes created by deployment.\n\nfunction addNginxConfig {\n    kubectl create configmap nginx-es-frontend-conf --from-file=../nginx/frontend.conf\n}\n\nfunction createFrontendDeployment {\n    kubectl create -f ../deployments/frontend-es.yaml\n}\n\nfunction createFrontendService {\n    kubectl create -f ../services/frontend-es.yaml\n}\n\n\nThe next part is basically what we described in the section about nginx setup.\n\nfunction createDisks {\n    local count=$1\n    for ((c=1; c<=$count; c++ ))\n    do\n        if gcloud compute disks list esdisk-$c | grep esdisk-$c; then\n            echo \"creating disk: esdisk-$c\" \n            gcloud compute disks create --size=10GB esdisk-$c\n        else\n            echo \"disk already exists: esdisk-$c\"\n        fi\n    done\n}\n\n\nA simple helper function to create disks on google cloud.\n\nfunction createEsCluster {\n    local count=$1\n    createSpecs $count\n    createDeployments $count\n    createEsService\n}\n\nfunction createFrontEnd {\n    addNginxConfig\n    createFrontendDeployment\n    createFrontendService\n}\n\n\nThe last two functions is just to make it easier to read what is going on.\n\ninit\nvalidateInput $1 #sets the variable $count\ncreateDisks $count\ncreateEsCluster $count\ncreateFrontEnd\n\n\nWith all the functions defined it is quite clear what is going on in this\nscript.\n\nTest\nThe easier way to test it is to create the cluster:\n\n./create_cluster.sh 5\n\n\nAdd some data to the cluster. You do that by finding the external IP of the\nnginx service and then follow the getting started instructions\n[http://docs.geteventstore.com/introduction/3.9.0/] for Event Store to add some\ndata.\n\nWith that in place you can kill pods as you like to simulate failures with:\n\nkubectl delete pod --now <pod id>\n\n\nWhen you kill a pod a new should be created, but this time it should use the\nsame persistent disk as the old one.\n\nYou can even delete the whole cluster and rebuild it again. I have added delete\ncluster script\n[https://github.com/mastoj/eventstore-kubernetes/blob/master/scripts/delete_cluster.sh] \nthat will delete everything but the disks. If you then create the cluster again\nthe data you added should still be there, since we are using the same disks for\nthe cluster.\n\nIf you want to change the size of the cluster you can actually do that as well,\njust delete the cluster and create it again with a larger cluster size and that\nshould work.\n\nNote that if you delete and create the cluster you might end up with a new IP.\n\nSummary\nI wouldn't say that this is perfect, but it definitely a start. One drawback is\nthat it doesn't allow for zero downtime increase of cluster size. It could\nprobably be added, but it is out of scope for this post. I haven't tested the\nperformance either, and that is probably something you should do before actually\nusing it. As I mentioned earlier, in a production environment you shouldn't\nexpose Event Store to the public.\n\nThere is probably a lot more comments one can have about this setup, but I leave\nthat up to you. Feel free to come with both positive and negative comments :).","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-11-21T22:06:20.000Z","updated_at":"2016-11-23T22:40:25.000Z","published_at":"2016-11-23T22:40:25.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0},{"id":"5e382b29f81c630018abe077","uuid":"19428d7a-c076-4690-b25f-bb354538341a","title":"Introducing F# intro","slug":"introducing-f-intro","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"card-markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"A couple of months back I was asked if could do a introductory workshop about F# for [Active Solution](https://www.activesolution.se/) during their internal conference in the Pyrenees, Spain. Of course I said yes and that was the start of this brand new workshop material for F#.\\n\\nTL;DR: https://mastoj.github.io/FSharpIntro/#/\\n\\nWhen I have previously done introductory workshops in F# I have used the awesome material from [http://www.fsharpworkshop.com/](http://www.fsharpworkshop.com/) by [Jorge Fioranelli](https://twitter.com/jorgefioranelli), but this time decided to create my own material. The reason for doing so is that I wanted the material to feel more like my own while presenting it, and I also wanted to contribute back to the community. Both Jorge's workshop and my have a lot of similarities and cover much of the same material, as they should since they are introductory workshops, but there are small differences in the way it is structured. I really do recommend the workshop by Jorge's as well, and I even think it make sense to go through both for repeated learning.\\n\\nI felt that the workshop was well received, but I leave that to the kind people at Active Solution to confirm :). The initial plan was that I would get four hours, but due to some delays I got roughly 2-2.5 hours, so you will need at least four hours to cover everything in a workshop setting. It is likely that it will take even longer if you have an curious crowd, as I had, since that will most likely result in a lot of questions, which I think is more important to answer than actual progress in the workshop. So when doing the workshop I recommend time boxing the workshop and do as much as you can. \\n\\nYou can find the start of the workshop at this link https://mastoj.github.io/FSharpIntro/#/, which contains all the presentation slides and links to github repo and exercises. \\n\\nLet me know if you have any questions. If you find anything that is wrong in the material or have any question regarding the workshop, please comment here or open an issue on [github](https://github.com/mastoj/FSharpIntro/issues).\"}]],\"sections\":[[10,0]]}","html":"<!--kg-card-begin: markdown--><p>A couple of months back I was asked if could do a introductory workshop about F# for <a href=\"https://www.activesolution.se/\">Active Solution</a> during their internal conference in the Pyrenees, Spain. Of course I said yes and that was the start of this brand new workshop material for F#.</p>\n<p>TL;DR: <a href=\"https://mastoj.github.io/FSharpIntro/#/\">https://mastoj.github.io/FSharpIntro/#/</a></p>\n<p>When I have previously done introductory workshops in F# I have used the awesome material from <a href=\"http://www.fsharpworkshop.com/\">http://www.fsharpworkshop.com/</a> by <a href=\"https://twitter.com/jorgefioranelli\">Jorge Fioranelli</a>, but this time decided to create my own material. The reason for doing so is that I wanted the material to feel more like my own while presenting it, and I also wanted to contribute back to the community. Both Jorge's workshop and my have a lot of similarities and cover much of the same material, as they should since they are introductory workshops, but there are small differences in the way it is structured. I really do recommend the workshop by Jorge's as well, and I even think it make sense to go through both for repeated learning.</p>\n<p>I felt that the workshop was well received, but I leave that to the kind people at Active Solution to confirm :). The initial plan was that I would get four hours, but due to some delays I got roughly 2-2.5 hours, so you will need at least four hours to cover everything in a workshop setting. It is likely that it will take even longer if you have an curious crowd, as I had, since that will most likely result in a lot of questions, which I think is more important to answer than actual progress in the workshop. So when doing the workshop I recommend time boxing the workshop and do as much as you can.</p>\n<p>You can find the start of the workshop at this link <a href=\"https://mastoj.github.io/FSharpIntro/#/\">https://mastoj.github.io/FSharpIntro/#/</a>, which contains all the presentation slides and links to github repo and exercises.</p>\n<p>Let me know if you have any questions. If you find anything that is wrong in the material or have any question regarding the workshop, please comment here or open an issue on <a href=\"https://github.com/mastoj/FSharpIntro/issues\">github</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"69","plaintext":"A couple of months back I was asked if could do a introductory workshop about F#\nfor Active Solution [https://www.activesolution.se/] during their internal\nconference in the Pyrenees, Spain. Of course I said yes and that was the start\nof this brand new workshop material for F#.\n\nTL;DR: https://mastoj.github.io/FSharpIntro/#/\n\nWhen I have previously done introductory workshops in F# I have used the awesome\nmaterial from http://www.fsharpworkshop.com/ by Jorge Fioranelli\n[https://twitter.com/jorgefioranelli], but this time decided to create my own\nmaterial. The reason for doing so is that I wanted the material to feel more\nlike my own while presenting it, and I also wanted to contribute back to the\ncommunity. Both Jorge's workshop and my have a lot of similarities and cover\nmuch of the same material, as they should since they are introductory workshops,\nbut there are small differences in the way it is structured. I really do\nrecommend the workshop by Jorge's as well, and I even think it make sense to go\nthrough both for repeated learning.\n\nI felt that the workshop was well received, but I leave that to the kind people\nat Active Solution to confirm :). The initial plan was that I would get four\nhours, but due to some delays I got roughly 2-2.5 hours, so you will need at\nleast four hours to cover everything in a workshop setting. It is likely that it\nwill take even longer if you have an curious crowd, as I had, since that will\nmost likely result in a lot of questions, which I think is more important to\nanswer than actual progress in the workshop. So when doing the workshop I\nrecommend time boxing the workshop and do as much as you can.\n\nYou can find the start of the workshop at this link \nhttps://mastoj.github.io/FSharpIntro/#/, which contains all the presentation\nslides and links to github repo and exercises.\n\nLet me know if you have any questions. If you find anything that is wrong in the\nmaterial or have any question regarding the workshop, please comment here or\nopen an issue on github [https://github.com/mastoj/FSharpIntro/issues].","feature_image":null,"featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-05-21T20:37:34.000Z","updated_at":"2017-05-21T21:08:39.000Z","published_at":"2017-05-21T21:08:39.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","send_email_when_published":0}],"posts_authors":[{"id":"5e382c43236c35002d5c4808","post_id":"5e382b29f81c630018abe035","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4809","post_id":"5e382b29f81c630018abe036","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480a","post_id":"5e382b29f81c630018abe037","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480b","post_id":"5e382b29f81c630018abe038","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480c","post_id":"5e382b29f81c630018abe039","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480d","post_id":"5e382b29f81c630018abe03a","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480e","post_id":"5e382b29f81c630018abe03b","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c480f","post_id":"5e382b29f81c630018abe03c","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4810","post_id":"5e382b29f81c630018abe03d","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4811","post_id":"5e382b29f81c630018abe03e","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4812","post_id":"5e382b29f81c630018abe03f","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4813","post_id":"5e382b29f81c630018abe040","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4814","post_id":"5e382b29f81c630018abe041","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4815","post_id":"5e382b29f81c630018abe042","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4816","post_id":"5e382b29f81c630018abe043","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4817","post_id":"5e382b29f81c630018abe044","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4818","post_id":"5e382b29f81c630018abe045","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4819","post_id":"5e382b29f81c630018abe046","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481a","post_id":"5e382b29f81c630018abe047","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481b","post_id":"5e382b29f81c630018abe048","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481c","post_id":"5e382b29f81c630018abe049","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481d","post_id":"5e382b29f81c630018abe04a","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481e","post_id":"5e382b29f81c630018abe04b","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c481f","post_id":"5e382b29f81c630018abe04c","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4820","post_id":"5e382b29f81c630018abe04d","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4821","post_id":"5e382b29f81c630018abe04e","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4822","post_id":"5e382b29f81c630018abe04f","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4823","post_id":"5e382b29f81c630018abe050","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4824","post_id":"5e382b29f81c630018abe051","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4825","post_id":"5e382b29f81c630018abe052","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4826","post_id":"5e382b29f81c630018abe053","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4827","post_id":"5e382b29f81c630018abe054","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4828","post_id":"5e382b29f81c630018abe055","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4829","post_id":"5e382b29f81c630018abe056","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482a","post_id":"5e382b29f81c630018abe057","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482b","post_id":"5e382b29f81c630018abe058","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482c","post_id":"5e382b29f81c630018abe059","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482d","post_id":"5e382b29f81c630018abe05a","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482e","post_id":"5e382b29f81c630018abe05b","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c482f","post_id":"5e382b29f81c630018abe05c","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4830","post_id":"5e382b29f81c630018abe05d","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4831","post_id":"5e382b29f81c630018abe05e","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4832","post_id":"5e382b29f81c630018abe05f","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4833","post_id":"5e382b29f81c630018abe060","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4834","post_id":"5e382b29f81c630018abe061","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4835","post_id":"5e382b29f81c630018abe062","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4836","post_id":"5e382b29f81c630018abe063","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4837","post_id":"5e382b29f81c630018abe064","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4838","post_id":"5e382b29f81c630018abe065","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4839","post_id":"5e382b29f81c630018abe066","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c483a","post_id":"5e382b29f81c630018abe067","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c483b","post_id":"5e382b29f81c630018abe068","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c483c","post_id":"5e382b29f81c630018abe069","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c483d","post_id":"5e382b29f81c630018abe06a","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c483e","post_id":"5e382b29f81c630018abe06b","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4840","post_id":"5e382b29f81c630018abe06d","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4841","post_id":"5e382b29f81c630018abe06e","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4843","post_id":"5e382b29f81c630018abe070","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4844","post_id":"5e382b29f81c630018abe071","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4845","post_id":"5e382b29f81c630018abe072","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4846","post_id":"5e382b29f81c630018abe073","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4847","post_id":"5e382b29f81c630018abe074","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c4848","post_id":"5e382b29f81c630018abe075","author_id":"1","sort_order":0},{"id":"5e382c43236c35002d5c484a","post_id":"5e382b29f81c630018abe077","author_id":"1","sort_order":0}],"posts_meta":[],"posts_tags":[{"id":"5e382b29f81c630018abe079","post_id":"5e382b29f81c630018abe035","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b29f81c630018abe07a","post_id":"5e382b29f81c630018abe035","tag_id":"5e382b29f81c630018abdff6","sort_order":1},{"id":"5e382b29f81c630018abe07b","post_id":"5e382b29f81c630018abe036","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b29f81c630018abe07c","post_id":"5e382b29f81c630018abe036","tag_id":"5e382b29f81c630018abdff7","sort_order":1},{"id":"5e382b29f81c630018abe07d","post_id":"5e382b29f81c630018abe036","tag_id":"5e382b29f81c630018abdff8","sort_order":2},{"id":"5e382b29f81c630018abe07e","post_id":"5e382b29f81c630018abe037","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b29f81c630018abe07f","post_id":"5e382b29f81c630018abe038","tag_id":"5e382b29f81c630018abdff9","sort_order":0},{"id":"5e382b2af81c630018abe080","post_id":"5e382b29f81c630018abe039","tag_id":"5e382b29f81c630018abdffa","sort_order":0},{"id":"5e382b2af81c630018abe081","post_id":"5e382b29f81c630018abe039","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2af81c630018abe082","post_id":"5e382b29f81c630018abe03a","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe083","post_id":"5e382b29f81c630018abe03b","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe084","post_id":"5e382b29f81c630018abe03b","tag_id":"5e382b29f81c630018abdffc","sort_order":1},{"id":"5e382b2af81c630018abe085","post_id":"5e382b29f81c630018abe03c","tag_id":"5e382b29f81c630018abdff9","sort_order":0},{"id":"5e382b2af81c630018abe086","post_id":"5e382b29f81c630018abe03d","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe087","post_id":"5e382b29f81c630018abe03d","tag_id":"5e382b29f81c630018abdffd","sort_order":1},{"id":"5e382b2af81c630018abe088","post_id":"5e382b29f81c630018abe03d","tag_id":"5e382b29f81c630018abdffe","sort_order":2},{"id":"5e382b2af81c630018abe089","post_id":"5e382b29f81c630018abe03e","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe08a","post_id":"5e382b29f81c630018abe03f","tag_id":"5e382b29f81c630018abdffa","sort_order":0},{"id":"5e382b2af81c630018abe08b","post_id":"5e382b29f81c630018abe03f","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2af81c630018abe08c","post_id":"5e382b29f81c630018abe040","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe08d","post_id":"5e382b29f81c630018abe040","tag_id":"5e382b29f81c630018abe000","sort_order":1},{"id":"5e382b2af81c630018abe08e","post_id":"5e382b29f81c630018abe041","tag_id":"5e382b29f81c630018abe001","sort_order":0},{"id":"5e382b2af81c630018abe08f","post_id":"5e382b29f81c630018abe042","tag_id":"5e382b29f81c630018abe002","sort_order":0},{"id":"5e382b2af81c630018abe090","post_id":"5e382b29f81c630018abe042","tag_id":"5e382b29f81c630018abe003","sort_order":1},{"id":"5e382b2af81c630018abe091","post_id":"5e382b29f81c630018abe042","tag_id":"5e382b29f81c630018abe004","sort_order":2},{"id":"5e382b2af81c630018abe092","post_id":"5e382b29f81c630018abe043","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2af81c630018abe093","post_id":"5e382b29f81c630018abe043","tag_id":"5e382b29f81c630018abe005","sort_order":1},{"id":"5e382b2bf81c630018abe094","post_id":"5e382b29f81c630018abe044","tag_id":"5e382b29f81c630018abe006","sort_order":0},{"id":"5e382b2bf81c630018abe095","post_id":"5e382b29f81c630018abe044","tag_id":"5e382b29f81c630018abe007","sort_order":1},{"id":"5e382b2bf81c630018abe096","post_id":"5e382b29f81c630018abe045","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2bf81c630018abe097","post_id":"5e382b29f81c630018abe045","tag_id":"5e382b29f81c630018abe008","sort_order":1},{"id":"5e382b2bf81c630018abe098","post_id":"5e382b29f81c630018abe045","tag_id":"5e382b29f81c630018abe009","sort_order":2},{"id":"5e382b2bf81c630018abe099","post_id":"5e382b29f81c630018abe046","tag_id":"5e382b29f81c630018abe007","sort_order":0},{"id":"5e382b2bf81c630018abe09a","post_id":"5e382b29f81c630018abe046","tag_id":"5e382b29f81c630018abe00a","sort_order":1},{"id":"5e382b2bf81c630018abe09b","post_id":"5e382b29f81c630018abe047","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2bf81c630018abe09c","post_id":"5e382b29f81c630018abe047","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2bf81c630018abe09d","post_id":"5e382b29f81c630018abe047","tag_id":"5e382b29f81c630018abe00c","sort_order":2},{"id":"5e382b2bf81c630018abe09e","post_id":"5e382b29f81c630018abe047","tag_id":"5e382b29f81c630018abe00d","sort_order":3},{"id":"5e382b2bf81c630018abe09f","post_id":"5e382b29f81c630018abe048","tag_id":"5e382b29f81c630018abe002","sort_order":0},{"id":"5e382b2bf81c630018abe0a0","post_id":"5e382b29f81c630018abe049","tag_id":"5e382b29f81c630018abdff6","sort_order":0},{"id":"5e382b2bf81c630018abe0a1","post_id":"5e382b29f81c630018abe049","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2bf81c630018abe0a2","post_id":"5e382b29f81c630018abe049","tag_id":"5e382b29f81c630018abe00e","sort_order":2},{"id":"5e382b2bf81c630018abe0a3","post_id":"5e382b29f81c630018abe04a","tag_id":"5e382b29f81c630018abe007","sort_order":0},{"id":"5e382b2bf81c630018abe0a4","post_id":"5e382b29f81c630018abe04a","tag_id":"5e382b29f81c630018abe00f","sort_order":1},{"id":"5e382b2bf81c630018abe0a5","post_id":"5e382b29f81c630018abe04a","tag_id":"5e382b29f81c630018abe010","sort_order":2},{"id":"5e382b2bf81c630018abe0a6","post_id":"5e382b29f81c630018abe04a","tag_id":"5e382b29f81c630018abe011","sort_order":3},{"id":"5e382b2bf81c630018abe0a7","post_id":"5e382b29f81c630018abe04a","tag_id":"5e382b29f81c630018abe012","sort_order":4},{"id":"5e382b2bf81c630018abe0a8","post_id":"5e382b29f81c630018abe04b","tag_id":"5e382b29f81c630018abe013","sort_order":0},{"id":"5e382b2bf81c630018abe0a9","post_id":"5e382b29f81c630018abe04c","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2bf81c630018abe0aa","post_id":"5e382b29f81c630018abe04c","tag_id":"5e382b29f81c630018abe014","sort_order":1},{"id":"5e382b2bf81c630018abe0ab","post_id":"5e382b29f81c630018abe04c","tag_id":"5e382b29f81c630018abe015","sort_order":2},{"id":"5e382b2bf81c630018abe0ac","post_id":"5e382b29f81c630018abe04d","tag_id":"5e382b29f81c630018abdffb","sort_order":0},{"id":"5e382b2bf81c630018abe0ad","post_id":"5e382b29f81c630018abe04d","tag_id":"5e382b29f81c630018abe014","sort_order":1},{"id":"5e382b2bf81c630018abe0ae","post_id":"5e382b29f81c630018abe04d","tag_id":"5e382b29f81c630018abe015","sort_order":2},{"id":"5e382b2bf81c630018abe0af","post_id":"5e382b29f81c630018abe04f","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2bf81c630018abe0b0","post_id":"5e382b29f81c630018abe04f","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2bf81c630018abe0b1","post_id":"5e382b29f81c630018abe04f","tag_id":"5e382b29f81c630018abe017","sort_order":2},{"id":"5e382b2bf81c630018abe0b2","post_id":"5e382b29f81c630018abe04f","tag_id":"5e382b29f81c630018abe018","sort_order":3},{"id":"5e382b2bf81c630018abe0b3","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2bf81c630018abe0b4","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2bf81c630018abe0b5","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe016","sort_order":2},{"id":"5e382b2bf81c630018abe0b6","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe017","sort_order":3},{"id":"5e382b2bf81c630018abe0b7","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe018","sort_order":4},{"id":"5e382b2bf81c630018abe0b8","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe01a","sort_order":5},{"id":"5e382b2bf81c630018abe0b9","post_id":"5e382b29f81c630018abe050","tag_id":"5e382b29f81c630018abe01b","sort_order":6},{"id":"5e382b2bf81c630018abe0ba","post_id":"5e382b29f81c630018abe051","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2bf81c630018abe0bb","post_id":"5e382b29f81c630018abe051","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2bf81c630018abe0bc","post_id":"5e382b29f81c630018abe051","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2cf81c630018abe0bd","post_id":"5e382b29f81c630018abe052","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0be","post_id":"5e382b29f81c630018abe052","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0bf","post_id":"5e382b29f81c630018abe052","tag_id":"5e382b29f81c630018abe017","sort_order":2},{"id":"5e382b2cf81c630018abe0c0","post_id":"5e382b29f81c630018abe052","tag_id":"5e382b29f81c630018abe018","sort_order":3},{"id":"5e382b2cf81c630018abe0c1","post_id":"5e382b29f81c630018abe053","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0c2","post_id":"5e382b29f81c630018abe053","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0c3","post_id":"5e382b29f81c630018abe053","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2cf81c630018abe0c4","post_id":"5e382b29f81c630018abe054","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0c5","post_id":"5e382b29f81c630018abe054","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0c6","post_id":"5e382b29f81c630018abe054","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2cf81c630018abe0c7","post_id":"5e382b29f81c630018abe055","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0c8","post_id":"5e382b29f81c630018abe055","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0c9","post_id":"5e382b29f81c630018abe055","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2cf81c630018abe0ca","post_id":"5e382b29f81c630018abe056","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0cb","post_id":"5e382b29f81c630018abe056","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0cc","post_id":"5e382b29f81c630018abe056","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2cf81c630018abe0cd","post_id":"5e382b29f81c630018abe056","tag_id":"5e382b29f81c630018abe019","sort_order":3},{"id":"5e382b2cf81c630018abe0ce","post_id":"5e382b29f81c630018abe057","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0cf","post_id":"5e382b29f81c630018abe057","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0d0","post_id":"5e382b29f81c630018abe057","tag_id":"5e382b29f81c630018abe017","sort_order":2},{"id":"5e382b2cf81c630018abe0d1","post_id":"5e382b29f81c630018abe057","tag_id":"5e382b29f81c630018abe018","sort_order":3},{"id":"5e382b2cf81c630018abe0d2","post_id":"5e382b29f81c630018abe057","tag_id":"5e382b29f81c630018abe01a","sort_order":4},{"id":"5e382b2cf81c630018abe0d3","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe009","sort_order":0},{"id":"5e382b2cf81c630018abe0d4","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2cf81c630018abe0d5","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe017","sort_order":2},{"id":"5e382b2cf81c630018abe0d6","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe018","sort_order":3},{"id":"5e382b2cf81c630018abe0d7","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe01a","sort_order":4},{"id":"5e382b2cf81c630018abe0d8","post_id":"5e382b29f81c630018abe058","tag_id":"5e382b29f81c630018abe01b","sort_order":5},{"id":"5e382b2cf81c630018abe0d9","post_id":"5e382b29f81c630018abe059","tag_id":"5e382b29f81c630018abdffd","sort_order":0},{"id":"5e382b2cf81c630018abe0da","post_id":"5e382b29f81c630018abe059","tag_id":"5e382b29f81c630018abe014","sort_order":1},{"id":"5e382b2cf81c630018abe0db","post_id":"5e382b29f81c630018abe059","tag_id":"5e382b29f81c630018abe019","sort_order":2},{"id":"5e382b2cf81c630018abe0dc","post_id":"5e382b29f81c630018abe059","tag_id":"5e382b29f81c630018abe01c","sort_order":3},{"id":"5e382b2cf81c630018abe0dd","post_id":"5e382b29f81c630018abe05a","tag_id":"5e382b29f81c630018abe014","sort_order":0},{"id":"5e382b2cf81c630018abe0de","post_id":"5e382b29f81c630018abe05a","tag_id":"5e382b29f81c630018abe015","sort_order":1},{"id":"5e382b2cf81c630018abe0df","post_id":"5e382b29f81c630018abe05b","tag_id":"5e382b29f81c630018abe013","sort_order":0},{"id":"5e382b2cf81c630018abe0e0","post_id":"5e382b29f81c630018abe05b","tag_id":"5e382b29f81c630018abe01d","sort_order":1},{"id":"5e382b2cf81c630018abe0e1","post_id":"5e382b29f81c630018abe05c","tag_id":"5e382b29f81c630018abe010","sort_order":0},{"id":"5e382b2cf81c630018abe0e2","post_id":"5e382b29f81c630018abe05c","tag_id":"5e382b29f81c630018abe01e","sort_order":1},{"id":"5e382b2cf81c630018abe0e3","post_id":"5e382b29f81c630018abe05c","tag_id":"5e382b29f81c630018abe01f","sort_order":2},{"id":"5e382b2cf81c630018abe0e4","post_id":"5e382b29f81c630018abe05d","tag_id":"5e382b29f81c630018abe020","sort_order":0},{"id":"5e382b2cf81c630018abe0e5","post_id":"5e382b29f81c630018abe05d","tag_id":"5e382b29f81c630018abe021","sort_order":1},{"id":"5e382b2df81c630018abe0e6","post_id":"5e382b29f81c630018abe05e","tag_id":"5e382b29f81c630018abdffd","sort_order":0},{"id":"5e382b2df81c630018abe0e7","post_id":"5e382b29f81c630018abe05e","tag_id":"5e382b29f81c630018abe009","sort_order":1},{"id":"5e382b2df81c630018abe0e8","post_id":"5e382b29f81c630018abe05e","tag_id":"5e382b29f81c630018abe00b","sort_order":2},{"id":"5e382b2df81c630018abe0e9","post_id":"5e382b29f81c630018abe05e","tag_id":"5e382b29f81c630018abe016","sort_order":3},{"id":"5e382b2df81c630018abe0ea","post_id":"5e382b29f81c630018abe05e","tag_id":"5e382b29f81c630018abe018","sort_order":4},{"id":"5e382b2df81c630018abe0eb","post_id":"5e382b29f81c630018abe05f","tag_id":"5e382b29f81c630018abe014","sort_order":0},{"id":"5e382b2df81c630018abe0ec","post_id":"5e382b29f81c630018abe05f","tag_id":"5e382b29f81c630018abe022","sort_order":1},{"id":"5e382b2df81c630018abe0ed","post_id":"5e382b29f81c630018abe05f","tag_id":"5e382b29f81c630018abe023","sort_order":2},{"id":"5e382b2df81c630018abe0ee","post_id":"5e382b29f81c630018abe05f","tag_id":"5e382b29f81c630018abe015","sort_order":3},{"id":"5e382b2df81c630018abe0ef","post_id":"5e382b29f81c630018abe060","tag_id":"5e382b29f81c630018abdffd","sort_order":0},{"id":"5e382b2df81c630018abe0f0","post_id":"5e382b29f81c630018abe060","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2df81c630018abe0f1","post_id":"5e382b29f81c630018abe060","tag_id":"5e382b29f81c630018abe018","sort_order":2},{"id":"5e382b2df81c630018abe0f2","post_id":"5e382b29f81c630018abe060","tag_id":"5e382b29f81c630018abe01a","sort_order":3},{"id":"5e382b2df81c630018abe0f3","post_id":"5e382b29f81c630018abe060","tag_id":"5e382b29f81c630018abe01c","sort_order":4},{"id":"5e382b2df81c630018abe0f4","post_id":"5e382b29f81c630018abe061","tag_id":"5e382b29f81c630018abe00e","sort_order":0},{"id":"5e382b2df81c630018abe0f5","post_id":"5e382b29f81c630018abe061","tag_id":"5e382b29f81c630018abe01c","sort_order":1},{"id":"5e382b2df81c630018abe0f6","post_id":"5e382b29f81c630018abe061","tag_id":"5e382b29f81c630018abe024","sort_order":2},{"id":"5e382b2df81c630018abe0f7","post_id":"5e382b29f81c630018abe062","tag_id":"5e382b29f81c630018abe025","sort_order":0},{"id":"5e382b2df81c630018abe0f8","post_id":"5e382b29f81c630018abe062","tag_id":"5e382b29f81c630018abe026","sort_order":1},{"id":"5e382b2df81c630018abe0f9","post_id":"5e382b29f81c630018abe062","tag_id":"5e382b29f81c630018abdffb","sort_order":2},{"id":"5e382b2df81c630018abe0fa","post_id":"5e382b29f81c630018abe062","tag_id":"5e382b29f81c630018abe015","sort_order":3},{"id":"5e382b2df81c630018abe0fb","post_id":"5e382b29f81c630018abe063","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b2df81c630018abe0fc","post_id":"5e382b29f81c630018abe063","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2df81c630018abe0fd","post_id":"5e382b29f81c630018abe063","tag_id":"5e382b29f81c630018abe025","sort_order":2},{"id":"5e382b2df81c630018abe0fe","post_id":"5e382b29f81c630018abe063","tag_id":"5e382b29f81c630018abe026","sort_order":3},{"id":"5e382b2df81c630018abe0ff","post_id":"5e382b29f81c630018abe064","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b2df81c630018abe100","post_id":"5e382b29f81c630018abe064","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2df81c630018abe101","post_id":"5e382b29f81c630018abe064","tag_id":"5e382b29f81c630018abe026","sort_order":2},{"id":"5e382b2df81c630018abe102","post_id":"5e382b29f81c630018abe064","tag_id":"5e382b29f81c630018abe027","sort_order":3},{"id":"5e382b2df81c630018abe103","post_id":"5e382b29f81c630018abe065","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b2df81c630018abe104","post_id":"5e382b29f81c630018abe065","tag_id":"5e382b29f81c630018abe015","sort_order":1},{"id":"5e382b2df81c630018abe105","post_id":"5e382b29f81c630018abe065","tag_id":"5e382b29f81c630018abe026","sort_order":2},{"id":"5e382b2df81c630018abe106","post_id":"5e382b29f81c630018abe065","tag_id":"5e382b29f81c630018abe027","sort_order":3},{"id":"5e382b2df81c630018abe107","post_id":"5e382b29f81c630018abe065","tag_id":"5e382b29f81c630018abdffb","sort_order":4},{"id":"5e382b2df81c630018abe108","post_id":"5e382b29f81c630018abe066","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b2df81c630018abe109","post_id":"5e382b29f81c630018abe066","tag_id":"5e382b29f81c630018abe015","sort_order":1},{"id":"5e382b2df81c630018abe10a","post_id":"5e382b29f81c630018abe066","tag_id":"5e382b29f81c630018abe026","sort_order":2},{"id":"5e382b2df81c630018abe10b","post_id":"5e382b29f81c630018abe066","tag_id":"5e382b29f81c630018abe027","sort_order":3},{"id":"5e382b2df81c630018abe10c","post_id":"5e382b29f81c630018abe066","tag_id":"5e382b29f81c630018abdffb","sort_order":4},{"id":"5e382b2df81c630018abe10d","post_id":"5e382b29f81c630018abe067","tag_id":"5e382b29f81c630018abdff5","sort_order":0},{"id":"5e382b2df81c630018abe10e","post_id":"5e382b29f81c630018abe067","tag_id":"5e382b29f81c630018abe026","sort_order":1},{"id":"5e382b2df81c630018abe10f","post_id":"5e382b29f81c630018abe067","tag_id":"5e382b29f81c630018abe028","sort_order":2},{"id":"5e382b2df81c630018abe110","post_id":"5e382b29f81c630018abe067","tag_id":"5e382b29f81c630018abdffb","sort_order":3},{"id":"5e382b2df81c630018abe111","post_id":"5e382b29f81c630018abe068","tag_id":"5e382b29f81c630018abe026","sort_order":0},{"id":"5e382b2df81c630018abe112","post_id":"5e382b29f81c630018abe068","tag_id":"5e382b29f81c630018abe029","sort_order":1},{"id":"5e382b2df81c630018abe113","post_id":"5e382b29f81c630018abe068","tag_id":"5e382b29f81c630018abdffb","sort_order":2},{"id":"5e382b2df81c630018abe114","post_id":"5e382b29f81c630018abe069","tag_id":"5e382b29f81c630018abdffd","sort_order":0},{"id":"5e382b2df81c630018abe115","post_id":"5e382b29f81c630018abe069","tag_id":"5e382b29f81c630018abe01c","sort_order":1},{"id":"5e382b2df81c630018abe116","post_id":"5e382b29f81c630018abe069","tag_id":"5e382b29f81c630018abe02a","sort_order":2},{"id":"5e382b2ef81c630018abe117","post_id":"5e382b29f81c630018abe06a","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe118","post_id":"5e382b29f81c630018abe06a","tag_id":"5e382b29f81c630018abe02b","sort_order":1},{"id":"5e382b2ef81c630018abe119","post_id":"5e382b29f81c630018abe06b","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe11a","post_id":"5e382b29f81c630018abe06b","tag_id":"5e382b29f81c630018abe02b","sort_order":1},{"id":"5e382b2ef81c630018abe11b","post_id":"5e382b29f81c630018abe06b","tag_id":"5e382b29f81c630018abe02c","sort_order":2},{"id":"5e382b2ef81c630018abe11c","post_id":"5e382b29f81c630018abe06d","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe11d","post_id":"5e382b29f81c630018abe06d","tag_id":"5e382b29f81c630018abdffb","sort_order":1},{"id":"5e382b2ef81c630018abe11e","post_id":"5e382b29f81c630018abe06e","tag_id":"5e382b29f81c630018abe02d","sort_order":0},{"id":"5e382b2ef81c630018abe11f","post_id":"5e382b29f81c630018abe06e","tag_id":"5e382b29f81c630018abe01c","sort_order":1},{"id":"5e382b2ef81c630018abe120","post_id":"5e382b29f81c630018abe06e","tag_id":"5e382b29f81c630018abe02e","sort_order":2},{"id":"5e382b2ef81c630018abe121","post_id":"5e382b29f81c630018abe06e","tag_id":"5e382b29f81c630018abe00f","sort_order":3},{"id":"5e382b2ef81c630018abe122","post_id":"5e382b29f81c630018abe06e","tag_id":"5e382b29f81c630018abe00e","sort_order":4},{"id":"5e382b2ef81c630018abe123","post_id":"5e382b29f81c630018abe070","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe124","post_id":"5e382b29f81c630018abe070","tag_id":"5e382b29f81c630018abe02d","sort_order":1},{"id":"5e382b2ef81c630018abe125","post_id":"5e382b29f81c630018abe070","tag_id":"5e382b29f81c630018abe032","sort_order":2},{"id":"5e382b2ef81c630018abe126","post_id":"5e382b29f81c630018abe071","tag_id":"5e382b29f81c630018abe026","sort_order":0},{"id":"5e382b2ef81c630018abe127","post_id":"5e382b29f81c630018abe071","tag_id":"5e382b29f81c630018abe02f","sort_order":1},{"id":"5e382b2ef81c630018abe128","post_id":"5e382b29f81c630018abe071","tag_id":"5e382b29f81c630018abe030","sort_order":2},{"id":"5e382b2ef81c630018abe129","post_id":"5e382b29f81c630018abe071","tag_id":"5e382b29f81c630018abe031","sort_order":3},{"id":"5e382b2ef81c630018abe12a","post_id":"5e382b29f81c630018abe072","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe12b","post_id":"5e382b29f81c630018abe072","tag_id":"5e382b29f81c630018abe00b","sort_order":1},{"id":"5e382b2ef81c630018abe12c","post_id":"5e382b29f81c630018abe072","tag_id":"5e382b29f81c630018abe009","sort_order":2},{"id":"5e382b2ef81c630018abe12d","post_id":"5e382b29f81c630018abe072","tag_id":"5e382b29f81c630018abdffd","sort_order":3},{"id":"5e382b2ef81c630018abe12e","post_id":"5e382b29f81c630018abe073","tag_id":"5e382b29f81c630018abe01c","sort_order":0},{"id":"5e382b2ef81c630018abe12f","post_id":"5e382b29f81c630018abe073","tag_id":"5e382b29f81c630018abdffd","sort_order":1},{"id":"5e382b2ef81c630018abe130","post_id":"5e382b29f81c630018abe074","tag_id":"5e382b29f81c630018abe02c","sort_order":0},{"id":"5e382b2ef81c630018abe131","post_id":"5e382b29f81c630018abe074","tag_id":"5e382b29f81c630018abe01c","sort_order":1},{"id":"5e382b2ef81c630018abe132","post_id":"5e382b29f81c630018abe074","tag_id":"5e382b29f81c630018abe033","sort_order":2},{"id":"5e382b2ef81c630018abe133","post_id":"5e382b29f81c630018abe075","tag_id":"5e382b29f81c630018abe01a","sort_order":0},{"id":"5e382b2ef81c630018abe134","post_id":"5e382b29f81c630018abe075","tag_id":"5e382b29f81c630018abe033","sort_order":1},{"id":"5e382b2ef81c630018abe135","post_id":"5e382b29f81c630018abe077","tag_id":"5e382b29f81c630018abe01c","sort_order":0}],"roles":[{"id":"5e382b26f81c630018abdf17","name":"Administrator","description":"Administrators","created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf18","name":"Editor","description":"Editors","created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf19","name":"Author","description":"Authors","created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf1a","name":"Contributor","description":"Contributors","created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e382b26f81c630018abdf1b","name":"Owner","description":"Blog Owner","created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z"},{"id":"5e3838d2bc678a002d6a7e25","name":"Admin Integration","description":"External Apps","created_at":"2020-02-03T15:14:26.000Z","updated_at":"2020-02-03T15:14:26.000Z"},{"id":"5e3838d8bc678a002d6a7e86","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"},{"id":"5e3838d8bc678a002d6a7e8f","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2020-02-03T15:14:32.000Z","updated_at":"2020-02-03T15:14:32.000Z"}],"roles_users":[{"id":"5e382b27f81c630018abdf51","role_id":"5e382b26f81c630018abdf19","user_id":"5951f5fca366002ebd5dbef7"},{"id":"5e382b28f81c630018abdfd3","role_id":"5e382b26f81c630018abdf1b","user_id":"1"}],"settings":[{"id":"5e382b28f81c630018abdfd4","key":"db_hash","value":"c4159e90-3c36-42c2-b9f1-e8488618ab2f","type":"string","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"core","flags":null},{"id":"5e382b28f81c630018abdfd5","key":"next_update_check","value":"1594930955","type":"number","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-07-15T20:22:35.000Z","group":"core","flags":null},{"id":"5e382b28f81c630018abdfd6","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"045f7e0c-5305-44bf-8b32-6f955d461212\",\"custom\":true,\"createdAt\":\"2018-08-21T19:05:35.000Z\",\"type\":\"info\",\"top\":true,\"message\":\"<strong>Ghost 2.0 is now available</strong> - This is a major update which requires a manual upgrade. <a href=\\\"https://my.ghost.org\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Click here</a> to get started!\",\"seen\":false,\"addedAt\":\"2020-02-03T14:20:56.938Z\"}]","type":"array","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:20:56.000Z","group":"core","flags":null},{"id":"5e382b28f81c630018abdfd7","key":"title","value":"Tomas Jansson","type":"string","created_at":"2014-02-09T20:21:56.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":"PUBLIC"},{"id":"5e382b28f81c630018abdfd8","key":"description","value":"Me, myself and I","type":"string","created_at":"2014-02-09T20:21:56.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":"PUBLIC"},{"id":"5e382b28f81c630018abdfd9","key":"logo","value":"/content/images/2014/Mar/tomas_jansson_bw_thumb.jpg","type":"string","created_at":"2014-02-09T20:21:56.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":"PUBLIC"},{"id":"5e382b28f81c630018abdfda","key":"cover_image","value":"/content/images/2014/Mar/Background_blog.jpg","type":"string","created_at":"2014-02-09T20:21:56.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":null},{"id":"5e382b28f81c630018abdfdb","key":"icon","value":null,"type":"string","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"site","flags":null},{"id":"5e382b28f81c630018abdfe0","key":"amp","value":"true","type":"boolean","created_at":"2017-01-12T18:19:21.000Z","updated_at":"2017-01-12T18:19:21.000Z","group":"amp","flags":null},{"id":"5e382b28f81c630018abdfe3","key":"facebook","value":null,"type":"string","created_at":"2016-05-18T11:52:34.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":null},{"id":"5e382b28f81c630018abdfe4","key":"twitter","value":null,"type":"string","created_at":"2016-05-18T11:52:34.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":null},{"id":"5e382b28f81c630018abdfe5","key":"labs","value":"{\"publicAPI\":true,\"members\":false}","type":"object","created_at":"2015-01-12T21:17:49.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"labs","flags":null},{"id":"5e382b28f81c630018abdfe6","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"Me me me\",\"url\":\"/aboutme\"}]","type":"array","created_at":"2015-02-28T21:39:31.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"site","flags":null},{"id":"5e382b28f81c630018abdfe7","key":"slack","value":"[{\"url\":\"\"}]","type":"array","created_at":"2016-05-18T11:52:34.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"slack","flags":null},{"id":"5e382b28f81c630018abdfe8","key":"unsplash","value":"{\"isActive\": true}","type":"object","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"unsplash","flags":null},{"id":"5e382b28f81c630018abdfe9","key":"active_theme","value":"casper-1.4","type":"string","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:15.000Z","group":"theme","flags":"RO"},{"id":"5e382b28f81c630018abdfea","key":"active_apps","value":"[]","type":"app","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"core","flags":null},{"id":"5e382b28f81c630018abdfeb","key":"installed_apps","value":"[]","type":"app","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"core","flags":null},{"id":"5e382b28f81c630018abdfec","key":"is_private","value":"false","type":"boolean","created_at":"2015-05-14T16:10:11.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"private","flags":null},{"id":"5e382b28f81c630018abdfed","key":"password","value":"null","type":"string","created_at":"2015-05-14T16:10:11.000Z","updated_at":"2016-07-27T09:55:33.000Z","group":"private","flags":null},{"id":"5e382b28f81c630018abdfee","key":"public_hash","value":"c8690991b07bd4ab83b0bcc58c028e","type":"string","created_at":"2020-02-03T14:16:08.000Z","updated_at":"2020-02-03T14:16:08.000Z","group":"private","flags":null},{"id":"5e3839e593bb0a00389a2935","key":"meta_title","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a2936","key":"meta_description","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a2937","key":"og_image","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a2938","key":"og_title","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a2939","key":"og_description","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a293a","key":"twitter_image","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a293b","key":"twitter_title","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a293c","key":"twitter_description","value":null,"type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"site","flags":null},{"id":"5e3839e593bb0a00389a2941","key":"default_content_visibility","value":"public","type":"string","created_at":"2020-02-03T15:19:01.000Z","updated_at":"2020-02-03T15:19:01.000Z","group":"members","flags":null},{"id":"5e389e3c7ebf4d003858434a","key":"secondary_navigation","value":"[]","type":"array","created_at":"2020-02-03T22:27:08.000Z","updated_at":"2020-02-03T22:27:08.000Z","group":"site","flags":null},{"id":"5eb3dfc8095d0b003964c681","key":"session_secret","value":"fd25b65eb09ac97a9c1f3814a32523ba289b996a295be0e85b63e3caefe3e570","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c682","key":"theme_session_secret","value":"de838af805070a229e55ff500491b68da0bad7b6b2da9c8d5f716fc637dbe80d","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c683","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJkUuJgxhFL/n6qV00ShOXVqWUGKttErZ1vs5pJYXz8cKn8Vi6dgOy3Pgln65eHX\nRhTBytLCf29DV8YiHIUDoVcc0/8wXXTlgX/1BdZt54QVuTPJPov+oz6hgoYY2FRRU6alwhju\nSYpYHW54VI0spwl8oyQsteMkg45mCS0KOfnVAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c684","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICWwIBAAKBgQCZFLiYMYRS/5+qldNEoTl1allBirbRK2db7OaSWF8/HCp/FYunYDstz4JZ\n+uXh10YUwcrSwn9vQ1fGIhyFA6FXHNP/MF105YF/9QXWbeeEFbkzyT6L/qM+oYKGGNhUUVOm\npcIY7kmKWB1ueFSNLKcJfKMkLLXjJIOOZgktCjn51QIDAQABAoGAMjloLxbf8VNJEXDCthun\nfGx+CFD4ljwFV1WseItiBJS7JKLvBOU/xVH11IvK7IPyzFCguPFzWg/gNnNuo/2C60oLGZng\njnlwDEJIy5YCyXCedcFj65QdiLf2qCkdVNczT0xu77dt3Ck/vkX4vmukKNUV8iDUDZRHImdV\n9v8N6IECQQD850s6iNdBBMmnImZw0bt4MlvOyfESsG0wDDMGaHBWvNjqoc704tu6jWvcbKwu\nAx3w+9Yb3mks6MsNRkpkcZSlAkEAmvSKkFfQdPPku7ITx6vBq1zPvcNi/MtBxOftx5Q6UFtv\nXjvWXSdM1M6Rvqinb+floBk6yvnU+MNIVGY626JZcQJAYDcKitFmuyi8IybWPFO+c8MM5IY1\nffu6/o2Vl/mBy637BIDcwJPSLo4BHBIIC1VKPdVBpsad0uUZ4wn74WzItQJANDcbSjCNKhvV\ng/op+CrEhD4uMr/YZ18GtpeA5LPqQpHNIXAAeDt6BGvnaNvkMC1wj1ZxHVy6i2kiwm4Af2ph\nwQJAEcmGoZB0H+OQRUuZ0Hsiku1nyQIy1KH1jrHbwurT0EvjHxoN0kQHhRi5WDYX7YgD0O5Y\niAAlwBY602YMaWiEPQ==\n-----END RSA PRIVATE KEY-----\n","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c685","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAJt+DEGP8BH+FXQWAIuWAShI1i0JnGPEDRPiTTCoeHFrU687ZxLgduw6BJHFKV0H\n+5c4wrRGw2T3Bo6vUY+MDlfln9a8dgx0RlymWVXms0xR8iNPojbgPrnGtupWTDFGtGJoFRon\nTzWGSlqLxEuLX+6WbUiMZIoQ78W4HTX4zdfxAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c686","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICWwIBAAKBgQCbfgxBj/AR/hV0FgCLlgEoSNYtCZxjxA0T4k0wqHhxa1OvO2cS4HbsOgSR\nxSldB/uXOMK0RsNk9waOr1GPjA5X5Z/WvHYMdEZcpllV5rNMUfIjT6I24D65xrbqVkwxRrRi\naBUaJ081hkpai8RLi1/ulm1IjGSKEO/FuB01+M3X8QIDAQABAoGATYuzOQ87vv1kXZyLcRHC\nSgDpL1TXtbmUfBIkRNwAFTBdb8rEQNnI9U6J8NjPJCJTCru3/og78iJwFsDAgEJs9AYHtyEH\nYXzmrXPhdstaPmxQB4oSD8DaG+e/TIpB+7qTM17NcMltsycpfZnno8M3X3AEy2TP/lBY7ldT\ndmVDv0ECQQDzolWPSajXU5/UMVXtYOIRoRDmn755QVMQaDb1uyaKpWn9nXccQ8hH45hPHpuG\noSasdHcC32QKLdWk0Q+sWg/NAkEAo2JxGSI357N2lvNQDDzAe8sy2hl7224XJmHXXRGzMX4t\ngUz/4jurvbEBb1R3Yd2e2Dy3RRFz2SWEC+OuTD9ctQJAbqyAp32zmFVhlpfuy82mBJIhRlKs\nyRJWtG5TAR/KK1NnKZF9iTZ1ZcrK5q4lwmu0UMT7Ry/JlyIpGhzqPMYQxQJAN6bJZUk9h6Oe\nak5YXsK2wbP2D0oM4TI/mmhLRj1ILzyOXXOP2Ux/9C39kiFQRLqLZ5ZOyqzmK/nWX4JfFUOj\n3QJASrTNMVzQVq2RxTRL9Dwf2Nq8EuY6okEXDIocuwb1GonZRLpOscRw51EsqYQ9vvXCp7hU\nzOLXnhkLw8xYTVZRNQ==\n-----END RSA PRIVATE KEY-----\n","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5eb3dfc8095d0b003964c688","key":"members_email_auth_secret","value":"ce3d68c8dbeebb9abadc91595cbf26b4e07ab211afa5beadb1e285d58ee610bb1fe82ac7269dfb019894e1438b6d87d8a672597db5344dc520e873bd12b59669","type":"string","created_at":"2020-05-07T10:15:36.000Z","updated_at":"2020-05-07T10:15:36.000Z","group":"core","flags":null},{"id":"5ee206e6b22c3e003914fb02","key":"shared_views","value":"{}","type":"array","created_at":"2020-06-11T11:26:46.000Z","updated_at":"2020-06-11T11:26:46.000Z","group":"views","flags":null},{"id":"5efa0f2c84a96b00395e1c10","key":"portal_name","value":"true","type":"boolean","created_at":"2020-06-29T15:56:28.000Z","updated_at":"2020-06-29T15:56:28.000Z","group":"portal","flags":null},{"id":"5efa0f2c84a96b00395e1c11","key":"portal_button","value":"false","type":"boolean","created_at":"2020-06-29T15:56:28.000Z","updated_at":"2020-06-29T15:56:28.000Z","group":"portal","flags":null},{"id":"5efa0f2c84a96b00395e1c12","key":"portal_plans","value":"[\"free\", \"monthly\", \"yearly\"]","type":"array","created_at":"2020-06-29T15:56:28.000Z","updated_at":"2020-06-29T15:56:28.000Z","group":"portal","flags":null},{"id":"5f0de91a2a9a26002dab0380","key":"accent_color","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"site","flags":"PUBLIC"},{"id":"5f0de91a2a9a26002dab0381","key":"lang","value":"en","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"site","flags":null},{"id":"5f0de91a2a9a26002dab0382","key":"timezone","value":"Etc/UTC","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"site","flags":null},{"id":"5f0de91a2a9a26002dab0383","key":"codeinjection_head","value":"<!-- You can safely delete this line if your theme does not require jQuery -->\n<script type=\"text/javascript\">\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\nm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\nga('create', 'UA-16648515-3', '2mas.xyz');\nga('send', 'pageview');\n</script>\n","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"site","flags":null},{"id":"5f0de91a2a9a26002dab0384","key":"codeinjection_foot","value":"\n","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"site","flags":null},{"id":"5f0de91a2a9a26002dab0385","key":"members_allow_free_signup","value":"true","type":"boolean","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab0386","key":"members_from_address","value":"noreply@blog.2mas.xyz","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":"RO"},{"id":"5f0de91a2a9a26002dab0387","key":"stripe_product_name","value":"Ghost Subscription","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab0388","key":"stripe_secret_key","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab0389","key":"stripe_publishable_key","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038a","key":"stripe_plans","value":"[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":0},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":0}]","type":"array","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038b","key":"stripe_connect_publishable_key","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038c","key":"stripe_connect_secret_key","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038d","key":"stripe_connect_livemode","value":"","type":"boolean","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038e","key":"stripe_connect_display_name","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab038f","key":"stripe_connect_account_id","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"members","flags":""},{"id":"5f0de91a2a9a26002dab0390","key":"portal_button_style","value":"icon-and-text","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"portal","flags":null},{"id":"5f0de91a2a9a26002dab0391","key":"portal_button_icon","value":null,"type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"portal","flags":null},{"id":"5f0de91a2a9a26002dab0392","key":"portal_button_signup_text","value":"Subscribe","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"portal","flags":null},{"id":"5f0de91a2a9a26002dab0393","key":"mailgun_domain","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"email","flags":null},{"id":"5f0de91a2a9a26002dab0394","key":"mailgun_api_key","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"email","flags":null},{"id":"5f0de91a2a9a26002dab0395","key":"mailgun_base_url","value":"","type":"string","created_at":"2020-07-14T18:19:22.000Z","updated_at":"2020-07-14T18:19:22.000Z","group":"email","flags":null},{"id":"5f0efe39739523002dcccf81","key":"members_stripe_webhook_id","value":null,"type":"string","created_at":"2020-07-15T14:01:45.000Z","updated_at":"2020-07-15T14:01:45.000Z","group":"core","flags":null},{"id":"5f0efe39739523002dcccf82","key":"members_stripe_webhook_secret","value":null,"type":"string","created_at":"2020-07-15T14:01:45.000Z","updated_at":"2020-07-15T14:01:45.000Z","group":"core","flags":null}],"tags":[{"id":"5e382b26f81c630018abdf12","name":"Getting Started","slug":"getting-started","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2020-02-03T14:16:06.000Z","updated_at":"2020-02-03T14:16:06.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdff5","name":"asp.net mvc","slug":"asp-net-mvc","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:00:30.000Z","updated_at":"2014-03-01T09:00:30.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdff6","name":"security","slug":"security","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:00:43.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdff7","name":"javascript","slug":"javascript","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:06:38.000Z","updated_at":"2014-03-01T09:06:38.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdff8","name":"jQuery","slug":"jquery","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:06:38.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdff9","name":"nuget","slug":"nuget","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:10:25.000Z","updated_at":"2014-03-01T09:10:25.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdffa","name":"WCF","slug":"wcf","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:11:48.000Z","updated_at":"2014-03-01T09:11:48.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdffb","name":".NET","slug":"net","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:13:05.000Z","updated_at":"2016-01-15T22:19:57.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdffc","name":"watin","slug":"watin","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:15:58.000Z","updated_at":"2014-03-01T09:15:58.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdffd","name":"functional","slug":"functional","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:19:24.000Z","updated_at":"2014-03-01T09:19:24.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdffe","name":"refactoring","slug":"refactoring","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:19:24.000Z","updated_at":"2014-03-01T09:19:24.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abdfff","name":"tag","slug":"post-tag","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:20:37.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe000","name":"ADO.NET","slug":"ado-net","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:24:14.000Z","updated_at":"2014-03-01T09:24:14.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe001","name":"SQL","slug":"sql","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:25:39.000Z","updated_at":"2014-03-01T09:25:39.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe002","name":"git","slug":"git","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:26:55.000Z","updated_at":"2014-03-01T09:26:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe003","name":"gitolite","slug":"gitolite","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:26:55.000Z","updated_at":"2014-03-01T09:26:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe004","name":"bash","slug":"bash","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:26:55.000Z","updated_at":"2014-03-01T09:26:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe005","name":"linq","slug":"linq","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:28:00.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe006","name":"IIS Express","slug":"iis-express","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:29:04.000Z","updated_at":"2014-03-01T09:29:04.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe007","name":"PowerShell","slug":"powershell","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:29:04.000Z","updated_at":"2014-03-01T09:29:04.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe008","name":"NServiceBus","slug":"nservicebus","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:30:19.000Z","updated_at":"2014-03-01T09:30:19.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe009","name":"CQRS","slug":"cqrs","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:30:19.000Z","updated_at":"2014-03-01T09:30:19.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00a","name":"json","slug":"json","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:31:54.000Z","updated_at":"2014-03-01T09:31:54.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00b","name":"Event Sourcing","slug":"event-sourcing","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:33:16.000Z","updated_at":"2014-03-01T09:33:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00c","name":"NoSQL","slug":"nosql","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:33:16.000Z","updated_at":"2014-03-01T09:33:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00d","name":"DDD","slug":"ddd","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:33:16.000Z","updated_at":"2014-03-01T09:33:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00e","name":"Azure","slug":"azure","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:37:25.000Z","updated_at":"2014-03-01T09:37:25.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe00f","name":"Octopus Deploy","slug":"octopus-deploy","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:39:10.000Z","updated_at":"2014-03-01T09:39:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe010","name":"DevOps","slug":"devops","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:39:10.000Z","updated_at":"2014-03-01T09:39:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe011","name":"TeamCity","slug":"teamcity","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:39:10.000Z","updated_at":"2014-03-01T09:39:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe012","name":"Continuous Deployment","slug":"continuous-deployment","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:39:10.000Z","updated_at":"2014-03-01T09:39:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe013","name":"speaking","slug":"speaking","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:40:09.000Z","updated_at":"2014-03-01T09:40:09.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe014","name":"OWIN","slug":"owin","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:41:31.000Z","updated_at":"2014-03-01T09:41:31.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe015","name":"asp.net","slug":"asp-net","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-03-01T09:41:31.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe016","name":"cqrs","slug":"cqrs-2","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-24T18:41:26.000Z","updated_at":"2014-06-24T18:41:26.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe017","name":"elasticsearch","slug":"elasticsearch","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-24T18:41:26.000Z","updated_at":"2014-06-24T18:41:26.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe018","name":"CQRSShop","slug":"cqrsshop","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-24T19:29:04.000Z","updated_at":"2014-06-24T19:29:04.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe019","name":"Simple.Web","slug":"simple-web","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-27T20:44:53.000Z","updated_at":"2014-06-27T20:44:53.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01a","name":"eventstore","slug":"eventstore","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-27T21:21:04.000Z","updated_at":"2014-06-27T21:21:04.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01b","name":"neo4j","slug":"neo4j","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-06-28T11:47:44.000Z","updated_at":"2014-06-28T11:47:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01c","name":"FSharp","slug":"fsharp","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-07-07T18:11:31.000Z","updated_at":"2014-07-07T18:11:31.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01d","name":"conferences","slug":"conferences","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-10-04T18:57:44.000Z","updated_at":"2014-10-04T18:57:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01e","name":"puppet","slug":"puppet","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-10-13T14:25:43.000Z","updated_at":"2014-10-13T14:25:43.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe01f","name":"vagrant","slug":"vagrant","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-10-13T14:25:43.000Z","updated_at":"2014-10-13T14:25:43.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe020","name":"work","slug":"work","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-11-09T14:53:21.000Z","updated_at":"2014-11-09T14:53:21.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe021","name":"microsoft","slug":"microsoft","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-11-09T14:53:21.000Z","updated_at":"2015-09-03T13:23:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe022","name":"vnext","slug":"vnext","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-11-09T17:50:45.000Z","updated_at":"2014-11-09T17:50:45.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe023","name":"middleware","slug":"middleware","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2014-11-09T17:50:45.000Z","updated_at":"2014-11-09T17:50:45.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe024","name":"Azure Active Directory","slug":"azure-active-directory","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-01-13T19:25:17.000Z","updated_at":"2015-01-13T19:25:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe025","name":"mvc 6","slug":"mvc-6","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-12T16:33:19.000Z","updated_at":"2015-04-12T16:33:19.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe026","name":"asp.net 5","slug":"asp-net-5","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-12T19:38:49.000Z","updated_at":"2015-04-12T19:38:49.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe027","name":"mvc","slug":"mvc","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-14T16:34:31.000Z","updated_at":"2015-04-14T16:34:31.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe028","name":"dnx","slug":"dnx","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-14T18:11:57.000Z","updated_at":"2015-05-14T18:11:57.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe029","name":"docker","slug":"docker","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-18T15:50:13.000Z","updated_at":"2015-05-18T15:50:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02a","name":"NDC","slug":"ndc","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-20T12:02:49.000Z","updated_at":"2015-06-20T12:02:49.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02b","name":"Topshelf","slug":"topshelf","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-28T17:01:37.000Z","updated_at":"2015-06-28T17:01:37.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02c","name":"Suave","slug":"suave","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-28T17:13:54.000Z","updated_at":"2015-06-28T17:13:54.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02d","name":"FAKE","slug":"fake","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-10-24T11:23:13.000Z","updated_at":"2015-10-24T11:23:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02e","name":"AppVeyor","slug":"appveyor","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-10-24T11:23:13.000Z","updated_at":"2015-10-24T11:23:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe02f","name":"react","slug":"react","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-11-15T22:09:35.000Z","updated_at":"2015-11-15T22:09:35.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe030","name":"react-hot-load","slug":"react-hot-load","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-11-15T22:09:35.000Z","updated_at":"2015-11-15T22:09:35.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe031","name":"webpack","slug":"webpack","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-11-15T22:09:35.000Z","updated_at":"2015-11-15T22:09:35.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe032","name":"npm","slug":"npm","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-11-15T22:34:03.000Z","updated_at":"2015-11-15T22:34:03.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5e382b29f81c630018abe033","name":"kubernetes","slug":"kubernetes","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-11-23T22:40:10.000Z","updated_at":"2016-11-23T22:40:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null}],"users":[{"id":"1","name":"Tomas Jansson","slug":"tomas-jansson","password":"$2a$10$w76f0jbwpu4mJAh0KRqxc.N4qDy4q2wOm59zMeC1T3LP45.7hQ27u","email":"tomas.jansson@gmail.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":"[\"getting-started\"]","last_seen":"2020-07-15T20:24:16.000Z","created_at":"2014-02-09T20:26:08.000Z","updated_at":"2020-07-15T20:26:26.000Z"}],"webhooks":[]}}]}